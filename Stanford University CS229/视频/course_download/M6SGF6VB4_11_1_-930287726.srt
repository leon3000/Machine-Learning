1
00:00:23,230 --> 00:00:24,680
教员(Andrew Ng):好的

2
00:00:24,810 --> 00:00:25,730
早上好  欢迎回来

3
00:00:27,450 --> 00:00:31,530
今天我要结束

4
00:00:31,600 --> 00:00:32,690
我们对

5
00:00:32,760 --> 00:00:34,410
学习理论的讲解

6
00:00:34,480 --> 00:00:37,950
我们会将

7
00:00:38,070 --> 00:00:38,970
贝叶斯统计

8
00:00:39,040 --> 00:00:39,790
和规范化

9
00:00:39,870 --> 00:00:43,410
之后简单地

10
00:00:43,490 --> 00:00:44,720
讲一下在线学习

11
00:00:44,790 --> 00:00:47,190
今天的大部分时间

12
00:00:47,270 --> 00:00:49,290
我们都会花在

13
00:00:49,360 --> 00:00:50,620
关于如何应用机器

14
00:00:50,700 --> 00:00:52,280
学习算法解决具体问题上

15
00:00:52,350 --> 00:00:53,110
例如  对于你们的课程项目

16
00:00:53,180 --> 00:00:54,530
或者你们毕业后

17
00:00:54,610 --> 00:00:55,950
可能需要处理的一些问题

18
00:00:56,030 --> 00:00:58,290
让我们开始讲

19
00:00:58,360 --> 00:00:59,280
贝叶斯统计

20
00:00:59,360 --> 00:00:59,920
和规范化

21
00:01:01,480 --> 00:01:03,950
记得上一周

22
00:01:04,030 --> 00:01:05,390
我们开始讲

23
00:01:05,470 --> 00:01:06,300
学习理论

24
00:01:06,380 --> 00:01:07,970
我们讲到了方差和偏差

25
00:01:08,040 --> 00:01:10,650
上一讲

26
00:01:10,730 --> 00:01:12,180
我们花了大部分时间

27
00:01:12,270 --> 00:01:15,380
来讲模型选择

28
00:01:15,460 --> 00:01:18,140
和特征选择算法

29
00:01:18,210 --> 00:01:19,950
我们讲到了交叉验证  对吗?

30
00:01:20,030 --> 00:01:22,350
上一讲的

31
00:01:22,430 --> 00:01:24,790
大多数方法

32
00:01:24,870 --> 00:01:27,070
都是尝试简化模型

33
00:01:27,150 --> 00:01:27,910
例如

34
00:01:27,990 --> 00:01:29,350
我们讲到的

35
00:01:29,420 --> 00:01:30,010
特征选择算法

36
00:01:30,080 --> 00:01:30,780
提供了一种方式

37
00:01:30,860 --> 00:01:32,480
可以消除一部分特征

38
00:01:32,550 --> 00:01:34,560
从而减少了

39
00:01:34,640 --> 00:01:35,690
需要拟合的参数数量

40
00:01:35,770 --> 00:01:37,150
降低了过拟合的风险

41
00:01:37,240 --> 00:01:38,240
对吗?还记得吗?

42
00:01:38,320 --> 00:01:39,690
特征选择算法

43
00:01:39,760 --> 00:01:43,260
选择了一个特征的子集

44
00:01:43,340 --> 00:01:45,420
所以你们的参数数量更少

45
00:01:45,490 --> 00:01:47,110
从而过拟合的可能性也会更低

46
00:01:47,200 --> 00:01:47,740
对吗?

47
00:01:48,040 --> 00:01:49,930
我今天要讲的是

48
00:01:50,010 --> 00:01:52,200
另外一种防止过拟合的方法

49
00:01:52,280 --> 00:01:54,620
我们将这种方法

50
00:01:54,700 --> 00:01:56,250
称之为规范化

51
00:01:56,330 --> 00:01:57,950
这种方法会保留所有的参数

52
00:01:58,940 --> 00:02:00,810
方法的思想是这样的

53
00:02:00,890 --> 00:02:03,150
我要用一个例子来说明

54
00:02:03,230 --> 00:02:05,490
比如说  线性回归

55
00:02:13,230 --> 00:02:14,250
线性回归模型

56
00:02:14,330 --> 00:02:15,710
是我们学到的

57
00:02:15,790 --> 00:02:17,020
第一个模型  对吗?

58
00:02:17,090 --> 00:02:19,170
我们会选择参数

59
00:02:19,250 --> 00:02:24,440
使得似然性最大化

60
00:02:26,410 --> 00:02:29,650
对吗?

61
00:02:29,750 --> 00:02:31,830
这意味着

62
00:02:31,900 --> 00:02:33,750
你会选择参数θ

63
00:02:35,050 --> 00:02:41,760
使得数据出现的概率最大化

64
00:02:41,840 --> 00:02:44,210
也就是选择θ

65
00:02:44,290 --> 00:02:45,640
使我们观测到

66
00:02:45,730 --> 00:02:46,990
该组数据的概率最大化 对吗?

67
00:02:50,400 --> 00:02:52,400
给这样的方法

68
00:02:52,480 --> 00:02:53,460
取个名字

69
00:02:53,540 --> 00:02:54,690
它是一个被称为

70
00:02:55,950 --> 00:02:57,820
"频率学家方法"的例子

71
00:02:59,660 --> 00:03:00,870
这里提到的频率学家

72
00:03:00,950 --> 00:03:02,100
你可以认为

73
00:03:02,180 --> 00:03:03,820
它代表了统计学的一个学派

74
00:03:03,900 --> 00:03:07,060
这个公式之后的

75
00:03:07,150 --> 00:03:09,420
哲学观点是这样的

76
00:03:09,490 --> 00:03:10,960
在数据背后

77
00:03:11,050 --> 00:03:12,620
有一组参数θ

78
00:03:12,700 --> 00:03:14,040
用来生成这些x和y

79
00:03:14,130 --> 00:03:15,330
会有一些真实的参数θ

80
00:03:15,410 --> 00:03:17,910
控制了房价y

81
00:03:17,990 --> 00:03:19,590
它是x的函数

82
00:03:19,670 --> 00:03:21,190
但是我们并不知道

83
00:03:21,260 --> 00:03:22,440
θ的值是什么

84
00:03:22,530 --> 00:03:23,830
我们要做的

85
00:03:23,900 --> 00:03:25,120
是用一些方法

86
00:03:25,210 --> 00:03:26,750
来估计参数θ的值

87
00:03:26,830 --> 00:03:27,510
明白吗?

88
00:03:27,590 --> 00:03:29,070
所以极大似然性

89
00:03:29,140 --> 00:03:31,520
是一种可以估计?值的

90
00:03:31,600 --> 00:03:33,690
一种可能的方法

91
00:03:35,160 --> 00:03:39,120
我们再来正式地说一遍

92
00:03:39,190 --> 00:03:40,960
θ不是一个随机变量 对吗?

93
00:03:41,040 --> 00:03:41,550
这就是为什么我们要说

94
00:03:41,630 --> 00:03:43,760
θ有真实的值

95
00:03:43,840 --> 00:03:44,900
它不是随机的

96
00:03:44,980 --> 00:03:46,130
我们只是不知道它的真实的值

97
00:03:46,210 --> 00:03:47,120
所以我们用一种

98
00:03:47,190 --> 00:03:48,300
称为极大似然性的方法

99
00:03:48,370 --> 00:03:50,940
来估计θ的真实值

100
00:03:51,020 --> 00:03:52,270
所以是一个

101
00:03:52,340 --> 00:03:53,320
频率学方法的例子

102
00:03:53,400 --> 00:03:57,290
除了频率学派之外

103
00:03:57,370 --> 00:03:58,970
还有另外一种学派

104
00:03:59,050 --> 00:04:01,010
那就是贝叶斯学派

105
00:04:03,500 --> 00:04:07,380
贝叶斯学派的观点认为

106
00:04:07,460 --> 00:04:08,370
我们不知道θ的值

107
00:04:08,450 --> 00:04:12,040
所以我们为θ的值

108
00:04:12,120 --> 00:04:14,860
赋予一个先验概率 明白吗?

109
00:04:14,940 --> 00:04:16,430
一个贝叶斯学派的

110
00:04:16,510 --> 00:04:17,120
学者会说

111
00:04:17,200 --> 00:04:18,380
"我们不知道

112
00:04:18,450 --> 00:04:19,100
θ的值是什么

113
00:04:19,190 --> 00:04:21,080
所以让我们用先验概率

114
00:04:21,160 --> 00:04:22,630
来表示θ取值的不确定性 "

115
00:04:27,270 --> 00:04:32,640
例如  θ的先验概率

116
00:04:32,740 --> 00:04:36,920
可能是一个高斯分布

117
00:04:37,000 --> 00:04:38,950
均值是0  协方差矩阵

118
00:04:39,040 --> 00:04:41,740
是τ^2 I  明白吗?

119
00:04:41,820 --> 00:04:46,680
我用S表示

120
00:04:46,760 --> 00:04:55,580
我的训练集合

121
00:04:55,660 --> 00:04:58,170
P(θ)表示了

122
00:04:58,250 --> 00:04:58,920
在没有见到任何数据的时候

123
00:04:58,990 --> 00:05:00,760
我所认为的参数的分布

124
00:05:00,840 --> 00:05:02,210
在没有看见任何数据的时候

125
00:05:02,290 --> 00:05:04,450
我将θ表示成

126
00:05:04,530 --> 00:05:06,430
我认为

127
00:05:06,500 --> 00:05:08,300
它最可能的形式

128
00:05:09,920 --> 00:05:11,490
给定训练集合S

129
00:05:11,570 --> 00:05:14,070
根据贝叶斯方法

130
00:05:14,150 --> 00:05:22,850
我们会计算

131
00:05:22,930 --> 00:05:25,350
给定了训练集合之后

132
00:05:25,430 --> 00:05:27,460
的后验概率

133
00:05:27,530 --> 00:05:31,120
让我们将它写在下一块黑板上

134
00:05:31,200 --> 00:05:34,200
给定了训练集合之后

135
00:05:34,290 --> 00:05:35,150
参数的后验概率应该是

136
00:05:35,230 --> 00:05:36,140
根据贝叶斯公式

137
00:05:36,220 --> 00:05:39,690
它应该和这个式子成正比

138
00:05:49,740 --> 00:05:51,580
对吗?

139
00:05:51,650 --> 00:05:52,770
这是根据贝叶斯公式得到的

140
00:05:55,690 --> 00:05:57,560
我们将其称之为后验概率

141
00:06:00,640 --> 00:06:03,360
这个分布表示

142
00:06:03,440 --> 00:06:04,920
在看到训练集合之后

143
00:06:05,000 --> 00:06:06,660
我所认为的θ的分布

144
00:06:07,640 --> 00:06:09,350
当你想

145
00:06:09,430 --> 00:06:10,170
对一个新房屋的价格

146
00:06:10,240 --> 00:06:11,250
进行预测时

147
00:06:18,950 --> 00:06:23,260
对于输入x  我会认为

148
00:06:23,330 --> 00:06:25,530
对于我尝试预测价格的

149
00:06:25,600 --> 00:06:28,100
这个新房屋的

150
00:06:28,170 --> 00:06:29,390
价格的概率分布

151
00:06:29,460 --> 00:06:31,300
给定房屋的大小

152
00:06:31,370 --> 00:06:32,720
也就是x代表的房屋的特征

153
00:06:32,800 --> 00:06:35,400
以及我之前给定的训练集合

154
00:06:36,280 --> 00:06:47,830
新房屋价格的概率分布

155
00:06:47,910 --> 00:06:50,660
应该等于:

156
00:06:50,730 --> 00:06:53,540
∫_θ

157
00:06:53,630 --> 00:06:55,980
〖P(y│x,θ)P(θ|S) dθ)〗

158
00:06:56,050 --> 00:06:58,250
对吗?

159
00:06:58,330 --> 00:07:01,730
特别地

160
00:07:01,810 --> 00:07:03,740
如果你想要在给定输入x

161
00:07:03,820 --> 00:07:10,170
和训练集合S的情况下

162
00:07:10,360 --> 00:07:11,470
预测y的期望值

163
00:07:11,610 --> 00:07:13,720
你需要这样求:

164
00:07:13,870 --> 00:07:24,460
∫_y?〖yP(y|x  S) dy)〗

165
00:07:24,620 --> 00:07:26,110
对吗?

166
00:07:26,260 --> 00:07:27,910
你需要根据

167
00:07:28,070 --> 00:07:28,970
后验概率

168
00:07:29,120 --> 00:07:32,520
来求y的期望值  对吗?

169
00:07:32,680 --> 00:07:36,660
当我写

170
00:07:36,850 --> 00:07:37,600
这个公式的时候

171
00:07:37,760 --> 00:07:39,250
根据贝叶斯公式

172
00:07:39,410 --> 00:07:40,560
我这里写的是

173
00:07:40,720 --> 00:07:42,350
y|x,θ

174
00:07:42,510 --> 00:07:44,970
因为这个式子表示

175
00:07:45,130 --> 00:07:46,950
y在给定x和

176
00:07:47,100 --> 00:07:48,720
θ下的条件概率

177
00:07:49,080 --> 00:07:50,120
所以我这里没有写成

178
00:07:50,280 --> 00:07:51,480
y|x;θ  而写的是y|x  θ

179
00:07:51,650 --> 00:07:53,530
因为我现在

180
00:07:53,700 --> 00:07:57,170
将θ视为随机变量

181
00:07:58,270 --> 00:08:01,030
这里的公式有些抽象

182
00:08:01,640 --> 00:08:04,760
但它是-

183
00:08:04,930 --> 00:08:05,810
让我看看

184
00:08:05,960 --> 00:08:07,000
你们有什么问题吗?

185
00:08:10,330 --> 00:08:14,490
没有?好的

186
00:08:14,650 --> 00:08:15,960
让我们使它变得更具体一些

187
00:08:16,120 --> 00:08:16,990
事实证明

188
00:08:17,160 --> 00:08:18,680
对于许多问题

189
00:08:18,850 --> 00:08:23,070
这些步骤

190
00:08:23,220 --> 00:08:24,350
很难计算

191
00:08:24,500 --> 00:08:27,110
因为θ是一个

192
00:08:27,270 --> 00:08:28,430
n+1维的向量

193
00:08:28,580 --> 00:08:29,930
一个n+1维的

194
00:08:30,100 --> 00:08:30,760
参数向量

195
00:08:30,920 --> 00:08:32,180
这里是一个

196
00:08:32,340 --> 00:08:33,410
n+1维向量上的积分

197
00:08:33,560 --> 00:08:35,360
n+1维的实数向量

198
00:08:35,520 --> 00:08:37,570
在数值上

199
00:08:37,740 --> 00:08:39,950
很难在

200
00:08:40,100 --> 00:08:42,340
很高的维度空间上求积分

201
00:08:42,500 --> 00:08:43,350
对吗?

202
00:08:43,500 --> 00:08:45,830
通常情况下  这个积分

203
00:08:45,980 --> 00:08:47,310
--通常情况下

204
00:08:47,460 --> 00:08:48,880
很难计算θ的后验概率

205
00:08:49,050 --> 00:08:51,270
因此很难计算这个积分

206
00:08:51,420 --> 00:08:52,390
如果θ维度很高的话

207
00:08:52,550 --> 00:08:54,720
在一些特殊情况下

208
00:08:54,880 --> 00:08:56,580
这个式子可以

209
00:08:56,740 --> 00:08:57,410
被表示成解析的形式

210
00:08:57,560 --> 00:08:59,000
但是对于许多学习算法

211
00:08:59,170 --> 00:09:01,010
例如贝叶斯logistic回归

212
00:09:01,180 --> 00:09:02,660
这个式子都很难计算

213
00:09:02,830 --> 00:09:07,500
所以我们通常情况下

214
00:09:07,680 --> 00:09:08,800
不会计算

215
00:09:08,960 --> 00:09:11,130
完整的后验概率

216
00:09:11,290 --> 00:09:13,080
P(θ|S)

217
00:09:13,240 --> 00:09:16,750
我们会尝试

218
00:09:16,930 --> 00:09:18,210
使右边的

219
00:09:18,380 --> 00:09:21,520
这个式子的值

220
00:09:21,680 --> 00:09:22,480
最大化

221
00:09:22,640 --> 00:09:23,450
所以让我写下来

222
00:09:23,600 --> 00:09:26,570
通常情况下

223
00:09:26,720 --> 00:09:27,810
我们不会计算

224
00:09:27,970 --> 00:09:29,370
完整的后验概率

225
00:09:29,520 --> 00:09:42,170
而是选择这样的?  明白吗?

226
00:09:42,320 --> 00:09:43,040
我们将这种选择方法

227
00:09:43,190 --> 00:09:44,630
称为MAP估计

228
00:09:44,830 --> 00:09:45,820
或者对?的后验概率

229
00:09:45,970 --> 00:09:47,440
最大化估计

230
00:09:47,600 --> 00:09:49,480
我们选择最有可能出现的θ

231
00:09:49,650 --> 00:09:51,380
在后验概率下

232
00:09:51,540 --> 00:09:52,870
最有可能出现的?

233
00:09:53,020 --> 00:10:08,360
它应该等于这个式子

234
00:10:08,520 --> 00:10:12,530
当你需要进行

235
00:10:12,720 --> 00:10:21,740
预测的时候

236
00:10:21,890 --> 00:10:28,050
你需要

237
00:10:34,180 --> 00:10:37,110
仍然使用往常的假设

238
00:10:37,280 --> 00:10:40,300
这里的参数向量

239
00:10:40,460 --> 00:10:44,510
使用MAP

240
00:10:44,660 --> 00:10:45,600
估计得到的参数值

241
00:10:45,750 --> 00:10:46,650
明白吗?

242
00:10:46,810 --> 00:10:48,260
需要注意

243
00:10:48,440 --> 00:10:49,640
这个方法和标准的

244
00:10:49,850 --> 00:10:51,050
极大似然估计的唯一不同是--

245
00:10:51,210 --> 00:10:53,320
当我们做选择的时候

246
00:10:53,470 --> 00:10:54,630
我们不选择

247
00:10:54,790 --> 00:10:55,890
最大化θ的似然性

248
00:10:56,060 --> 00:10:58,160
而是选择最大化这个式子

249
00:10:58,320 --> 00:10:59,430
这一部分和极大似然估计

250
00:10:59,590 --> 00:11:00,600
是相同的

251
00:11:00,750 --> 00:11:02,420
这里再乘上

252
00:11:02,580 --> 00:11:04,380
先验概率 明白吗?

253
00:11:04,690 --> 00:11:09,880
让我们看看

254
00:11:10,040 --> 00:11:12,290
一种直观理解是

255
00:11:12,440 --> 00:11:21,660
如果你的先验概率

256
00:11:21,850 --> 00:11:24,130
是一个高斯分布  均值是0

257
00:11:24,280 --> 00:11:25,390
并且有一定的协方差

258
00:11:25,540 --> 00:11:27,610
那么对于这样的一个分部

259
00:11:27,780 --> 00:11:29,250
大多数概率质量

260
00:11:29,400 --> 00:11:30,710
都集中在0附近  对吗?

261
00:11:30,860 --> 00:11:31,760
这个高斯分布

262
00:11:31,920 --> 00:11:33,250
以0为中心

263
00:11:33,410 --> 00:11:34,610
所以大多数概率质量

264
00:11:34,770 --> 00:11:35,840
都在0附近

265
00:11:37,730 --> 00:11:39,280
所以这是先验概率

266
00:11:39,440 --> 00:11:41,310
的含义就是说

267
00:11:41,470 --> 00:11:43,110
多数参数都应该接近于0

268
00:11:43,270 --> 00:11:46,490
如果你们记得

269
00:11:46,660 --> 00:11:47,530
我们关于特征选择的讨论

270
00:11:47,680 --> 00:11:48,830
如果你消除了一个

271
00:11:48,990 --> 00:11:50,070
需要考虑的特征

272
00:11:50,220 --> 00:11:51,890
那么这就等同于

273
00:11:52,050 --> 00:11:54,450
你将某个θ的值

274
00:11:54,610 --> 00:11:55,610
设成了0

275
00:11:55,770 --> 00:11:56,910
明白吗?

276
00:11:57,060 --> 00:11:57,920
如果你令

277
00:11:58,070 --> 00:11:59,510
θ_5=0

278
00:11:59,660 --> 00:12:01,040
那么这就等同于

279
00:12:01,190 --> 00:12:02,130
从假设中

280
00:12:02,310 --> 00:12:03,570
消除第5个特征

281
00:12:03,730 --> 00:12:06,470
所以这个先验概率

282
00:12:06,650 --> 00:12:08,860
将大多数的参数值设为0--


283
00:12:09,020 --> 00:12:11,050
接近于0

284
00:12:11,210 --> 00:12:12,130
你可以类比地

285
00:12:12,280 --> 00:12:14,110
看这个问题--

286
00:12:14,270 --> 00:12:15,330
你可以将其

287
00:12:15,480 --> 00:12:16,420
和特征选择进行类比

288
00:12:16,580 --> 00:12:17,340
明白吗?

289
00:12:17,500 --> 00:12:18,500
实际上

290
00:12:18,670 --> 00:12:19,920
这种方式并不会

291
00:12:20,160 --> 00:12:21,660
将参数严格地设为0

292
00:12:21,980 --> 00:12:23,160
但是多数参数

293
00:12:23,510 --> 00:12:24,450
都会非常接近于0

294
00:12:24,730 --> 00:12:27,820
用图来表示

295
00:12:31,210 --> 00:12:35,110
如果你记得

296
00:12:35,270 --> 00:12:37,950
如果你有

297
00:12:38,110 --> 00:12:39,170
五个数据点  并且你用

298
00:12:39,320 --> 00:12:41,150
一个四次多项式函数进行拟合

299
00:12:47,630 --> 00:12:49,070
--我觉得似乎不太光滑

300
00:12:49,220 --> 00:12:50,420
但是没关系

301
00:12:50,580 --> 00:12:51,950
如果你用一个

302
00:12:52,110 --> 00:12:54,090
非常高次的多项式

303
00:12:54,250 --> 00:12:55,490
拟合一个非常小的数据集合

304
00:12:55,660 --> 00:12:56,330
那么你会发现

305
00:12:56,490 --> 00:12:57,440
函数曲线中会有很大的摆动

306
00:12:57,600 --> 00:12:59,370
如果你用极大似然估计的话

307
00:12:59,520 --> 00:13:00,850
就会出现这种问题  明白吗?

308
00:13:01,010 --> 00:13:02,100
相反地

309
00:13:02,260 --> 00:13:03,130
如果你使用

310
00:13:03,280 --> 00:13:04,290
贝叶斯规范化的方法

311
00:13:04,440 --> 00:13:06,390
你仍然会得到

312
00:13:06,550 --> 00:13:07,590
一个高次多项式

313
00:13:07,750 --> 00:13:11,570
但是在拟合数据时

314
00:13:11,720 --> 00:13:13,690
会变得越来越平滑

315
00:13:13,880 --> 00:13:15,480
因为你减小了τ

316
00:13:15,650 --> 00:13:16,860
所以参数会

317
00:13:17,020 --> 00:13:18,120
越来越接近于0

318
00:13:18,290 --> 00:13:19,870
实际上

319
00:13:20,020 --> 00:13:21,110
这一点不容易看出来

320
00:13:21,280 --> 00:13:22,400
但是你可以先记住我的话

321
00:13:22,560 --> 00:13:24,080
当τ变得越来越小

322
00:13:24,240 --> 00:13:26,430
拟合数据的曲线

323
00:13:26,590 --> 00:13:28,070
也会越来越平滑

324
00:13:28,230 --> 00:13:30,290
过拟合的几率也会越来越小

325
00:13:30,440 --> 00:13:31,850
即使当你拟合

326
00:13:32,020 --> 00:13:32,970
很大数量的参数时这一点也是成立的

327
00:13:33,110 --> 00:13:34,260
明白吗?

328
00:13:36,660 --> 00:13:45,370
让我们再来看

329
00:13:45,520 --> 00:13:47,410
一种直观理解

330
00:13:47,570 --> 00:13:49,010
这些概念你们可以

331
00:13:49,160 --> 00:13:51,280
在problem set 3中

332
00:13:51,440 --> 00:13:53,010
仔细理解

333
00:13:53,170 --> 00:13:54,310
我大概这周晚些时候

334
00:13:54,480 --> 00:13:56,020
会放到网上

335
00:13:56,180 --> 00:13:59,030
极大似然估计

336
00:13:59,200 --> 00:14:12,430
尝试最小化这个式子  对吗?

337
00:14:12,590 --> 00:14:13,590
使用极大似然估计的算法

338
00:14:13,740 --> 00:14:14,730
例如线性回归

339
00:14:14,890 --> 00:14:16,430
实际上目的是使这个式子最小化

340
00:14:16,590 --> 00:14:18,280
实际上如果你

341
00:14:18,440 --> 00:14:20,690
加入了先验概率的因素

342
00:14:20,880 --> 00:14:21,950
那么事实证明

343
00:14:22,100 --> 00:14:24,090
优化目标函数

344
00:14:24,240 --> 00:14:27,360
应该变成这样的形式

345
00:14:27,510 --> 00:14:29,630
加入了额外的一项

346
00:14:29,790 --> 00:14:31,790
使得参数值过大时

347
00:14:31,950 --> 00:14:33,110
会惩罚目标函数

348
00:14:33,270 --> 00:14:35,670
这个算法

349
00:14:35,830 --> 00:14:36,940
和极大似然估计非常类似

350
00:14:37,100 --> 00:14:38,510
除了这个算法

351
00:14:38,670 --> 00:14:40,040
倾向于选择比较小的参数值

352
00:14:40,210 --> 00:14:42,030
这就是最终的效果

353
00:14:42,200 --> 00:14:43,590
这些结论可能很难看出来

354
00:14:43,740 --> 00:14:44,700
但是先记住我说的话

355
00:14:44,850 --> 00:14:45,810
这样的方法得到的参数

356
00:14:45,960 --> 00:14:47,320
会使得拟合出的函数曲线

357
00:14:47,480 --> 00:14:49,330
更为平滑

358
00:14:49,500 --> 00:14:52,050
从而减少过拟合的可能性 明白吗?

359
00:14:54,990 --> 00:14:58,300
希望你们

360
00:14:58,470 --> 00:15:00,060
通过下一个problem set的练习

361
00:15:00,210 --> 00:15:01,010
可以对它有更深的了解

362
00:15:01,170 --> 00:15:02,420
让我看看

363
00:15:02,580 --> 00:15:03,610
你们有什么问题

364
00:15:11,000 --> 00:15:13,660
S:你所说的平滑效应似乎是依赖于

365
00:15:13,840 --> 00:15:16,330
用高斯分布作为先验概率这一事实

366
00:15:16,490 --> 00:15:18,020
如果换用不同的先验概率

367
00:15:18,170 --> 00:15:19,540
还会有这样的效应吗?

368
00:15:19,700 --> 00:15:20,890
I:让我看看

369
00:15:21,040 --> 00:15:22,150
是的 这种效应依赖于

370
00:15:22,320 --> 00:15:24,030
--如果你的先验概率中

371
00:15:24,190 --> 00:15:25,460
大多数的概率密度都接近于0

372
00:15:25,620 --> 00:15:26,790
那么就会有这样的效应

373
00:15:26,960 --> 00:15:28,220
习惯上

374
00:15:28,380 --> 00:15:31,040
高斯先验概率

375
00:15:31,190 --> 00:15:33,360
对于logistic回归

376
00:15:33,520 --> 00:15:35,050
或者线性回归这样的问题来说

377
00:15:35,200 --> 00:15:36,710
是最为常用的

378
00:15:36,870 --> 00:15:39,150
有时候还可以使用

379
00:15:39,310 --> 00:15:40,020
其他先验概率分布

380
00:15:40,200 --> 00:15:41,080
例如:拉普拉斯先验概率

381
00:15:41,240 --> 00:15:42,290
但是这些先验概率都

382
00:15:42,450 --> 00:15:43,970
具有这样的平滑效应

383
00:15:45,790 --> 00:15:50,100
很好

384
00:15:50,250 --> 00:15:51,210
事实证明

385
00:15:51,360 --> 00:15:54,030
像文本分类这样的问题

386
00:15:54,180 --> 00:15:56,840
可能包含30000

387
00:15:57,010 --> 00:15:58,510
或50000个特征

388
00:15:58,660 --> 00:16:01,130
这种情况下

389
00:16:01,280 --> 00:16:02,460
logistic回归这样的算法

390
00:16:02,620 --> 00:16:04,010
非常容易过拟合

391
00:16:04,170 --> 00:16:04,830
想象一下你尝试

392
00:16:04,990 --> 00:16:05,510
建立一个垃圾邮件分类器

393
00:16:05,670 --> 00:16:07,770
可能你有100个训练样本

394
00:16:07,940 --> 00:16:09,490
但是有30000或

395
00:16:09,650 --> 00:16:13,130
50000个特征  这样显然

396
00:16:13,510 --> 00:16:14,980
很容易过拟合

397
00:16:15,140 --> 00:16:16,070
事实证明

398
00:16:16,230 --> 00:16:18,860
使用这种贝叶斯规范化

399
00:16:19,010 --> 00:16:21,640
用高斯分布作为先验概率

400
00:16:21,790 --> 00:16:23,710
logistic回归会是一个

401
00:16:23,860 --> 00:16:24,950
非常有效的文本

402
00:16:25,110 --> 00:16:26,120
分类算法

403
00:16:26,280 --> 00:16:29,700
Alex?

404
00:16:29,870 --> 00:16:32,780
S:[听不见]

405
00:16:32,940 --> 00:16:33,660
I:是的

406
00:16:33,810 --> 00:16:34,920
你可以选择

407
00:16:35,090 --> 00:16:36,310
τ^2或λ的值

408
00:16:36,480 --> 00:16:37,780
λ应该等于

409
00:16:37,930 --> 00:16:39,500
1/τ^2

410
00:16:39,650 --> 00:16:40,470
所以你可以选择

411
00:16:40,630 --> 00:16:41,310
τ^2或λ的值

412
00:16:41,480 --> 00:16:42,810
可以通过交叉验证来选择

413
00:16:42,980 --> 00:16:46,870
明白吗?很好

414
00:16:47,030 --> 00:16:52,250
好的  关于这个

415
00:16:52,400 --> 00:16:53,070
防止过拟合的方法

416
00:16:53,240 --> 00:16:55,180
我就讲完了

417
00:16:55,340 --> 00:16:57,480
接下来

418
00:16:57,640 --> 00:16:59,500
我要花五分钟时间

419
00:16:59,660 --> 00:17:01,270
讲一下在线学习

420
00:17:01,430 --> 00:17:03,710
这个内容并不属于主体内容

421
00:17:03,860 --> 00:17:05,330
你知道

422
00:17:05,490 --> 00:17:07,830
当你设计一门课的课程大纲时

423
00:17:08,000 --> 00:17:09,720
有时会有一些概念

424
00:17:09,910 --> 00:17:11,270
你想介绍它们

425
00:17:11,430 --> 00:17:12,790
但是你不知道

426
00:17:12,950 --> 00:17:13,730
将它们放在哪里合适

427
00:17:13,890 --> 00:17:14,900
我接下来要讲的就是这样一个概念

428
00:17:15,050 --> 00:17:17,220
它可能和其它部分

429
00:17:17,390 --> 00:17:18,220
有些脱节

430
00:17:18,370 --> 00:17:18,930
我只是想

431
00:17:19,100 --> 00:17:19,890
简单地介绍一下

432
00:17:22,600 --> 00:17:30,460
好的  这个概念是这样的

433
00:17:31,870 --> 00:17:35,080
我们目前所讲的

434
00:17:35,240 --> 00:17:36,630
所有学习算法

435
00:17:36,790 --> 00:17:37,820
都被称为批学习算法

436
00:17:37,970 --> 00:17:39,750
给定一个训练集合

437
00:17:39,910 --> 00:17:40,790
之后在训练集合上

438
00:17:40,940 --> 00:17:42,110
运行学习算法

439
00:17:42,260 --> 00:17:43,050
之后再用其它

440
00:17:43,210 --> 00:17:44,470
一些测试集合进行测试

441
00:17:44,630 --> 00:17:48,000
除此之外还有另外一类学习算法

442
00:17:48,160 --> 00:17:49,380
称为在线学习

443
00:17:49,530 --> 00:17:50,660
这种情况下

444
00:17:50,820 --> 00:17:52,230
即使你在学习的过程中

445
00:17:52,410 --> 00:17:53,320
仍然需要进行预测

446
00:17:53,480 --> 00:17:56,230
这个问题是这样的

447
00:17:56,390 --> 00:17:57,370
看好了

448
00:17:57,530 --> 00:17:58,740
我会给你一个输入:x^((1) )

449
00:17:58,900 --> 00:18:00,560
比如说这是一个分类问题

450
00:18:00,710 --> 00:18:03,020
所以我先给你x^((1) )

451
00:18:03,170 --> 00:18:04,800
之后会问你

452
00:18:04,960 --> 00:18:06,190
"你能预测出x^((1) )的标记吗?

453
00:18:06,350 --> 00:18:07,490
是0还是1?"

454
00:18:07,660 --> 00:18:08,950
这个时候你还没有见过任何数据

455
00:18:09,120 --> 00:18:11,130
之后你做出了一个猜测  对吗?

456
00:18:11,290 --> 00:18:13,150
我们将你的猜测

457
00:18:13,310 --> 00:18:14,210
表示为y ?^((1) )

458
00:18:14,360 --> 00:18:17,060
在你做出预测之后

459
00:18:17,250 --> 00:18:18,370
我会告诉你

460
00:18:18,520 --> 00:18:21,270
y^((1) )的真实值

461
00:18:21,430 --> 00:18:22,850
在没有见过任何数据的情况下

462
00:18:23,000 --> 00:18:24,920
你猜对的概率

463
00:18:25,080 --> 00:18:26,150
只有50%  对吗?

464
00:18:26,330 --> 00:18:27,370
如果你随机猜得话

465
00:18:29,440 --> 00:18:31,420
之后我给你x^((2) )

466
00:18:31,570 --> 00:18:33,200
之后我会问你

467
00:18:33,370 --> 00:18:34,510
"你能对x^((2) )进行预测吗?"

468
00:18:34,670 --> 00:18:36,680
根据之前的数据

469
00:18:36,850 --> 00:18:37,580
可能你现在会给出一个

470
00:18:37,750 --> 00:18:38,690
稍微靠谱点的猜测

471
00:18:38,850 --> 00:18:39,640
将其表示为y ?^((2) )

472
00:18:39,790 --> 00:18:41,350
你作出猜测之后

473
00:18:41,520 --> 00:18:42,880
我将其真正的类别标记告诉你

474
00:18:43,030 --> 00:18:45,460
之后我给你x^((3) )

475
00:18:45,620 --> 00:18:47,160
之后你进行猜测

476
00:18:47,320 --> 00:18:50,720
整个学习过程就是这样的

477
00:18:50,880 --> 00:18:54,930
这个模型和

478
00:18:55,070 --> 00:18:56,140
批学习模型是不同的

479
00:18:56,300 --> 00:18:57,800
在这个模型中

480
00:18:57,980 --> 00:19:00,750
即使在进行预测的时候

481
00:19:00,910 --> 00:19:02,050
也要进行学习  明白吗?

482
00:19:02,220 --> 00:19:04,270
想象一下

483
00:19:04,420 --> 00:19:06,220
你有一个网站

484
00:19:06,390 --> 00:19:07,230
用户可以访问它

485
00:19:07,380 --> 00:19:08,390
当用户进行访问时

486
00:19:08,720 --> 00:19:09,760
你开始

487
00:19:09,960 --> 00:19:11,010
对用户的好恶

488
00:19:11,170 --> 00:19:12,050
进行预测

489
00:19:12,240 --> 00:19:13,560
随着你预测的越来越多

490
00:19:13,730 --> 00:19:15,020
你的训练样本数

491
00:19:15,190 --> 00:19:16,580
也会越来越多

492
00:19:16,730 --> 00:19:18,980
在线学习中

493
00:19:19,130 --> 00:19:20,240
我们需要关注

494
00:19:20,380 --> 00:19:21,570
总在线误差

495
00:19:26,210 --> 00:19:30,920
它应该等于

496
00:19:31,060 --> 00:19:32,440
这个式子

497
00:19:32,590 --> 00:19:37,780
其中m是样本的数量

498
00:19:37,950 --> 00:19:40,900
明白吗?

499
00:19:41,060 --> 00:19:43,480
所以总在线误差等于

500
00:19:43,640 --> 00:19:46,000
整个样本序列的

501
00:19:46,170 --> 00:19:47,330
预测错误次数

502
00:19:47,500 --> 00:19:53,470
事实证明

503
00:19:53,630 --> 00:19:55,450
许多你们已经

504
00:19:55,620 --> 00:19:56,920
学过的学习算法

505
00:19:57,080 --> 00:19:57,950
都可以应用于

506
00:19:58,100 --> 00:19:59,120
这种在线学习的形式

507
00:19:59,290 --> 00:20:01,370
你可以这样做

508
00:20:01,530 --> 00:20:04,520
当你需要对

509
00:20:04,690 --> 00:20:06,270
y ?^((3) )进行预测时

510
00:20:06,420 --> 00:20:08,410
你可以将这个点之前

511
00:20:08,570 --> 00:20:10,000
所见过的所有其他训练样本

512
00:20:10,160 --> 00:20:11,820
作为训练集合

513
00:20:11,980 --> 00:20:13,100
并基于其运行学习算法

514
00:20:13,260 --> 00:20:15,680
例如这里你需要基于

515
00:20:15,840 --> 00:20:17,520
y^((3) )之前的样本学习

516
00:20:17,680 --> 00:20:19,050
为了对一个样本做出预测

517
00:20:19,210 --> 00:20:20,120
首先用之前所见过的

518
00:20:20,270 --> 00:20:22,470
所有样本进行训练

519
00:20:22,640 --> 00:20:24,120
之后再用

520
00:20:24,280 --> 00:20:25,460
训练好的模型

521
00:20:25,620 --> 00:20:26,420
对新样本进行预测

522
00:20:26,580 --> 00:20:28,830
实际上还有一些

523
00:20:28,990 --> 00:20:30,460
其他的算法

524
00:20:30,610 --> 00:20:32,160
例如我们之前讲过的

525
00:20:32,330 --> 00:20:33,540
随机梯度下降算法

526
00:20:33,710 --> 00:20:34,790
可以更好地

527
00:20:34,950 --> 00:20:35,610
适用于这种情形

528
00:20:35,770 --> 00:20:37,820
举一个具体的例子

529
00:20:40,650 --> 00:20:42,270
你们可能还记得

530
00:20:42,430 --> 00:20:45,370
感知器算法

531
00:20:45,540 --> 00:20:49,160
首先将参数

532
00:20:49,320 --> 00:20:50,700
θ初始化为0

533
00:20:50,860 --> 00:20:58,550
见到第i个训练样本后

534
00:20:58,700 --> 00:21:01,440
需要更新参数

535
00:21:09,670 --> 00:21:15,500
你们可能见过

536
00:21:15,650 --> 00:21:17,560
这个学习规则很多次了

537
00:21:17,730 --> 00:21:18,630
它被称为

538
00:21:18,780 --> 00:21:20,890
标准感知器学习规则

539
00:21:21,050 --> 00:21:22,170
同样道理

540
00:21:22,330 --> 00:21:24,380
如果你使用logistic回归

541
00:21:24,540 --> 00:21:27,140
之后对于每个样本

542
00:21:27,300 --> 00:21:29,000
你可以仅仅对该样本

543
00:21:29,160 --> 00:21:31,470
使用一次

544
00:21:31,680 --> 00:21:34,250
随机梯度下降

545
00:21:34,820 --> 00:21:37,200
明白吗?

546
00:21:37,350 --> 00:21:41,360
之所以我

547
00:21:41,510 --> 00:21:43,470
将这个知识点

548
00:21:43,640 --> 00:21:44,630
放在学习理论部分讲  是因为

549
00:21:44,800 --> 00:21:46,510
当你使用这样的算法时

550
00:21:46,670 --> 00:21:47,760
有的时候你可以证明出

551
00:21:47,910 --> 00:21:50,570
一些关于总在线误差的

552
00:21:50,730 --> 00:21:52,420
惊人的结论

553
00:21:52,590 --> 00:21:55,900
我不会在课上

554
00:21:56,060 --> 00:21:57,700
花时间证明

555
00:21:57,850 --> 00:21:58,950
但是  例如

556
00:21:59,120 --> 00:22:00,090
你可以证明

557
00:22:00,250 --> 00:22:03,070
对于感知器算法

558
00:22:03,230 --> 00:22:11,590
即使x^((i) )

559
00:22:11,750 --> 00:22:13,570
是无限维的特征向量

560
00:22:13,730 --> 00:22:15,770
像我们在SVM中看到的那样

561
00:22:15,930 --> 00:22:16,590
有些时候

562
00:22:16,770 --> 00:22:17,890
无限维的特征向量

563
00:22:18,050 --> 00:22:20,550
可以用核来表示

564
00:22:20,710 --> 00:22:21,860
实际上可以证明

565
00:22:22,030 --> 00:22:23,320
当你使用感知器算法时

566
00:22:23,480 --> 00:22:25,220
即使数据的维度

567
00:22:25,420 --> 00:22:28,490
可能非常高

568
00:22:28,650 --> 00:22:29,740
你也可以证明

569
00:22:29,890 --> 00:22:32,780
只要正负样本

570
00:22:32,950 --> 00:22:34,630
能够以某个间隔被分隔开

571
00:22:34,780 --> 00:22:39,050
在这个无限维空间中

572
00:22:39,220 --> 00:22:44,210
只要正负样本能够

573
00:22:44,380 --> 00:22:49,930
以某个间隔被分隔开

574
00:22:50,100 --> 00:22:53,450
那么感知器算法会收敛

575
00:22:53,600 --> 00:22:54,810
到一个能够将

576
00:22:54,970 --> 00:22:59,670
正负样本完美分隔的假设

577
00:22:59,860 --> 00:23:02,770
明白吗?

578
00:23:02,920 --> 00:23:04,540
所以见到有限个样本之后

579
00:23:04,690 --> 00:23:07,320
它会收敛到

580
00:23:07,480 --> 00:23:09,060
一个能够

581
00:23:09,230 --> 00:23:12,100
将正负样本

582
00:23:12,260 --> 00:23:13,560
完美分隔的边界上

583
00:23:13,730 --> 00:23:14,760
即使输入向量

584
00:23:14,920 --> 00:23:15,740
可能是无限维的

585
00:23:15,910 --> 00:23:17,510
明白吗?

586
00:23:17,660 --> 00:23:21,190
让我看看

587
00:23:21,340 --> 00:23:24,000
讲这个证明

588
00:23:24,160 --> 00:23:25,280
需要花费一节课的时间

589
00:23:25,450 --> 00:23:27,320
但是我们还有

590
00:23:27,460 --> 00:23:28,680
其他内容要讲

591
00:23:28,830 --> 00:23:30,000
如果你们想自己看

592
00:23:30,150 --> 00:23:31,250
证明过程的话

593
00:23:31,400 --> 00:23:32,650
我已经将它写在讲义里了

594
00:23:32,810 --> 00:23:33,890
你们可以从网上得到

595
00:23:34,040 --> 00:23:35,980
根据课程大纲

596
00:23:36,130 --> 00:23:38,280
这个结果的证明过程

597
00:23:38,430 --> 00:23:39,510
我们不作要求

598
00:23:39,680 --> 00:23:40,740
这意味着

599
00:23:41,140 --> 00:23:42,550
它不会出现在

600
00:23:42,710 --> 00:23:44,000
期中考试

601
00:23:44,160 --> 00:23:45,200
或者problem set中

602
00:23:45,350 --> 00:23:48,270
但是我认为--


603
00:23:48,420 --> 00:23:49,600
我知道你们有些人

604
00:23:49,760 --> 00:23:51,380
对于上节课的一些

605
00:23:51,530 --> 00:23:53,450
结论的证明过程很好奇

606
00:23:53,620 --> 00:23:55,870
例如:SVM的VC维是有界的

607
00:23:56,030 --> 00:23:58,360
即使在无限维的空间中

608
00:23:58,530 --> 00:23:59,980
怎样在无限维的空间中

609
00:24:00,150 --> 00:24:01,670
证明这些

610
00:24:01,830 --> 00:24:03,440
学习理论方面的结论?

611
00:24:03,600 --> 00:24:05,450
这个感知器的界的结论

612
00:24:05,620 --> 00:24:06,830
是我知道的

613
00:24:06,980 --> 00:24:08,470
最简单的一个例子

614
00:24:08,630 --> 00:24:10,180
你们可以回去

615
00:24:10,350 --> 00:24:11,530
用大概半个小时的时间阅读并理解它

616
00:24:11,700 --> 00:24:12,550
如果你们感兴趣

617
00:24:12,710 --> 00:24:15,080
网上有关于

618
00:24:15,240 --> 00:24:16,770
这个结论

619
00:24:16,930 --> 00:24:17,870
证明过程的讲义

620
00:24:18,020 --> 00:24:19,400
并不是很难

621
00:24:19,560 --> 00:24:20,950
你们可以用一页纸左右证明出来

622
00:24:21,110 --> 00:24:22,610
如果感兴趣的话

623
00:24:22,780 --> 00:24:24,490
可以回去自己看看 好吗?

624
00:24:24,660 --> 00:24:25,530
除了这个

625
00:24:25,690 --> 00:24:28,560
理论结果之外

626
00:24:28,720 --> 00:24:29,630
在线学习的情景

627
00:24:29,790 --> 00:24:30,620
是一种

628
00:24:30,780 --> 00:24:32,050
非常合理的情景

629
00:24:32,210 --> 00:24:33,870
像随机梯度下降

630
00:24:34,180 --> 00:24:35,180
这样的算法

631
00:24:35,350 --> 00:24:36,270
非常适用于这种情况

632
00:24:36,440 --> 00:24:39,400
在我继续之前

633
00:24:39,560 --> 00:24:40,470
还有问题要问吗?

634
00:24:47,430 --> 00:24:48,920
很好

635
00:24:49,090 --> 00:24:51,350
我最后要讲的

636
00:24:51,540 --> 00:24:53,240
也是我们今天的主要内容

637
00:24:53,400 --> 00:24:54,710
请帮我切到PPT的界面

638
00:24:54,880 --> 00:24:57,240
这节课的大部分内容

639
00:24:57,410 --> 00:24:59,090
我们都会讲这些内容

640
00:24:59,250 --> 00:25:00,230
会讲一些应用

641
00:25:00,400 --> 00:25:01,630
不同的机器学习算法的建议

642
00:25:10,350 --> 00:25:13,370
你们知道

643
00:25:13,530 --> 00:25:14,870
你们现在已经对这些强大的

644
00:25:15,040 --> 00:25:18,510
机器学习工具

645
00:25:18,660 --> 00:25:19,980
有了很好的了解

646
00:25:20,140 --> 00:25:20,980
是不是?

647
00:25:21,140 --> 00:25:23,030
我今天要

648
00:25:23,200 --> 00:25:24,520
给你们一些建议

649
00:25:24,670 --> 00:25:25,910
关于如何有效地使用它们

650
00:25:26,070 --> 00:25:28,420
比如说

651
00:25:28,580 --> 00:25:30,220
对于同样的

652
00:25:30,390 --> 00:25:31,560
机器学习算法

653
00:25:31,720 --> 00:25:32,590
例如:logistic回归

654
00:25:32,750 --> 00:25:34,510
你可以让两个人同时利用它们

655
00:25:34,680 --> 00:25:35,630
解决同样的问题

656
00:25:35,790 --> 00:25:38,510
有的时候其中一个人

657
00:25:38,660 --> 00:25:39,580
会做出非常惊人的工作

658
00:25:39,730 --> 00:25:40,750
效果也会非常好

659
00:25:40,910 --> 00:25:42,040
但是另外一个人可能发现

660
00:25:42,200 --> 00:25:43,910
算法根本没用

661
00:25:44,060 --> 00:25:45,190
即使两个人确实用的是

662
00:25:45,350 --> 00:25:46,390
同样的算法

663
00:25:46,550 --> 00:25:49,040
我今天在剩下的时间里

664
00:25:49,210 --> 00:25:50,650
要告诉你们一些方法

665
00:25:50,810 --> 00:25:53,160
是你们真正了解

666
00:25:53,320 --> 00:25:55,250
应该怎样

667
00:25:55,410 --> 00:25:57,340
用机器

668
00:25:57,500 --> 00:25:58,320
学习算法

669
00:25:58,480 --> 00:25:59,580
来解决具体问题

670
00:25:59,740 --> 00:26:06,530
在剩下的课上

671
00:26:06,690 --> 00:26:07,790
我会告诉你们

672
00:26:07,960 --> 00:26:09,470
一些需要注意的地方

673
00:26:09,630 --> 00:26:12,660
我接下要讲的东西

674
00:26:12,820 --> 00:26:13,870
数学性不是很强

675
00:26:14,020 --> 00:26:16,350
但都是最难的一部分

676
00:26:16,530 --> 00:26:19,190
多数都是观念上的

677
00:26:19,360 --> 00:26:20,630
而且比较难以理解

678
00:26:20,780 --> 00:26:21,390
懂了吗?

679
00:26:21,540 --> 00:26:23,510
我要讲的东西和数学无关

680
00:26:23,670 --> 00:26:24,560
但是并不容易

681
00:26:24,710 --> 00:26:27,350
另外我今天讲的一些东西

682
00:26:27,500 --> 00:26:28,710
是存在争议的

683
00:26:28,880 --> 00:26:30,270
我认为一个好的

684
00:26:30,430 --> 00:26:31,560
机器学习领域的人

685
00:26:31,730 --> 00:26:32,420
会同意我说的大多数

686
00:26:32,580 --> 00:26:33,540
但是不是全部

687
00:26:33,700 --> 00:26:36,340
另外我的一些建议

688
00:26:36,500 --> 00:26:37,270
对于机器学习研究来说

689
00:26:37,430 --> 00:26:38,610
并不是很好的建议

690
00:26:38,780 --> 00:26:39,630
稍后我会说明

691
00:26:39,780 --> 00:26:42,750
我今天所关注的重点是

692
00:26:42,910 --> 00:26:44,630
怎样使用机器学习算法

693
00:26:44,780 --> 00:26:45,770
如果你在公司工作

694
00:26:45,940 --> 00:26:47,110
需要推出一个新产品

695
00:26:47,260 --> 00:26:49,420
建立一个系统

696
00:26:49,570 --> 00:26:50,510
你希望让你的

697
00:26:50,690 --> 00:26:52,160
机器学习系统能够工作

698
00:26:52,310 --> 00:26:53,190
我今天讲的东西

699
00:26:53,340 --> 00:26:55,500
对于发明新的机器学习算法而言

700
00:26:55,650 --> 00:26:56,970
并不是很好的建议

701
00:26:57,120 --> 00:26:58,250
但是这些建议

702
00:26:58,410 --> 00:26:59,810
对于应用

703
00:26:59,970 --> 00:27:01,390
机器学习部署系统来说

704
00:27:01,540 --> 00:27:02,790
将会非常有帮助

705
00:27:02,950 --> 00:27:06,690
我要讲的三个主要内容是

706
00:27:06,860 --> 00:27:09,640
一:学习算法的

707
00:27:09,790 --> 00:27:10,770
调试诊断法

708
00:27:10,930 --> 00:27:13,540
二:误差分析

709
00:27:13,700 --> 00:27:15,150
和销蚀分析

710
00:27:15,310 --> 00:27:18,970
第三:我要给出一些建议

711
00:27:19,120 --> 00:27:20,710
关于怎样求解

712
00:27:20,870 --> 00:27:23,490
机器学习问题

713
00:27:23,650 --> 00:27:26,290
我们之后会讲到的一个主题是

714
00:27:26,440 --> 00:27:28,610
你们都听说过的过早优化问题

715
00:27:28,790 --> 00:27:30,690
这是软件编写中

716
00:27:30,860 --> 00:27:31,890
经常会遇到的问题

717
00:27:32,050 --> 00:27:32,590
这个问题指的是

718
00:27:32,760 --> 00:27:35,130
在软件编写早期的过度设计

719
00:27:35,290 --> 00:27:36,340
比如某个人

720
00:27:36,490 --> 00:27:37,330
在写代码的时候

721
00:27:37,500 --> 00:27:39,170
他可能会对某个函数

722
00:27:39,320 --> 00:27:41,250
过度优化

723
00:27:41,410 --> 00:27:42,760
例如  用汇编语言或其他语言

724
00:27:42,920 --> 00:27:43,930
来写这个函数

725
00:27:44,080 --> 00:27:46,300
我们中的许多人

726
00:27:46,480 --> 00:27:48,200
都有过过早优化的习惯

727
00:27:48,360 --> 00:27:49,570
我们尝试

728
00:27:49,730 --> 00:27:50,840
让一段代码高速地运行

729
00:27:50,990 --> 00:27:52,430
我们可能选取一段代码

730
00:27:52,630 --> 00:27:54,150
然后用汇编语言实现

731
00:27:54,310 --> 00:27:55,250
这确实能够

732
00:27:55,400 --> 00:27:56,330
使该段代码高速执行

733
00:27:56,500 --> 00:27:57,860
但是实际上对于整个程序来说

734
00:27:58,010 --> 00:27:59,480
这段代码可能根本不是瓶颈

735
00:27:59,630 --> 00:28:00,500
我们将这种行为

736
00:28:00,670 --> 00:28:01,560
称为过早优化

737
00:28:01,720 --> 00:28:03,320
在本科生的编程课上

738
00:28:03,470 --> 00:28:04,990
我们反复地警告人们

739
00:28:05,150 --> 00:28:06,610
不要进行过早优化

740
00:28:06,760 --> 00:28:08,350
但是还是不断地有人这样做

741
00:28:08,510 --> 00:28:11,960
实际上

742
00:28:12,120 --> 00:28:13,290
在建立机器学习系统时

743
00:28:13,460 --> 00:28:15,100
也存在相似的问题

744
00:28:15,260 --> 00:28:17,300
很多人都会犯这样的错误

745
00:28:17,470 --> 00:28:18,870
称之为过早统计优化

746
00:28:19,030 --> 00:28:21,510
他们会对

747
00:28:21,680 --> 00:28:23,880
机器学习系统的

748
00:28:24,040 --> 00:28:25,880
一个非常次要的部分

749
00:28:26,040 --> 00:28:27,630
进行过度优化

750
00:28:27,790 --> 00:28:28,880
我们之后会讨论这个问题

751
00:28:29,050 --> 00:28:30,890
让我们先关注

752
00:28:31,040 --> 00:28:32,130
学习算法的调试问题

753
00:28:38,460 --> 00:28:40,560
来看一个例子

754
00:28:40,720 --> 00:28:43,840
比如说你想建立一个

755
00:28:44,000 --> 00:28:45,140
垃圾邮件识别系统

756
00:28:45,300 --> 00:28:47,900
经过仔细地筛选

757
00:28:48,070 --> 00:28:49,900
你选择了100个词

758
00:28:50,060 --> 00:28:51,640
作为特征

759
00:28:51,790 --> 00:28:52,930
而不是原来的50000个词

760
00:28:53,080 --> 00:28:53,590
你选择了一个

761
00:28:53,740 --> 00:28:54,630
具有100个特征的很小的集合

762
00:28:54,790 --> 00:28:56,700
来建立垃圾邮件识别系统

763
00:28:56,860 --> 00:28:59,660
你使用的算法是

764
00:28:59,830 --> 00:29:00,850
贝叶斯logistic回归

765
00:29:01,010 --> 00:29:02,210
用梯度下降算法实现

766
00:29:02,370 --> 00:29:04,010
测试误差为20%

767
00:29:04,170 --> 00:29:06,570
这是一个不可接受的误差

768
00:29:06,740 --> 00:29:08,580
这是贝叶斯logistic回归

769
00:29:08,750 --> 00:29:10,290
它看起来像

770
00:29:10,480 --> 00:29:12,000
极大似然估计

771
00:29:12,170 --> 00:29:14,260
但是这里多了额外的一项

772
00:29:14,430 --> 00:29:16,110
我们这里是要最大化

773
00:29:16,260 --> 00:29:17,780
而不是最小化

774
00:29:17,950 --> 00:29:19,980
因此这里是减去这一项

775
00:29:20,130 --> 00:29:21,340
而不是加上这一项

776
00:29:21,500 --> 00:29:24,200
我们的问题是

777
00:29:24,360 --> 00:29:26,050
当你实现了贝叶斯

778
00:29:26,220 --> 00:29:27,200
logistic回归

779
00:29:27,360 --> 00:29:29,840
并且经过测试

780
00:29:29,990 --> 00:29:31,180
得到了一个非常高的测试误差

781
00:29:31,340 --> 00:29:33,060
你接下来应该怎么办?

782
00:29:35,220 --> 00:29:38,260
你可以做的一件事

783
00:29:38,420 --> 00:29:39,400
是想方设法地

784
00:29:39,550 --> 00:29:40,470
改造算法

785
00:29:40,620 --> 00:29:41,430
这可能是

786
00:29:41,590 --> 00:29:42,900
多数人会做的

787
00:29:43,050 --> 00:29:44,240
"让我们来看看

788
00:29:44,390 --> 00:29:45,630
哪里有问题  然后

789
00:29:45,790 --> 00:29:46,790
我们尝试改进算法 "

790
00:29:46,940 --> 00:29:49,410
显然  使用更多的训练数据

791
00:29:49,580 --> 00:29:50,360
会有帮助

792
00:29:50,510 --> 00:29:51,430
所以你可以

793
00:29:51,570 --> 00:29:52,350
提供更多的训练样本

794
00:29:54,190 --> 00:29:55,470
也许你会怀疑

795
00:29:55,610 --> 00:29:57,060
100个特征是不是太多了

796
00:29:57,220 --> 00:29:58,180
所以你可以使用

797
00:29:58,350 --> 00:29:59,950
更少的特征

798
00:30:00,110 --> 00:30:03,000
你也可以怀疑

799
00:30:03,160 --> 00:30:04,110
你选取的特征是不是不够好

800
00:30:04,270 --> 00:30:05,240
所以你可能花更多的时间

801
00:30:05,390 --> 00:30:06,140
来查看邮件标题

802
00:30:06,300 --> 00:30:07,510
看看能否找到

803
00:30:07,670 --> 00:30:08,600
更好的特征

804
00:30:08,750 --> 00:30:11,950
来确定垃圾邮件

805
00:30:12,110 --> 00:30:15,320
你可以仔细寻找看看

806
00:30:15,470 --> 00:30:16,660
是否能够找到更好的特征

807
00:30:16,820 --> 00:30:18,700
例如:邮件标题

808
00:30:18,870 --> 00:30:22,090
你可能会怀疑

809
00:30:22,250 --> 00:30:23,560
梯度下降还没有完全收敛

810
00:30:23,730 --> 00:30:25,030
所以可以再运行几次

811
00:30:25,190 --> 00:30:26,310
梯度下降来

812
00:30:26,460 --> 00:30:27,140
看看是否有效

813
00:30:27,290 --> 00:30:28,320
显然这不会有任何影响

814
00:30:28,470 --> 00:30:30,280
只需要再运行几次

815
00:30:30,440 --> 00:30:33,040
或者你可能记得

816
00:30:33,190 --> 00:30:34,710
课上讲过

817
00:30:34,850 --> 00:30:36,450
牛顿方法可能收敛效果会更好

818
00:30:36,610 --> 00:30:37,620
所以可以试一试

819
00:30:39,250 --> 00:30:40,280
你可以调节

820
00:30:40,440 --> 00:30:41,190
λ的值

821
00:30:41,370 --> 00:30:42,120
如果你不确定它的值

822
00:30:42,280 --> 00:30:42,950
是否合适的话

823
00:30:43,120 --> 00:30:45,350
或者你可以使用SVM算法

824
00:30:45,510 --> 00:30:47,150
因为有可能SVM算法

825
00:30:47,300 --> 00:30:48,830
比logistic回归效果更好

826
00:30:48,990 --> 00:30:51,840
我这里列出了八种改进的可能

827
00:30:51,990 --> 00:30:52,480
你可以想象一下

828
00:30:52,630 --> 00:30:54,030
当你建立

829
00:30:54,180 --> 00:30:55,240
机器学习系统的时候

830
00:30:55,390 --> 00:30:57,010
你可以进行的改进可能性太多了

831
00:30:57,170 --> 00:30:58,660
你可以找到

832
00:30:58,810 --> 00:31:00,150
几百种改进系统的方式

833
00:31:00,320 --> 00:31:01,930
其中的一些方式肯定有效

834
00:31:02,100 --> 00:31:03,460
例如  提供更多的训练样本

835
00:31:03,620 --> 00:31:05,070
多使用这样的方式

836
00:31:05,220 --> 00:31:06,760
将会节省你的时间

837
00:31:06,920 --> 00:31:11,770
事实上这种

838
00:31:11,930 --> 00:31:13,910
选取可能的

839
00:31:14,060 --> 00:31:15,530
改进方向的方法

840
00:31:15,680 --> 00:31:17,410
也许能够帮助你

841
00:31:17,560 --> 00:31:18,850
逐渐建立一个

842
00:31:19,010 --> 00:31:20,090
能够工作的系统

843
00:31:20,240 --> 00:31:21,630
但是通常情况下这是非常费时的

844
00:31:21,820 --> 00:31:24,000
而且这种方法

845
00:31:24,160 --> 00:31:25,410
经常会很依赖于运气

846
00:31:25,580 --> 00:31:26,730
运气好的话

847
00:31:26,880 --> 00:31:27,690
你就能很快地解决问题

848
00:31:27,850 --> 00:31:30,300
具体地  这八种改进方法

849
00:31:30,460 --> 00:31:32,270
处理的都是不同的问题

850
00:31:32,440 --> 00:31:34,500
可能其中的一些改进

851
00:31:34,670 --> 00:31:35,560
你根本就不需要

852
00:31:35,720 --> 00:31:38,330
如果你可以排除

853
00:31:38,490 --> 00:31:40,440
其中的六种

854
00:31:40,590 --> 00:31:41,420
你可以--

855
00:31:41,590 --> 00:31:42,570
如果能够通过某种方式

856
00:31:42,730 --> 00:31:43,780
深入地分析问题

857
00:31:43,940 --> 00:31:44,670
你可以发现

858
00:31:44,830 --> 00:31:45,920
到底这八种方案中

859
00:31:46,080 --> 00:31:47,140
哪些才是有必要的

860
00:31:47,290 --> 00:31:49,480
这样你就可以节省很多时间

861
00:31:49,640 --> 00:31:51,260
让我们看看

862
00:31:51,410 --> 00:31:52,330
我们应该怎样做

863
00:31:55,390 --> 00:31:58,280
我所认识的工业界和学术界的

864
00:31:58,440 --> 00:32:00,090
成功人士

865
00:32:00,260 --> 00:32:02,370
都不会尝试

866
00:32:02,530 --> 00:32:03,960
随机地改变学习算法

867
00:32:04,120 --> 00:32:05,880
有很多方法可以明显地

868
00:32:06,080 --> 00:32:07,580
改进学习算法

869
00:32:07,760 --> 00:32:08,680
但是问题是

870
00:32:08,840 --> 00:32:10,050
这些方法如此地多

871
00:32:10,220 --> 00:32:11,060
你根本不知道应该如何做

872
00:32:11,210 --> 00:32:14,030
所以你需要

873
00:32:14,190 --> 00:32:16,110
通过不同的诊断方法

874
00:32:16,270 --> 00:32:17,300
来确定问题

875
00:32:17,450 --> 00:32:20,060
之后再修正这些问题

876
00:32:20,220 --> 00:32:25,550
对于我们举的那个例子

877
00:32:25,710 --> 00:32:26,820
一个贝叶斯logistic回归算法的

878
00:32:26,990 --> 00:32:28,410
测试误差达到了20%

879
00:32:28,580 --> 00:32:30,380
这个误差太高了

880
00:32:30,530 --> 00:32:33,030
假设你们怀疑是

881
00:32:33,190 --> 00:32:35,300
过拟合导致的这个问题

882
00:32:35,450 --> 00:32:37,210
或者高偏差导致的问题

883
00:32:37,370 --> 00:32:39,160
或者你们怀疑

884
00:32:39,310 --> 00:32:40,160
我们使用的特征

885
00:32:40,300 --> 00:32:41,780
太少了  所以--

886
00:32:41,930 --> 00:32:42,740
哦  对不起

887
00:32:42,900 --> 00:32:45,730
我想我写错了

888
00:32:45,880 --> 00:32:48,410
让我们先不管

889
00:32:48,570 --> 00:32:49,490
先不管它

890
00:32:49,640 --> 00:32:50,580
假设你怀疑问题出在

891
00:32:50,730 --> 00:32:51,640
高偏差或高方差上

892
00:32:51,800 --> 00:32:52,850
幻灯片中的某些文本

893
00:32:53,000 --> 00:32:53,820
没有意义

894
00:32:53,980 --> 00:32:55,820
如果你认为是

895
00:32:55,980 --> 00:32:57,940
过拟合导致的

896
00:32:58,090 --> 00:32:59,310
(这会导致高方差)

897
00:32:59,470 --> 00:33:00,700
或者你认为是

898
00:33:00,860 --> 00:33:01,860
过少的特征导致的

899
00:33:02,020 --> 00:33:02,840
(这会导致高偏差)

900
00:33:02,980 --> 00:33:04,910
幻灯片里我这两部分写反了  对不起

901
00:33:05,060 --> 00:33:06,720
那么你应该如何区分

902
00:33:06,880 --> 00:33:09,940
到底是高偏差

903
00:33:10,090 --> 00:33:12,600
还是高方差呢?

904
00:33:14,830 --> 00:33:16,460
实际上有一个

905
00:33:16,610 --> 00:33:18,050
简单的诊断方法告诉你

906
00:33:18,210 --> 00:33:19,940
到底是高偏差

907
00:33:20,100 --> 00:33:21,170
还是高方差

908
00:33:21,340 --> 00:33:25,070
如果你记得我们之前的

909
00:33:25,220 --> 00:33:26,860
关于高方差问题的示意图的话

910
00:33:27,010 --> 00:33:28,000
你会发现当方差很高时

911
00:33:28,150 --> 00:33:31,140
训练误差会

912
00:33:31,290 --> 00:33:32,450
远远小于测试误差

913
00:33:32,640 --> 00:33:33,380
对吗?

914
00:33:33,550 --> 00:33:34,490
如果是高方差

915
00:33:34,650 --> 00:33:35,340
那么你对于

916
00:33:35,510 --> 00:33:36,980
训练集合的拟合将会非常好

917
00:33:37,140 --> 00:33:38,380
例如

918
00:33:38,550 --> 00:33:39,210
你可能用10次多项式

919
00:33:39,370 --> 00:33:41,520
拟合11个点

920
00:33:41,680 --> 00:33:42,510
所以当你数据拟合

921
00:33:42,660 --> 00:33:43,550
非常好的时候

922
00:33:43,700 --> 00:33:44,720
你的训练误差

923
00:33:44,870 --> 00:33:45,980
将会远远小于测试误差

924
00:33:46,130 --> 00:33:48,560
相反地  如果是高偏差

925
00:33:48,710 --> 00:33:50,680
那么训练误差

926
00:33:50,850 --> 00:33:52,720
也将会很高

927
00:33:52,870 --> 00:33:54,260
如果你的数据是二次的

928
00:33:54,430 --> 00:33:56,170
而你用一个线性函数去拟合它

929
00:33:56,320 --> 00:33:57,380
所以即使是训练集合

930
00:33:57,540 --> 00:33:58,580
拟合效果也会很差

931
00:33:58,730 --> 00:34:03,180
用图来表示

932
00:34:03,330 --> 00:34:05,860
这是一个典型的

933
00:34:06,030 --> 00:34:07,720
高方差学习曲线

934
00:34:07,880 --> 00:34:10,170
横轴表示的是

935
00:34:10,320 --> 00:34:12,340
训练集合的大小m

936
00:34:12,500 --> 00:34:14,730
纵轴表示的

937
00:34:14,890 --> 00:34:15,590
是误差

938
00:34:15,750 --> 00:34:19,710
因此

939
00:34:19,870 --> 00:34:20,640
当你增加--

940
00:34:20,800 --> 00:34:22,570
如果存在高方差问题

941
00:34:22,730 --> 00:34:23,750
你会注意到

942
00:34:23,900 --> 00:34:25,470
随着训练集合大小m的增长

943
00:34:25,630 --> 00:34:27,440
测试集误差将会

944
00:34:27,600 --> 00:34:28,330
持续减小

945
00:34:28,500 --> 00:34:30,770
这意味着

946
00:34:30,940 --> 00:34:31,720
如果你进一步增加

947
00:34:31,870 --> 00:34:32,840
训练集合的大小

948
00:34:33,010 --> 00:34:34,240
这条绿线还会

949
00:34:34,410 --> 00:34:35,730
进一步向下延伸

950
00:34:35,910 --> 00:34:37,220
测试集误差也会

951
00:34:37,380 --> 00:34:38,480
进一步减小

952
00:34:38,640 --> 00:34:39,450
对吗?

953
00:34:39,610 --> 00:34:41,470
另外一条有用的线--

954
00:34:41,620 --> 00:34:44,090
我们用这条红色的线

955
00:34:44,250 --> 00:34:45,360
来表示

956
00:34:45,530 --> 00:34:46,250
尝试接近的误差

957
00:34:46,420 --> 00:34:47,720
另外一条有用的线

958
00:34:47,880 --> 00:34:49,900
是训练误差

959
00:34:50,060 --> 00:34:50,890
事实证明

960
00:34:51,040 --> 00:34:53,380
训练误差会

961
00:34:53,540 --> 00:34:54,970
随着训练集合大小的增加而增加

962
00:34:55,130 --> 00:35:00,900
因为训练集合越大

963
00:35:01,060 --> 00:35:03,880
就越难完美地

964
00:35:04,040 --> 00:35:05,550
进行拟合 对吗?

965
00:35:05,710 --> 00:35:06,860
这只是示意图

966
00:35:07,030 --> 00:35:08,030
不要太认真

967
00:35:08,190 --> 00:35:08,830
通常情况下

968
00:35:08,990 --> 00:35:10,000
训练误差会

969
00:35:10,160 --> 00:35:12,620
随着训练集合的大小单调递增

970
00:35:12,780 --> 00:35:13,910
对于小的训练集合

971
00:35:14,070 --> 00:35:15,110
如果只有一个点

972
00:35:15,260 --> 00:35:16,300
那么很容易精确拟合

973
00:35:16,450 --> 00:35:18,460
但是如果你有10000个点

974
00:35:18,620 --> 00:35:20,030
那么就很难精确拟合

975
00:35:20,200 --> 00:35:21,070
对吗?

976
00:35:21,230 --> 00:35:24,580
这是另外一个

977
00:35:24,730 --> 00:35:25,640
高方差的诊断方法

978
00:35:25,800 --> 00:35:27,240
我经常会用

979
00:35:27,410 --> 00:35:28,420
就是比较训练误差

980
00:35:28,580 --> 00:35:29,980
和测试误差之间的差异

981
00:35:30,140 --> 00:35:31,470
如果它们之间的差异很大

982
00:35:31,620 --> 00:35:34,210
那么这意味着

983
00:35:34,360 --> 00:35:35,180
提供更多的训练数据

984
00:35:35,330 --> 00:35:36,770
可以帮助你缩小它们之间的差距

985
00:35:36,930 --> 00:35:38,380
明白吗?

986
00:35:39,600 --> 00:35:43,010
这是高方差情况下

987
00:35:43,170 --> 00:35:45,950
的示意图

988
00:35:46,110 --> 00:35:50,400
这是高偏差时

989
00:35:50,550 --> 00:35:51,810
的示意图

990
00:35:51,980 --> 00:35:53,730
如果你仔细看这条曲线

991
00:35:53,890 --> 00:35:55,860
你会发现曲线到这里时测试误差

992
00:35:57,500 --> 00:35:58,660
已经开始不变了

993
00:35:58,810 --> 00:36:01,670
这是一个标志

994
00:36:01,830 --> 00:36:02,900
此时即使你增加

995
00:36:03,070 --> 00:36:04,850
再多的训练样本

996
00:36:05,010 --> 00:36:05,830
曲线的右侧

997
00:36:06,000 --> 00:36:07,660
也不太可能

998
00:36:07,810 --> 00:36:08,780
再向下延伸了

999
00:36:08,930 --> 00:36:10,690
这是高偏差的一个性质:

1000
00:36:10,850 --> 00:36:11,970
使用更多的训练数据

1001
00:36:12,140 --> 00:36:13,230
不一定是有效的

1002
00:36:13,390 --> 00:36:16,390
但是这又告诉了我们

1003
00:36:16,550 --> 00:36:17,800
一种有效的诊断方法

1004
00:36:17,960 --> 00:36:21,010
如果你将训练误差的曲线

1005
00:36:21,170 --> 00:36:22,130
也画出来

1006
00:36:22,300 --> 00:36:23,680
如果你对比一下训练误差

1007
00:36:23,840 --> 00:36:24,770
和保留测试误差

1008
00:36:24,930 --> 00:36:27,140
你会发现

1009
00:36:27,300 --> 00:36:29,700
当你的训练误差很高时

1010
00:36:31,220 --> 00:36:33,020
增加再多的训练数据

1011
00:36:33,180 --> 00:36:34,930
也是没有用的

1012
00:36:35,090 --> 00:36:36,430
对吗?

1013
00:36:36,590 --> 00:36:39,950
实际上

1014
00:36:40,130 --> 00:36:44,900
训练误差会

1015
00:36:45,060 --> 00:36:46,190
随着训练集合大小单调递增

1016
00:36:46,350 --> 00:36:50,670
所以如果你发现

1017
00:36:50,830 --> 00:36:51,980
训练误差已经超过了

1018
00:36:52,150 --> 00:36:53,480
预期的误差

1019
00:36:53,640 --> 00:36:58,020
那么及时增加

1020
00:36:58,180 --> 00:36:59,440
再多的训练数据

1021
00:36:59,590 --> 00:37:01,250
都不可能将训练误差

1022
00:37:01,410 --> 00:37:03,010
拉回到预期误差之下

1023
00:37:03,160 --> 00:37:04,410
因为

1024
00:37:04,560 --> 00:37:05,770
随着训练样本数的增加

1025
00:37:05,940 --> 00:37:06,880
训练误差只可能变得越来越差

1026
00:37:07,060 --> 00:37:08,860
所以当曲线向右取值时

1027
00:37:09,020 --> 00:37:10,440
蓝色的线

1028
00:37:10,600 --> 00:37:11,840
不太可能回到

1029
00:37:12,000 --> 00:37:13,260
预期的误差之下

1030
00:37:13,420 --> 00:37:15,390
它会一直待在上面

1031
00:37:15,550 --> 00:37:19,170
对我个人来说

1032
00:37:19,320 --> 00:37:21,040
对于这条绿色的

1033
00:37:21,190 --> 00:37:22,440
表示测试误差的线

1034
00:37:22,610 --> 00:37:23,790
我发现判断曲线

1035
00:37:23,960 --> 00:37:25,730
是否还会继续向下

1036
00:37:25,880 --> 00:37:27,570
还是保持水平非常困难

1037
00:37:27,730 --> 00:37:28,590
有的时候你可以分辨出来

1038
00:37:28,740 --> 00:37:29,340
但是通常情况下

1039
00:37:29,490 --> 00:37:30,240
都比较模棱两可

1040
00:37:30,390 --> 00:37:32,290
所以我个人最常用的

1041
00:37:32,450 --> 00:37:34,380
判断偏差和方差问题的方法

1042
00:37:34,540 --> 00:37:35,930
是检查训练误差

1043
00:37:36,080 --> 00:37:38,440
和测试误差之间的差异

1044
00:37:38,600 --> 00:37:39,460
看它们的差异

1045
00:37:39,610 --> 00:37:40,800
是很小

1046
00:37:40,960 --> 00:37:42,100
还是很大

1047
00:37:42,260 --> 00:37:43,580
明白吗?

1048
00:37:43,730 --> 00:37:47,860
回到修正措施的列表

1049
00:37:52,370 --> 00:37:53,580
对于第一个修正措施

1050
00:37:53,750 --> 00:37:55,290
获得更多的训练样本

1051
00:37:55,450 --> 00:37:58,850
可以修正高方差问题

1052
00:37:59,000 --> 00:38:00,140
如果出现了高方差问题

1053
00:38:00,290 --> 00:38:01,010
使用更多的训练样本

1054
00:38:01,160 --> 00:38:02,030
会有帮助

1055
00:38:02,180 --> 00:38:04,340
使用更少的特征

1056
00:38:04,490 --> 00:38:06,940
也可以修正高方差问题

1057
00:38:07,090 --> 00:38:09,920
对吗?

1058
00:38:11,810 --> 00:38:13,050
使用更多的特征

1059
00:38:13,210 --> 00:38:14,430
或者使用email特征

1060
00:38:15,860 --> 00:38:17,660
这些可以修正高偏差问题

1061
00:38:17,820 --> 00:38:19,010
对吗?

1062
00:38:19,160 --> 00:38:21,740
如果你的假设过于简单

1063
00:38:21,890 --> 00:38:23,020
或者说没有考虑足够多的特征

1064
00:38:23,170 --> 00:38:25,020
就会产生高偏差

1065
00:38:26,500 --> 00:38:29,940
你经常会看到

1066
00:38:30,110 --> 00:38:30,800
很多人面对

1067
00:38:30,970 --> 00:38:32,020
机器学习方面的问题时

1068
00:38:32,180 --> 00:38:35,650
他们会记得通常情况下

1069
00:38:35,810 --> 00:38:37,060
使用更多的训练样本会有帮助

1070
00:38:37,230 --> 00:38:38,480
当他们建立一个学习系统

1071
00:38:38,640 --> 00:38:40,280
例如一个垃圾邮件分类系统时

1072
00:38:40,450 --> 00:38:41,190
如果系统工作效果不好

1073
00:38:41,370 --> 00:38:42,900
那么他们会将

1074
00:38:43,050 --> 00:38:44,580
大量的时间和精力

1075
00:38:44,750 --> 00:38:45,770
花费在收集更多的训练数据上

1076
00:38:45,940 --> 00:38:46,520
因为他们会说

1077
00:38:46,680 --> 00:38:47,650
"哦  使用更多的数据

1078
00:38:47,820 --> 00:38:48,540
一定会有帮助 "

1079
00:38:48,710 --> 00:38:51,550
但是如果他们面对的

1080
00:38:51,720 --> 00:38:52,490
是一个高偏差问题

1081
00:38:52,640 --> 00:38:53,480
而不是一个高方差问题

1082
00:38:53,640 --> 00:38:55,810
那么很有可能

1083
00:38:55,970 --> 00:38:57,500
他们会花三个月到六个月时间

1084
00:38:57,650 --> 00:38:59,080
寻找这些更多的训练数据

1085
00:38:59,230 --> 00:39:00,520
却不知道

1086
00:39:00,680 --> 00:39:02,980
这根本不会有什么帮助

1087
00:39:03,140 --> 00:39:06,880
这种情况经常发生

1088
00:39:07,040 --> 00:39:09,000
例如在硅谷的很多公司里

1089
00:39:09,150 --> 00:39:10,050
这种情况也经常发生

1090
00:39:10,210 --> 00:39:12,830
经常会有人

1091
00:39:12,990 --> 00:39:14,320
在建立机器学习系统的时候

1092
00:39:14,480 --> 00:39:17,000
他们经常会

1093
00:39:17,140 --> 00:39:18,090
花费几个月的时间

1094
00:39:18,260 --> 00:39:20,010
修正学习算法中的问题

1095
00:39:20,160 --> 00:39:21,760
可能你在几个月前

1096
00:39:21,920 --> 00:39:22,880
就已经告诉过他们

1097
00:39:23,060 --> 00:39:25,060
这样没有用

1098
00:39:25,220 --> 00:39:26,670
因为他们不知道

1099
00:39:26,830 --> 00:39:27,560
问题所在

1100
00:39:27,720 --> 00:39:30,090
所以他们很容易就花费好几个月的时间

1101
00:39:30,240 --> 00:39:31,310
用于引入新特征

1102
00:39:31,470 --> 00:39:32,170
或者一些其他事情

1103
00:39:32,330 --> 00:39:35,080
这样的情况会令人惊讶地经常出现

1104
00:39:35,240 --> 00:39:36,810
这有点让人沮丧

1105
00:39:36,980 --> 00:39:38,030
你经常会忍不住想对他们说

1106
00:39:38,190 --> 00:39:39,710
"我在六个月前就告诉过你

1107
00:39:39,870 --> 00:39:41,300
这样做没有用 "

1108
00:39:41,460 --> 00:39:43,250
六个月并不是一个笑话

1109
00:39:43,400 --> 00:39:44,220
这种情形很常见

1110
00:39:46,630 --> 00:39:48,460
相反地

1111
00:39:48,610 --> 00:39:50,510
如果你能够区分出你的问题

1112
00:39:50,670 --> 00:39:51,690
是高偏差还是高方差

1113
00:39:51,850 --> 00:39:53,610
那么你就可以排除掉

1114
00:39:53,770 --> 00:39:55,130
两种可能的解决方案

1115
00:39:55,320 --> 00:39:56,100
从而减少几个月

1116
00:39:56,270 --> 00:39:58,550
没有结果的努力

1117
00:39:58,700 --> 00:40:01,300
我想讲一下

1118
00:40:01,480 --> 00:40:02,480
下面的这四种措施

1119
00:40:02,640 --> 00:40:03,640
但是在我开始之前

1120
00:40:03,800 --> 00:40:04,490
让我看看

1121
00:40:04,660 --> 00:40:05,480
你们对我刚刚说的内容

1122
00:40:05,650 --> 00:40:06,340
是否有问题

1123
00:40:14,770 --> 00:40:17,880
没有?很好

1124
00:40:18,020 --> 00:40:21,210
由偏差和方差产生的问题

1125
00:40:21,370 --> 00:40:23,440
是很常见的问题

1126
00:40:23,600 --> 00:40:24,920
这两种问题的判断

1127
00:40:25,070 --> 00:40:26,190
是有通用的判断方法的

1128
00:40:26,350 --> 00:40:30,360
对于其他的机器学习问题

1129
00:40:30,520 --> 00:40:31,690
则需要发挥

1130
00:40:31,840 --> 00:40:32,820
你们自己的聪明才智

1131
00:40:32,970 --> 00:40:34,200
设计诊断方法

1132
00:40:34,360 --> 00:40:35,640
来判断到底出了什么问题

1133
00:40:35,800 --> 00:40:37,230
如果一个机器学习算法

1134
00:40:37,390 --> 00:40:38,010
不工作

1135
00:40:38,170 --> 00:40:40,440
通常需要你自己来判断

1136
00:40:40,600 --> 00:40:42,220
去自己设计测试方法

1137
00:40:42,380 --> 00:40:43,590
例如像刚才我们说的那样

1138
00:40:43,750 --> 00:40:44,640
检查训练误差和测试误差之间的差异

1139
00:40:44,810 --> 00:40:45,920
你们需要检查一下别的方面

1140
00:40:46,090 --> 00:40:47,610
通常需要你们自己发挥聪明才智

1141
00:40:47,760 --> 00:40:49,240
去设计检验方法

1142
00:40:49,390 --> 00:40:50,530
来找出问题所在

1143
00:40:50,690 --> 00:40:53,800
我们再来看一个例子

1144
00:40:53,960 --> 00:40:55,050
注意了

1145
00:40:55,210 --> 00:40:56,480
虽然这个例子有点不自然

1146
00:40:56,640 --> 00:40:58,860
但是它可以说明

1147
00:40:59,010 --> 00:41:00,460
另外一个常见的问题

1148
00:41:00,630 --> 00:41:03,180
一个应用学习算法时

1149
00:41:03,350 --> 00:41:05,420
最为常见的问题之一

1150
00:41:05,580 --> 00:41:06,600
这个例子是这样的

1151
00:41:06,770 --> 00:41:07,820
稍微有点不自然

1152
00:41:07,970 --> 00:41:09,640
比如说  你实现了贝叶斯

1153
00:41:09,820 --> 00:41:10,710
logistic回归

1154
00:41:10,880 --> 00:41:13,300
在判断垃圾邮件时

1155
00:41:13,460 --> 00:41:14,350
有2%的误差

1156
00:41:14,510 --> 00:41:16,240
在判断非垃圾邮件时有2%的误差

1157
00:41:16,400 --> 00:41:18,720
系统会拒绝

1158
00:41:18,890 --> 00:41:22,700
98%的邮件

1159
00:41:22,850 --> 00:41:23,700
会放过2%的垃圾邮件

1160
00:41:23,860 --> 00:41:25,040
这样没什么问题

1161
00:41:25,200 --> 00:41:27,700
但是同时

1162
00:41:27,880 --> 00:41:29,440
它会拒绝2%的好邮件

1163
00:41:29,620 --> 00:41:31,420
也就是说2%的

1164
00:41:31,580 --> 00:41:32,480
来自朋友的邮件都会被拒绝

1165
00:41:32,630 --> 00:41:34,250
这是不可接受的

1166
00:41:34,410 --> 00:41:39,950
对于另外一个使用SVM

1167
00:41:40,100 --> 00:41:41,220
和线性核的解决方案

1168
00:41:41,380 --> 00:41:43,090
在判断垃圾邮件时会有10%的误差

1169
00:41:43,240 --> 00:41:45,700
而在判断非垃圾邮件时会有0.01%的误差

1170
00:41:45,850 --> 00:41:47,300
这种解决方案的性能

1171
00:41:47,460 --> 00:41:48,500
更可以接受

1172
00:41:48,650 --> 00:41:50,180
这个例子说的是

1173
00:41:50,350 --> 00:41:51,710
要建立一个

1174
00:41:51,860 --> 00:41:52,920
垃圾邮件判定系统  对吗?

1175
00:41:53,080 --> 00:41:55,050
可能你非常想

1176
00:41:55,210 --> 00:41:58,430
使用logistic回归来实现

1177
00:41:58,590 --> 00:41:59,890
可能因为它计算效率很高

1178
00:42:00,040 --> 00:42:01,140
可能因为你需要

1179
00:42:01,300 --> 00:42:02,660
随时进行学习

1180
00:42:02,830 --> 00:42:04,430
或者因为logistic回归

1181
00:42:04,600 --> 00:42:05,980
运行更简单

1182
00:42:06,140 --> 00:42:07,220
更快速

1183
00:42:07,370 --> 00:42:07,940
所以比如说

1184
00:42:08,100 --> 00:42:09,480
你需要部署logistic回归的实现版本

1185
00:42:09,630 --> 00:42:10,550
但是它的工作效果并不好

1186
00:42:10,710 --> 00:42:14,080
所以  你应该怎么办?

1187
00:42:14,240 --> 00:42:18,320
事实证明

1188
00:42:18,480 --> 00:42:20,250
伴随着这个问题的

1189
00:42:20,410 --> 00:42:22,450
是另外一种

1190
00:42:22,600 --> 00:42:25,440
非常常见的问题:

1191
00:42:25,590 --> 00:42:26,980
算法的收敛性

1192
00:42:27,120 --> 00:42:28,690
你可能会怀疑

1193
00:42:28,840 --> 00:42:31,080
logistic回归

1194
00:42:31,250 --> 00:42:32,510
没有收敛

1195
00:42:32,660 --> 00:42:34,630
你需要进行更多的迭代

1196
00:42:34,790 --> 00:42:37,860
实际上  如果你

1197
00:42:38,000 --> 00:42:39,020
考虑logistic回归的优化目标

1198
00:42:39,180 --> 00:42:40,900
比如说  logistic回归是

1199
00:42:41,070 --> 00:42:42,580
J(θ)的话

1200
00:42:42,740 --> 00:42:44,660
那么如果你

1201
00:42:44,810 --> 00:42:45,690
将这个函数

1202
00:42:45,860 --> 00:42:47,280
视为迭代次数的函数

1203
00:42:50,060 --> 00:42:52,610
如果你观察函数曲线的话

1204
00:42:52,760 --> 00:42:53,440
你会发现

1205
00:42:53,600 --> 00:42:54,990
函数上升之后

1206
00:42:55,140 --> 00:42:56,210
趋于平缓

1207
00:42:56,370 --> 00:42:58,380
当你看这些曲线的时候

1208
00:42:58,530 --> 00:42:59,900
会很难区分

1209
00:43:00,050 --> 00:43:02,420
曲线是否已经变为水平

1210
00:43:02,580 --> 00:43:03,220
对吗?

1211
00:43:03,370 --> 00:43:04,940
当你看这条曲线的时候

1212
00:43:05,100 --> 00:43:05,700
你可能会问:

1213
00:43:05,850 --> 00:43:07,050
算法是否收敛?

1214
00:43:07,210 --> 00:43:08,370
如果你仅仅观察J(θ)

1215
00:43:08,520 --> 00:43:09,500
那么会很难辨认

1216
00:43:09,650 --> 00:43:11,350
你可以进行10次迭代之后

1217
00:43:11,500 --> 00:43:12,620
再看曲线是否水平

1218
00:43:12,780 --> 00:43:14,520
可能进行10次迭代之后

1219
00:43:14,670 --> 00:43:16,290
你仍然发现

1220
00:43:16,450 --> 00:43:17,950
函数上升非常慢

1221
00:43:18,110 --> 00:43:22,480
所以相对于查看曲线的方式

1222
00:43:22,630 --> 00:43:24,140
我们需要更好的判断

1223
00:43:24,300 --> 00:43:25,580
logistic回归是否收敛的方法

1224
00:43:25,730 --> 00:43:30,110
你可能感兴趣的


1225
00:43:30,290 --> 00:43:32,060
另一个问题是--

1226
00:43:32,220 --> 00:43:32,720
对于一个问题

1227
00:43:32,900 --> 00:43:34,940
你是否找到了正确的优化函数

1228
00:43:35,100 --> 00:43:39,670
在垃圾邮件识别的例子里

1229
00:43:39,820 --> 00:43:42,850
你可能会关心这样的

1230
00:43:43,010 --> 00:43:45,590
一个加权准确率函数

1231
00:43:45,750 --> 00:43:47,570
a(θ)是对

1232
00:43:47,720 --> 00:43:49,600
所有正确的样本

1233
00:43:49,760 --> 00:43:51,280
进行加权求和

1234
00:43:51,440 --> 00:43:53,840
非垃圾邮件的权值

1235
00:43:54,000 --> 00:43:55,680
可能比垃圾邮件要高一些

1236
00:43:55,830 --> 00:43:57,850
因为相对于垃圾邮件

1237
00:43:58,030 --> 00:43:59,440
你更希望正确地

1238
00:43:59,600 --> 00:44:01,160
预测非垃圾邮件

1239
00:44:01,320 --> 00:44:03,020
比如说a(θ)是

1240
00:44:03,220 --> 00:44:06,780
你关心的

1241
00:44:06,950 --> 00:44:08,060
优化目标函数

1242
00:44:08,230 --> 00:44:12,380
但是贝叶斯logistic回归

1243
00:44:12,530 --> 00:44:14,970
要优化的是这样的一个式子 对吗?

1244
00:44:15,130 --> 00:44:16,530
它基本上就是最大似然性

1245
00:44:16,690 --> 00:44:19,580
只不过这里

1246
00:44:19,740 --> 00:44:21,130
多了个惩罚项

1247
00:44:21,280 --> 00:44:22,120
我们之前讲过

1248
00:44:22,280 --> 00:44:23,380
你可能想知道:

1249
00:44:23,530 --> 00:44:24,600
我们尝试优化的这个函数

1250
00:44:24,760 --> 00:44:26,670
是正确的吗?

1251
00:44:26,830 --> 00:44:28,600
我是否需要

1252
00:44:28,760 --> 00:44:29,810
改变λ的值

1253
00:44:29,970 --> 00:44:32,120
或者改变使用的参数?

1254
00:44:32,280 --> 00:44:34,800
或者说我是否应该

1255
00:44:34,960 --> 00:44:36,460
改用SVM的

1256
00:44:36,640 --> 00:44:37,660
优化目标?

1257
00:44:37,800 --> 00:44:40,140
明白吗?

1258
00:44:40,290 --> 00:44:43,280
我要讲的第二种诊断方法

1259
00:44:43,440 --> 00:44:45,640
可以帮助你判断

1260
00:44:45,790 --> 00:44:48,440
问题到底是

1261
00:44:48,600 --> 00:44:49,780
因为算法的收敛上

1262
00:44:49,940 --> 00:44:51,310
还是出在

1263
00:44:51,460 --> 00:44:52,850
一开始

1264
00:44:53,020 --> 00:44:55,700
对于目标函数的选择上

1265
00:44:55,860 --> 00:44:59,060
你可以用这样的诊断方法

1266
00:44:59,210 --> 00:45:04,940
让我们重新回顾一下这个例子

1267
00:45:05,100 --> 00:45:08,810
对于你的问题

1268
00:45:08,970 --> 00:45:10,460
SVM比贝叶斯

1269
00:45:10,620 --> 00:45:11,430
logistic回归效果更好

1270
00:45:11,590 --> 00:45:13,540
但是你想用

1271
00:45:13,700 --> 00:45:14,550
贝叶斯logistic回归

1272
00:45:14,710 --> 00:45:18,390
让我们用θ_SVM表示

1273
00:45:18,560 --> 00:45:20,430
由SVM生成的参数

1274
00:45:20,600 --> 00:45:23,190
用θ_BLR表示

1275
00:45:23,340 --> 00:45:24,600
由贝叶斯logistic回归

1276
00:45:24,750 --> 00:45:25,710
生成的参数

1277
00:45:25,860 --> 00:45:29,620
你关心的

1278
00:45:29,770 --> 00:45:30,830
优化目标是

1279
00:45:30,990 --> 00:45:32,090
我刚刚讲的

1280
00:45:32,260 --> 00:45:33,530
加权准确率

1281
00:45:33,680 --> 00:45:38,750
SVM比贝叶斯

1282
00:45:38,910 --> 00:45:40,340
logistic回归

1283
00:45:40,500 --> 00:45:41,910
效果更好

1284
00:45:42,070 --> 00:45:43,110
这也就意味着

1285
00:45:43,280 --> 00:45:44,560
SVM的加权准确率

1286
00:45:44,710 --> 00:45:46,790
要好于

1287
00:45:46,980 --> 00:45:48,280
贝叶斯logistic回归

1288
00:45:48,440 --> 00:45:54,850
贝叶斯logistic回归

1289
00:45:55,000 --> 00:45:56,220
尝试最优化

1290
00:45:56,370 --> 00:45:58,460
目标函数

1291
00:45:58,610 --> 00:46:00,420
J(θ)

1292
00:46:00,580 --> 00:46:04,510
所以  我们的诊断方法就是

1293
00:46:04,670 --> 00:46:08,700
要比较J(θSVM)

1294
00:46:08,850 --> 00:46:11,900
和J(θ_BLR)的大小

1295
00:46:12,050 --> 00:46:13,350
我会在下一张讲义解释

1296
00:46:13,510 --> 00:46:16,360
我们知道了两个事实

1297
00:46:16,510 --> 00:46:18,410
我们现在已知一个事实

1298
00:46:18,570 --> 00:46:19,590
我们知道--

1299
00:46:19,740 --> 00:46:22,460
SVM的加权准确率要

1300
00:46:22,630 --> 00:46:25,140
大于贝叶斯logistic回归

1301
00:46:25,300 --> 00:46:26,680
的加权准确率

1302
00:46:26,830 --> 00:46:29,830
我为了找到

1303
00:46:29,990 --> 00:46:31,100
问题到底是

1304
00:46:31,270 --> 00:46:32,140
出在算法没有收敛上

1305
00:46:32,300 --> 00:46:33,180
还是出在

1306
00:46:33,340 --> 00:46:34,130
没有选择合适的目标函数上

1307
00:46:34,280 --> 00:46:36,230
我们需要做的

1308
00:46:36,390 --> 00:46:37,200
就是看

1309
00:46:37,370 --> 00:46:38,860
这个不等式是否成立

1310
00:46:39,030 --> 00:46:40,230
明白吗?

1311
00:46:40,390 --> 00:46:41,500
让我解释一下

1312
00:46:41,660 --> 00:46:44,940
一种情况是

1313
00:46:45,120 --> 00:46:47,530
等式成立

1314
00:46:47,690 --> 00:46:51,090
这种情况下

1315
00:46:51,250 --> 00:46:53,560
J(θ_SVM )>

1316
00:46:54,330 --> 00:46:57,930
J(θ_BLR)

1317
00:46:58,090 --> 00:47:03,170
我们知道

1318
00:47:03,350 --> 00:47:05,340
我们需要使

1319
00:47:05,500 --> 00:47:07,890
J（θ）最大化 这是

1320
00:47:08,050 --> 00:47:09,640
贝叶斯logistic回归的定义

1321
00:47:09,810 --> 00:47:16,610
这意味着

1322
00:47:16,760 --> 00:47:18,840
我们目前由贝叶斯logistic回归

1323
00:47:18,990 --> 00:47:20,060
输出的参数

1324
00:47:20,220 --> 00:47:22,230
无法使得J(θ)最大化

1325
00:47:22,390 --> 00:47:25,160
因为SVM

1326
00:47:25,320 --> 00:47:26,480
生成的参数

1327
00:47:26,660 --> 00:47:28,970
可以使J(θ)

1328
00:47:29,140 --> 00:47:30,120
取一个更大的值

1329
00:47:30,290 --> 00:47:32,220
所以  这告诉我们

1330
00:47:32,390 --> 00:47:33,930
贝叶斯logistic回归

1331
00:47:34,090 --> 00:47:36,360
没有使J(θ)最大化

1332
00:47:36,510 --> 00:47:38,630
因此  我们的问题

1333
00:47:38,790 --> 00:47:40,240
出在算法上

1334
00:47:40,400 --> 00:47:41,600
算法没有收敛

1335
00:47:41,760 --> 00:47:46,590
这是另外一种情况

1336
00:47:46,750 --> 00:47:51,570
J(θ_SVM )《

1337
00:47:51,730 --> 00:47:53,010
J(θ_BLR)

1338
00:47:53,160 --> 00:47:56,880
这种情况意味着什么?

1339
00:47:57,040 --> 00:47:59,590
这意味着

1340
00:47:59,750 --> 00:48:00,680
在目标函数

1341
00:48:00,850 --> 00:48:04,640
的最大化上

1342
00:48:04,800 --> 00:48:07,290
贝叶斯logistic回归

1343
00:48:07,450 --> 00:48:08,850
比SVM做的更好

1344
00:48:09,020 --> 00:48:11,930
但是对于我们的问题

1345
00:48:12,090 --> 00:48:15,640
所关心的加权准确率

1346
00:48:15,800 --> 00:48:18,220
SVM却获得了

1347
00:48:18,380 --> 00:48:20,440
更好的结果

1348
00:48:23,250 --> 00:48:25,180
所以这意味着

1349
00:48:25,340 --> 00:48:26,790
即使某些参数

1350
00:48:26,950 --> 00:48:28,920
不能够使目标函数J最大化

1351
00:48:29,070 --> 00:48:30,490
也能获得更好的

1352
00:48:30,650 --> 00:48:32,770
加权准确率

1353
00:48:32,930 --> 00:48:34,690
这意味着

1354
00:48:34,850 --> 00:48:38,590
最大化J(θ)

1355
00:48:38,750 --> 00:48:39,930
并不一定意味着

1356
00:48:40,140 --> 00:48:41,600
最大化加权准确率

1357
00:48:41,760 --> 00:48:44,260
因此  这告诉你

1358
00:48:44,420 --> 00:48:46,120
也许J(θ)并不是

1359
00:48:46,270 --> 00:48:47,560
一个错误的目标函数

1360
00:48:47,750 --> 00:48:49,640
对吗?

1361
00:48:49,800 --> 00:48:50,950
J(θ)并不是

1362
00:48:51,110 --> 00:48:52,210
一个好的目标函数

1363
00:48:52,380 --> 00:48:54,340
如果你关注的是

1364
00:48:54,510 --> 00:48:55,460
加权准确率的话

1365
00:48:55,640 --> 00:48:58,480
明白吗?

1366
00:49:00,520 --> 00:49:03,290
明白的话

1367
00:49:03,450 --> 00:49:06,330
请举手 很好

1368
00:49:10,520 --> 00:49:12,830
这告诉我们

1369
00:49:12,980 --> 00:49:14,330
问题是出在优化算法上

1370
00:49:14,490 --> 00:49:17,440
还是出在

1371
00:49:17,600 --> 00:49:18,520
目标函数上

1372
00:49:18,680 --> 00:49:20,490
回到这张讲义

1373
00:49:20,650 --> 00:49:23,390
我们有八种解决问题的方法

1374
00:49:23,560 --> 00:49:25,610
你可以在梯度下降算法中

1375
00:49:25,770 --> 00:49:27,930
进行更多次的迭代

1376
00:49:28,090 --> 00:49:30,210
这可以解决优化算法问题

1377
00:49:30,380 --> 00:49:32,020
使用牛顿方法

1378
00:49:32,180 --> 00:49:34,460
也可以解决优化算法问题

1379
00:49:34,610 --> 00:49:36,650
使用不同的λ值

1380
00:49:36,810 --> 00:49:38,170
也就是λ|(|θ| )|^2中的那个参数

1381
00:49:38,330 --> 00:49:39,970
可以解决优化目标函数的问题

1382
00:49:40,130 --> 00:49:42,810
尝试使用SVM

1383
00:49:42,970 --> 00:49:44,560
也可以解决

1384
00:49:44,710 --> 00:49:46,520
优化目标问题 明白吗?

1385
00:49:46,680 --> 00:49:49,900
你会经常

1386
00:49:50,060 --> 00:49:51,180
见到这种情形--

1387
00:49:51,350 --> 00:49:53,490
经常会出现这样的情况

1388
00:49:53,650 --> 00:49:56,240
很多人的

1389
00:49:56,400 --> 00:49:57,350
优化目标有问题

1390
00:49:57,500 --> 00:49:59,480
但是他们却在

1391
00:49:59,640 --> 00:50:02,250
不断地尝试修正优化算法

1392
00:50:02,400 --> 00:50:05,550
很多情况下

1393
00:50:05,710 --> 00:50:06,850
问题出在

1394
00:50:07,010 --> 00:50:08,120
J(θ)的公式中

1395
00:50:08,280 --> 00:50:10,180
但是很多人却在

1396
00:50:10,330 --> 00:50:11,040
不断地尝试

1397
00:50:11,200 --> 00:50:12,360
对梯度下降算法进行更多的迭代

1398
00:50:12,520 --> 00:50:13,700
或者使用牛顿方法

1399
00:50:13,850 --> 00:50:16,060
或者尝试使用更多的

1400
00:50:16,220 --> 00:50:17,200
疯狂的优化算法

1401
00:50:17,350 --> 00:50:20,440
而实际的问题却是:

1402
00:50:20,600 --> 00:50:22,050
J(θ)的选择

1403
00:50:22,220 --> 00:50:23,150
是有问题的

1404
00:50:23,300 --> 00:50:24,180
明白吗?

1405
00:50:24,340 --> 00:50:25,640
这是另外一个

1406
00:50:25,800 --> 00:50:27,980
诊断方法的例子

1407
00:50:28,150 --> 00:50:29,250
可以帮助你们分析出

1408
00:50:29,400 --> 00:50:30,280
到底是

1409
00:50:30,440 --> 00:50:31,990
优化算法的问题

1410
00:50:32,140 --> 00:50:34,030
还是优化目标的问题

1411
00:50:34,180 --> 00:50:36,980
明白吗?

1412
00:50:37,140 --> 00:50:39,710
让我看看还有多少时间

1413
00:50:39,860 --> 00:50:46,090
我们还有时间

1414
00:50:46,250 --> 00:50:49,020
让我给你们讲

1415
00:50:49,170 --> 00:50:51,220
最后一个诊断方法

1416
00:50:51,380 --> 00:50:53,080
这个问题来自于

1417
00:50:53,230 --> 00:50:54,400
我和我学生的工作

1418
00:50:54,560 --> 00:50:55,660
直升机的自动驾驶

1419
00:50:55,830 --> 00:51:00,610
这个问题是

1420
00:51:00,770 --> 00:51:03,250
我今天所举的

1421
00:51:03,430 --> 00:51:04,370
三个例子中最复杂的一个

1422
00:51:04,530 --> 00:51:06,780
我会讲的很快

1423
00:51:06,930 --> 00:51:08,910
因为这部分内容

1424
00:51:09,060 --> 00:51:10,520
涉及到强化学习

1425
00:51:10,690 --> 00:51:11,480
我们可能在

1426
00:51:11,640 --> 00:51:12,680
课程快结束的时候

1427
00:51:12,830 --> 00:51:14,120
才会讲

1428
00:51:14,300 --> 00:51:17,160
它是一个更复杂的

1429
00:51:17,320 --> 00:51:18,470
诊断方法的例子

1430
00:51:18,610 --> 00:51:20,650
我可能会

1431
00:51:20,810 --> 00:51:21,770
讲得非常快

1432
00:51:21,920 --> 00:51:23,080
当我们之后讲到

1433
00:51:23,240 --> 00:51:24,410
强化学习的时候

1434
00:51:24,570 --> 00:51:25,590
我可能还会

1435
00:51:25,750 --> 00:51:26,840
再讲一遍这个例子

1436
00:51:26,990 --> 00:51:28,430
那时你们对它的理解

1437
00:51:28,590 --> 00:51:29,590
会更深刻

1438
00:51:29,740 --> 00:51:33,280
你们中有些人知道

1439
00:51:33,440 --> 00:51:34,150
我和我的学生尝试

1440
00:51:34,310 --> 00:51:35,190
让直升机自动驾驶

1441
00:51:35,350 --> 00:51:37,060
那么你们应该怎样设计

1442
00:51:37,230 --> 00:51:39,320
直升机的控制器呢?

1443
00:51:39,480 --> 00:51:42,670
我们是这样做的

1444
00:51:42,880 --> 00:51:44,830
首先

1445
00:51:45,000 --> 00:51:46,240
实现一个模拟器

1446
00:51:46,410 --> 00:51:47,490
看

1447
00:51:47,650 --> 00:51:48,750
这是模拟器的截图

1448
00:51:48,900 --> 00:51:50,330
它就像是

1449
00:51:50,480 --> 00:51:51,870
一个操纵杆的模拟器

1450
00:51:52,030 --> 00:51:53,320
可以模拟直升机的飞行

1451
00:51:53,490 --> 00:51:56,570
之后需要选择一个函数

1452
00:51:56,730 --> 00:51:57,780
它实际上被称作

1453
00:51:57,940 --> 00:51:58,810
"   "函数

1454
00:51:58,970 --> 00:51:59,590
这里我们将其

1455
00:51:59,750 --> 00:52:00,390
称为成本函数

1456
00:52:00,550 --> 00:52:02,860
例如  用J(θ)表示

1457
00:52:03,010 --> 00:52:04,360
直升机位置的

1458
00:52:04,520 --> 00:52:06,550
期望平方误差

1459
00:52:06,710 --> 00:52:08,150
那么这个J(θ)可能是

1460
00:52:08,310 --> 00:52:09,520
期望平方误差

1461
00:52:09,680 --> 00:52:10,680
或者正好是平方误差

1462
00:52:10,850 --> 00:52:13,020
我们运行一个

1463
00:52:13,180 --> 00:52:14,720
强化学习算法

1464
00:52:14,880 --> 00:52:16,870
你们几周之后会学到

1465
00:52:17,020 --> 00:52:17,840
将其表示为RL算法

1466
00:52:18,000 --> 00:52:19,280
之后在模拟器中

1467
00:52:19,440 --> 00:52:21,550
执行强化学习算法

1468
00:52:21,710 --> 00:52:24,330
使这个成本函数最小化

1469
00:52:24,500 --> 00:52:25,840
尝试使

1470
00:52:26,010 --> 00:52:27,610
直升机位置的

1471
00:52:27,760 --> 00:52:29,550
平方误差最小化

1472
00:52:31,380 --> 00:52:32,620
强化学习算法

1473
00:52:32,780 --> 00:52:33,780
会输出一些参数

1474
00:52:33,930 --> 00:52:36,160
我将其表示为θ_RL

1475
00:52:36,320 --> 00:52:39,030
之后我们会用这些参数

1476
00:52:39,200 --> 00:52:40,110
来进行直升机的控制

1477
00:52:41,590 --> 00:52:42,930
假设你执行这个学习算法

1478
00:52:43,090 --> 00:52:45,300
并且得到了

1479
00:52:45,450 --> 00:52:46,660
一组控制函数

1480
00:52:46,810 --> 00:52:48,000
θ_RL

1481
00:52:48,160 --> 00:52:49,930
但是和人工驾驶相比

1482
00:52:50,090 --> 00:52:50,970
实际效果很差

1483
00:52:51,130 --> 00:52:52,820
你接下来应该怎样做?

1484
00:52:52,980 --> 00:52:54,880
实际上

1485
00:52:55,040 --> 00:52:56,390
根据上面三步

1486
00:52:56,550 --> 00:52:58,140
你可以很自然地

1487
00:52:58,300 --> 00:52:59,150
想到做三件事

1488
00:52:59,310 --> 00:53:00,750
你可以尝试--

1489
00:53:00,910 --> 00:53:02,870
讲义的底部

1490
00:53:03,040 --> 00:53:03,790
可能看不见

1491
00:53:03,950 --> 00:53:05,310
你可以尝试改进模拟器

1492
00:53:05,470 --> 00:53:07,930
也许你觉得

1493
00:53:08,100 --> 00:53:09,170
你的模拟器不够精确

1494
00:53:09,320 --> 00:53:10,570
你需要捕获到

1495
00:53:10,720 --> 00:53:11,830
更精确的空气动力学的因素

1496
00:53:11,990 --> 00:53:13,400
你需要更精确地

1497
00:53:13,560 --> 00:53:14,920
捕获直升机周围的

1498
00:53:15,080 --> 00:53:16,190
气流扰动效应

1499
00:53:16,350 --> 00:53:18,610
或者你需要

1500
00:53:18,770 --> 00:53:19,630
修改成本函数

1501
00:53:19,780 --> 00:53:20,980
也许平方误差并不适合

1502
00:53:21,180 --> 00:53:22,500
也许对于一个人类飞行员来说

1503
00:53:22,660 --> 00:53:23,720
他所考虑的并不是平方误差

1504
00:53:23,880 --> 00:53:24,890
而是一些更复杂的东西

1505
00:53:25,050 --> 00:53:27,380
或者说你可以认为

1506
00:53:27,550 --> 00:53:28,300
强化学习算法效果不好

1507
00:53:28,460 --> 00:53:29,660
它不能很好地收敛

1508
00:53:29,820 --> 00:53:30,930
明白吗?

1509
00:53:31,110 --> 00:53:33,640
这是我

1510
00:53:33,800 --> 00:53:35,100
用到的诊断方法

1511
00:53:35,260 --> 00:53:36,190
我和我的学生

1512
00:53:36,410 --> 00:53:37,240
用它来检查问题所在

1513
00:53:38,660 --> 00:53:41,790
你们可以

1514
00:53:41,950 --> 00:53:42,800
花一点时间思考一下

1515
00:53:42,950 --> 00:53:44,030
应该怎么做  之后

1516
00:53:44,180 --> 00:53:45,300
我会告诉你们我们是怎样做的

1517
00:54:45,230 --> 00:54:48,160
好的  让我告诉你


1518
00:54:48,320 --> 00:54:49,220
我们的做法

1519
00:54:49,400 --> 00:54:51,030
看看和你们想的是不是一样的

1520
00:54:51,180 --> 00:54:52,350
如果你们有比我更好的想法

1521
00:54:52,500 --> 00:54:53,320
请让我知道

1522
00:54:53,480 --> 00:54:54,530
我会在我的直升机上尝试

1523
00:54:54,680 --> 00:54:58,360
我进行试验的推理过程

1524
00:54:58,500 --> 00:55:00,500
是这样的

1525
00:55:00,660 --> 00:55:02,000
可以说

1526
00:55:02,150 --> 00:55:04,350
我们的强化学习算法

1527
00:55:04,510 --> 00:55:05,420
输出的控制器的效果很差

1528
00:55:05,570 --> 00:55:10,560
假设下面的

1529
00:55:10,710 --> 00:55:11,720
三个条件都成立

1530
00:55:11,870 --> 00:55:13,600
我们进行相反的假设

1531
00:55:13,760 --> 00:55:15,960
假设直升机的模拟器很精确

1532
00:55:16,110 --> 00:55:18,190
假设我们

1533
00:55:18,350 --> 00:55:19,980
对直升机的建模是准确的

1534
00:55:21,820 --> 00:55:23,050
让我们假设

1535
00:55:23,220 --> 00:55:24,580
强化学习算法

1536
00:55:24,740 --> 00:55:26,400
能够在模拟条件下

1537
00:55:26,540 --> 00:55:28,080
正确地控制直升机

1538
00:55:28,250 --> 00:55:30,080
我们是在

1539
00:55:30,250 --> 00:55:31,310
模拟情况下运行的

1540
00:55:31,480 --> 00:55:32,730
所以即使直升机坠毁

1541
00:55:32,900 --> 00:55:33,920
也没关系 对吗?

1542
00:55:34,070 --> 00:55:35,500
假设我们的

1543
00:55:35,660 --> 00:55:37,010
强化学习算法能够

1544
00:55:37,180 --> 00:55:38,540
使J(θ)最小化

1545
00:55:38,700 --> 00:55:39,260
从而能够正确地

1546
00:55:39,420 --> 00:55:40,680
控制直升机

1547
00:55:40,840 --> 00:55:42,640
再假设

1548
00:55:42,790 --> 00:55:44,120
最小化J(θ)

1549
00:55:44,270 --> 00:55:46,340
对应着精确飞行

1550
00:55:46,500 --> 00:55:47,640
或正确地自动飞行

1551
00:55:47,800 --> 00:55:50,330
如果所有这些条件都成立

1552
00:55:50,490 --> 00:55:54,260
那么参数θ_RL

1553
00:55:54,460 --> 00:55:55,770
就可以精确地

1554
00:55:55,920 --> 00:55:59,670
控制直升机了 对吗?

1555
00:55:59,830 --> 00:56:02,100
但是事实是

1556
00:56:02,260 --> 00:56:04,180
参数θ_RL并不能

1557
00:56:04,330 --> 00:56:06,680
很好地控制直升机

1558
00:56:06,840 --> 00:56:08,960
这意味着

1559
00:56:09,120 --> 00:56:10,200
至少有一个假设

1560
00:56:10,400 --> 00:56:11,190
不成立

1561
00:56:11,350 --> 00:56:12,340
我们希望

1562
00:56:12,500 --> 00:56:15,640
找出问题的所在

1563
00:56:17,000 --> 00:56:20,680
我们用的诊断方法是这样的

1564
00:56:20,870 --> 00:56:26,280
首先  我们会检查控制器

1565
00:56:26,450 --> 00:56:27,310
是否能够在

1566
00:56:27,470 --> 00:56:28,760
模拟环境下正常工作

1567
00:56:28,910 --> 00:56:30,560
明白吗?

1568
00:56:30,720 --> 00:56:32,360
我们在直升机的

1569
00:56:32,510 --> 00:56:33,560
模拟器上测试

1570
00:56:33,720 --> 00:56:36,060
如果我们的直升机能够

1571
00:56:36,220 --> 00:56:37,530
在模拟环境中正常飞行

1572
00:56:37,690 --> 00:56:39,900
而在真实条件下不能正常飞行

1573
00:56:40,080 --> 00:56:42,580
那么我知道

1574
00:56:42,740 --> 00:56:44,290
问题可能出在模拟器上

1575
00:56:44,450 --> 00:56:45,240
对吗?

1576
00:56:45,400 --> 00:56:47,780
我的模拟器预测出的控制器

1577
00:56:47,950 --> 00:56:49,040
可以让它在模拟器中正常飞行

1578
00:56:49,200 --> 00:56:50,440
但是在真实条件下却不可以

1579
00:56:50,590 --> 00:56:52,120
所以问题

1580
00:56:52,290 --> 00:56:53,300
出在模拟器上

1581
00:56:53,460 --> 00:56:54,110
我们需要花时间

1582
00:56:54,280 --> 00:56:55,800
改进模拟器的精确性

1583
00:56:55,960 --> 00:57:00,020
除此之外

1584
00:57:00,190 --> 00:57:02,460
让我们用θ_human

1585
00:57:02,610 --> 00:57:04,370
表示人类的控制策略

1586
00:57:04,530 --> 00:57:08,100
我们会让人去

1587
00:57:08,270 --> 00:57:10,520
驾驶直升机

1588
00:57:10,700 --> 00:57:11,540
可能是在模拟器中

1589
00:57:11,690 --> 00:57:12,380
也可能是在真实条件中

1590
00:57:12,530 --> 00:57:14,270
之后我们测量

1591
00:57:14,440 --> 00:57:15,570
人类飞行员的

1592
00:57:15,720 --> 00:57:17,960
平方误差

1593
00:57:18,110 --> 00:57:21,450
之后我们会检查

1594
00:57:21,610 --> 00:57:22,880
人类飞行员的控制策略

1595
00:57:23,040 --> 00:57:25,260
相比于模拟出的控制策略

1596
00:57:25,410 --> 00:57:27,270
是否可以

1597
00:57:27,440 --> 00:57:30,780
使J(θ)更小

1598
00:57:30,940 --> 00:57:33,900
如果一个

1599
00:57:34,050 --> 00:57:37,300
很好的人类飞行员得到了

1600
00:57:37,470 --> 00:57:39,450
比我们的学习算法

1601
00:57:39,610 --> 00:57:41,250
更小的

1602
00:57:41,400 --> 00:57:43,150
优化目标的值

1603
00:57:47,690 --> 00:57:49,750
那么问题出在

1604
00:57:49,910 --> 00:57:50,940
强化学习算法上

1605
00:57:51,110 --> 00:57:52,750
因为我们的强化学习算法

1606
00:57:52,910 --> 00:57:54,460
尝试最小化J(θ)

1607
00:57:54,620 --> 00:57:56,880
但是人类飞行员

1608
00:57:57,040 --> 00:57:58,530
得到了一个

1609
00:57:58,690 --> 00:58:00,140
比我的算法更小的J(θ)

1610
00:58:00,300 --> 00:58:02,900
这告诉我们

1611
00:58:03,060 --> 00:58:05,570
我们的算法没有成功地

1612
00:58:05,720 --> 00:58:08,160
使J(θ)取最小值

1613
00:58:08,330 --> 00:58:09,300
所以问题出现在

1614
00:58:09,470 --> 00:58:10,440
强化学习算法上

1615
00:58:12,360 --> 00:58:14,120
最后  


1616
00:58:14,280 --> 00:58:15,960
如果人类飞行员

1617
00:58:16,130 --> 00:58:17,970
得到了一个更大的J(θ)

1618
00:58:18,140 --> 00:58:19,120
对不起

1619
00:58:19,270 --> 00:58:20,740
如果人类得到了

1620
00:58:20,900 --> 00:58:22,180
一个更大的J(θ)

1621
00:58:22,340 --> 00:58:23,900
这意味着

1622
00:58:24,060 --> 00:58:25,680
人类的控制策略的

1623
00:58:25,830 --> 00:58:27,730
直升机位置的

1624
00:58:27,880 --> 00:58:29,110
平方误差更大

1625
00:58:29,280 --> 00:58:31,890
但是实际上

1626
00:58:32,050 --> 00:58:33,070
人类的飞行效果

1627
00:58:33,240 --> 00:58:34,640
要比强化学习算法更好

1628
00:58:34,800 --> 00:58:35,950
如果这种情形出现

1629
00:58:36,110 --> 00:58:37,880
那么很明显问题

1630
00:58:38,030 --> 00:58:39,470
出现在成本函数上

1631
00:58:39,630 --> 00:58:40,870
因为人类飞行员的

1632
00:58:41,050 --> 00:58:41,970
成本函数的值

1633
00:58:42,120 --> 00:58:43,510
比算法的函数值更糟

1634
00:58:43,670 --> 00:58:44,760
但是飞得要更好

1635
00:58:44,920 --> 00:58:46,930
所以这意味着问题

1636
00:58:47,080 --> 00:58:47,910
出现在成本函数上

1637
00:58:48,080 --> 00:58:48,750
这意味着--

1638
00:58:48,910 --> 00:58:50,580
对不起  我这里想说的是最小化

1639
00:58:50,730 --> 00:58:51,870
不是最大化

1640
00:58:52,030 --> 00:58:53,180
讲义上写错了

1641
00:58:53,330 --> 00:58:54,290
这意味着

1642
00:58:54,450 --> 00:58:55,700
最小化成本函数--

1643
00:58:55,840 --> 00:58:58,380
这意味着我的算法相比于人类飞行员

1644
00:58:58,540 --> 00:58:59,580
能够取得更小的成本函数值

1645
00:58:59,740 --> 00:59:01,230
但是飞行效果却不如人类飞行员

1646
00:59:01,380 --> 00:59:02,240
所以这意味着

1647
00:59:02,400 --> 00:59:03,910
使成本函数最小化

1648
00:59:04,070 --> 00:59:06,250
并不意味着好的飞行效果

1649
00:59:06,410 --> 00:59:07,680
所以你们应该

1650
00:59:07,840 --> 00:59:09,300
尝试修改J(θ)

1651
00:59:09,450 --> 00:59:10,680
明白吗?

1652
00:59:10,880 --> 00:59:13,540
对于这些

1653
00:59:13,690 --> 00:59:14,770
强化学习问题

1654
00:59:14,930 --> 00:59:16,290
如果效果不好--

1655
00:59:16,450 --> 00:59:19,020
通常强化学习算法

1656
00:59:19,180 --> 00:59:20,040
是可以的

1657
00:59:20,190 --> 00:59:21,180
但是万一如果它们效果不好

1658
00:59:21,340 --> 00:59:22,400
你可以用这些诊断方法

1659
00:59:22,560 --> 00:59:23,620
来判断

1660
00:59:23,780 --> 00:59:25,010
到底是应该改进模拟器

1661
00:59:25,170 --> 00:59:27,190
还是应该改变成本函数

1662
00:59:27,350 --> 00:59:28,580
或者是应该改变

1663
00:59:28,750 --> 00:59:29,690
强化学习算法

1664
00:59:29,850 --> 00:59:32,400
再说一次

1665
00:59:32,550 --> 00:59:33,540
如果你们不弄清楚

1666
00:59:33,700 --> 00:59:34,960
到底出了什么问题

1667
00:59:35,120 --> 00:59:37,100
你们完全有可能

1668
00:59:37,260 --> 00:59:39,620
花掉两年时间

1669
00:59:39,780 --> 00:59:41,000
去建立一个更好的

1670
00:59:41,180 --> 00:59:41,990
直升机的模拟器

1671
00:59:42,150 --> 00:59:43,040
事实上

1672
00:59:43,200 --> 00:59:44,850
对直升机的空气动力学进行建模

1673
00:59:45,010 --> 00:59:46,610
是一个很活跃的研究领域

1674
00:59:46,770 --> 00:59:47,600
有很多人的PHD论文

1675
00:59:47,750 --> 00:59:48,940
就是研究的这个

1676
00:59:49,100 --> 00:59:51,350
所以完全有可能

1677
00:59:51,510 --> 00:59:52,100
花掉六年的时间

1678
00:59:52,270 --> 00:59:53,100
完成一篇PHD论文

1679
00:59:53,270 --> 00:59:54,180
建立一个更好的直升机模拟器

1680
00:59:54,330 --> 00:59:55,650
但是由于

1681
00:59:55,800 --> 00:59:56,530
你们没有找到对应的问题

1682
00:59:56,700 --> 00:59:57,480
所以这不会有帮助的

1683
01:00:03,410 --> 01:00:05,760
通常情况下

1684
01:00:05,910 --> 01:00:06,710
你们需要建立

1685
01:00:06,860 --> 01:00:07,760
自己的诊断方法

1686
01:00:07,920 --> 01:00:09,020
来找出

1687
01:00:09,180 --> 01:00:09,980
算法出现问题的原因

1688
01:00:10,140 --> 01:00:14,690
不巧的是我不知道很多方法

1689
01:00:14,850 --> 01:00:16,730
我讲的都是

1690
01:00:16,890 --> 01:00:18,460
我用过的

1691
01:00:18,610 --> 01:00:20,750
最常用的诊断方法

1692
01:00:20,910 --> 01:00:21,950
很多问题中都能用到

1693
01:00:22,110 --> 01:00:22,760
但是很多时候

1694
01:00:22,920 --> 01:00:24,200
你们还是要对特定的问题

1695
01:00:24,370 --> 01:00:25,630
建立特殊的诊断方法

1696
01:00:25,800 --> 01:00:28,790
我要强调的是

1697
01:00:28,940 --> 01:00:30,140
即使算法没有问题

1698
01:00:30,310 --> 01:00:32,520
使用一些诊断方法

1699
01:00:32,690 --> 01:00:33,990
也是不错的主意

1700
01:00:34,160 --> 01:00:35,150
比如说我刚刚提到的一些方法

1701
01:00:35,310 --> 01:00:36,420
因为这可以使你

1702
01:00:36,580 --> 01:00:38,000
真正搞清楚问题的实质

1703
01:00:38,160 --> 01:00:39,080
这样做有几个原因

1704
01:00:39,230 --> 01:00:40,060
一个原因是

1705
01:00:40,230 --> 01:00:41,790
这样的诊断方法

1706
01:00:41,950 --> 01:00:43,880
可以帮助你

1707
01:00:44,040 --> 01:00:45,570
更好地理解问题

1708
01:00:45,730 --> 01:00:49,070
你们中的一部分人

1709
01:00:49,240 --> 01:00:49,990
从Stanford毕业之后

1710
01:00:50,130 --> 01:00:50,940
可能会从事一些

1711
01:00:51,120 --> 01:00:52,450
能带来巨大经济利益的

1712
01:00:52,640 --> 01:00:53,900
高薪职业

1713
01:00:54,060 --> 01:00:56,350
并用机器学习算法

1714
01:00:56,500 --> 01:00:58,330
解决一些问题

1715
01:00:58,490 --> 01:01:01,460
你们可能会花几个月

1716
01:01:01,620 --> 01:01:03,390
甚至几年的时间

1717
01:01:03,560 --> 01:01:04,510
来开发一个

1718
01:01:04,660 --> 01:01:05,690
非常重要的机器学习应用

1719
01:01:05,840 --> 01:01:09,160
对你个人来说

1720
01:01:09,330 --> 01:01:11,060
一件非常有价值的事  


1721
01:01:11,220 --> 01:01:13,500
是形成对这些问题的直观理解

1722
01:01:13,690 --> 01:01:15,310
包括什么东西对问题有用

1723
01:01:15,460 --> 01:01:16,580
什么东西对问题没用

1724
01:01:16,750 --> 01:01:18,950
现在在工业界

1725
01:01:19,120 --> 01:01:20,550
包括硅谷和世界上的很多公司

1726
01:01:20,720 --> 01:01:22,550
都在处理

1727
01:01:22,710 --> 01:01:23,940
机器学习问题

1728
01:01:24,110 --> 01:01:24,820
有许多人在处理

1729
01:01:25,010 --> 01:01:26,190
相同的机器学习问题

1730
01:01:26,350 --> 01:01:27,430
这通常会花几个月

1731
01:01:27,580 --> 01:01:28,600
甚至几年的时间

1732
01:01:28,760 --> 01:01:32,310
当你从事这种工作时

1733
01:01:32,470 --> 01:01:33,390
利用学习算法

1734
01:01:33,540 --> 01:01:34,440
来解决重要的问题时

1735
01:01:34,610 --> 01:01:35,860
一件非常有价值的事

1736
01:01:36,020 --> 01:01:37,460
是对问题

1737
01:01:37,610 --> 01:01:39,960
形成你个人的直观理解

1738
01:01:40,120 --> 01:01:43,930
我刚才讲过的这些诊断方法

1739
01:01:44,080 --> 01:01:45,400
就是一种能够

1740
01:01:45,560 --> 01:01:46,580
帮助你们

1741
01:01:46,730 --> 01:01:48,400
深入理解问题的方式

1742
01:01:48,560 --> 01:01:50,640
事实上

1743
01:01:50,800 --> 01:01:51,990
一些硅谷的公司会将

1744
01:01:52,160 --> 01:01:53,270
需要机器学习算法的项目

1745
01:01:53,440 --> 01:01:54,560
部分外包出去

1746
01:01:54,720 --> 01:01:55,590
有的时候

1747
01:01:55,750 --> 01:01:57,620
硅谷的一些公司会

1748
01:01:57,780 --> 01:01:58,930
雇佣纽约的一家公司

1749
01:01:59,110 --> 01:02:00,570
来帮助它们

1750
01:02:00,720 --> 01:02:02,960
发明机器学习算法

1751
01:02:03,110 --> 01:02:04,530
我不是一个商人

1752
01:02:04,690 --> 01:02:06,020
但是我个人认为

1753
01:02:06,190 --> 01:02:07,940
这是一个很糟的主意

1754
01:02:08,100 --> 01:02:11,130
如果你有很多专业知识

1755
01:02:11,290 --> 01:02:13,650
而且能够理解这些数据

1756
01:02:13,800 --> 01:02:15,920
那么在几个月的

1757
01:02:16,080 --> 01:02:17,210
开发过程中

1758
01:02:17,360 --> 01:02:18,410
最重要的东西

1759
01:02:18,570 --> 01:02:21,360
是你对问题

1760
01:02:21,520 --> 01:02:23,290
还有数据的理解

1761
01:02:23,460 --> 01:02:24,680
这些东西很有价值

1762
01:02:24,850 --> 01:02:26,630
但是如果你

1763
01:02:26,790 --> 01:02:27,770
将这些工作外包出去

1764
01:02:27,930 --> 01:02:28,830
这些知识你就不会领悟到

1765
01:02:28,990 --> 01:02:30,070
我个人认为

1766
01:02:30,220 --> 01:02:30,970
这是一个很糟的主意

1767
01:02:31,120 --> 01:02:32,380
但是我不是商人

1768
01:02:32,540 --> 01:02:33,750
我只是经常看到有人这样做

1769
01:02:33,900 --> 01:02:37,800
让我看看

1770
01:02:37,960 --> 01:02:40,490
另一个使用

1771
01:02:40,640 --> 01:02:41,990
诊断方法的原因

1772
01:02:42,160 --> 01:02:43,200
是因为研究性论文的需要

1773
01:02:43,350 --> 01:02:46,980
诊断方法和错误分析

1774
01:02:47,160 --> 01:02:48,310
稍后我会讲到

1775
01:02:48,470 --> 01:02:50,330
能够帮助你更好地

1776
01:02:50,490 --> 01:02:51,530
传达问题的精髓

1777
01:02:51,690 --> 01:02:53,110
从而帮助你更好地阐明你的研究论点

1778
01:02:55,130 --> 01:02:56,990
例如

1779
01:02:57,150 --> 01:02:58,890
我们不写论文

1780
01:02:59,080 --> 01:03:00,110
而是说

1781
01:03:00,280 --> 01:03:01,480
"这个算法很有效

1782
01:03:01,640 --> 01:03:02,790
我用它控制直升机飞起来了"

1783
01:03:02,950 --> 01:03:04,210
或者

1784
01:03:04,370 --> 01:03:05,100
更有趣地说

1785
01:03:05,260 --> 01:03:06,440
"这个算法有效果了

1786
01:03:06,590 --> 01:03:08,830
这是因为x

1787
01:03:08,990 --> 01:03:11,180
这个诊断方法

1788
01:03:11,340 --> 01:03:12,490
可以证明

1789
01:03:12,860 --> 01:03:14,170
是x修正了

1790
01:03:14,320 --> 01:03:15,170
原来的问题

1791
01:03:15,330 --> 01:03:16,460
所以算法有效果了" \

1792
01:03:16,620 --> 01:03:17,380
明白吗?

1793
01:03:17,540 --> 01:03:21,790
所以这帮我们引出了

1794
01:03:21,940 --> 01:03:22,920
对于错误分析的讨论

1795
01:03:23,070 --> 01:03:24,710
这是很好的机器学习的实践

1796
01:03:24,870 --> 01:03:27,000
可以帮助你们很好地理解

1797
01:03:27,170 --> 01:03:28,450
误差产生的原因

1798
01:03:28,610 --> 01:03:33,050
所以我将其称之为误差分析 


1799
01:03:33,210 --> 01:03:34,940
有问题吗?

1800
01:03:39,550 --> 01:03:41,350
什么?

1801
01:03:41,500 --> 01:03:42,240
S:那个直升机

1802
01:03:42,410 --> 01:03:43,340
到底出了什么问题?

1803
01:03:43,500 --> 01:03:46,260
I:哦

1804
01:03:46,410 --> 01:03:47,210
让我看看

1805
01:03:47,380 --> 01:03:48,550
我们飞了很多次

1806
01:03:48,710 --> 01:03:51,360
对于直升机来说

1807
01:03:51,510 --> 01:03:52,920
最难的是--

1808
01:03:53,070 --> 01:03:55,410
我不知道  通常会有变化

1809
01:03:55,590 --> 01:03:56,800
通常是由于模拟器

1810
01:03:56,970 --> 01:03:58,000
创建一个精确的

1811
01:03:58,160 --> 01:03:59,370
直升机模拟器是非常难的

1812
01:03:59,530 --> 01:04:04,280
误差分析是一种方法

1813
01:04:04,440 --> 01:04:07,080
可以帮你找出

1814
01:04:07,250 --> 01:04:08,360
算法的哪些部分

1815
01:04:08,510 --> 01:04:09,320
效果好或者不好

1816
01:04:09,470 --> 01:04:12,450
我们要举两个例子

1817
01:04:12,560 --> 01:04:18,060
有很多学习系统


1818
01:04:18,220 --> 01:04:19,930
有很多AI系统

1819
01:04:20,090 --> 01:04:21,070
和机器学习系统

1820
01:04:21,220 --> 01:04:23,190
是由很多独立的组件

1821
01:04:23,340 --> 01:04:24,490
通过流水线的方式

1822
01:04:24,650 --> 01:04:26,060
组合在一起的

1823
01:04:26,210 --> 01:04:28,540
这里是一个例子

1824
01:04:28,690 --> 01:04:29,970
和其他的系统

1825
01:04:30,130 --> 01:04:30,880
没有什么不同

1826
01:04:31,040 --> 01:04:31,870
不如说你希望

1827
01:04:32,030 --> 01:04:33,960
从图片中识别出人像

1828
01:04:34,120 --> 01:04:35,540
这是我的一个朋友的照片

1829
01:04:35,700 --> 01:04:39,010
你可以用图片作为输入

1830
01:04:39,160 --> 01:04:41,080
并用一个很长的流水线来处理

1831
01:04:41,240 --> 01:04:42,230
例如

1832
01:04:42,400 --> 01:04:44,020
你要做的第一件事

1833
01:04:44,180 --> 01:04:45,600
是进行预处理

1834
01:04:45,770 --> 01:04:46,630
去掉背景

1835
01:04:46,780 --> 01:04:48,200
所以我们去掉背景

1836
01:04:48,360 --> 01:04:51,940
之后你执行一个

1837
01:04:52,100 --> 01:04:52,950
人脸检测算法

1838
01:04:53,110 --> 01:04:53,950
所以算法

1839
01:04:54,100 --> 01:04:55,470
会检测出人脸  对吗?

1840
01:04:55,630 --> 01:04:57,130
比如说你想

1841
01:04:57,290 --> 01:04:58,350
识别出人的特征

1842
01:04:58,560 --> 01:04:59,230
假如说这就是

1843
01:04:59,400 --> 01:05:00,180
你的应用的目的

1844
01:05:00,340 --> 01:05:02,410
你之后

1845
01:05:02,560 --> 01:05:04,760
用不同的学习算法

1846
01:05:04,920 --> 01:05:05,710
识别出眼睛

1847
01:05:05,880 --> 01:05:07,710
鼻子和嘴等等

1848
01:05:07,860 --> 01:05:09,210
我猜她看到这个效果之后

1849
01:05:09,360 --> 01:05:10,700
就不想做我的朋友了

1850
01:05:10,860 --> 01:05:14,430
找到所有这些特征之后

1851
01:05:14,590 --> 01:05:16,040
根据鼻子

1852
01:05:16,200 --> 01:05:17,050
眼睛的样子

1853
01:05:17,210 --> 01:05:18,540
你可以

1854
01:05:18,700 --> 01:05:19,670
将这些特征

1855
01:05:19,830 --> 01:05:20,600
提供给logistic回归算法

1856
01:05:20,750 --> 01:05:22,610
Logistic回归算法

1857
01:05:22,770 --> 01:05:24,040
或者一些其他的算法

1858
01:05:24,200 --> 01:05:26,370
会告诉你这个人的特征

1859
01:05:26,570 --> 01:05:27,780
明白吗?

1860
01:05:27,950 --> 01:05:33,660
误差分析是这样的

1861
01:05:33,820 --> 01:05:37,050
你用一个很长很复杂的流水线

1862
01:05:37,200 --> 01:05:39,850
将许多机器学习组件组合起来

1863
01:05:40,000 --> 01:05:42,050
许多这些都可以

1864
01:05:42,220 --> 01:05:43,180
用机器学习算法

1865
01:05:43,350 --> 01:05:46,890
如果你能分析出

1866
01:05:47,040 --> 01:05:48,890
误差是缘于哪些组件

1867
01:05:49,150 --> 01:05:51,080
这将会非常有帮助

1868
01:05:54,840 --> 01:05:57,950
通常的误差分析过程

1869
01:05:58,110 --> 01:06:00,280
是逐渐地用基准值

1870
01:06:00,430 --> 01:06:02,030
代替每个组件的输出

1871
01:06:02,170 --> 01:06:03,350
并观察准确率的变化

1872
01:06:03,510 --> 01:06:05,680
我要说的过程

1873
01:06:05,850 --> 01:06:08,280
展示在左下角

1874
01:06:08,440 --> 01:06:10,020
不  是右下角

1875
01:06:10,180 --> 01:06:11,830
我们的系统的精度是85%

1876
01:06:11,990 --> 01:06:14,610
我想知道我们的

1877
01:06:14,770 --> 01:06:16,200
15%的误差来自哪里 对吗?

1878
01:06:16,360 --> 01:06:18,070
我要做的是

1879
01:06:18,230 --> 01:06:20,200
利用手动编码的方式

1880
01:06:20,360 --> 01:06:23,410
去除背景

1881
01:06:23,570 --> 01:06:24,890
而不是使用

1882
01:06:25,040 --> 01:06:27,840
去除背景的算法

1883
01:06:27,990 --> 01:06:29,380
所以通过这种方式

1884
01:06:29,540 --> 01:06:30,790
我们提供了正确的

1885
01:06:30,960 --> 01:06:32,410
去除背景的结果

1886
01:06:32,580 --> 01:06:33,880
如果我这样做的话

1887
01:06:34,030 --> 01:06:36,140
让我们用蓝色表示

1888
01:06:36,310 --> 01:06:37,670
我在测试集合中

1889
01:06:37,830 --> 01:06:38,780
用的基准值

1890
01:06:38,940 --> 01:06:40,040
假设准确率

1891
01:06:40,200 --> 01:06:42,840
增加到了85.1% 明白吗?

1892
01:06:42,990 --> 01:06:45,530
现在我要将

1893
01:06:45,690 --> 01:06:47,280
人脸识别的输出

1894
01:06:47,440 --> 01:06:48,780
替换成基准值

1895
01:06:48,950 --> 01:06:51,190
我会在测试集合中

1896
01:06:51,340 --> 01:06:52,330
告诉算法

1897
01:06:52,490 --> 01:06:53,090
人脸的位置

1898
01:06:53,250 --> 01:06:54,090
如果我这样做的话

1899
01:06:54,250 --> 01:06:56,010
准确率会增加到91%

1900
01:06:56,170 --> 01:06:58,340
以此类推

1901
01:06:58,510 --> 01:07:00,030
然后我会

1902
01:07:00,190 --> 01:07:05,920
将每个组件的输出

1903
01:07:06,070 --> 01:07:07,570
替换成基准值

1904
01:07:07,720 --> 01:07:08,960
例如

1905
01:07:09,120 --> 01:07:10,800
对于鼻子信息

1906
01:07:10,970 --> 01:07:12,130
提取算法

1907
01:07:12,280 --> 01:07:13,780
我会告诉它鼻子在哪里

1908
01:07:13,950 --> 01:07:15,650
这样它就不用找了

1909
01:07:15,820 --> 01:07:17,550
我就这样

1910
01:07:17,710 --> 01:07:19,040
一个接一个地替换

1911
01:07:19,210 --> 01:07:20,630
最终得到了

1912
01:07:20,790 --> 01:07:22,210
100%的准确率

1913
01:07:23,400 --> 01:07:24,950
你可以看一下这张表--

1914
01:07:25,100 --> 01:07:26,570
很抱歉底下看不到了

1915
01:07:26,730 --> 01:07:27,940
它说的是利用logistic回归

1916
01:07:28,090 --> 01:07:28,890
达到了100%的精度

1917
01:07:29,050 --> 01:07:31,750
你可以看一下这张表

1918
01:07:31,920 --> 01:07:34,060
可以清楚地看出  将每个组件的输出

1919
01:07:34,210 --> 01:07:35,710
替换成基准值

1920
01:07:35,870 --> 01:07:36,820
可以带来

1921
01:07:36,980 --> 01:07:37,910
多大的准确率提升

1922
01:07:38,060 --> 01:07:39,430
特别的

1923
01:07:39,590 --> 01:07:40,500
从这张表中可以看出

1924
01:07:40,660 --> 01:07:42,510
当我将人脸检测

1925
01:07:42,670 --> 01:07:44,000
替换成基准值时

1926
01:07:44,170 --> 01:07:45,810
我的性能直接

1927
01:07:45,970 --> 01:07:47,250
由85.1%提升到

1928
01:07:47,420 --> 01:07:50,090
91%  对吗?

1929
01:07:50,240 --> 01:07:51,360
这告诉我

1930
01:07:51,520 --> 01:07:53,250
如果我可以使用更好的人脸识别算法

1931
01:07:53,420 --> 01:07:55,350
那么我可以将准确率

1932
01:07:55,510 --> 01:07:59,080
提升6% 相反地

1933
01:07:59,240 --> 01:08:01,520
如果我们使用更好的

1934
01:08:01,680 --> 01:08:04,590
背景去除算法

1935
01:08:04,740 --> 01:08:06,790
准确率只会从85%提升到85.1%

1936
01:08:06,950 --> 01:08:09,290
这个诊断方法

1937
01:08:09,460 --> 01:08:10,440
还告诉你

1938
01:08:10,600 --> 01:08:12,040
如果你的目标是

1939
01:08:12,200 --> 01:08:12,820
提升整个系统的性能的话

1940
01:08:12,980 --> 01:08:14,400
尝试改进背景去除算法

1941
01:08:14,570 --> 01:08:16,190
可能是在浪费时间

1942
01:08:16,340 --> 01:08:18,070
因为即使你

1943
01:08:18,220 --> 01:08:19,510
使用基准值

1944
01:08:19,670 --> 01:08:20,720
也只会带来0.1%的准确率提升

1945
01:08:20,880 --> 01:08:22,760
如果你改进人脸检测算法

1946
01:08:22,930 --> 01:08:24,240
会有更大的

1947
01:08:24,420 --> 01:08:26,350
可能的提升 明白吗?

1948
01:08:26,510 --> 01:08:27,750
这类诊断方法

1949
01:08:27,910 --> 01:08:31,290
是非常有用的

1950
01:08:31,450 --> 01:08:32,320
如果你的目标是

1951
01:08:32,480 --> 01:08:34,040
要改善整个系统的性能的话

1952
01:08:34,210 --> 01:08:35,220
你可以选择

1953
01:08:35,360 --> 01:08:36,360
未来的三个月改进哪个方面

1954
01:08:36,520 --> 01:08:37,650
选择正确的改进方面

1955
01:08:37,810 --> 01:08:39,970
是至关重要的

1956
01:08:40,120 --> 01:08:41,150
这样的诊断方法可以告诉你

1957
01:08:41,320 --> 01:08:42,750
哪方面的努力

1958
01:08:42,910 --> 01:08:43,670
是最值得的

1959
01:08:47,080 --> 01:08:49,760
我要讲另外一种

1960
01:08:49,930 --> 01:08:51,150
和刚才方法

1961
01:08:51,300 --> 01:08:52,050
相反的分析方法

1962
01:08:52,220 --> 01:08:54,650
误差分析尝试

1963
01:08:54,810 --> 01:08:56,520
将现有的系统性能和

1964
01:08:56,690 --> 01:08:57,890
完美的性能进行对比

1965
01:08:58,050 --> 01:09:00,820
而销蚀分析

1966
01:09:00,970 --> 01:09:02,770
尝试解释

1967
01:09:02,920 --> 01:09:04,590
当前系统性能和

1968
01:09:04,740 --> 01:09:05,520
一些效果很差的

1969
01:09:05,680 --> 01:09:06,730
底线性能之间的差异

1970
01:09:06,890 --> 01:09:09,590
举个例子

1971
01:09:09,750 --> 01:09:10,960
假设你建立了一个

1972
01:09:11,120 --> 01:09:12,770
很好的垃圾邮件分类器

1973
01:09:12,920 --> 01:09:14,450
并对logistic回归算法

1974
01:09:14,620 --> 01:09:16,100
加入了许多非常聪明的特征

1975
01:09:16,250 --> 01:09:17,360
例如  你加入了

1976
01:09:17,550 --> 01:09:18,740
拼写检查的特征

1977
01:09:18,900 --> 01:09:19,680
发送者主机的特征

1978
01:09:19,840 --> 01:09:20,530
邮件头的特征

1979
01:09:20,690 --> 01:09:22,380
邮件文本分析的特征

1980
01:09:22,540 --> 01:09:23,770
JavaScript解析的特征

1981
01:09:23,920 --> 01:09:25,460
嵌入图片的特征

1982
01:09:25,620 --> 01:09:26,490
等等

1983
01:09:26,650 --> 01:09:27,430
假如说

1984
01:09:27,580 --> 01:09:28,150
你想总结一下这个系统

1985
01:09:28,320 --> 01:09:29,670
并且指出每种特性

1986
01:09:29,840 --> 01:09:31,310
对于系统性能的改进

1987
01:09:31,500 --> 01:09:32,330
有多少贡献

1988
01:09:32,480 --> 01:09:34,280
可能你想

1989
01:09:34,440 --> 01:09:35,670
写一篇论文来说明这一点

1990
01:09:35,840 --> 01:09:36,790
以指出你的系统

1991
01:09:36,940 --> 01:09:37,590
和之前的系统的不同

1992
01:09:37,750 --> 01:09:39,180
你该怎样说明

1993
01:09:39,340 --> 01:09:39,870
并证明呢?

1994
01:09:40,030 --> 01:09:41,850
利用销蚀分析

1995
01:09:42,010 --> 01:09:43,800
我们可以这样做

1996
01:09:43,950 --> 01:09:46,700
在这个例子中  假设

1997
01:09:46,860 --> 01:09:47,790
你用了简单的logistic回归

1998
01:09:47,950 --> 01:09:49,610
没有做任何的改进

1999
01:09:49,780 --> 01:09:50,890
得到了94%的性能提升

2000
01:09:51,050 --> 01:09:53,070
你希望指出每一种改进

2001
01:09:53,230 --> 01:09:54,310
对性能提升

2002
01:09:54,480 --> 01:09:56,350
有多少贡献

2003
01:09:58,230 --> 01:10:00,900
利用销蚀分析

2004
01:10:01,060 --> 01:10:02,420
我们不会每次添加一个组件

2005
01:10:02,570 --> 01:10:03,850
相反地  我们每次会

2006
01:10:04,010 --> 01:10:05,390
去除一个组件  来观察准确率的变化

2007
01:10:05,550 --> 01:10:08,880
从系统的现有准确率

2008
01:10:09,040 --> 01:10:10,320
99%开始

2009
01:10:10,470 --> 01:10:12,740
我们去除拼写校正的特性

2010
01:10:12,900 --> 01:10:14,200
性能下降了

2011
01:10:14,370 --> 01:10:15,800
之后我们去除

2012
01:10:15,970 --> 01:10:17,180
发送者主机的特性

2013
01:10:17,340 --> 01:10:18,500
性能又下降了

2014
01:10:18,660 --> 01:10:23,910
所以

2015
01:10:24,070 --> 01:10:25,150
在这个例子中

2016
01:10:25,300 --> 01:10:29,540
你看到

2017
01:10:29,700 --> 01:10:31,510
当你去除文本分析的特性时

2018
01:10:31,670 --> 01:10:33,640
性能下降的最厉害

2019
01:10:33,800 --> 01:10:36,200
所以你可以确信

2020
01:10:36,370 --> 01:10:37,830
文本分析

2021
01:10:38,000 --> 01:10:39,430
带来的改进

2022
01:10:39,580 --> 01:10:41,340
是最大的

2023
01:10:41,510 --> 01:10:42,230
你还可以看出

2024
01:10:42,390 --> 01:10:45,080
例如说

2025
01:10:45,240 --> 01:10:46,700
当我们去除

2026
01:10:46,860 --> 01:10:47,730
发送者主机这一特征时

2027
01:10:47,890 --> 01:10:51,310
准确率只会从99.9%降低到98.9%

2028
01:10:51,460 --> 01:10:52,470
这意味着

2029
01:10:52,630 --> 01:10:53,750
如果你希望

2030
01:10:53,920 --> 01:10:55,100
去除某个特征

2031
01:10:55,280 --> 01:10:56,580
来提升性能时

2032
01:10:56,740 --> 01:10:57,900
发送者主机这一特征

2033
01:10:58,060 --> 01:10:59,310
是一个很好的选择 明白吗?

2034
01:10:59,480 --> 01:11:01,080
S:如果改变

2035
01:11:01,250 --> 01:11:02,340
特征去除

2036
01:11:02,510 --> 01:11:03,810
的顺序的话

2037
01:11:03,960 --> 01:11:04,960
结果会不会一样?

2038
01:11:05,110 --> 01:11:06,400
I:我再

2039
01:11:06,570 --> 01:11:07,170
陈述一下这个问题:

2040
01:11:07,350 --> 01:11:08,110
当你改变删除顺序的时候

2041
01:11:08,270 --> 01:11:08,780
结果会不会一样?

2042
01:11:08,880 --> 01:11:09,480
答案是否定的

2043
01:11:09,560 --> 01:11:10,240
不能保证

2044
01:11:10,320 --> 01:11:11,160
得到相似的结果

2045
01:11:11,240 --> 01:11:12,750
实际上

2046
01:11:12,840 --> 01:11:15,040
有时会有非常自然的分析顺序

2047
01:11:15,220 --> 01:11:16,970
对于误差分析

2048
01:11:17,140 --> 01:11:18,260
和销蚀分析都是如此

2049
01:11:18,410 --> 01:11:19,680
有时会有非常自然的

2050
01:11:19,850 --> 01:11:21,070
添加特性

2051
01:11:21,220 --> 01:11:22,130
或删除特性的顺序

2052
01:11:22,310 --> 01:11:23,330
但是有时没有

2053
01:11:23,500 --> 01:11:25,380
有的时候  你需要

2054
01:11:25,540 --> 01:11:27,450
自己决定顺序

2055
01:11:27,620 --> 01:11:29,430
不要将这些分析方法

2056
01:11:29,620 --> 01:11:31,160
看成是一成不变的

2057
01:11:31,320 --> 01:11:32,070
我的意思是

2058
01:11:32,230 --> 01:11:33,270
你可以自己发明自己的实验方法

2059
01:11:33,430 --> 01:11:35,910
一件经常做的事情是:

2060
01:11:36,070 --> 01:11:38,910
对于整个系统

2061
01:11:39,070 --> 01:11:40,170
去除一个特性

2062
01:11:40,320 --> 01:11:41,030
之后将该特性放回来

2063
01:11:41,180 --> 01:11:41,990
之后去掉另外一个特性

2064
01:11:42,150 --> 01:11:42,860
之后再放回来

2065
01:11:43,020 --> 01:11:44,300
直到所有的特性都尝试过一遍

2066
01:11:44,460 --> 01:11:46,420
就是这样

2067
01:11:46,580 --> 01:11:49,490
我最后要讲的是

2068
01:11:49,650 --> 01:11:51,200
一些关于开始

2069
01:11:51,360 --> 01:11:52,260
解决机器学习问题的

2070
01:11:52,430 --> 01:11:53,180
一些通用建议

2071
01:11:53,350 --> 01:11:59,490
这张图展示了

2072
01:11:59,640 --> 01:12:01,570
两种开始解决问题的方式

2073
01:12:01,730 --> 01:12:04,490
第一种是

2074
01:12:04,670 --> 01:12:07,020
仔细设计系统

2075
01:12:07,170 --> 01:12:08,220
你需要很长的时间

2076
01:12:08,400 --> 01:12:09,980
提取正确的特征

2077
01:12:10,140 --> 01:12:11,300
收集正确的数据集合

2078
01:12:11,460 --> 01:12:12,540
设计正确的算法结构

2079
01:12:12,700 --> 01:12:14,230
之后实现它

2080
01:12:14,390 --> 01:12:15,650
并希望它有效 明白吗?

2081
01:12:15,800 --> 01:12:18,760
这样做的好处是

2082
01:12:18,930 --> 01:12:19,860
你可能会得到更漂亮

2083
01:12:20,010 --> 01:12:21,200
伸缩性更强的算法

2084
01:12:21,360 --> 01:12:23,190
也许你会得到

2085
01:12:23,350 --> 01:12:24,720
更优雅的算法

2086
01:12:24,890 --> 01:12:26,260
如果你的目标是

2087
01:12:26,420 --> 01:12:28,080
对机器学习算法的基础研究

2088
01:12:28,240 --> 01:12:28,880
做出贡献的话

2089
01:12:29,080 --> 01:12:29,760
如果你的目标是要发明

2090
01:12:29,930 --> 01:12:30,660
新的机器学习算法的话

2091
01:12:30,850 --> 01:12:32,360
将速度慢下来

2092
01:12:32,530 --> 01:12:34,570
仔细深入地思考问题

2093
01:12:34,730 --> 01:12:35,940
并发明新的解决方法

2094
01:12:36,090 --> 01:12:37,210
这才是解决问题的

2095
01:12:37,370 --> 01:12:38,780
正确方式

2096
01:12:38,940 --> 01:12:44,030
第二种方法

2097
01:12:44,180 --> 01:12:45,780
可以称之为"创建--修改"法

2098
01:12:45,940 --> 01:12:47,140
这意味着我们需要先实现一些

2099
01:12:47,290 --> 01:12:47,980
快速简陋的解决方案

2100
01:12:48,140 --> 01:12:49,660
之后利用误差分析

2101
01:12:49,820 --> 01:12:50,580
这样的诊断方法

2102
01:12:50,730 --> 01:12:51,660
来看看哪里有问题

2103
01:12:51,820 --> 01:12:52,970
之后再修改

2104
01:12:53,150 --> 01:12:55,550
第二种方法的好处是

2105
01:12:55,720 --> 01:12:57,260
你可以

2106
01:12:57,420 --> 01:12:59,060
非常快的建立系统

2107
01:12:59,220 --> 01:13:01,730
尤其是你们中的很多人

2108
01:13:01,880 --> 01:13:02,880
你们可能会去公司工作

2109
01:13:03,040 --> 01:13:04,330
如果你在公司工作的话

2110
01:13:04,500 --> 01:13:06,060
很多时候

2111
01:13:06,220 --> 01:13:08,460
胜出的产品并不是最好的产品

2112
01:13:08,610 --> 01:13:10,710
而是第一个占有市场的产品

2113
01:13:10,870 --> 01:13:11,800
所以

2114
01:13:11,960 --> 01:13:12,950
对于工业界

2115
01:13:13,110 --> 01:13:14,080
需要做的一些事情是

2116
01:13:14,230 --> 01:13:15,680
经常需要快速地建立

2117
01:13:15,850 --> 01:13:16,910
并部署系统

2118
01:13:17,070 --> 01:13:19,750
第二种方法需要

2119
01:13:19,910 --> 01:13:21,300
快速地建立一个简陋的系统

2120
01:13:21,470 --> 01:13:23,380
先实现再修改

2121
01:13:23,540 --> 01:13:24,780
可以让你的系统

2122
01:13:24,930 --> 01:13:26,920
更快地

2123
01:13:27,080 --> 01:13:28,300
工作起来

2124
01:13:28,450 --> 01:13:33,240
原因是  通常情况下

2125
01:13:33,420 --> 01:13:34,670
系统的哪个部分

2126
01:13:34,840 --> 01:13:35,980
更容易实现

2127
01:13:36,130 --> 01:13:37,730
哪些部分需要你花更多时间

2128
01:13:37,890 --> 01:13:38,890
这些都不是那么明确的

2129
01:13:39,050 --> 01:13:41,190
这是我刚才

2130
01:13:41,360 --> 01:13:43,180
讲过的例子  对吗?

2131
01:13:43,340 --> 01:13:47,670
对于人的特征的识别

2132
01:13:47,830 --> 01:13:49,640
像这样大的一个

2133
01:13:49,810 --> 01:13:50,740
复杂的学习系统

2134
01:13:50,910 --> 01:13:52,130
像这样的一个复杂的流水线

2135
01:13:52,290 --> 01:13:54,650
哪个部分

2136
01:13:54,810 --> 01:13:56,730
需要仔细设计

2137
01:13:56,910 --> 01:13:58,300
开始的时候并不明显  对吗?

2138
01:13:58,590 --> 01:13:59,670
如果你不知道

2139
01:13:59,840 --> 01:14:00,910
预处理部分不是

2140
01:14:01,070 --> 01:14:01,970
最重要的组件的话

2141
01:14:02,120 --> 01:14:03,660
你很有可能花

2142
01:14:03,820 --> 01:14:04,580
三个月的时间

2143
01:14:04,740 --> 01:14:05,810
尝试改进背景去除算法

2144
01:14:05,970 --> 01:14:07,250
而不知道它的改进效果

2145
01:14:07,400 --> 01:14:08,290
对整个系统是无关紧要的

2146
01:14:08,440 --> 01:14:11,220
所以找到这些组件的

2147
01:14:11,390 --> 01:14:12,420
唯一方法是

2148
01:14:12,580 --> 01:14:13,420
快速地实现一个系统

2149
01:14:13,590 --> 01:14:14,350
之后找出哪些组件

2150
01:14:14,510 --> 01:14:18,100
是最应该好好实现的

2151
01:14:18,250 --> 01:14:19,260
关键部分

2152
01:14:19,420 --> 01:14:20,620
这些关键部分

2153
01:14:20,790 --> 01:14:22,100
对于性能的影响最大

2154
01:14:22,260 --> 01:14:24,040
实际上

2155
01:14:24,190 --> 01:14:25,310
如果你的目标是

2156
01:14:25,470 --> 01:14:27,160
建立一个人像识别系统

2157
01:14:27,320 --> 01:14:28,250
一个这样的系统

2158
01:14:28,410 --> 01:14:29,450
作为你们的第一个系统而言

2159
01:14:29,610 --> 01:14:30,780
有些太复杂了

2160
01:14:30,950 --> 01:14:32,450
可能你得先设计一些

2161
01:14:32,620 --> 01:14:33,780
系统的原型

2162
01:14:33,940 --> 01:14:34,700
之后才能得到一个这样的系统

2163
01:14:34,870 --> 01:14:35,900
作为你们设计的

2164
01:14:36,060 --> 01:14:37,070
第一个系统

2165
01:14:37,230 --> 01:14:37,950
这有些过于复杂了

2166
01:14:41,040 --> 01:14:43,600
这是一个

2167
01:14:43,760 --> 01:14:44,680
非常实用的建议

2168
01:14:44,830 --> 01:14:46,720
也适用于你们项目

2169
01:14:46,880 --> 01:14:48,530
如果你的目标是

2170
01:14:48,700 --> 01:14:49,970
建立一个工作的应用

2171
01:14:50,130 --> 01:14:52,260
那么你们要做的第一步可能不是

2172
01:14:52,420 --> 01:14:53,650
建立一个这样的系统

2173
01:14:53,810 --> 01:14:54,830
第一步应该是

2174
01:14:54,990 --> 01:14:55,680
将你的数据画出来

2175
01:14:55,840 --> 01:14:57,800
通常情况下

2176
01:14:57,960 --> 01:14:58,990
如果你将那些

2177
01:14:59,160 --> 01:15:00,360
你尝试预测的数据画出来

2178
01:15:00,540 --> 01:15:02,430
画出x和y

2179
01:15:02,600 --> 01:15:03,590
将这些数据

2180
01:15:03,750 --> 01:15:05,910
画出来

2181
01:15:06,080 --> 01:15:07,950
有一半的时间你会看着它们说

2182
01:15:08,110 --> 01:15:09,300
"天哪  为什么这些数

2183
01:15:09,450 --> 01:15:10,150
都是负的?

2184
01:15:10,320 --> 01:15:11,090
我认为它们应该是正的

2185
01:15:11,260 --> 01:15:12,280
数据集合有问题 "

2186
01:15:12,440 --> 01:15:14,250
大概有一半的时间

2187
01:15:14,410 --> 01:15:15,010
你会找到数据中

2188
01:15:15,170 --> 01:15:16,360
一些明显错误的地方

2189
01:15:16,520 --> 01:15:17,380
或者一些非常令人惊讶的地方

2190
01:15:17,530 --> 01:15:19,270
这些你仅仅通过

2191
01:15:19,490 --> 01:15:20,540
将数据画出来就可以发现

2192
01:15:20,690 --> 01:15:22,630
根本不需要先是

2193
01:15:22,790 --> 01:15:23,530
提出一个

2194
01:15:23,700 --> 01:15:24,800
复杂的学习算法

2195
01:15:26,540 --> 01:15:29,190
将点画出来看起来如此简单

2196
01:15:29,330 --> 01:15:30,520
这个建议

2197
01:15:30,680 --> 01:15:31,380
我们很多人都会给出来

2198
01:15:31,550 --> 01:15:32,630
但是很少有人会照着做

2199
01:15:32,800 --> 01:15:34,760
你可以试着这样做  这是值得的

2200
01:15:38,270 --> 01:15:39,360
让我再说一遍

2201
01:15:39,530 --> 01:15:41,050
如果你想实现

2202
01:15:41,210 --> 01:15:43,230
新的学习算法的话

2203
01:15:43,380 --> 01:15:45,270
这些建议并不是好建议

2204
01:15:45,430 --> 01:15:47,270
对我而言

2205
01:15:47,430 --> 01:15:49,230
我最常用的学习算法

2206
01:15:49,410 --> 01:15:50,870
可能就是logistic回归了

2207
01:15:51,040 --> 01:15:52,260
因为我周围到处都是代码

2208
01:15:52,420 --> 01:15:54,110
给我一个学习问题

2209
01:15:54,260 --> 01:15:55,150
我可能不会先考虑

2210
01:15:55,300 --> 01:15:55,920
任何复杂的学习算法

2211
01:15:56,070 --> 01:15:57,470
而是会优先尝试logistic回归

2212
01:15:57,620 --> 01:15:58,750
只有进行一些

2213
01:15:58,900 --> 01:15:59,860
简单的尝试之后

2214
01:16:00,020 --> 01:16:01,110
你才能找到哪些部分是容易的

2215
01:16:01,270 --> 01:16:02,110
哪些部分是困难的

2216
01:16:02,270 --> 01:16:03,130
这样你才能知道努力的方向

2217
01:16:03,280 --> 01:16:04,770
但是  如果你的目标是发明

2218
01:16:04,920 --> 01:16:06,330
新的机器学习算法时

2219
01:16:06,490 --> 01:16:07,500
那么你要做的

2220
01:16:07,660 --> 01:16:08,420
不是快速实现一个东西

2221
01:16:08,580 --> 01:16:09,790
再修正问题

2222
01:16:09,970 --> 01:16:11,700
之后再出现问题  再修正

2223
01:16:11,860 --> 01:16:12,790
如果你的目标是

2224
01:16:12,930 --> 01:16:14,020
进行一些新奇的机器学习研究

2225
01:16:14,180 --> 01:16:15,220
那么你们应该

2226
01:16:15,380 --> 01:16:16,520
更深入地思考问题

2227
01:16:16,680 --> 01:16:17,790
而不要遵从这些建议

2228
01:16:20,180 --> 01:16:22,850
噢  好吧

2229
01:16:23,000 --> 01:16:23,750
我已经来不及了

2230
01:16:23,910 --> 01:16:25,070
但是还有两张讲义

2231
01:16:25,230 --> 01:16:26,430
所以我会讲的很快

2232
01:16:26,590 --> 01:16:30,780
我接下来要讲的问题

2233
01:16:30,940 --> 01:16:32,510
成为过早统计优化

2234
01:16:32,670 --> 01:16:35,080
正如代码编写中的

2235
01:16:35,250 --> 01:16:36,690
过早优化现象

2236
01:16:36,860 --> 01:16:39,310
很多人在实现一个

2237
01:16:39,470 --> 01:16:41,510
大的复杂的机器学习系统时

2238
01:16:41,660 --> 01:16:43,000
会过早地优化其中的一个组件

2239
01:16:43,170 --> 01:16:45,830
还有两张讲义

2240
01:16:45,980 --> 01:16:49,960
这张图


2241
01:16:50,120 --> 01:16:51,520
极大地影响了我的思维方式

2242
01:16:51,690 --> 01:16:52,520
它来自于

2243
01:16:52,680 --> 01:16:53,900
Christos Papadimitriou的一篇论文

2244
01:16:54,050 --> 01:16:58,350
这个过程在开发

2245
01:16:58,510 --> 01:17:00,320
和研究过程中

2246
01:17:00,470 --> 01:17:01,360
经常会出现

2247
01:17:01,530 --> 01:17:02,870
比如说你希望

2248
01:17:03,040 --> 01:17:03,710
发明一个邮件投递机器人

2249
01:17:03,870 --> 01:17:05,090
所以我这里用一个圆圈

2250
01:17:05,250 --> 01:17:06,280
表示邮件投递机器人

2251
01:17:06,440 --> 01:17:07,470
这个东西看起来很有用

2252
01:17:07,620 --> 01:17:08,360
对吗?

2253
01:17:08,520 --> 01:17:09,440
它可以将人们解放出来

2254
01:17:09,620 --> 01:17:10,400
不用再亲自进行邮件投递

2255
01:17:10,550 --> 01:17:13,270
为了投递邮件

2256
01:17:13,420 --> 01:17:14,960
很显然你需要一个机器人

2257
01:17:15,130 --> 01:17:16,790
能够在室内环境中行走

2258
01:17:16,940 --> 01:17:19,090
并且能够操作物体

2259
01:17:19,250 --> 01:17:20,430
并捡起信封

2260
01:17:20,590 --> 01:17:21,760
所以为了得到一个邮件机器人

2261
01:17:21,920 --> 01:17:22,830
你需要这两个组件

2262
01:17:22,990 --> 01:17:24,230
所以我画出这两个组件

2263
01:17:24,390 --> 01:17:26,350
并用两个箭头

2264
01:17:26,510 --> 01:17:28,020
表示这种依赖关系

2265
01:17:28,180 --> 01:17:30,430
实现一个机器人的话

2266
01:17:30,590 --> 01:17:32,880
躲避障碍也是需要的功能

2267
01:17:33,040 --> 01:17:36,360
为了能够躲避障碍

2268
01:17:36,520 --> 01:17:38,010
你需要机器人

2269
01:17:38,170 --> 01:17:39,070
能够浏览

2270
01:17:39,240 --> 01:17:40,180
并检测物体

2271
01:17:40,350 --> 01:17:41,220
这样才可以躲避障碍物

2272
01:17:41,400 --> 01:17:43,560
我们要用计算机视觉

2273
01:17:43,710 --> 01:17:45,230
检测这些物体

2274
01:17:45,400 --> 01:17:48,110
我们还知道

2275
01:17:48,270 --> 01:17:49,550
外界的光线会改变

2276
01:17:49,710 --> 01:17:51,130
例如  早晨  中午  和晚上的光线

2277
01:17:51,290 --> 01:17:51,920
是不一样的

2278
01:17:52,110 --> 01:17:53,690
这些光线的变化

2279
01:17:53,850 --> 01:17:55,080
会导致物体的颜色的改变

2280
01:17:55,240 --> 01:17:57,240
所以你需要

2281
01:17:57,390 --> 01:17:58,800
独立于物体颜色的

2282
01:17:58,970 --> 01:18:01,070
物体检测系统 对吗?

2283
01:18:01,230 --> 01:18:02,340
因为光线会变化

2284
01:18:02,500 --> 01:18:06,120
颜色  或者RGB的值

2285
01:18:06,300 --> 01:18:08,970
是由三维向量表示的

2286
01:18:09,130 --> 01:18:10,200
你希望学到

2287
01:18:10,360 --> 01:18:12,470
两种颜色可能表示同样的物体

2288
01:18:12,640 --> 01:18:15,400
当光线改变的时候

2289
01:18:15,560 --> 01:18:16,740
同样的物体

2290
01:18:16,890 --> 01:18:18,110
会显示出不同的颜色

2291
01:18:18,270 --> 01:18:21,490
为了理解这些特性

2292
01:18:21,640 --> 01:18:22,530
我们需要学习

2293
01:18:22,690 --> 01:18:24,120
三维表面的微分几何

2294
01:18:24,280 --> 01:18:25,320
因为这样可以帮助我们

2295
01:18:25,470 --> 01:18:26,460
建立一个合理的理论

2296
01:18:26,620 --> 01:18:27,960
来设计我们的

2297
01:18:28,140 --> 01:18:29,340
三维相似性学习算法

2298
01:18:31,640 --> 01:18:33,120
为了理解

2299
01:18:33,270 --> 01:18:34,600
这些问题的基本方面

2300
01:18:34,750 --> 01:18:36,870
我们需要学习

2301
01:18:37,040 --> 01:18:37,990
非黎曼几何的复杂性

2302
01:18:38,150 --> 01:18:41,040
渐渐地

2303
01:18:41,210 --> 01:18:42,250
你还需要证明

2304
01:18:42,400 --> 01:18:43,830
"非单调逻辑取样的

2305
01:18:44,000 --> 01:18:44,870
收敛界"

2306
01:18:45,020 --> 01:18:46,240
我也不知道这是什么

2307
01:18:46,410 --> 01:18:46,920
因为这是我编的

2308
01:18:47,090 --> 01:18:50,830
而实际上

2309
01:18:50,990 --> 01:18:53,250
这条链接有可能是不存在的

2310
01:18:53,420 --> 01:18:56,090
颜色的变化可能

2311
01:18:56,240 --> 01:18:57,160
不会影响物体的识别

2312
01:18:57,310 --> 01:18:58,200
这也是我编的

2313
01:18:58,370 --> 01:19:00,910
也许微分几何也不会

2314
01:19:01,070 --> 01:19:02,320
帮助三维相似性的学习

2315
01:19:02,480 --> 01:19:03,850
所以这条链接也可能不存在

2316
01:19:04,000 --> 01:19:04,610
明白吗?

2317
01:19:04,760 --> 01:19:05,970
这些圆还可以

2318
01:19:06,120 --> 01:19:06,900
表示一个人

2319
01:19:07,060 --> 01:19:08,010
或者一个研究团体

2320
01:19:08,190 --> 01:19:09,270
或者你的一个念头

2321
01:19:09,440 --> 01:19:10,990
非常有可能

2322
01:19:11,150 --> 01:19:12,590
之所以这里写着

2323
01:19:12,750 --> 01:19:14,210
需要三维表面的微分几何

2324
01:19:14,370 --> 01:19:15,460
是因为

2325
01:19:15,620 --> 01:19:17,300
有人告诉作者

2326
01:19:17,470 --> 01:19:18,730
它会有助于三维相似性学习

2327
01:19:18,890 --> 01:19:20,830
这有点类似于

2328
01:19:20,990 --> 01:19:21,990
"我的一个朋友告诉我

2329
01:19:22,140 --> 01:19:23,120
颜色无关会

2330
01:19:23,270 --> 01:19:24,010
有助于进行物体识别

2331
01:19:24,160 --> 01:19:25,110
所以我需要研究颜色无关性

2332
01:19:25,270 --> 01:19:26,250
现在我告诉我的一个朋友

2333
01:19:26,420 --> 01:19:28,070
这个知识会帮助解决我的问题

2334
01:19:28,240 --> 01:19:29,490
他会告诉他的朋友

2335
01:19:29,650 --> 01:19:30,810
这个知识会解决他的问题 "

2336
01:19:30,970 --> 01:19:32,510
不久之后  你就开始研究

2337
01:19:32,670 --> 01:19:34,130
"非单调逻辑取样的

2338
01:19:34,290 --> 01:19:35,900
收敛界"了 而实际上

2339
01:19:36,080 --> 01:19:37,880
这些知识都不会

2340
01:19:38,040 --> 01:19:39,790
对实现邮件投递机器人有任何帮助

2341
01:19:39,950 --> 01:19:43,960
我并不是在批判这些理论的作用

2342
01:19:44,120 --> 01:19:45,840
这些都是非常伟大的理论

2343
01:19:46,000 --> 01:19:48,070
例如VC维理论

2344
01:19:48,240 --> 01:19:49,950
它是非常非常伟大的理论

2345
01:19:50,110 --> 01:19:51,430
VC维是一个

2346
01:19:51,610 --> 01:19:52,630
非常抽象的理论

2347
01:19:52,790 --> 01:19:54,860
它对于很多应用

2348
01:19:55,010 --> 01:19:56,090
都有巨大的影响

2349
01:19:56,250 --> 01:19:57,570
极大地促进了

2350
01:19:57,730 --> 01:19:58,960
机器学习的发展

2351
01:19:59,110 --> 01:20:00,530
另外一个例子是

2352
01:20:00,690 --> 01:20:01,770
NP难理论

2353
01:20:01,930 --> 01:20:03,480
它也是个非常抽象的理论

2354
01:20:03,660 --> 01:20:04,820
NP难理论在

2355
01:20:04,970 --> 01:20:07,220
计算机科学的所有领域

2356
01:20:07,380 --> 01:20:08,330
都有重要的应用

2357
01:20:08,490 --> 01:20:11,520
但是先不要研究

2358
01:20:11,680 --> 01:20:13,240
这些高深的理论

2359
01:20:13,390 --> 01:20:13,990
对我而言  我认为

2360
01:20:14,140 --> 01:20:15,320
需要时刻保持这样的想法:

2361
01:20:15,480 --> 01:20:17,690
我需要研究

2362
01:20:17,850 --> 01:20:19,340
VC维吗?

2363
01:20:19,510 --> 01:20:20,420
我需要研究

2364
01:20:20,570 --> 01:20:21,700
"非单调逻辑取样的

2365
01:20:21,850 --> 01:20:22,550
收敛界"吗?

2366
01:20:22,710 --> 01:20:23,800
它们对于

2367
01:20:23,950 --> 01:20:25,650
你应用的效果

2368
01:20:25,800 --> 01:20:27,830
是非常次要的

2369
01:20:28,250 --> 01:20:31,170
对我个人而言

2370
01:20:31,330 --> 01:20:33,210
我倾向于认为--

2371
01:20:33,360 --> 01:20:35,400
对不起  对我个人而言

2372
01:20:35,550 --> 01:20:36,530
这是个人选择

2373
01:20:36,680 --> 01:20:38,900
当我在开发某个应用时

2374
01:20:39,050 --> 01:20:40,420
我倾向于相信那些

2375
01:20:40,570 --> 01:20:41,830
我可以确信会和应用之间

2376
01:20:41,990 --> 01:20:43,630
存在关联的理论

2377
01:20:43,790 --> 01:20:47,100
如果我不能亲自确定

2378
01:20:47,260 --> 01:20:48,020
某个理论

2379
01:20:48,170 --> 01:20:49,170
和我的应用之间

2380
01:20:49,330 --> 01:20:51,230
存在关联

2381
01:20:51,400 --> 01:20:52,820
我可能回去钻研理论

2382
01:20:52,980 --> 01:20:54,810
但是我不会轻易确信

2383
01:20:54,960 --> 01:20:56,500
我所钻研的理论

2384
01:20:56,650 --> 01:20:57,860
和我的应用之间是相关的

2385
01:20:58,010 --> 01:20:58,940
除非我亲自确定

2386
01:20:59,100 --> 01:21:00,080
它们之间确实存在关联

2387
01:21:01,460 --> 01:21:02,800
总结一下

2388
01:21:04,630 --> 01:21:07,510
我们应该学到的一点是

2389
01:21:07,680 --> 01:21:09,260
花在设计

2390
01:21:09,440 --> 01:21:10,640
诊断方法上的时间

2391
01:21:10,800 --> 01:21:11,690
通常都是有价值的

2392
01:21:11,850 --> 01:21:13,820
通常情况下你需要发挥自己的聪明才智

2393
01:21:13,970 --> 01:21:15,340
去发明伟大的诊断方法

2394
01:21:15,490 --> 01:21:17,090
对我个人而言

2395
01:21:17,250 --> 01:21:18,060
当我设计机器学习算法时

2396
01:21:18,230 --> 01:21:20,350
花费三分之一

2397
01:21:20,510 --> 01:21:22,140
甚至是一半的时间

2398
01:21:22,290 --> 01:21:24,300
去设计诊断方法并不少见

2399
01:21:24,450 --> 01:21:25,400
通过它们我可以找出

2400
01:21:25,550 --> 01:21:26,590
哪里工作正常  哪里出了问题

2401
01:21:26,750 --> 01:21:29,510
有些时候也许不需要这样做

2402
01:21:29,680 --> 01:21:30,620
因为你希望实现

2403
01:21:30,780 --> 01:21:31,450
机器学习算法

2404
01:21:31,610 --> 01:21:32,730
你不希望

2405
01:21:32,890 --> 01:21:33,860
将所有的时间

2406
01:21:34,020 --> 01:21:35,040
花在对算法进行测试上

2407
01:21:35,200 --> 01:21:36,340
这感觉不像在做事情

2408
01:21:36,510 --> 01:21:38,400
但是当我实现

2409
01:21:38,550 --> 01:21:39,300
学习算法时

2410
01:21:39,470 --> 01:21:40,860
我至少会花三分之一  通常是

2411
01:21:41,010 --> 01:21:42,100
一半的时间

2412
01:21:42,260 --> 01:21:43,880
来设计诊断方法

2413
01:21:44,050 --> 01:21:44,920
并将其实现出来找到问题

2414
01:21:45,110 --> 01:21:46,130
我觉得花费

2415
01:21:46,280 --> 01:21:47,260
这些时间是值得的

2416
01:21:48,950 --> 01:21:50,760
之后我们讲了误差分析

2417
01:21:50,920 --> 01:21:51,620
与销蚀分析

2418
01:21:51,780 --> 01:21:54,910
之后讲了

2419
01:21:55,060 --> 01:21:56,120
不同的诊断方法和

2420
01:21:56,280 --> 01:21:58,620
过早统计优化的风险

2421
01:21:58,780 --> 01:22:00,230
好的

2422
01:22:00,400 --> 01:22:01,400
很抱歉拖堂了

2423
01:22:01,570 --> 01:22:03,150
我会在这里待几分钟

2424
01:22:03,340 --> 01:22:04,100
来回答你们的问题

2425
01:22:04,250 --> 01:22:06,490
今天就到这里

