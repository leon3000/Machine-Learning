1
00:00:21,250 --> 00:00:24,320
好的  早上好

2
00:00:24,420 --> 00:00:28,180
欢迎来到第三讲

3
00:00:28,260 --> 00:00:31,720
这是我今天要讲的

4
00:00:31,800 --> 00:00:35,190
其中的一些主题

5
00:00:35,280 --> 00:00:36,930
可能有点儿跳跃

6
00:00:37,040 --> 00:00:38,710
从一个主题跳跃到另一个主题

7
00:00:38,790 --> 00:00:40,860
这里是我今天

8
00:00:40,940 --> 00:00:42,580
要讲的东西的一个大纲

9
00:00:42,660 --> 00:00:46,000
上一讲我们讲到了线性回归

10
00:00:46,080 --> 00:00:48,450
今天我要讲它的一种变化版本

11
00:00:48,540 --> 00:00:51,420
称之为局部加权回归

12
00:00:51,500 --> 00:00:52,820
它是一个非常流行的算法

13
00:00:52,880 --> 00:00:55,530
实际上它可能是我之前的导师

14
00:00:55,550 --> 00:00:57,940
最喜欢用的算法

15
00:00:58,030 --> 00:01:01,190
我们之后会讨论另外一种

16
00:01:01,270 --> 00:01:03,270
可能的对于线性回归的解释

17
00:01:03,350 --> 00:01:07,390
并且基于它来开始讲我们的第一个分类算法

18
00:01:07,480 --> 00:01:08,660
被称之为:logistic回归

19
00:01:08,740 --> 00:01:10,850
之后再简要地讲一些额外的

20
00:01:10,940 --> 00:01:12,610
关于感知器

21
00:01:12,690 --> 00:01:13,850
算法的知识

22
00:01:13,930 --> 00:01:15,180
我们之后还会再来讲它

23
00:01:15,260 --> 00:01:16,060
如果时间允许

24
00:01:16,060 --> 00:01:19,280
我希望能够讲完牛顿方法

25
00:01:19,360 --> 00:01:20,590
它是一个用来对logistic

26
00:01:20,670 --> 00:01:22,450
回归模型进行拟合的算法

27
00:01:22,530 --> 00:01:25,730
这是

28
00:01:25,750 --> 00:01:27,990
我上一讲讲的要点

29
00:01:28,070 --> 00:01:32,470
记得我定义了符号

30
00:01:32,540 --> 00:01:36,330
用来表示

31
00:01:36,440 --> 00:01:40,420
第i个训练样本

32
00:01:40,530 --> 00:01:51,290
当我们讲到线性回归

33
00:01:51,390 --> 00:01:54,670
或线性最小二乘时

34
00:01:54,760 --> 00:01:57,660
我们用这个符号

35
00:01:57,740 --> 00:02:02,250
表示我的预测H对于输入的预测值

36
00:02:02,340 --> 00:02:04,930
我的假设基于

37
00:02:05,010 --> 00:02:08,740
参数向量θ

38
00:02:08,810 --> 00:02:15,960
它等于

39
00:02:16,060 --> 00:02:21,550
可以写成我们约定

40
00:02:21,640 --> 00:02:24,720
所以它可以作为

41
00:02:24,810 --> 00:02:28,110
我们的线性

42
00:02:28,210 --> 00:02:29,960
回归模型中的一项

43
00:02:30,050 --> 00:02:34,330
这里的n是我用来表示

44
00:02:34,390 --> 00:02:38,620
训练集合的特征数量的符号  明白吗

45
00:02:38,720 --> 00:02:40,160
所以在

46
00:02:40,260 --> 00:02:41,960
房价预测的那个例子中

47
00:02:42,070 --> 00:02:43,370
我们有两个特征

48
00:02:43,450 --> 00:02:45,060
房子的大小和卧室的数量

49
00:02:45,160 --> 00:02:47,160
我们有两个特征

50
00:02:47,250 --> 00:02:49,170
因此n=2

51
00:02:49,300 --> 00:02:56,190
为了完成我们的要点回顾

52
00:02:56,300 --> 00:02:58,900
还要定义

53
00:02:59,010 --> 00:03:00,570
二次成本函数

54
00:03:00,660 --> 00:03:02,540
这是对于

55
00:03:02,630 --> 00:03:12,490
训练集合中的

56
00:03:12,580 --> 00:03:15,400
m个训练样本的

57
00:03:15,480 --> 00:03:16,380
求和

58
00:03:16,480 --> 00:03:19,060
m是我用来表示

59
00:03:19,150 --> 00:03:20,840
训练样本数目的符号

60
00:03:20,930 --> 00:03:22,160
也就是训练集合的大小

61
00:03:22,270 --> 00:03:24,770
在上一讲的最后

62
00:03:24,870 --> 00:03:27,960
我们导出了

63
00:03:28,050 --> 00:03:29,360
使这个式子

64
00:03:29,430 --> 00:03:36,640
最小化的θ的值等于

65
00:03:36,730 --> 00:03:43,830
所以在我们今天讲课的过程中

66
00:03:43,920 --> 00:03:46,420
我还会继续使用这些符号

67
00:03:46,490 --> 00:03:48,430
我知道需要记住

68
00:03:48,430 --> 00:03:49,540
很多符号

69
00:03:49,640 --> 00:03:52,810
所以如果讲课过程中你忘记了

70
00:03:52,890 --> 00:03:55,010
比如你想不起m或者n

71
00:03:55,110 --> 00:03:57,630
或者其它符号的含义

72
00:03:57,720 --> 00:03:59,220
请举手提问

73
00:03:59,300 --> 00:04:06,030
当我们上次讲线性回归时

74
00:04:06,120 --> 00:04:08,700
我们使用了两个特征

75
00:04:08,760 --> 00:04:10,750
一个特征是

76
00:04:10,850 --> 00:04:12,020
房子的大小

77
00:04:12,100 --> 00:04:13,570
也就是房子的生活面积

78
00:04:13,650 --> 00:04:15,180
另一个特征是

79
00:04:15,290 --> 00:04:16,690
房子的卧室数量

80
00:04:16,780 --> 00:04:20,760
通常情况下  我们会应用机器学习算法

81
00:04:20,840 --> 00:04:22,130
来解决你们关心的问题

82
00:04:22,240 --> 00:04:24,820
对于特征的选择也是

83
00:04:24,910 --> 00:04:26,070
由你们进行的  对吧

84
00:04:26,160 --> 00:04:29,440
你选择交给学习算法处理的特征

85
00:04:29,530 --> 00:04:31,270
的方式对于算法的工作过程

86
00:04:31,420 --> 00:04:33,530
有很大的影响

87
00:04:33,650 --> 00:04:40,820
举个例子

88
00:04:40,870 --> 00:04:42,720
我们上次用表示房间大小

89
00:04:42,820 --> 00:04:45,270
现在先不管卧室数量

90
00:04:45,360 --> 00:04:47,120
这个特征  比如说我们现在

91
00:04:47,200 --> 00:04:49,300
根本没有关于卧室数量的数据

92
00:04:49,390 --> 00:04:52,970
你可以做的一件事情是定义

93
00:04:53,070 --> 00:05:03,090
哦  让我们把它画出来

94
00:05:03,160 --> 00:05:05,930
比如说这里是房子的大小

95
00:05:06,030 --> 00:05:07,240
这里是房子的价格

96
00:05:07,300 --> 00:05:11,780
这条线可能是你得到的

97
00:05:11,880 --> 00:05:16,310
一个线性模型

98
00:05:16,390 --> 00:05:20,520
如果你选择

99
00:05:20,600 --> 00:05:26,080
让我把同样的数据复制到这里

100
00:05:26,170 --> 00:05:28,350
你可以定义这样的特征集合

101
00:05:28,440 --> 00:05:30,090
其中表示房子大小

102
00:05:30,190 --> 00:05:38,500
表示房子大小的平方 明白吗

103
00:05:38,600 --> 00:05:41,210
所以表示房子的大小

104
00:05:41,320 --> 00:05:44,210
比如说平方英尺

105
00:05:44,290 --> 00:05:46,460
表示房子的平方英尺数的平方

106
00:05:46,540 --> 00:05:47,830
这是另外一种

107
00:05:47,910 --> 00:05:49,070
生成特征的方式

108
00:05:49,150 --> 00:05:52,370
所以如果你使用相同的算法

109
00:05:52,440 --> 00:05:57,350
你将会拟合得到一个二次函数

110
00:05:57,430 --> 00:06:01,640
明白吗

111
00:06:01,720 --> 00:06:07,100
因为这一项就是

112
00:06:07,200 --> 00:06:11,330
基于数据的特点

113
00:06:11,400 --> 00:06:13,190
可能这样的拟合会更好

114
00:06:13,280 --> 00:06:24,570
你们还可以更进一步地进行推导  对吗

115
00:06:24,630 --> 00:06:26,710
我们看一下

116
00:06:26,810 --> 00:06:28,780
我有七个训练样本

117
00:06:28,890 --> 00:06:30,900
所以你们最多可以得到一个

118
00:06:30,980 --> 00:06:32,270
6次的多项式

119
00:06:32,360 --> 00:06:34,440
你们可以拟合

120
00:06:34,500 --> 00:06:42,640
得到对于这7个数据

121
00:06:42,740 --> 00:06:50,360
我们拟合出了一个

122
00:06:50,430 --> 00:06:54,640
关于的6次多项式

123
00:06:54,740 --> 00:06:57,610
如果你这样做了  你会发现

124
00:06:57,690 --> 00:06:58,780
你找到了一个能够

125
00:06:58,780 --> 00:07:00,280
精确拟合所有数据的模型

126
00:07:00,380 --> 00:07:03,090
我猜想在我举的这个例子中

127
00:07:03,180 --> 00:07:04,790
我们有7个点数据

128
00:07:04,880 --> 00:07:08,840
所以如果

129
00:07:08,930 --> 00:07:09,880
你拟合出了一个

130
00:07:09,980 --> 00:07:11,290
6次多项式模型

131
00:07:11,380 --> 00:07:13,370
你可以得到一条完美地通过所有点的曲线

132
00:07:13,450 --> 00:07:18,020
你可能发现你得到的曲线是这样的

133
00:07:18,100 --> 00:07:19,080
一方面

134
00:07:19,080 --> 00:07:22,430
从完美拟合所有训练数据的意义上来说

135
00:07:22,510 --> 00:07:24,610
这是一个伟大的模型

136
00:07:24,700 --> 00:07:27,160
但是从另一方面来看  这个模型也许不是

137
00:07:27,260 --> 00:07:28,500
一个非常好的模型  因为没有人

138
00:07:28,600 --> 00:07:30,440
会认真地认为这个房子大小的函数

139
00:07:30,550 --> 00:07:32,140
是一个对于房价的

140
00:07:32,220 --> 00:07:34,480
非常好的预测  不是吗

141
00:07:34,560 --> 00:07:37,660
所以我们之后还会回到这一部分

142
00:07:37,770 --> 00:07:40,960
事实上  对于我们这里的模型

143
00:07:41,040 --> 00:07:44,680
我感觉也许用二次模型来拟合效果最好

144
00:07:44,770 --> 00:07:47,800
用线性模型的话  看起来

145
00:07:47,890 --> 00:07:51,360
数据中的某些二次成分在线性函数中

146
00:07:51,440 --> 00:07:54,900
没有被捕获到

147
00:07:54,990 --> 00:07:58,440
所以我们之后会回来

148
00:07:58,520 --> 00:08:00,840
讨论这些和拟合模型有关的问题

149
00:08:00,910 --> 00:08:02,480
这些问题或者由于使用了

150
00:08:02,570 --> 00:08:04,040
过小的特征集合使得模型过于简单

151
00:08:04,120 --> 00:08:06,210
或者由于使用了过大的特征集合

152
00:08:06,280 --> 00:08:09,920
使得模型过于复杂

153
00:08:10,020 --> 00:08:12,490
为了给这边的问题一个名字

154
00:08:12,560 --> 00:08:19,890
我们将其称之为:欠拟合(underfitting)

155
00:08:20,000 --> 00:08:22,760
可以将它非正式地

156
00:08:22,840 --> 00:08:24,720
理解为一种情形

157
00:08:24,800 --> 00:08:25,480
在这种情形下

158
00:08:25,480 --> 00:08:26,600
数据中的某些非常明显的模式

159
00:08:26,700 --> 00:08:28,240
没有被成功地拟合出来

160
00:08:28,320 --> 00:08:33,240
这边的这个问题我们可以称之为过拟合

161
00:08:33,330 --> 00:08:35,780
可以再一次非正式地

162
00:08:35,860 --> 00:08:38,550
将其理解为一种情形  这种情形下算法拟合

163
00:08:38,630 --> 00:08:39,940
出的结果仅仅反映了

164
00:08:39,940 --> 00:08:41,400
所给的特定数据的特质

165
00:08:41,490 --> 00:08:44,390
例如我们在Portland地区

166
00:08:44,470 --> 00:08:46,480
或者其它的什么地方

167
00:08:46,530 --> 00:08:47,880
采集的7个样本数据

168
00:08:47,940 --> 00:08:50,120
它们碰巧比较贵

169
00:08:50,240 --> 00:08:51,610
或者碰巧比较便宜

170
00:08:51,690 --> 00:08:56,010
所以我们拟合出的6次多项式函数

171
00:08:56,080 --> 00:08:58,400
仅仅反映了该数据集合的特质

172
00:08:58,490 --> 00:09:00,640
而不是隐藏在其下的

173
00:09:00,730 --> 00:09:03,120
房屋价格随房屋大小

174
00:09:03,220 --> 00:09:04,720
变化的一般化规律明白吗

175
00:09:04,830 --> 00:09:06,320
所以这是两类差异很大的问题

176
00:09:06,410 --> 00:09:07,990
我们之后会对它们进行更加正式的定义

177
00:09:08,080 --> 00:09:10,800
并且讨论这些问题的解决方法

178
00:09:10,880 --> 00:09:14,970
但是现在我仅仅希望你们能够领会到

179
00:09:15,050 --> 00:09:16,270
在特征选择中存在这样的问题

180
00:09:16,360 --> 00:09:24,650
有几个方法可以用来

181
00:09:24,750 --> 00:09:26,120
解决这类学习问题

182
00:09:26,200 --> 00:09:30,430
我们之后讲到

183
00:09:30,500 --> 00:09:31,470
特征选择算法

184
00:09:31,550 --> 00:09:33,180
这是一类自动化的算法

185
00:09:33,290 --> 00:09:36,990
可以在这类回归问题中选择要用到的特征

186
00:09:37,080 --> 00:09:40,880
今天我还会讲到一类算法

187
00:09:40,960 --> 00:09:43,930
称之为

188
00:09:43,980 --> 00:09:46,480
非参数学习算法

189
00:09:46,560 --> 00:09:49,580
可以缓解你们对于选取特征的需求

190
00:09:49,820 --> 00:09:51,380
这会帮我们引出对于

191
00:09:51,450 --> 00:09:53,310
局部加权回归的讨论

192
00:09:53,400 --> 00:10:13,130
首先定义这个概念

193
00:10:13,200 --> 00:10:14,180
我们已经定义过的线性回归

194
00:10:14,280 --> 00:10:16,770
是参数学习算法(parametric learning algorithm)的一个例子

195
00:10:16,840 --> 00:10:20,270
参数学习算法是一类

196
00:10:20,350 --> 00:10:22,490
有固定数目的参数

197
00:10:22,570 --> 00:10:24,180
以用来进行

198
00:10:24,270 --> 00:10:26,620
数据拟合的算法

199
00:10:26,710 --> 00:10:28,800
所以在线性回归中

200
00:10:28,880 --> 00:10:33,130
我们有一个固定的参数集合θ 对吗

201
00:10:33,190 --> 00:10:34,700
它一定可以拟合数据

202
00:10:34,790 --> 00:10:41,160
相反的  我接下来要讨论的

203
00:10:41,240 --> 00:10:43,550
是我们的第一个非参数学习算法

204
00:10:43,630 --> 00:11:02,010
它的正式定义不是非常直观

205
00:11:02,090 --> 00:11:04,390
所以我将它替换成了一个更直观的定义

206
00:11:04,490 --> 00:11:08,260
非参数学习算法的正式定义说的是

207
00:11:08,350 --> 00:11:09,950
它是一个参数数量

208
00:11:10,030 --> 00:11:11,080
会随着m增长的

209
00:11:11,120 --> 00:11:22,660
算法

210
00:11:22,730 --> 00:11:24,190
m表示训练集合大小

211
00:11:24,290 --> 00:11:26,620
通常它被定义为参数的数目

212
00:11:26,700 --> 00:11:28,650
会随着训练集合的大小线性增长

213
00:11:28,740 --> 00:11:31,720
这是正式定义

214
00:11:31,820 --> 00:11:35,900
一个稍微不那么正式的定义是说

215
00:11:36,020 --> 00:11:39,660
你的算法所需要的东西

216
00:11:39,740 --> 00:11:41,800
会随着训练集合

217
00:11:41,890 --> 00:11:43,130
线性增长

218
00:11:43,200 --> 00:11:44,330
或者换句话说

219
00:11:44,410 --> 00:11:46,420
算法的维持是

220
00:11:46,520 --> 00:11:48,340
基于整个训练集合的

221
00:11:48,440 --> 00:11:49,930
即使在学习以后 明白吗

222
00:11:50,010 --> 00:11:52,970
不用过于担心这个定义

223
00:11:53,070 --> 00:11:56,650
我现在要描述一个

224
00:11:56,750 --> 00:11:58,450
特定的非参数学习算法

225
00:11:58,540 --> 00:12:00,450
称之为局部加权回归

226
00:12:00,550 --> 00:12:11,850
它还有一些其他的名字

227
00:12:11,970 --> 00:12:18,640
由于一些历史原因

228
00:12:18,740 --> 00:12:20,370
也可被称为Loess

229
00:12:20,460 --> 00:12:22,590
Loess通常被拼写为L O E S S

230
00:12:22,700 --> 00:12:24,720
有时也可以这样拼写

231
00:12:24,800 --> 00:12:26,340
我只把它称为局部加权回归

232
00:12:26,410 --> 00:12:28,100
算法主要思想是这样的

233
00:12:28,180 --> 00:12:36,450
这个算法可以

234
00:12:36,520 --> 00:12:39,310
让我们不必太担心

235
00:12:39,410 --> 00:12:41,430
对于特征的选择

236
00:12:41,520 --> 00:12:49,850
作为一个例子

237
00:12:49,940 --> 00:12:59,650
比如说我们有一个看起来像这样的训练集合

238
00:12:59,740 --> 00:13:02,390
这是x  这是y

239
00:13:02,490 --> 00:13:08,250
如果你对这组数据进行线性回归

240
00:13:08,350 --> 00:13:10,930
你可能会拟合出一个线性函数

241
00:13:11,020 --> 00:13:13,650
你可能会得到一条有些单调平坦的直线

242
00:13:13,740 --> 00:13:15,650
不能够很好地拟合这组数据

243
00:13:15,750 --> 00:13:18,410
你可以坐在那儿盯着这里来

244
00:13:18,500 --> 00:13:20,170
决定是否使用了正确的特征

245
00:13:20,280 --> 00:13:21,970
也许你会决定使用二次函数

246
00:13:22,060 --> 00:13:24,170
但是它同样不是二次的

247
00:13:24,280 --> 00:13:28,810
所以你也许认为这个模型是x加上

248
00:13:28,900 --> 00:13:31,210
或者再加上sin(x)或者一些其它的东西

249
00:13:31,310 --> 00:13:32,740
你实际上就坐在那儿不断地修改着特征

250
00:13:32,850 --> 00:13:36,080
之后你可能会得到一组特征

251
00:13:36,170 --> 00:13:37,430
并得到适当的模型

252
00:13:37,540 --> 00:13:39,230
但是让我们来讨论一个

253
00:13:39,310 --> 00:13:41,410
使你们不需要做这些工作的算法

254
00:13:41,500 --> 00:13:55,220
所以如果 假设你希望

255
00:13:55,330 --> 00:14:00,160
对于一个确定的查询点x

256
00:14:00,270 --> 00:14:06,560
在x处对你的假设h进行求值

257
00:14:06,670 --> 00:14:09,050
例如  你想知道

258
00:14:09,150 --> 00:14:13,650
x在这个位置时y的预测值  对吗

259
00:14:13,770 --> 00:14:19,510
对于线性回归  我们要做的

260
00:14:19,620 --> 00:14:29,290
是首先拟合出θ

261
00:14:29,390 --> 00:14:35,540
使得最小化

262
00:14:35,660 --> 00:14:42,290
之后返回 明白吗

263
00:14:42,410 --> 00:14:43,810
这是线性回归

264
00:14:43,920 --> 00:14:48,720
相对而言  在局部加权线性回归中

265
00:14:48,830 --> 00:14:50,380
你需要做的工作会有些不同

266
00:14:50,500 --> 00:14:52,770
当你要处理x点时

267
00:14:52,870 --> 00:14:55,790
我会检查数据集合

268
00:14:55,890 --> 00:15:00,240
并且只考虑那些位于x周围

269
00:15:00,330 --> 00:15:03,020
固定区域内的数据点 明白吗

270
00:15:03,130 --> 00:15:05,940
我们来看一下对假设求值需要考虑的区域

271
00:15:06,020 --> 00:15:10,980
我会只考虑这个邻近区域内的点

272
00:15:11,060 --> 00:15:12,730
来进行假设的求值

273
00:15:12,830 --> 00:15:16,900
之后我会只取  比如说

274
00:15:17,010 --> 00:15:18,130
这些点

275
00:15:18,240 --> 00:15:23,730
之后我会对这个数据子集使用线性回归

276
00:15:23,820 --> 00:15:26,230
来拟合出一条直线 明白吗

277
00:15:26,330 --> 00:15:27,730
我会使用这个子项或者子集

278
00:15:27,830 --> 00:15:29,460
让我们待会儿再讨论它

279
00:15:29,560 --> 00:15:30,910
所以我们所用这个数据集合

280
00:15:30,910 --> 00:15:32,140
并且对它拟合出一条直线

281
00:15:32,250 --> 00:15:34,760
也许是这样的一条直线

282
00:15:34,890 --> 00:15:40,820
我之后要做的是

283
00:15:40,920 --> 00:15:42,280
根据这条直线求出具体的值

284
00:15:42,390 --> 00:15:47,200
作为我的算法的返回结果

285
00:15:47,310 --> 00:15:50,570
我认为这将是在

286
00:15:50,660 --> 00:15:55,480
局部加权回归中的假设的输出结果

287
00:15:55,570 --> 00:15:59,120
或者预测值 明白吗?

288
00:15:59,220 --> 00:16:06,390
所以让我们继续

289
00:16:06,440 --> 00:16:08,060
让我们继续使其正规化

290
00:16:08,130 --> 00:16:11,360
在局部加权回归中

291
00:16:11,460 --> 00:16:31,090
我们将要拟合出θ

292
00:16:31,190 --> 00:16:34,240
使得最小化

293
00:16:34,330 --> 00:16:36,290
其中称之为权值(weights)

294
00:16:36,380 --> 00:16:39,120
有很多可能的对于权值

295
00:16:39,200 --> 00:16:40,370
的选择  我只写出一种形式

296
00:16:40,440 --> 00:16:46,240
可以

297
00:16:46,320 --> 00:16:52,030
写成

298
00:16:52,130 --> 00:16:53,010
注意

299
00:16:53,120 --> 00:16:57,000
假设你有一个训练样本

300
00:16:57,090 --> 00:17:02,560
如果x与非常接近  所以这一项会非常小  对吗

301
00:17:02,660 --> 00:17:08,240
如果非常小

302
00:17:08,320 --> 00:17:10,040
也就是说接近0

303
00:17:10,130 --> 00:17:12,400
那么e的 0次方

304
00:17:12,480 --> 00:17:17,640
和e的0次方都是1所以如果和x很接近

305
00:17:17,740 --> 00:17:23,290
那么就会接近于1 换句话说

306
00:17:23,370 --> 00:17:25,040
和第i个训练样本

307
00:17:25,130 --> 00:17:27,180
相关的权值会接近于1

308
00:17:27,260 --> 00:17:29,150
如果与x彼此接近的话

309
00:17:29,250 --> 00:17:40,370
相反地  如果非常大

310
00:17:40,450 --> 00:17:44,710
我不知道  结果将会是什么?

311
00:17:44,800 --> 00:17:48,000
S:0

312
00:17:48,090 --> 00:17:49,550
I:没错  0 接近于0

313
00:17:49,640 --> 00:17:53,280
所以如果离x非常远的话

314
00:17:53,350 --> 00:17:55,810
e的次数将是

315
00:17:55,930 --> 00:17:58,330
一个很大的数的负数

316
00:17:58,410 --> 00:18:00,540
所以值近似为0 明白吗

317
00:18:00,640 --> 00:18:14,570
在这张图里  如果我在查询一个确定的点x

318
00:18:14,660 --> 00:18:22,540
在x轴上画出来  如果我的数据集合

319
00:18:22,620 --> 00:18:25,900
比如说  看起来像这个样子

320
00:18:25,980 --> 00:18:27,610
我将要给那些离得近的点赋予较大的权值

321
00:18:27,720 --> 00:18:30,090
而给那些离得远的点赋予较小的权值

322
00:18:30,200 --> 00:18:37,330
对于那些离得远的点

323
00:18:37,410 --> 00:18:39,180
将会近似为0

324
00:18:39,280 --> 00:18:41,610
所以对于那些离得很远的点

325
00:18:41,720 --> 00:18:45,570
它们对于这个求和式的贡献

326
00:18:45,690 --> 00:18:47,340
会非常小  对吗

327
00:18:47,410 --> 00:18:49,030
所以我认为  这个对于i的求和式

328
00:18:49,120 --> 00:18:52,450
等于1乘上若干个二次项加上0乘上

329
00:18:52,540 --> 00:18:55,830
那些离得较远的点对应的二次项

330
00:18:55,910 --> 00:18:58,060
所以使用这种权值计算方法的效果

331
00:18:58,120 --> 00:19:00,140
是通过使用局部加权回归

332
00:19:00,210 --> 00:19:01,820
拟合出一组参数向量

333
00:19:01,890 --> 00:19:04,110
来更多地注重

334
00:19:04,170 --> 00:19:07,390
对临近点的精确拟合

335
00:19:07,460 --> 00:19:11,930
同时忽略那些离得很远的点的贡献

336
00:19:12,000 --> 00:19:13,410
明白吗?什么

337
00:19:13,490 --> 00:19:17,060
S: 你的Y有问题

338
00:19:17,130 --> 00:19:18,330
I: 好的  我知道了

339
00:19:18,380 --> 00:19:20,230
实际上有很多其他的

340
00:19:20,320 --> 00:19:21,710
可以使用的权值函数

341
00:19:21,830 --> 00:19:24,520
实际上有许多非常不同的

342
00:19:24,640 --> 00:19:26,360
学者团体默认

343
00:19:26,450 --> 00:19:28,400
倾向于不同的选择

344
00:19:28,500 --> 00:19:31,740
有些文献在争论

345
00:19:31,820 --> 00:19:33,010
到底应该用哪种方法

346
00:19:33,010 --> 00:19:34,670
确切的说是到底应该用那种函数

347
00:19:34,770 --> 00:19:35,990
指数衰减函数恰好是

348
00:19:35,990 --> 00:19:37,350
一个合理而普遍的函数

349
00:19:37,460 --> 00:19:39,910
在很多问题中都是

350
00:19:39,980 --> 00:19:42,120
一个更为合理的选择

351
00:19:42,210 --> 00:19:43,950
但是你同样也可以使用其他函数

352
00:19:44,060 --> 00:19:47,420
我是否提到过

353
00:19:47,530 --> 00:19:48,780
对于你们那些

354
00:19:48,890 --> 00:19:52,270
对正态分布

355
00:19:52,380 --> 00:19:54,340
或者高斯分布很熟悉的人来说

356
00:19:54,450 --> 00:19:55,720
我写在这里的这个函数看起来

357
00:19:55,720 --> 00:19:57,000
有点像一个高斯分布

358
00:19:57,110 --> 00:20:00,880
对吗?但是事实上它和高斯分布

359
00:20:00,970 --> 00:20:03,720
一点关系都没有

360
00:20:03,800 --> 00:20:05,320
所以这并不表示是高斯分布的

361
00:20:05,420 --> 00:20:10,810
并没有这个含义

362
00:20:10,890 --> 00:20:12,540
这只是一个很方便的函数

363
00:20:12,630 --> 00:20:14,820
恰好是钟形的而已

364
00:20:14,890 --> 00:20:19,440
所以不要赋予它任何和高斯分布有关的含义

365
00:20:19,520 --> 00:20:20,910
明白吗

366
00:20:20,980 --> 00:20:23,730
所以  实际上如果你记得

367
00:20:23,810 --> 00:20:26,740
高斯分布的熟悉的钟形曲线

368
00:20:26,820 --> 00:20:29,950
这正是将这些点

369
00:20:30,000 --> 00:20:34,450
联系在一起的方式如果你想象着将这些点

370
00:20:34,510 --> 00:20:37,390
放置在一个以你想计算假设值h的位置

371
00:20:37,450 --> 00:20:39,430
为中心钟形的凸起上方

372
00:20:39,500 --> 00:20:43,560
那么可以说对于这个点

373
00:20:43,640 --> 00:20:46,010
我赋予的权值和高斯函数的高度

374
00:20:46,090 --> 00:20:48,050
对不起  和这个钟形函数

375
00:20:48,110 --> 00:20:49,920
在这一点的值的高度是成比例的

376
00:20:50,000 --> 00:20:52,520
这一点的权值

377
00:20:52,610 --> 00:20:53,990
对于这个训练样本来说

378
00:20:54,080 --> 00:20:58,290
也和高度成比例  以此类推 明白吗

379
00:20:58,360 --> 00:20:59,840
以离得很远的训练样本将会

380
00:20:59,900 --> 00:21:02,500
得到非常小的权值

381
00:21:02,580 --> 00:21:09,890
最后一个小的一般化的修改

382
00:21:09,970 --> 00:21:13,940
是为这个算法加上一个其他的参数

383
00:21:14,020 --> 00:21:16,810
我这里将其表示为?

384
00:21:16,900 --> 00:21:19,380
这个函数又一次非常可疑地

385
00:21:19,450 --> 00:21:20,970
看起来像是高斯函数

386
00:21:21,050 --> 00:21:22,280
但是这并不是高斯函数

387
00:21:22,360 --> 00:21:24,030
这只是一种便利的函数形式

388
00:21:24,120 --> 00:21:33,250
这个参数被称作波长函数

389
00:21:41,790 --> 00:21:43,370
从非正式的角度上说

390
00:21:43,460 --> 00:21:46,410
它控制了权值随距离下降的速率 明白吗

391
00:21:46,500 --> 00:21:49,990
我猜我得把图从另一边复制过来

392
00:21:53,760 --> 00:21:58,450
如果?的值非常小  如果这是查询值x

393
00:21:58,530 --> 00:22:01,860
那么你会得到一个相当窄的高斯函数

394
00:22:01,940 --> 00:22:04,030
对不起  一个相当窄的钟形

395
00:22:04,100 --> 00:22:06,540
所以举例较远的点的权值

396
00:22:06,600 --> 00:22:08,030
会降低的非常快

397
00:22:08,110 --> 00:22:10,330
如果?的值比较大

398
00:22:20,920 --> 00:22:22,770
那么你会得到一个权值函数

399
00:22:22,860 --> 00:22:24,580
它的值随着距离下降的速度

400
00:22:24,640 --> 00:22:27,520
将会相对较慢 明白吗

401
00:22:29,590 --> 00:22:34,280
所以我希望你因此能了解

402
00:22:34,360 --> 00:22:43,570
如果你在这样的一个数据集合上

403
00:22:43,660 --> 00:22:45,010
使用局部加权线性回归

404
00:22:45,080 --> 00:22:48,270
然后问你在这一点上你的假设的输出

405
00:22:48,370 --> 00:22:50,120
你会得到一条直线

406
00:22:50,220 --> 00:22:52,230
并且做出预测

407
00:22:52,330 --> 00:22:54,560
如果问你在这一点处会得到怎样的输出

408
00:22:54,630 --> 00:22:57,540
你只需要

409
00:22:57,620 --> 00:22:59,880
在这里放一条直线然后预测出它的值

410
00:22:59,960 --> 00:23:01,650
实际上  每一次你都在

411
00:23:01,740 --> 00:23:03,690
尝试改变你的假设

412
00:23:03,750 --> 00:23:05,330
每一次当你让你的学习算法

413
00:23:05,400 --> 00:23:07,130
去预测房屋价格

414
00:23:07,210 --> 00:23:08,410
或无论任何东西时

415
00:23:08,490 --> 00:23:11,800
你都需要重新进行一遍全新的拟合过程

416
00:23:11,890 --> 00:23:15,860
之后在这一点x处

417
00:23:15,930 --> 00:23:18,980
拟合出一条直线

418
00:23:19,060 --> 00:23:20,400
这一点就是你希望

419
00:23:20,480 --> 00:23:22,850
进行预测的那个查询 明白吗

420
00:23:22,920 --> 00:23:24,020
但是如果你沿着x轴

421
00:23:24,020 --> 00:23:25,220
对每个点都执行同样的操作

422
00:23:25,290 --> 00:23:28,210
那么你会发现对于这个数据集合来说

423
00:23:28,290 --> 00:23:29,880
局部加权回归的预测结果

424
00:23:29,960 --> 00:23:33,760
能够追踪这条非线性的曲线

425
00:23:33,830 --> 00:23:39,340
所以在作业的problem set中

426
00:23:39,430 --> 00:23:41,580
我们会让你们自己体会和操作这个算法

427
00:23:41,670 --> 00:23:43,120
所以在这里我就不多说了

428
00:23:43,200 --> 00:23:45,890
最后在开始下一个主题之前

429
00:23:45,970 --> 00:23:48,140
让我看看你们有什么问题 什么

430
00:23:48,210 --> 00:23:50,760
S:好像在这个算法中

431
00:23:50,820 --> 00:23:52,510
同样存在过拟合和欠拟合的问题

432
00:23:52,580 --> 00:23:54,160
当你改变?的值时

433
00:23:54,240 --> 00:23:57,820
你可以让它变得非常小

434
00:23:57,880 --> 00:24:00,140
I:是的  当然 是的

435
00:24:00,210 --> 00:24:03,650
所以局部加权回归

436
00:24:03,710 --> 00:24:06,130
并不能完全避免

437
00:24:06,220 --> 00:24:08,090
过拟合和欠拟合问题

438
00:24:08,170 --> 00:24:12,070
你使用局部加权回归的时候

439
00:24:12,150 --> 00:24:13,460
仍然会遇到同样的问题

440
00:24:13,550 --> 00:24:15,450
你刚才说的那些

441
00:24:16,340 --> 00:24:19,110
以及我说的一些东西我会留给你们

442
00:24:19,190 --> 00:24:20,890
在作业中讨论

443
00:24:20,950 --> 00:24:23,910
你会看到你刚才提到的问题 什么

444
00:24:23,910 --> 00:24:31,690
S: [听不清]

445
00:24:31,780 --> 00:24:32,610
I:是的

446
00:24:32,690 --> 00:24:34,290
S:这个算法能不能

447
00:24:34,340 --> 00:24:36,930
算是一个建模的过程

448
00:24:37,010 --> 00:24:37,960
I:好的

449
00:24:38,030 --> 00:24:41,030
问题有点类似于

450
00:24:41,090 --> 00:24:42,690
好像你并没有在建立一个模型

451
00:24:42,760 --> 00:24:44,320
因为你始终需要全部的数据集合

452
00:24:44,390 --> 00:24:46,780
对于这样的算法的另一种称呼

453
00:24:46,850 --> 00:24:48,220
就是非参数学习算法

454
00:24:48,280 --> 00:24:52,660
所以 我不知道


455
00:24:52,730 --> 00:24:55,570
我不会去辩论

456
00:24:55,650 --> 00:24:56,970
我们是否在建立模型

457
00:24:57,060 --> 00:24:59,840
但是这个算法很好

458
00:24:59,930 --> 00:25:03,230
如果你写了一个算法对于某个数据集合

459
00:25:03,320 --> 00:25:07,400
实现了局部加权线性回归

460
00:25:07,500 --> 00:25:09,450
那么我认为所有的东西一起  包括代码

461
00:25:09,540 --> 00:25:11,220
构成了对于模型的构建

462
00:25:11,320 --> 00:25:14,060
实际上

463
00:25:14,140 --> 00:25:16,810
我们将这个模型成功地

464
00:25:16,880 --> 00:25:18,170
用在了直升机的

465
00:25:18,230 --> 00:25:21,450
自动驾驶上 什么?

466
00:25:21,530 --> 00:25:23,550
S:我想问一下这个算法是否

467
00:25:23,620 --> 00:25:25,610
在基于数据学习权值

468
00:25:25,700 --> 00:25:28,580
I:学习权值

469
00:25:28,660 --> 00:25:29,770
哦  是权值

470
00:25:29,840 --> 00:25:32,430
S:是否需要学习算法来确定权值函数的参数

471
00:25:32,480 --> 00:25:33,400
I:我知道了

472
00:25:33,480 --> 00:25:35,700
实际上有几件事是你需要做的

473
00:25:35,770 --> 00:25:37,020
其中一件常见的是

474
00:25:37,080 --> 00:25:40,490
是怎样确定这个波长参数  对吗

475
00:25:40,540 --> 00:25:42,560
当用到数据的时候.我们之后会回来讨论它

476
00:25:42,620 --> 00:25:44,390
当我们讲到模型选择的时候

477
00:25:44,470 --> 00:25:46,370
什么?最后一个问题

478
00:25:46,460 --> 00:25:48,840
S:  权值函数

479
00:25:48,950 --> 00:25:52,300
是否应该具有和分布函数类似的一些性质

480
00:25:52,410 --> 00:25:59,230
I:哦  我看到了 让我们看看  孩子

481
00:25:59,320 --> 00:26:03,570
权值不是随机变量

482
00:26:03,680 --> 00:26:04,950
而且对于这个算法来说

483
00:26:05,040 --> 00:26:07,810
对其赋予其他可能的含义是没有用的

484
00:26:07,920 --> 00:26:10,660
你可以选择按照高斯函数定义公式

485
00:26:10,730 --> 00:26:12,280
但是这并不表示任何其他含义

486
00:26:12,360 --> 00:26:18,680
实际上  我只是恰好选择了

487
00:26:18,740 --> 00:26:22,680
这个钟形的函数来定义我的权值

488
00:26:22,760 --> 00:26:24,960
你的权值函数的积分的值

489
00:26:25,030 --> 00:26:26,350
不一定是1

490
00:26:26,470 --> 00:26:27,660
同样也可以

491
00:26:27,760 --> 00:26:29,020
是正无穷

492
00:26:29,230 --> 00:26:31,890
在这种意义下

493
00:26:31,990 --> 00:26:34,290
你可以将函数强行定义成高斯函数的形式

494
00:26:34,390 --> 00:26:35,620
但是这没什么用

495
00:26:35,700 --> 00:26:38,260
尤其当你可以用其他积分到正无穷的函数

496
00:26:38,320 --> 00:26:41,700
而不是用积分到1的函数的时候 明白吗

497
00:26:41,790 --> 00:26:43,900
最后一个问题  之后我们继续

498
00:26:43,990 --> 00:26:46,670
S:假设我们有一个巨大的训练集合

499
00:26:46,770 --> 00:26:49,600
例如  一个巨大的房屋数据的集合

500
00:26:49,690 --> 00:26:52,810
然后希望基于每一个房屋来预测线性模型

501
00:26:52,900 --> 00:26:56,760
对于每个输入都要得到一个结果

502
00:26:56,830 --> 00:26:59,360
我感觉这个过程会经常地

503
00:26:59,460 --> 00:27:00,570
I:是的  你是对的

504
00:27:00,640 --> 00:27:03,490
由于局部加权回归

505
00:27:03,580 --> 00:27:07,400
是一个非参数算法

506
00:27:07,510 --> 00:27:09,720
每次当你需要进行预测的时候

507
00:27:09,800 --> 00:27:10,910
你需要再一次根据你的整个训练集合拟合出?

508
00:27:11,000 --> 00:27:12,850
所以实际上你是对的

509
00:27:12,940 --> 00:27:14,950
如果你有一个很大的训练集合

510
00:27:15,040 --> 00:27:17,950
那么使用这个算法代价就会有点高

511
00:27:18,040 --> 00:27:19,480
因为每次当你需要进行预测的时候

512
00:27:19,540 --> 00:27:22,040
你就需要重新根据巨大的数据集合

513
00:27:22,130 --> 00:27:24,470
来拟合出一条直线

514
00:27:24,560 --> 00:27:26,970
实际上  有算法 实际上

515
00:27:27,050 --> 00:27:29,740
有方法可以让这个算法

516
00:27:29,840 --> 00:27:32,100
对于大型数据集合更加高效

517
00:27:32,200 --> 00:27:33,410
我不想讨论这些

518
00:27:33,500 --> 00:27:35,140
如果你感兴趣  可以查一下

519
00:27:35,200 --> 00:27:37,320
Andrew Moore的关于KD tree 的工作

520
00:27:37,410 --> 00:27:40,280
他提出了一些方法能够

521
00:27:40,330 --> 00:27:41,640
以更高效的方式拟合出这些模型

522
00:27:41,720 --> 00:27:44,070
这不是我今天要讲的  明白了吗

523
00:27:44,170 --> 00:27:45,830
让我们继续吧

524
00:27:45,910 --> 00:27:46,990
之后再来回答你们的问题

525
00:28:00,750 --> 00:28:02,320
好的 这就是局部加权回归

526
00:28:02,410 --> 00:28:07,990
记得我之前写的提纲

527
00:28:08,070 --> 00:28:09,260
就是我在上课之前写的那个

528
00:28:09,340 --> 00:28:12,030
我现在要讨论的是

529
00:28:12,110 --> 00:28:15,200
对于线性模型的概率解释

530
00:28:15,290 --> 00:28:16,780
尤其是

531
00:28:16,870 --> 00:28:19,340
关于这个概率解释

532
00:28:19,440 --> 00:28:23,350
我们继续讲logistic回归

533
00:28:23,440 --> 00:28:25,100
它将是我们的第一个分类算法

534
00:28:39,830 --> 00:28:42,130
我们现在先不看局部加权回归

535
00:28:42,170 --> 00:28:43,280
我们先来讨论

536
00:28:43,360 --> 00:28:45,410
通常的非加权的线性回归

537
00:28:45,490 --> 00:28:47,800
我们首先要问的一个问题是

538
00:28:47,800 --> 00:28:49,730
为什么要选择最小二乘

539
00:28:49,830 --> 00:28:51,280
有许多东西可以被优化

540
00:28:51,370 --> 00:28:54,040
为什么我们要选择这样的指标

541
00:28:54,130 --> 00:28:56,630
使得假设预测出的值

542
00:28:56,740 --> 00:28:58,330
和真正的y值之间的面积

543
00:28:58,400 --> 00:29:00,640
的平方最小化

544
00:29:00,720 --> 00:29:04,270
为什么不是面积的绝对值

545
00:29:04,360 --> 00:29:06,330
或者面积的四次方或者其它的呢

546
00:29:06,410 --> 00:29:11,440
我现在要给出一组假设

547
00:29:11,520 --> 00:29:13,900
来"证实"为什么

548
00:29:13,990 --> 00:29:17,270
我们要选择这样的指标 明白吗?

549
00:29:17,380 --> 00:29:20,360
实际上有许多假设

550
00:29:20,460 --> 00:29:23,610
足够可以证实为什么我们要选择最小二乘

551
00:29:23,700 --> 00:29:24,760
这仅仅是其中之一

552
00:29:24,880 --> 00:29:29,330
因为我仅仅提供一组假设

553
00:29:29,440 --> 00:29:31,400
在这组假设下最小二乘回归变得有意义

554
00:29:31,490 --> 00:29:34,010
但是这并不是唯一的一组假设

555
00:29:34,090 --> 00:29:36,480
所以  即使我这里描述的假设不成立

556
00:29:36,590 --> 00:29:38,510
最小二乘实际上

557
00:29:38,580 --> 00:29:39,580
在很多条件下仍然有意义

558
00:29:39,670 --> 00:29:41,770
这有点类似于帮助你

559
00:29:41,840 --> 00:29:43,320
并且给出使用最小二乘的

560
00:29:43,390 --> 00:29:44,610
理由和原因

561
00:29:44,710 --> 00:29:49,670
特别的  我要为

562
00:29:49,770 --> 00:29:53,010
最小二乘模型

563
00:29:53,110 --> 00:29:54,590
赋予概率意义

564
00:29:54,670 --> 00:29:59,480
在我们预测房价的

565
00:29:59,560 --> 00:30:00,690
那个例子中

566
00:30:00,760 --> 00:30:05,050
假设房屋的价格是

567
00:30:05,100 --> 00:30:09,610
一些特征的

568
00:30:09,690 --> 00:30:16,840
线性函数  加上

569
00:30:16,920 --> 00:30:20,990
其中是误差项(error term)

570
00:30:21,080 --> 00:30:23,880
你可以把误差项看成是

571
00:30:23,970 --> 00:30:26,840
对未建模的效应的捕获  比如

572
00:30:26,930 --> 00:30:29,430
也许房子还有其他的特征

573
00:30:29,510 --> 00:30:31,720
比如  房间有多少壁炉

574
00:30:31,800 --> 00:30:33,010
或者房间是否有花园

575
00:30:33,100 --> 00:30:36,320
或者一些其他的特征

576
00:30:36,380 --> 00:30:37,600
它表示了一种我们没有捕获到的特征

577
00:30:37,670 --> 00:30:39,750
或者你也可以把它看成一种随机的噪声

578
00:30:39,800 --> 00:30:41,280
ε是我们的误差项

579
00:30:41,360 --> 00:30:43,320
捕获了没有未建模的效应

580
00:30:43,400 --> 00:30:45,150
一些我们忘记建模的东西

581
00:30:45,210 --> 00:30:46,980
也许函数不是线性或其他之类的

582
00:30:47,070 --> 00:30:53,670
也可能是随机噪声  可能那天卖家的心情

583
00:30:53,760 --> 00:30:56,010
很不好  然后随随便便就卖了

584
00:30:56,120 --> 00:30:57,880
没有去寻找一些更为合理的价格

585
00:30:57,960 --> 00:30:59,760
或者一些其他的原因

586
00:30:59,840 --> 00:31:09,160
现在我要假设这些误差

587
00:31:09,200 --> 00:31:10,590
存在一个概率分布

588
00:31:10,680 --> 00:31:13,770
我会假设误差项

589
00:31:13,850 --> 00:31:19,510
服从某个概率分布

590
00:31:19,600 --> 00:31:21,580
用这样的方式表示

591
00:31:21,650 --> 00:31:26,130
这是一个高斯分布  均值是0

592
00:31:26,220 --> 00:31:28,590
方差是  明白吗

593
00:31:28,700 --> 00:31:30,480
我这里用了手写体N

594
00:31:30,560 --> 00:31:32,510
N表示normal  对吧

595
00:31:32,600 --> 00:31:34,320
这代表一个正态分布

596
00:31:34,400 --> 00:31:35,540
也称之为高斯分布

597
00:31:35,620 --> 00:31:38,700
均值是0  方差是

598
00:31:38,780 --> 00:31:42,670
如果你之前见过高斯分布

599
00:31:42,760 --> 00:31:44,680
快速地举一下手

600
00:31:44,770 --> 00:31:46,950
很好  大多数人

601
00:31:47,010 --> 00:31:52,030
几乎每一个人  换句话说你们之前

602
00:31:52,120 --> 00:31:55,630
也应该见过

603
00:31:55,720 --> 00:31:56,910
高斯分布的概率

604
00:31:56,990 --> 00:32:03,620
密度函数的

605
00:32:03,700 --> 00:32:08,860
概率密度函数  对吗

606
00:32:08,950 --> 00:32:17,270
概率密度函数

607
00:32:17,360 --> 00:32:23,630
是一个钟形的曲线

608
00:32:23,710 --> 00:32:28,990
标准差是Σ

609
00:32:29,090 --> 00:32:34,320
这就是这个钟形曲线

610
00:32:34,400 --> 00:32:37,530
看一下  我可以擦了它

611
00:32:37,620 --> 00:32:42,860
我能擦了这块黑板吗

612
00:32:42,920 --> 00:33:05,820
这表示在给定和参数的时候

613
00:33:05,890 --> 00:33:10,820
房屋的价格也服从高斯分布

614
00:33:10,910 --> 00:33:14,010
概率密度函数

615
00:33:14,070 --> 00:33:33,300
是这样的 明白吗

616
00:33:33,390 --> 00:33:37,870
换句话说  在给定房屋的特征

617
00:33:37,960 --> 00:33:43,590
与参数之后

618
00:33:43,650 --> 00:33:45,090
房屋的价格是一个服从

619
00:33:45,170 --> 00:33:48,220
高斯分布的随机变量

620
00:33:48,290 --> 00:33:54,060
均值为

621
00:33:54,140 --> 00:33:58,410
方差为  对吗

622
00:33:58,490 --> 00:34:02,100
因为我们想像房屋的价格

623
00:34:02,200 --> 00:34:05,930
是加上一些

624
00:34:06,000 --> 00:34:08,360
服从高斯分布

625
00:34:08,450 --> 00:34:10,600
且方差为的噪声

626
00:34:10,710 --> 00:34:14,190
所以房屋价格的均值是

627
00:34:14,260 --> 00:34:19,130
方差为 明白了吗

628
00:34:19,210 --> 00:34:21,000
如果明白的话

629
00:34:21,080 --> 00:34:25,090
请举手 好的

630
00:34:37,330 --> 00:34:40,340
很多人都明白了 关于符号 什么

631
00:34:40,430 --> 00:34:44,740
S:我们并不了解

632
00:34:44,790 --> 00:34:46,910
误差的任何性质

633
00:34:46,940 --> 00:34:47,950
为什么假设误差是服从高斯分布的

634
00:34:48,040 --> 00:34:49,220
I:好的  孩子

635
00:34:49,330 --> 00:34:55,500
为什么我认为误差服从高斯分布呢

636
00:34:55,610 --> 00:34:57,610
两个原因

637
00:34:57,700 --> 00:34:59,530
一个原因是这样会

638
00:34:59,630 --> 00:35:03,610
便于进行数学处理  另外一个  我不知道

639
00:35:03,710 --> 00:35:05,660
实际上我可以讲很多关于证明方面的内容

640
00:35:05,750 --> 00:35:07,550
例如中心极限定律之类的

641
00:35:07,650 --> 00:35:10,060
实际上如果你  对于绝大多数问题

642
00:35:10,160 --> 00:35:11,530
如果你像这样使用线性回归模型

643
00:35:11,650 --> 00:35:14,460
然后尝试测量误差的分布

644
00:35:14,560 --> 00:35:17,590
不一定是所有情况

645
00:35:17,650 --> 00:35:18,460
但是通常情况下你会发现

646
00:35:18,520 --> 00:35:19,480
误差就是高斯分布的

647
00:35:19,520 --> 00:35:21,650
高斯模型对于这类回归问题中的误差来说

648
00:35:21,750 --> 00:35:25,250
是一个很好的假设

649
00:35:25,330 --> 00:35:27,090
你们中的一些人也许听说过

650
00:35:27,140 --> 00:35:28,510
中心极限定律

651
00:35:28,590 --> 00:35:31,390
说的是许多独立随机变量之和

652
00:35:31,450 --> 00:35:32,590
趋向于服从高斯分布

653
00:35:32,680 --> 00:35:35,610
如果误差是由许多效应共同导致的

654
00:35:35,700 --> 00:35:38,630
例如:卖家的情绪  买家的情绪

655
00:35:38,710 --> 00:35:40,350
我们没有考虑到的其它特征

656
00:35:40,430 --> 00:35:42,900
房子是否有花园等等

657
00:35:42,980 --> 00:35:44,310
如果所有这些效应都是独立的

658
00:35:44,360 --> 00:35:47,320
那么根据中心极限定律

659
00:35:47,370 --> 00:35:50,720
你也许会愿意相信

660
00:35:50,780 --> 00:35:52,120
这些效应的总和会接近于服从高斯分布

661
00:35:52,180 --> 00:35:54,980
实际上  我想  答案应该是

662
00:35:55,030 --> 00:35:56,670
(1) 实际上这是一个

663
00:35:56,770 --> 00:35:58,930
合理准确的假设

664
00:35:59,010 --> 00:36:01,500
(2)实际上它会

665
00:36:01,610 --> 00:36:02,810
带来数学上的便利

666
00:36:02,920 --> 00:36:04,620
明白了吗? 什么

667
00:36:04,700 --> 00:36:08,720
S:如果我们假设模型周围的区域均值是0

668
00:36:08,770 --> 00:36:11,090
那么就是说这些区域

669
00:36:11,170 --> 00:36:14,420
以我们的模型为中心分布在周围

670
00:36:14,510 --> 00:36:17,680
这好像意味着我们尝试

671
00:36:17,750 --> 00:36:18,900
假设了我们试图证明的东西

672
00:36:18,990 --> 00:36:22,530
I:但是  是的

673
00:36:22,630 --> 00:36:24,850
你确实在假设误差的均值是0

674
00:36:24,960 --> 00:36:30,210
是的 我认为稍后

675
00:36:30,270 --> 00:36:31,530
我们会讲一些其他的东西

676
00:36:31,580 --> 00:36:33,610
但是现在仅仅把它看成是数学上的

677
00:36:33,690 --> 00:36:36,160
它实际上并不是一个没有道理的假设

678
00:36:36,250 --> 00:36:42,210
我想在机器学习中  我们所做的所有假设

679
00:36:42,300 --> 00:36:46,690
在绝对意义上都不是完全正确的  不是吗

680
00:36:46,760 --> 00:36:50,280
因为  例如  房价都是

681
00:36:50,380 --> 00:36:53,810
以美元和美分为单位的

682
00:36:53,890 --> 00:36:56,720
所以价格上的误差

683
00:36:56,780 --> 00:36:57,780
作为随机变量不可能是连续的

684
00:36:57,890 --> 00:37:01,340
因为房子只能定价为

685
00:37:01,400 --> 00:37:03,330
整数美元和整数美分

686
00:37:03,430 --> 00:37:05,560
你永远不会得到小数美分

687
00:37:05,670 --> 00:37:07,650
而服从高斯分布的随机变量却是连续的

688
00:37:07,720 --> 00:37:10,230
所以从这个意义上来看

689
00:37:10,330 --> 00:37:11,610
我们做的假设不可能是"绝对正确"的

690
00:37:11,720 --> 00:37:12,730
但是从实用目的出发

691
00:37:12,840 --> 00:37:15,910
这样的假设已经足够精确

692
00:37:15,960 --> 00:37:18,420
并且可以使用了.明白吗

693
00:37:18,510 --> 00:37:21,140
我想在一两周之后

694
00:37:21,250 --> 00:37:25,410
我们能够回顾一下我们做的若干种假设

695
00:37:25,510 --> 00:37:26,980
来看看什么时候它们会

696
00:37:27,060 --> 00:37:27,950
帮助我们的学习算法

697
00:37:28,030 --> 00:37:29,240
什么时候会妨碍它们

698
00:37:29,310 --> 00:37:31,210
当我们一两周之内讲到生成学习算法

699
00:37:31,300 --> 00:37:32,650
和判别学习算法

700
00:37:32,730 --> 00:37:35,680
时会说的更多一些 好吗

701
00:37:35,770 --> 00:37:42,720
关于符号问题我想再说一点

702
00:37:42,920 --> 00:37:46,040
当我写: 时

703
00:37:46,120 --> 00:37:48,910
（老师强调的是中间用的是分号  不是逗号)

704
00:37:49,030 --> 00:37:51,300
我会用这样的符号

705
00:37:51,370 --> 00:37:55,940
表示我们不把θ当成随机变量

706
00:37:56,000 --> 00:37:58,200
统计学中

707
00:37:58,280 --> 00:38:01,100
将其称之为频率学派的观点

708
00:38:01,210 --> 00:38:04,210
你可以认为?是有真正的值  用来产生数据

709
00:38:04,250 --> 00:38:05,280
但是我们并不知道

710
00:38:05,390 --> 00:38:06,780
这些真正的值到底是什么

711
00:38:06,850 --> 00:38:09,020
但是我们知道它不是一个随机变量θ

712
00:38:09,130 --> 00:38:11,650
明白吗

713
00:38:11,740 --> 00:38:13,680
并不是

714
00:38:13,900 --> 00:38:16,000
随机变量θ

715
00:38:16,090 --> 00:38:18,550
θ有它真正的值

716
00:38:18,590 --> 00:38:19,510
只是我们并不知道

717
00:38:19,590 --> 00:38:20,720
它的真正的值到底是多少

718
00:38:20,810 --> 00:38:25,110
所以如果θ不是随机变量

719
00:38:25,190 --> 00:38:29,950
所以我会避免写成这样的形式

720
00:38:30,030 --> 00:38:33,700
因为这意味着有可能

721
00:38:33,780 --> 00:38:35,560
是以和θ为条件的

722
00:38:35,640 --> 00:38:37,760
而你只能以随机变量为条件

723
00:38:37,860 --> 00:38:41,550
所以对于课程的这一部分来说

724
00:38:41,670 --> 00:38:44,250
我们会采用频率学派的观点

725
00:38:44,340 --> 00:38:45,620
而不是贝叶斯学派的观点

726
00:38:45,710 --> 00:38:47,390
对于课程的这一部分

727
00:38:47,470 --> 00:38:48,530
我们不把?看成是随机变量

728
00:38:48,620 --> 00:38:50,390
而仅仅看成是我们尝试估计的值

729
00:38:50,440 --> 00:38:51,960
所以这里我们使用分号.明白吗

730
00:38:52,060 --> 00:38:55,460
所以这个公式的读法应该是:

731
00:38:55,520 --> 00:39:00,410
给定  以θ为参数的的概率 明白吗

732
00:39:00,490 --> 00:39:02,910
所以这个分号应该读作:以 为参数

733
00:39:03,030 --> 00:39:04,750
同理  在这里我会说:

734
00:39:04,830 --> 00:39:07,750
给定  以θ为参数

735
00:39:07,840 --> 00:39:10,280
服从这样的高斯分布

736
00:39:36,150 --> 00:39:37,800
一个条件是假设

737
00:39:37,920 --> 00:39:50,670
不同输入

738
00:39:50,790 --> 00:39:52,950
对应的误差项

739
00:39:53,060 --> 00:39:55,890
之间是彼此独立的

740
00:39:56,000 --> 00:39:58,980
明白吗

741
00:39:59,050 --> 00:40:13,630
同分布意味着我假设

742
00:40:13,670 --> 00:40:15,460
它们服从均值和方差

743
00:40:15,530 --> 00:40:17,130
完全相同的高斯分布

744
00:40:18,890 --> 00:40:20,440
但是更为重要的是

745
00:40:20,550 --> 00:40:22,410
我假设不同的彼此之间

746
00:40:22,460 --> 00:40:23,730
是完全独立的

747
00:40:23,840 --> 00:40:26,910
让我们来讨论怎样拟合出一个模型

748
00:40:27,010 --> 00:40:33,530
我要给它

749
00:40:33,640 --> 00:40:38,100
一个另外的名字

750
00:40:38,210 --> 00:40:39,480
我会将其写成这样的形式

751
00:40:39,560 --> 00:40:41,790
将其称之为θ的似然性

752
00:40:41,870 --> 00:40:44,950
并将其定义为

753
00:40:45,000 --> 00:40:49,680
所以它等于

754
00:40:49,750 --> 00:40:57,490
也就是一系列

755
00:40:57,560 --> 00:41:04,380
高斯密度函数的乘积

756
00:41:04,450 --> 00:41:05,650
对吗

757
00:41:05,690 --> 00:41:08,650
明白了吗

758
00:41:08,770 --> 00:41:22,600
在这一部分的符号中

759
00:41:22,680 --> 00:41:25,560
我定义了这一项表示θ的似然性

760
00:41:25,640 --> 00:41:27,570
θ的似然性表示的

761
00:41:27,640 --> 00:41:29,720
是数据的概率  对吗

762
00:41:29,810 --> 00:41:31,380
给定X并且以θ为参数

763
00:41:31,460 --> 00:41:35,400
似然性和概率这两个概念非常令人迷惑

764
00:41:35,440 --> 00:41:40,160
θ的似然性和你看到的数据

765
00:41:40,240 --> 00:41:41,910
的概率是一样的

766
00:41:41,980 --> 00:41:43,140
所以似然性和概率

767
00:41:43,140 --> 00:41:44,880
这两个概念大致也是相同的

768
00:41:44,970 --> 00:41:47,190
除了当我使用似然性这个词的时候

769
00:41:47,590 --> 00:41:50,850
我在试图强调

770
00:41:50,960 --> 00:41:54,990
我将这一项视为一个θ的函数 明白吗

771
00:41:55,070 --> 00:41:56,640
所以似然性和概率

772
00:41:56,640 --> 00:41:58,800
实际上指的是同样的东西

773
00:41:58,910 --> 00:42:00,830
除了当我想将这些看成

774
00:42:00,950 --> 00:42:03,180
当X和固定时?的函数时

775
00:42:03,280 --> 00:42:06,910
我会使用似然性这个词

776
00:42:07,010 --> 00:42:11,030
明白吗?所以希望你们能

777
00:42:11,120 --> 00:42:12,360
听到我说参数的似然性

778
00:42:12,470 --> 00:42:15,490
和数据的概率

779
00:42:15,570 --> 00:42:16,890
而不是数据的似然性

780
00:42:17,000 --> 00:42:18,120
和参数的概率

781
00:42:18,230 --> 00:42:19,870
所以要试着保持术语的一致性

782
00:42:19,960 --> 00:42:25,600
好的  我会再做一个假设.

783
00:42:25,610 --> 00:42:32,990
我们假设误差项满足iid  明白吗

784
00:42:33,100 --> 00:42:35,390
iid表示独立同分布

785
00:42:35,500 --> 00:42:37,000
所以给定这些作为

786
00:42:37,000 --> 00:42:39,260
数据的概率和参数的似然性

787
00:42:39,370 --> 00:42:42,310
你们怎样去估计参数呢?

788
00:42:42,360 --> 00:42:43,510
你希望为你的模型选择怎样的参数

789
00:42:57,690 --> 00:43:00,980
极大似然估计

790
00:43:01,070 --> 00:43:10,070
原则解决的就是这样的问题

791
00:43:10,170 --> 00:43:12,130
你需要选择参数θ

792
00:43:12,240 --> 00:43:15,660
使得数据出现的可能性尽可能的大

793
00:43:15,760 --> 00:43:24,750
所以要选择θ使似然性最大化

794
00:43:24,870 --> 00:43:29,430
或者换句话说选择参数

795
00:43:29,630 --> 00:43:32,030
使得数据出现的可能性尽可能的大

796
00:43:32,140 --> 00:43:35,460
这样你就非常有可能统计到这样的数据

797
00:43:35,580 --> 00:43:37,040
所以关键是要

798
00:43:37,130 --> 00:43:39,910
选择参数使我们得到的数据

799
00:43:40,000 --> 00:43:41,460
出现的概率最大化

800
00:43:41,560 --> 00:43:48,150
所以为了数学上的便利

801
00:43:48,220 --> 00:43:50,760
让我定义l(θ)

802
00:43:50,830 --> 00:43:55,010
它称为对数似然函数

803
00:43:55,100 --> 00:43:59,650
它是对L(θ)取对数的结果

804
00:43:59,740 --> 00:44:04,490
所以它是对于

805
00:44:04,490 --> 00:44:10,030
这些乘积取对数

806
00:44:10,110 --> 00:44:11,630
我现在就先不把括号中的东西

807
00:44:11,690 --> 00:44:13,040
写出来了

808
00:44:13,110 --> 00:44:15,460
它的形式写在之前的黑板上

809
00:44:15,500 --> 00:44:18,350
对于一个乘积取对数

810
00:44:18,390 --> 00:44:19,690
等于对每项取对数之后再求和  对吗

811
00:44:20,760 --> 00:44:25,450
之后得到了对很多对数的求和式

812
00:44:25,560 --> 00:44:39,280
化简得到  加上

813
00:44:39,370 --> 00:44:48,680
之后对于幂式

814
00:44:48,720 --> 00:44:51,340
求对数会抵消掉  对吗

815
00:44:51,460 --> 00:44:52,390
对e的几次方求自然对数

816
00:44:52,480 --> 00:44:54,220
会得到它的指数项

817
00:44:54,320 --> 00:45:02,440
让我把它写在下一块黑板上

818
00:45:02,520 --> 00:45:51,430
好的 所以使L(?)最大化

819
00:45:51,520 --> 00:45:58,740
或者使l(?)最大化

820
00:45:58,820 --> 00:46:03,910
就是使这一项最小化

821
00:46:19,620 --> 00:46:23,010
好的  推出来了 明白了吗?因为这里有个负号

822
00:46:23,090 --> 00:46:25,180
所以使这一项最大化  由于负号的存在

823
00:46:25,250 --> 00:46:28,000
就等同于将里面这项作为θ的函数最小化

824
00:46:28,130 --> 00:46:36,170
这一项  当然就是我们之前用的

825
00:46:36,270 --> 00:46:38,250
同样的成本函数J(?)

826
00:46:38,370 --> 00:46:40,860
明白了吗

827
00:46:40,950 --> 00:46:45,960
所以我们展示了

828
00:46:46,040 --> 00:46:47,600
之前讲的一般的最小二乘算法目的

829
00:46:47,680 --> 00:46:50,020
实际上是在假设误差项满足高斯分布

830
00:46:50,080 --> 00:46:55,220
且独立同分布的情况下

831
00:46:55,310 --> 00:46:57,420
使似然性最大化

832
00:46:57,510 --> 00:47:03,430
明白了吗

833
00:47:03,540 --> 00:47:13,900
有个问题我们要

834
00:47:13,980 --> 00:47:15,750
留到下节课去讲

835
00:47:15,810 --> 00:47:17,540
注意到对于结果没有影响  对吗

836
00:47:17,590 --> 00:47:19,200
不管取什么值

837
00:47:19,260 --> 00:47:20,580
我的意思是

838
00:47:20,700 --> 00:47:22,250
一定是个整数

839
00:47:22,270 --> 00:47:23,450
它是高斯分布的方差

840
00:47:23,540 --> 00:47:24,900
所以不管取什么值

841
00:47:24,950 --> 00:47:28,800
由于它是个正数

842
00:47:28,900 --> 00:47:31,420
所以我们最后得到的结果都是相同的  对吗

843
00:47:31,540 --> 00:47:35,700
因为要使这个最小化

844
00:47:35,790 --> 00:47:37,900
不管取什么样的值你都会得到同样的θ

845
00:47:38,010 --> 00:47:40,650
所以似乎在这个模型中的值

846
00:47:40,720 --> 00:47:42,020
并没有影响

847
00:47:42,110 --> 00:47:45,260
记住这个性质

848
00:47:45,340 --> 00:47:46,540
下节课我们还会回到这里

849
00:47:46,620 --> 00:47:48,780
有问题吗

850
00:47:48,860 --> 00:47:53,600
让我先擦几块黑板

851
00:47:53,670 --> 00:47:55,080
然后再看你们有什么问题

852
00:47:55,180 --> 00:48:41,940
好的  有问题吗?什么

853
00:48:42,030 --> 00:48:44,470
S:我认为

854
00:48:44,540 --> 00:48:50,260
你在用错误的

855
00:48:50,340 --> 00:48:53,390
几率衡量似然性

856
00:48:53,490 --> 00:48:56,720
但是我认为

857
00:48:56,770 --> 00:49:00,000
你衡量的过程

858
00:49:00,100 --> 00:49:01,610
依赖于?

859
00:49:01,610 --> 00:49:04,040
(注:台词缺失)

860
00:49:04,130 --> 00:49:06,030
I:好的  我的意思是

861
00:49:06,100 --> 00:49:07,140
你在问关于过拟合的问题

862
00:49:07,220 --> 00:49:09,390
不管这是不是一个好模型 让我们

863
00:49:09,460 --> 00:49:12,990
你刚才提到的问题可能是

864
00:49:13,100 --> 00:49:15,420
关于学习算法的比较深的问题

865
00:49:15,510 --> 00:49:18,240
我们之后会再来研究它

866
00:49:18,350 --> 00:49:20,890
所以现在先不用研究的如此深入

867
00:49:20,980 --> 00:49:27,770
还有其他问题吗?好的

868
00:49:27,850 --> 00:49:36,010
所以这些为线性回归赋予了

869
00:49:36,100 --> 00:49:37,540
概率意义上的诠释

870
00:49:37,620 --> 00:49:40,910
实际上我想用这种概率

871
00:49:41,000 --> 00:49:44,470
解释引出我们的

872
00:49:44,560 --> 00:49:46,220
下一个学习算法

873
00:49:46,330 --> 00:49:49,820
它是我们的第一个分类算法

874
00:49:49,910 --> 00:49:56,620
你们回忆一下

875
00:49:56,690 --> 00:49:58,410
我说过回归问题中

876
00:49:58,490 --> 00:50:00,150
你们要尝试预测的变量y是连续变量

877
00:50:00,260 --> 00:50:02,790
现在我要讨论的

878
00:50:02,930 --> 00:50:05,550
第一个分类算法中

879
00:50:05,640 --> 00:50:09,040
你们要预测的变量y是离散的

880
00:50:09,100 --> 00:50:10,530
你们可以限定y只取很少的几个离散值

881
00:50:10,610 --> 00:50:11,950
在这个例子里

882
00:50:12,030 --> 00:50:13,810
我会讨论二元分类

883
00:50:13,910 --> 00:50:18,160
其中y只能取两个值

884
00:50:18,220 --> 00:50:20,890
你们可能会遇到一些分类问题

885
00:50:20,980 --> 00:50:22,280
例如你们可能要

886
00:50:22,380 --> 00:50:24,000
进行医学诊断

887
00:50:24,090 --> 00:50:25,600
尝试着基于一些特征

888
00:50:25,690 --> 00:50:28,250
来判断一个病人是否生病

889
00:50:28,350 --> 00:50:31,290
或者回到房屋销售的那个例子中

890
00:50:31,300 --> 00:50:33,560
也许你尝试去 判断一幢房屋是否

891
00:50:33,560 --> 00:50:35,160
会在未来的6个月之内卖掉

892
00:50:35,240 --> 00:50:36,710
答案或者为"是"  或者为"否"

893
00:50:36,800 --> 00:50:38,440
它在6个月之后或者被卖掉

894
00:50:38,490 --> 00:50:42,260
或者不被卖掉 另外一个典型的例子是

895
00:50:42,350 --> 00:50:43,890
你也许想要建立一个垃圾邮件过滤器

896
00:50:43,970 --> 00:50:45,090
这封邮件是不是垃圾邮件?

897
00:50:45,090 --> 00:50:45,930
这也是一个是否问题

898
00:50:46,010 --> 00:50:49,860
或者如果你  我的一些同事

899
00:50:49,940 --> 00:50:51,290
要尝试预测一个计算机系统

900
00:50:51,340 --> 00:50:52,510
是否会崩溃

901
00:50:52,610 --> 00:50:54,610
所以你需要一个学习算法

902
00:50:54,700 --> 00:50:56,560
来预测这个计算机集群

903
00:50:56,630 --> 00:50:58,540
是否会在未来24小时之内崩溃

904
00:50:58,610 --> 00:51:00,440
答案又一次只能为"是"或者"否"

905
00:51:00,570 --> 00:51:09,840
所以这里是x  这里是y在一个分类问题中

906
00:51:09,900 --> 00:51:16,600
y只能取两个值  0和1

907
00:51:16,670 --> 00:51:18,420
这就是二元分类

908
00:51:18,500 --> 00:51:19,960
你可以做什么呢

909
00:51:20,020 --> 00:51:22,810
一件你可以做的事是使用线性回归

910
00:51:22,890 --> 00:51:24,280
正如我们到现在为止讨论过的那样

911
00:51:24,340 --> 00:51:25,740
并且应用到这个问题上  对吗

912
00:51:25,830 --> 00:51:27,030
所以  你知道

913
00:51:27,100 --> 00:51:28,010
对于这样一个数据集合

914
00:51:28,010 --> 00:51:28,920
你可以拟合出一条直线

915
00:51:29,000 --> 00:51:31,940
可能你得到这样一条直线

916
00:51:32,000 --> 00:51:34,290
但是我画的这个数据集合

917
00:51:34,350 --> 00:51:36,690
是一个非常简单的分类问题

918
00:51:36,770 --> 00:51:39,690
它对于每个人来说都很明显  是吧

919
00:51:39,760 --> 00:51:41,010
x和y之间的关系是

920
00:51:41,080 --> 00:51:42,390
你仅仅需要在这附近取一个值

921
00:51:42,390 --> 00:51:43,000
如果点在它右边

922
00:51:43,060 --> 00:51:45,580
y值就是1  如果在它左边  y值就是0

923
00:51:45,660 --> 00:51:49,450
如果你对这个数据集合应用了线性回归

924
00:51:49,520 --> 00:51:51,370
并且得到了一个合理的拟合结果

925
00:51:51,440 --> 00:51:52,230
也许你就可以通过

926
00:51:52,310 --> 00:51:53,870
线性回归的假设得到这条直线

927
00:51:53,940 --> 00:51:57,670
并且将y=0.5处的点作为一个临界点

928
00:51:57,730 --> 00:52:00,310
如果你这样做  你一定会得到正确答案

929
00:52:00,370 --> 00:52:05,300
如果x在这个中点右侧

930
00:52:05,370 --> 00:52:07,710
那么y的值就是1

931
00:52:07,760 --> 00:52:09,950
如果在中点左侧y值就是0

932
00:52:10,030 --> 00:52:13,080
所以确实会有一些人这样做

933
00:52:13,150 --> 00:52:15,550
用线性回归解决分类问题

934
00:52:15,630 --> 00:52:18,160
有些时候它的效果很好

935
00:52:18,210 --> 00:52:21,350
但是通常情况下应用线性回归解决

936
00:52:21,410 --> 00:52:26,960
像这样的分类问题会是一个很糟糕的主意

937
00:52:26,990 --> 00:52:29,110
原因是这样的

938
00:52:29,190 --> 00:52:33,880
取

939
00:52:33,970 --> 00:52:35,770
这个值

940
00:52:35,870 --> 00:52:36,990
x比它

941
00:52:37,080 --> 00:52:41,270
大

942
00:52:41,360 --> 00:52:43,150
则y值为1

943
00:52:43,260 --> 00:52:44,670
x比它

944
00:52:44,750 --> 00:52:47,320
小

945
00:52:47,410 --> 00:52:49,410
则y值为0

946
00:52:49,500 --> 00:52:54,270
给你这个额外的训练样本

947
00:52:54,360 --> 00:52:56,050
实际上并没有改变任何事情

948
00:52:56,140 --> 00:52:58,510
我的意思是  我并没有表达更多的信息

949
00:52:58,600 --> 00:52:59,410
这个点对应的y值等于1

950
00:52:59,470 --> 00:53:00,780
这一点也不令人惊讶

951
00:53:00,860 --> 00:53:02,490
但是如果你现在对这个

952
00:53:02,500 --> 00:53:04,130
数据集合应用线性回归

953
00:53:04,210 --> 00:53:07,880
你会得到这样一条直线  我不知道

954
00:53:07,950 --> 00:53:09,910
也许看起来是这样  对吗

955
00:53:09,990 --> 00:53:13,790
现在你的假设的预测已经完全改变了

956
00:53:13,870 --> 00:53:14,950
如果你的临界点

957
00:53:14,960 --> 00:53:16,720
你的假设中临界点对应的y值

958
00:53:16,790 --> 00:53:20,200
仍然是0.5的话 明白吗

959
00:53:20,290 --> 00:53:24,500
所以 S:在那个中间的那个区间上

960
00:53:24,560 --> 00:53:27,470
有很多0  是吗

961
00:53:27,560 --> 00:53:28,500
对于那些离得远的点

962
00:53:28,580 --> 00:53:29,860
I:哦  你的意思是  像这样

963
00:53:29,940 --> 00:53:30,690
S:是的

964
00:53:30,750 --> 00:53:32,170
I:是的  很好

965
00:53:32,270 --> 00:53:34,230
是的  当然 一个像这样的数据集

966
00:53:34,280 --> 00:53:39,750
我想  这些 是的  你是对的

967
00:53:39,840 --> 00:53:40,790
但是这是一个例子

968
00:53:40,800 --> 00:53:43,570
而且这个例子能说明问题

969
00:53:43,610 --> 00:53:44,490
这个


970
00:53:44,570 --> 00:53:47,660
S:如果你加上这些的话

971
00:53:47,730 --> 00:53:49,950
拟合结果会变化的更厉害


972
00:53:50,000 --> 00:53:50,700
I:是的

973
00:53:50,770 --> 00:53:51,640
我觉得这样使得结果更糟

974
00:53:51,720 --> 00:53:53,100
你实际上会得到一条被拉得更远的直线.

975
00:53:53,190 --> 00:53:56,490
这是我的例子

976
00:53:56,580 --> 00:53:57,540
我想让它怎样它就怎样

977
00:53:57,600 --> 00:53:58,980
但是这

978
00:53:59,060 --> 00:54:00,630
并不是重点

979
00:54:00,710 --> 00:54:02,230
重点是我希望告诉你

980
00:54:02,300 --> 00:54:04,670
对分类问题应用线性回归

981
00:54:04,780 --> 00:54:05,880
是一个非常糟糕的主意.

982
00:54:05,880 --> 00:54:06,780
有时它的效果还可以

983
00:54:06,870 --> 00:54:08,510
但是通常情况下我不会这样做

984
00:54:08,580 --> 00:54:14,130
和这有关的有几个问题 一个问题是


985
00:54:14,220 --> 00:54:16,260
你希望分类做些什么

986
00:54:16,320 --> 00:54:21,420
如果你知道y的值在0和1之间

987
00:54:21,480 --> 00:54:24,330
那么为了开始完善问题

988
00:54:24,390 --> 00:54:30,260
让我们先开始改变我们假设的形式

989
00:54:30,330 --> 00:54:34,720
使得我的假设得到的值

990
00:54:34,780 --> 00:54:37,390
总是在0到1之间 明白吗

991
00:54:37,460 --> 00:54:42,780
所以如果我知道y不是0就是1

992
00:54:42,840 --> 00:54:44,750
那么我们至少应该让我们的

993
00:54:44,750 --> 00:54:45,870
假设预测出的值

994
00:54:45,960 --> 00:54:48,340
不会比1大太多  也不会比0小太多

995
00:54:48,420 --> 00:54:53,660
所以我们不会选择线性函数作为假设

996
00:54:53,720 --> 00:54:56,210
而是会选择一些

997
00:54:56,270 --> 00:54:57,380
稍微不同的形式

998
00:54:57,470 --> 00:55:03,980
所以  特别地

999
00:55:04,060 --> 00:55:08,190
我会选择这个函数

1000
00:55:08,240 --> 00:55:15,580
g是这样的函数

1001
00:55:15,650 --> 00:55:20,260
所以这一项

1002
00:55:20,320 --> 00:55:23,920
变成了

1003
00:55:24,000 --> 00:55:28,750
g(z)被称为sigmoid函数

1004
00:55:28,800 --> 00:55:35,110
通常也被称为logistic函数

1005
00:55:35,190 --> 00:55:39,760
叫哪个名字都可以

1006
00:55:39,820 --> 00:55:43,620
g(z)函数看起来是这样的

1007
00:55:43,680 --> 00:55:49,200
当你有一条水平坐标轴  我把这里标为z

1008
00:55:49,280 --> 00:55:58,070
所以g(z)看起来是这样的 明白吗

1009
00:55:58,130 --> 00:56:04,270
我画的不太好

1010
00:56:04,350 --> 00:56:08,990
当z变得非常小的时候

1011
00:56:09,080 --> 00:56:13,910
g(z)会趋向于0  当z变得非常大的时候

1012
00:56:13,990 --> 00:56:19,370
g(z)会趋向于1  它和纵轴相交在0.5

1013
00:56:19,430 --> 00:56:20,930
这就是sigmoid函数

1014
00:56:21,010 --> 00:56:23,140
也常值为logistic函数 什么问题

1015
00:56:23,220 --> 00:56:25,030
S:(注:解释不通)

1016
00:56:25,110 --> 00:56:26,270
I:能再说一遍吗?

1017
00:56:26,330 --> 00:56:28,990
S:为什么要用

1018
00:56:29,050 --> 00:56:31,430
这个函数

1019
00:56:31,510 --> 00:56:32,380
I:好的

1020
00:56:32,440 --> 00:56:33,920
让我们一会儿回来再看 实际上

1021
00:56:33,990 --> 00:56:36,100
你问的是我从哪里得到的这个函数是吗

1022
00:56:36,160 --> 00:56:38,370
我仅仅是把这个函数写出来

1023
00:56:38,480 --> 00:56:40,670
实际上我们用这个函数

1024
00:56:40,750 --> 00:56:42,460
主要有两个原因

1025
00:56:42,540 --> 00:56:45,280
一个是  我们讨论过一般化的线性模型

1026
00:56:45,370 --> 00:56:46,870
作为一类更宽泛的模型类型

1027
00:56:46,970 --> 00:56:48,700
我们会很自然地想到它

1028
00:56:48,770 --> 00:56:53,030
另外一个原因我们下周会再来讨论

1029
00:56:53,140 --> 00:56:55,910
实际上我认为有几个非常漂亮的原因

1030
00:56:55,990 --> 00:56:58,830
使我们选择了logistic函数

1031
00:56:58,910 --> 00:57:00,190
我们之后会看到一些

1032
00:57:00,270 --> 00:57:02,750
但是现在仅仅让我把它定义出来

1033
00:57:02,840 --> 00:57:04,360
并且现在按照我说的先认为

1034
00:57:04,440 --> 00:57:06,510
这是一个合理的选择 好吗

1035
00:57:06,580 --> 00:57:08,570
但是注意到

1036
00:57:08,690 --> 00:57:11,880
现在我的假设产生的值会

1037
00:57:11,940 --> 00:57:13,220
一直在0和1之间

1038
00:57:13,290 --> 00:57:16,450
进一步地  就像我们之前

1039
00:57:16,460 --> 00:57:18,830
对线性回归做的那样

1040
00:57:18,870 --> 00:57:22,090
我要为我的假设

1041
00:57:22,160 --> 00:57:24,350
和这些输出进行概率意义上的解释

1042
00:57:24,370 --> 00:57:28,660
现在

1043
00:57:28,710 --> 00:57:32,710
我要

1044
00:57:32,760 --> 00:57:38,600
假设:

1045
00:57:38,660 --> 00:57:43,240
换句话说

1046
00:57:43,300 --> 00:57:44,560
我的假设会产生

1047
00:57:44,640 --> 00:57:45,670
0和1之间的数

1048
00:57:45,750 --> 00:57:48,620
我会认为我的假设

1049
00:57:48,670 --> 00:57:51,550
要尝试估计y=1的概率 明白吗

1050
00:57:51,640 --> 00:58:01,920
由于y或者是0  或者是1

1051
00:58:02,000 --> 00:58:05,550
所以y=0的概率会是这样的

1052
00:58:05,600 --> 00:58:13,670
对吗?为了简化形式

1053
00:58:13,750 --> 00:58:16,910
我们可以将这两个公式简洁地写在一起

1054
00:58:16,990 --> 00:58:22,900
P(y|x;θ)

1055
00:58:22,990 --> 00:58:29,260
H(x;θ)的Y*(1- H(x))次方

1056
00:58:29,340 --> 00:58:33,840
H/x;θ的Y*(1- H(x))次方

1057
00:58:33,940 --> 00:58:36,890
将会是1-y  明白吗?

1058
00:58:36,990 --> 00:58:39,140
我知道这看起来多少有些奇怪

1059
00:58:39,200 --> 00:58:41,840
但是这实际上会让变量变得更加漂亮

1060
00:58:41,920 --> 00:58:46,440
所以如果y=1

1061
00:58:46,540 --> 00:58:48,620
那么这个等式就是的1次方

1062
00:58:48,700 --> 00:58:50,300
乘上一些东西的0次方

1063
00:58:50,380 --> 00:58:54,590
任何东西的0次方都是1  对吗

1064
00:58:54,660 --> 00:58:57,620
所以y=1那么这就是一些东西的0次方

1065
00:58:57,680 --> 00:58:59,330
所以就是1

1066
00:58:59,430 --> 00:59:02,360
所以如果y=1

1067
00:59:02,440 --> 00:59:06,190
那么就是说P(y=1|x;θ)等于 明白吗

1068
00:59:06,260 --> 00:59:08,260
同样的道理  如果y=0

1069
00:59:08,350 --> 00:59:12,490
那么P(y=0|x;?)等于这些东西的0次方

1070
00:59:12,600 --> 00:59:14,390
所以消失了

1071
00:59:14,460 --> 00:59:17,330
就等于1乘以这些东西的1次方 明白吗

1072
00:59:17,440 --> 00:59:20,630
所以这是一种简洁的写法

1073
00:59:20,740 --> 00:59:24,460
可以将两个公式写成一行

1074
00:59:24,530 --> 00:59:33,100
所以让我们解决参数拟合的问题吧

1075
00:59:33,180 --> 00:59:34,770
所以你又一次会问

1076
00:59:34,860 --> 00:59:37,040
给定这个基于数据的模型

1077
00:59:37,150 --> 00:59:40,570
我该怎样为我的模型拟合出参数θ呢

1078
00:59:40,640 --> 00:59:45,360
所以参数的似然性  像之前那样

1079
00:59:45,440 --> 00:59:48,610
是数据的概率  对吗

1080
00:59:48,710 --> 00:59:51,510
也就是说

1081
00:59:51,600 --> 00:59:57,990
等于 XI=θ

1082
00:59:58,040 --> 01:00:09,130
只需要把这些带入

1083
01:00:09,230 --> 01:00:11,380
我省略了下标θ

1084
01:00:11,430 --> 01:00:18,230
这样可以少写一点

1085
01:00:18,330 --> 01:00:27,990
哦  抱歉  这里应该是和

1086
01:00:52,970 --> 01:00:55,460
所以  像往常一样

1087
01:00:55,550 --> 01:00:57,800
我们需要找到参数θ的一个极大似然估计

1088
01:00:57,890 --> 01:00:59,470
所以我们需要找到

1089
01:00:59,530 --> 01:01:01,230
参数θ使得似然性

1090
01:01:01,280 --> 01:01:04,660
L(θ)最大化

1091
01:01:04,740 --> 01:01:10,460
实际上这很常见

1092
01:01:10,540 --> 01:01:13,490
当你需要进行推导时

1093
01:01:13,530 --> 01:01:15,340
使似然性的对数最大化

1094
01:01:15,430 --> 01:01:17,370
要比使似然性最大化

1095
01:01:17,460 --> 01:01:18,530
要容易得多

1096
01:01:18,630 --> 01:01:24,420
θ的对数似然率l是对L取对数

1097
01:01:24,500 --> 01:01:48,490
因此  它应该等于这些项的和

1098
01:01:48,600 --> 01:02:04,240
为了拟合出我们模型中的参数θ

1099
01:02:04,310 --> 01:02:06,370
我们需要找到θ

1100
01:02:06,470 --> 01:02:09,150
使得对数似然性最大化 什么

1101
01:02:09,240 --> 01:02:12,110
S: [听不清]

1102
01:02:12,200 --> 01:02:12,770
I:能再说一遍吗?

1103
01:02:12,840 --> 01:02:14,230
S:(注:学生提醒老师多写了)

1104
01:02:14,310 --> 01:02:18,720
I:哦  是的  谢谢

1105
01:02:18,820 --> 01:02:23,080
为了使这个函数最大化

1106
01:02:23,180 --> 01:02:25,910
实际上我们可以使用同样的

1107
01:02:25,980 --> 01:02:30,350
曾经学习过的梯度下降算法

1108
01:02:30,430 --> 01:02:33,060
那是我们学习的第一个算法

1109
01:02:33,140 --> 01:02:34,380
用来使二次函数最小化

1110
01:02:34,460 --> 01:02:35,980
你们记得

1111
01:02:36,090 --> 01:02:37,430
当我们讨论最小二乘时

1112
01:02:37,510 --> 01:02:39,040
我们用的第一个用来使

1113
01:02:39,120 --> 01:02:41,880
二次函数最小化的算法就是梯度下降算法

1114
01:02:41,960 --> 01:02:44,930
所以我们实际上可以使用完全相同的算法

1115
01:02:45,020 --> 01:02:47,080
使对数似然性最大化

1116
01:02:47,140 --> 01:02:51,600
你们记得  那个算法

1117
01:02:51,660 --> 01:02:55,750
就是不断地将θ的值替换为

1118
01:02:55,830 --> 01:03:00,720
它之前的值加上学习率θ

1119
01:03:00,780 --> 01:03:06,350
乘上成本函数的梯度

1120
01:03:06,440 --> 01:03:10,050
对数似然性是和θ有关的

1121
01:03:10,100 --> 01:03:11,000
一个小的改变是

1122
01:03:11,070 --> 01:03:12,980
之前我们尝试

1123
01:03:13,060 --> 01:03:16,620
最小化二次函数的值

1124
01:03:16,700 --> 01:03:19,080
而现在我们尝试最大化而不是最小化

1125
01:03:19,160 --> 01:03:20,680
所以这里不是符号而是正号

1126
01:03:20,770 --> 01:03:22,160
所以这就是梯度下降算法

1127
01:03:22,220 --> 01:03:27,470
但是是为了最大化而不是最小化

1128
01:03:27,570 --> 01:03:29,750
所以我们实际上称它为梯度上升算法

1129
01:03:29,840 --> 01:03:30,990
实际上是同样的算法

1130
01:03:31,100 --> 01:03:37,960
为了指出梯度是什么

1131
01:03:38,070 --> 01:03:40,560
也就是说为了

1132
01:03:40,630 --> 01:03:45,660
导出梯度下降的方向

1133
01:03:45,740 --> 01:03:48,770
你需要关于每个参数的分量

1134
01:03:48,830 --> 01:03:50,310
对目标函数求偏导数θI  对吧

1135
01:03:50,380 --> 01:03:53,830
实际上

1136
01:03:53,920 --> 01:04:01,740
你可以计算出这个偏导数

1137
01:04:01,820 --> 01:04:05,690
按照这个公式  l(θ)

1138
01:04:05,770 --> 01:04:08,950
哦  写错了

1139
01:04:09,070 --> 01:04:11,660
如果对l(θ)

1140
01:04:11,740 --> 01:04:13,270
也就是θ的对数似然性

1141
01:04:13,350 --> 01:04:15,530
关于求偏导

1142
01:04:15,600 --> 01:04:23,150
结果应该等于 θI

1143
01:04:45,580 --> 01:04:48,100
好的 实际上推导过程

1144
01:04:48,100 --> 01:04:49,860
并不是非常复杂

1145
01:04:49,920 --> 01:04:52,840
但是为了节省时间

1146
01:04:52,920 --> 01:04:54,010
防止让你们一直看我

1147
01:04:54,010 --> 01:04:55,230
写几个黑板的数学公式

1148
01:04:55,280 --> 01:04:57,210
我就直接把结果写出来了

1149
01:04:57,280 --> 01:04:59,950
你得到这个的方法

1150
01:05:00,030 --> 01:05:02,570
是将的定义代入到这里 F(θ)

1151
01:05:02,640 --> 01:05:05,150
然后求导

1152
01:05:05,230 --> 01:05:06,940
之后通过算术推导得出下面的公式

1153
01:05:07,030 --> 01:05:11,030
明白吗?

1154
01:05:11,100 --> 01:05:16,700
所以梯度上升的规则

1155
01:05:16,790 --> 01:05:20,620
是这样的

1156
01:05:20,690 --> 01:05:28,450
像这样更新 θJ+α

1157
01:05:44,810 --> 01:05:46,250
明白吗?这个公式

1158
01:05:46,250 --> 01:05:48,400
看起来像其它的什么公式吗

1159
01:05:48,500 --> 01:05:50,920
你们记得在上节课见到

1160
01:05:50,980 --> 01:05:55,460
过这个公式吗?好的

1161
01:05:55,500 --> 01:05:57,910
当我在最小二乘回归中

1162
01:05:57,960 --> 01:06:00,300
进行梯度下降时

1163
01:06:00,370 --> 01:06:04,020
我实际上写出了完全一样的东西

1164
01:06:04,100 --> 01:06:09,790
可能这里有个负号被修改了

1165
01:06:09,870 --> 01:06:12,390
但是本质上我用的是和最小二乘

1166
01:06:12,460 --> 01:06:17,000
回归完全一样的学习规则  对吗

1167
01:06:17,090 --> 01:06:21,720
那么这是一个相同的学习算法吗

1168
01:06:21,800 --> 01:06:22,850
它们有什么区别

1169
01:06:22,910 --> 01:06:23,900
为什么我之前说了那么多

1170
01:06:23,900 --> 01:06:24,660
用来说明最小二乘回归

1171
01:06:24,770 --> 01:06:27,820
对于分类问题是一个糟糕的主意

1172
01:06:27,880 --> 01:06:30,590
而之后我做了那么多数学推导

1173
01:06:30,660 --> 01:06:33,100
并且跳过了一些步骤

1174
01:06:33,150 --> 01:06:33,950
但是之后却在最后声称

1175
01:06:33,950 --> 01:06:34,820
他们实际上是相同的学习算法

1176
01:06:34,890 --> 01:06:38,440
S: [听不清]

1177
01:06:38,520 --> 01:06:39,260
I:能再说一遍吗?

1178
01:06:39,360 --> 01:06:41,150
S: [听不清]

1179
01:06:41,380 --> 01:06:42,910
I:是的  很好

1180
01:06:43,000 --> 01:06:44,510
S: [听不清]

1181
01:06:44,580 --> 01:06:45,300
I:好的  非常准确

1182
01:06:45,380 --> 01:06:47,260
所以就是说  这是不同的  对吗

1183
01:06:47,320 --> 01:06:50,230
原因就是  logistic回归

1184
01:06:50,310 --> 01:06:54,560
和之前的回归不同  对吗

1185
01:06:54,610 --> 01:06:58,080
和我

1186
01:06:58,140 --> 01:07:00,370
上一次课上的

1187
01:07:00,450 --> 01:07:02,030
定义不同

1188
01:07:02,080 --> 01:07:04,800
它并不是  它已经不再

1189
01:07:04,890 --> 01:07:07,830
是一个线性函数了

1190
01:07:07,890 --> 01:07:12,880
它是一个关于的logistic函数

1191
01:07:12,880 --> 01:07:13,710
明白吗

1192
01:07:13,820 --> 01:07:16,930
所以即使它们看起来很相似

1193
01:07:17,010 --> 01:07:19,490
即使他们表面上很相似

1194
01:07:19,560 --> 01:07:23,110
相对于我上一次基于最小二乘回归

1195
01:07:23,170 --> 01:07:25,500
推出的下降规则

1196
01:07:25,580 --> 01:07:26,680
这实际上是一个

1197
01:07:26,680 --> 01:07:28,730
完全不同的学习算法 明白吗

1198
01:07:28,810 --> 01:07:31,670
实际上你会以同样的学习规则结尾

1199
01:07:31,750 --> 01:07:33,270
并不是巧合

1200
01:07:33,360 --> 01:07:36,560
我们之后会更多地讨论通用的学习

1201
01:07:36,640 --> 01:07:38,270
模型

1202
01:07:38,350 --> 01:07:42,060
但是这是一种我们之后将会看到的

1203
01:07:42,160 --> 01:07:43,810
最为优雅的通用的学习模型

1204
01:07:43,900 --> 01:07:45,940
即使我们使用不同的模型

1205
01:07:46,030 --> 01:07:49,870
你实际上都会以一种看起来相同的

1206
01:07:49,960 --> 01:07:51,860
学习算法结束  这并不是巧合

1207
01:07:51,920 --> 01:07:56,230
很好

1208
01:07:56,330 --> 01:08:00,490
最后关于学习过程要说的一点是

1209
01:08:00,580 --> 01:08:04,420
这里我进行了求导

1210
01:08:04,470 --> 01:08:06,070
并且得到了这一行

1211
01:08:06,150 --> 01:08:09,840
我不想让你们将很长时间

1212
01:08:09,890 --> 01:08:11,740
花在代数推导上

1213
01:08:11,830 --> 01:08:14,880
但是晚些时候

1214
01:08:14,960 --> 01:08:17,180
希望你们能回去

1215
01:08:17,240 --> 01:08:19,360
好好看看讲义

1216
01:08:19,430 --> 01:08:20,600
上面有完整的推导过程

1217
01:08:20,680 --> 01:08:22,950
确定你们能看懂每一步

1218
01:08:23,050 --> 01:08:26,250
关于如何对这个对数似然性求偏导

1219
01:08:26,340 --> 01:08:28,630
并且得到这个公式 好吗

1220
01:08:28,680 --> 01:08:33,240
对于你们那些想认真学习

1221
01:08:33,300 --> 01:08:35,530
这些机器学习材料的人

1222
01:08:35,600 --> 01:08:38,030
当你们回去之后

1223
01:08:38,090 --> 01:08:39,970
我想对于大多数人来说

1224
01:08:40,060 --> 01:08:41,230
仔细阅读讲义的每一行

1225
01:08:41,320 --> 01:08:43,000
并且全部看懂

1226
01:08:43,090 --> 01:08:44,180
最终弄清楚怎样得到这一行

1227
01:08:44,240 --> 01:08:46,180
将是非常简单的

1228
01:08:46,260 --> 01:08:47,520
你们需要确定

1229
01:08:47,600 --> 01:08:51,890
你们真的理解这份材料

1230
01:08:51,990 --> 01:08:54,580
我的具体建议是  你们应该回去之后

1231
01:08:54,660 --> 01:08:56,870
认真阅读每一行  并且将推导过程盖起来

1232
01:08:56,950 --> 01:08:59,650
然后看看你是否

1233
01:08:59,720 --> 01:09:02,820
能够自己推导出来  明白吗

1234
01:09:02,910 --> 01:09:05,390
通常情况下  对于学习和机器学习

1235
01:09:05,450 --> 01:09:06,460
有关的技术性材料来说

1236
01:09:06,460 --> 01:09:07,300
这是一个非常好的建议

1237
01:09:07,360 --> 01:09:09,970
你看懂一个证明过程的每一个步骤

1238
01:09:10,030 --> 01:09:11,400
一种确定你真正理解它的方式

1239
01:09:11,510 --> 01:09:13,280
是把证明过程盖起来

1240
01:09:13,350 --> 01:09:14,490
然后看看你是否能够

1241
01:09:14,590 --> 01:09:16,090
独立地推导出全部过程

1242
01:09:16,200 --> 01:09:17,240
这是一个很好的方法

1243
01:09:17,340 --> 01:09:18,840
当我尝试学习各种不同的机器学习理论

1244
01:09:18,940 --> 01:09:20,940
和证明过程时

1245
01:09:21,040 --> 01:09:22,840
我会做很多类似的工作

1246
01:09:22,950 --> 01:09:24,960
实际上这是一种很好的学习方法

1247
01:09:25,030 --> 01:09:26,040
把原始的推导过程盖起来

1248
01:09:26,140 --> 01:09:28,000
看看你是否能够在不看原始推导的情况下

1249
01:09:28,110 --> 01:09:35,110
自己推导出来 好的

1250
01:09:35,200 --> 01:09:38,540
我今天可能不会讲牛顿方法了

1251
01:09:38,630 --> 01:09:41,390
我现在要岔开主题

1252
01:09:57,170 --> 01:09:58,950
讲另外一个算法

1253
01:09:59,060 --> 01:10:02,490
在之前可能已经提到过了

1254
01:10:02,590 --> 01:10:10,760
这个算法就是感知器算法

1255
01:10:10,850 --> 01:10:14,630
关于这个算法

1256
01:10:14,700 --> 01:10:15,970
我今天不会讲太多

1257
01:10:16,060 --> 01:10:17,770
之后我们还会再回过头来讲

1258
01:10:17,880 --> 01:10:21,350
之后我们会来讲学习理论	

1259
01:10:21,420 --> 01:10:28,570
在logistic回归中  我们说g(z)

1260
01:10:28,660 --> 01:10:30,540
也就是我们的假设

1261
01:10:30,630 --> 01:10:32,100
会生成0 1之间的小数

1262
01:10:32,200 --> 01:10:35,110
问题是你如何迫使

1263
01:10:35,220 --> 01:10:40,980
g(z)生成0或1?

1264
01:10:41,000 --> 01:10:46,940
所以感知器算法将g(z)定义成这样的形式

1265
01:10:47,060 --> 01:10:55,270
所以函数的图形看起来不会像

1266
01:10:55,300 --> 01:10:57,090
sigmoid函数那样

1267
01:10:57,190 --> 01:11:04,530
g(z)看起来阶梯函数(step function)一样

1268
01:11:04,620 --> 01:11:06,000
正如你们之前问到的那样

1269
01:11:06,080 --> 01:11:10,230
在这之前  我们可以令  这和之前是一样的

1270
01:11:10,330 --> 01:11:14,780
除了现在g(z)表示阶梯函数

1271
01:11:14,860 --> 01:11:15,780
实际上

1272
01:11:15,880 --> 01:11:17,070
这个算法的

1273
01:11:17,170 --> 01:11:19,190
感知器学习规则

1274
01:11:19,310 --> 01:11:23,220
和logistic回归的经典的

1275
01:11:23,320 --> 01:11:24,840
梯度上升规则

1276
01:11:24,920 --> 01:11:26,490
是相同的

1277
01:11:26,580 --> 01:11:29,520
学习规则

1278
01:11:29,620 --> 01:11:43,820
是这样的

1279
01:11:43,920 --> 01:11:50,630
所以它看起来就像是logistic回归的

1280
01:11:50,720 --> 01:11:55,470
经典梯度上升规则

1281
01:11:55,530 --> 01:11:58,750
这个算法的风格

1282
01:11:58,840 --> 01:12:01,230
和之前的最小二乘回归

1283
01:12:01,340 --> 01:12:03,560
以及logistic回归相比十分不同

1284
01:12:03,650 --> 01:12:06,300
因为它生成的值

1285
01:12:06,390 --> 01:12:08,390
只有0或1

1286
01:12:08,480 --> 01:12:10,160
实际上为它赋予概率意义非常困难

1287
01:12:10,280 --> 01:12:16,370
即使它 哦  对不起

1288
01:12:16,470 --> 01:12:19,510
好的

1289
01:12:19,600 --> 01:12:22,150
即使这个学习规则又一次看起来

1290
01:12:22,240 --> 01:12:23,420
和logistics回归中的学习规则出奇地相似

1291
01:12:23,500 --> 01:12:25,400
但和我们课上学过的

1292
01:12:25,500 --> 01:12:27,860
其他学习规则相比

1293
01:12:27,950 --> 01:12:30,440
它是一类非常不同的学习规则

1294
01:12:30,550 --> 01:12:36,520
它仅仅计算

1295
01:12:36,580 --> 01:12:39,220
并且你的临界值

1296
01:12:39,290 --> 01:12:40,160
与你的输出

1297
01:12:40,250 --> 01:12:42,960
只能是0或1

1298
01:12:43,020 --> 01:12:44,410
所以我认为这些是比

1299
01:12:44,500 --> 01:12:45,430
logistic回归更简单的算法

1300
01:12:45,520 --> 01:12:48,440
当我们之后讲学习理论的时候

1301
01:12:48,490 --> 01:12:51,260
我们会利用这个算法的简单性

1302
01:12:51,350 --> 01:12:54,160
来将其作为基本的构造步骤 好吗

1303
01:12:54,240 --> 01:12:55,800
但是今天关于这个算法

1304
01:12:55,800 --> 01:12:56,730
我就说这么多

