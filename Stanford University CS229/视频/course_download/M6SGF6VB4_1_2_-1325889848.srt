1
00:00:23,360 --> 00:00:24,660
Good morning.

2
00:00:24,890 --> 00:00:27,800
Welcome to CS229, the machine learning class.

3
00:00:28,200 --> 00:00:30,970
So what I wanna do today is just spend

4
00:00:31,110 --> 00:00:32,860
a little time going over the logistics

5
00:00:33,020 --> 00:00:35,530
of the class, and then we'll start to

6
00:00:35,670 --> 00:00:36,860
talk a bit about machine learning.

7
00:00:38,670 --> 00:00:40,940
By way of introduction, my name's Andrew Ng

8
00:00:41,070 --> 00:00:42,450
and I'll be instructor for this class.

9
00:00:42,640 --> 00:00:44,950
And so I personally work in machine learning,

10
00:00:45,120 --> 00:00:47,110
and I've worked on it for about 15 years now,

11
00:00:47,300 --> 00:00:49,330
and I actually think that machine learning

12
00:00:49,510 --> 00:00:52,810
is the most exciting field of all the computer sciences.

13
00:00:52,990 --> 00:00:55,790
So I'm actually always excited about teaching this class.

14
00:00:56,050 --> 00:00:58,580
Sometimes I actually think that machine learning

15
00:00:58,760 --> 00:01:01,500
is not only the most exciting thing in computer science,

16
00:01:01,660 --> 00:01:03,640
but the most exciting thing in all of human endeavor,

17
00:01:03,830 --> 00:01:06,270
so maybe a little bias there.

18
00:01:06,600 --> 00:01:08,960
I also want to introduce the TAs,

19
00:01:09,240 --> 00:01:11,560
who are all graduate students doing research in

20
00:01:11,720 --> 00:01:13,100
or related to the machine learning and all

21
00:01:13,360 --> 00:01:14,750
aspects of machine learning.

22
00:01:15,050 --> 00:01:17,940
Paul Baumstarck works in

23
00:01:18,080 --> 00:01:19,720
machine learning and computer vision.

24
00:01:19,930 --> 00:01:22,540
Catie Chang is actually a neuroscientist

25
00:01:22,890 --> 00:01:24,630
who applies machine learning algorithms

26
00:01:24,770 --> 00:01:26,440
to try to understand the human brain.

27
00:01:26,940 --> 00:01:29,040
Tom Do is another PhD student,

28
00:01:29,270 --> 00:01:30,490
works in computational biology

29
00:01:30,690 --> 00:01:33,940
and in sort of the basic fundamentals of human learning.

30
00:01:34,290 --> 00:01:36,820
Zico Kolter is the head TA —

31
00:01:37,050 --> 00:01:39,660
he's head TA two years in a row now —

32
00:01:40,110 --> 00:01:41,250
works in machine learning

33
00:01:41,460 --> 00:01:43,630
and applies them to a bunch of robots.

34
00:01:43,820 --> 00:01:47,380
And Daniel Ramage is — I guess he's not here —

35
00:01:47,650 --> 00:01:49,410
Daniel applies learning algorithms

36
00:01:49,570 --> 00:01:51,300
to problems in natural language processing.

37
00:01:53,150 --> 00:01:54,380
So you'll get to know the TAs

38
00:01:54,560 --> 00:01:56,130
and me much better throughout this quarter,

39
00:01:56,310 --> 00:01:59,510
but just from the sorts of things the TA's do,

40
00:01:59,780 --> 00:02:01,070
I hope you can already tell that

41
00:02:01,240 --> 00:02:04,370
machine learning is a highly interdisciplinary

42
00:02:04,580 --> 00:02:09,330
topic in which just the TAs find learning algorithms

43
00:02:09,500 --> 00:02:10,820
to problems in computer vision

44
00:02:11,050 --> 00:02:13,260
and biology and robotsand language.

45
00:02:13,560 --> 00:02:15,760
And machine learning is one of those things

46
00:02:15,970 --> 00:02:19,020
that has and is having a large

47
00:02:19,210 --> 00:02:21,100
impact on many applications.

48
00:02:21,680 --> 00:02:23,930
So just in my own daily work,

49
00:02:24,160 --> 00:02:26,440
I actually frequently end up talking to people like

50
00:02:26,660 --> 00:02:29,500
helicopter pilots to biologists to people in computer

51
00:02:29,670 --> 00:02:32,240
systems or databases to economists

52
00:02:32,490 --> 00:02:35,760
and sort of also an unending stream of

53
00:02:35,960 --> 00:02:38,430
people from industry coming to Stanford

54
00:02:38,690 --> 00:02:40,040
interested in applying machine learning

55
00:02:40,230 --> 00:02:44,260
methods to their own problems.

56
00:02:45,600 --> 00:02:48,330
So yeah, this is fun.

57
00:02:48,550 --> 00:02:49,430
A couple of weeks ago,

58
00:02:49,620 --> 00:02:51,290
a student actually forwarded to me an article

59
00:02:51,500 --> 00:02:53,150
in "Computer World"

60
00:02:53,340 --> 00:02:59,140
about the 12 IT skills that employers can't say no to.

61
00:02:59,350 --> 00:03:02,230
So it's about sort of the 12 most desirable

62
00:03:02,440 --> 00:03:05,380
skills in all of IT and all of information technology,

63
00:03:05,760 --> 00:03:08,380
and topping the list was actually machine learning.

64
00:03:08,640 --> 00:03:10,360
So I think this is a good time to be

65
00:03:10,560 --> 00:03:12,640
learning this stuff and learning algorithms

66
00:03:12,820 --> 00:03:13,980
and having a large impact on

67
00:03:14,170 --> 00:03:16,600
many segments of science and industry.

68
00:03:17,520 --> 00:03:19,520
I'm actually curious about something.

69
00:03:21,430 --> 00:03:22,890
Learning algorithms is one of the things that

70
00:03:23,090 --> 00:03:26,040
touches many areas of science and industries,

71
00:03:26,250 --> 00:03:27,290
and I'm just kind of curious.

72
00:03:27,470 --> 00:03:29,530
How many people here are computer science majors,

73
00:03:29,710 --> 00:03:31,530
are in the computer science department? Okay.

74
00:03:31,720 --> 00:03:32,850
About half of you.

75
00:03:33,020 --> 00:03:34,790
How many people are from EE?

76
00:03:35,520 --> 00:03:37,680
Oh, okay, maybe about a fifth.

77
00:03:37,960 --> 00:03:40,440
How many biologers are there here?

78
00:03:41,110 --> 00:03:42,820
Wow, just a few, not many.

79
00:03:43,020 --> 00:03:45,720
I'm surprised. Anyone from statistics?

80
00:03:46,070 --> 00:03:48,850
Okay, a few. So where are the rest of you from?

81
00:03:49,900 --> 00:03:51,350
Student : iCME.

82
00:03:51,550 --> 00:03:52,600
Instructor (Andrew Ng) : Say again?

83
00:03:52,740 --> 00:03:53,540
Student : iCME.

84
00:03:53,680 --> 00:03:55,020
Instructor (Andrew Ng) : iCME. Cool.

85
00:03:55,460 --> 00:03:56,280
Student : [Inaudible].

86
00:03:56,440 --> 00:03:57,610
Instructor (Andrew Ng) : Civi and what else?

87
00:03:57,770 --> 00:03:58,640
Student : [Inaudible]

88
00:03:58,790 --> 00:04:00,710
Instructor (Andrew Ng) : Synthesis, systems. Yeah, cool.

89
00:04:01,000 --> 00:04:01,930
Student : Chemi.

90
00:04:02,110 --> 00:04:03,140
Instructor (Andrew Ng) : Chemi. Cool.

91
00:04:03,300 --> 00:04:04,130
Student : [Inaudible].

92
00:04:04,300 --> 00:04:05,510
Instructor (Andrew Ng) : Aero/astro. Yes, right.

93
00:04:07,200 --> 00:04:09,190
Yeah, okay, cool. Anyone else?

94
00:04:09,360 --> 00:04:10,130
Student : [Inaudible].

95
00:04:10,290 --> 00:04:11,070
Instructor (Andrew Ng) : Pardon?

96
00:04:11,190 --> 00:04:11,950
MSNE. All right. Cool. Yeah.

97
00:04:12,110 --> 00:04:13,300
Student : [Inaudible].

98
00:04:13,430 --> 00:04:14,690
Instructor (Andrew Ng) : Pardon?

99
00:04:14,840 --> 00:04:15,730
Student : [Inaudible].

100
00:04:15,880 --> 00:04:16,920
Instructor (Andrew Ng) : Endo —

101
00:04:17,060 --> 00:04:18,050
Student : [Inaudible].

102
00:04:18,180 --> 00:04:19,910
Instructor (Andrew Ng) : Oh, I see, industry.

103
00:04:20,080 --> 00:04:21,800
Okay. Cool. Great, great.

104
00:04:21,960 --> 00:04:24,640
So as you can tell from a cross-section of this class,

105
00:04:24,810 --> 00:04:27,480
I think we're a very diverse audience in this room,

106
00:04:27,720 --> 00:04:28,830
and that's one of the things that

107
00:04:29,000 --> 00:04:31,980
makes this class fun to teach and fun to be in, I think.

108
00:04:33,480 --> 00:04:34,770
So in this class,

109
00:04:34,930 --> 00:04:36,120
we've tried to convey to you a broad

110
00:04:36,280 --> 00:04:38,760
set of principles and tools that will

111
00:04:38,910 --> 00:04:40,690
be useful for doing many, many things.

112
00:04:41,030 --> 00:04:43,870
And every time I teach this class,

113
00:04:44,070 --> 00:04:46,010
I can actually very confidently say that

114
00:04:46,300 --> 00:04:48,330
after December, no matter what

115
00:04:48,500 --> 00:04:50,280
you're going to do after this December

116
00:04:50,520 --> 00:04:52,280
when you've sort of completed this class,

117
00:04:52,570 --> 00:04:56,450
you'll find the things you learn in this class very useful,

118
00:04:56,940 --> 00:04:59,390
and these things will be useful pretty much no matter

119
00:04:59,580 --> 00:05:01,360
what you end up doing later in your life.

120
00:05:02,630 --> 00:05:06,920
So I have more logistics to go over later,

121
00:05:07,190 --> 00:05:09,350
but let's say a few more words about machine learning.

122
00:05:09,720 --> 00:05:13,490
I feel that machine learning grew out of

123
00:05:13,780 --> 00:05:17,140
early work in AI, early work in artificial intelligence.

124
00:05:17,680 --> 00:05:18,980
And over the last —

125
00:05:19,150 --> 00:05:21,940
I wanna say last 15 or last 20 years or so,

126
00:05:22,230 --> 00:05:25,120
it's been viewed as a sort of growing

127
00:05:25,390 --> 00:05:28,070
new capability for computers.

128
00:05:28,310 --> 00:05:30,300
And in particular, it turns out that

129
00:05:30,490 --> 00:05:31,540
there are many programs or there are

130
00:05:31,770 --> 00:05:33,780
many applications that you can't program by hand.

131
00:05:34,810 --> 00:05:38,450
For example, if you want to get a computer to read

132
00:05:38,670 --> 00:05:41,270
handwritten characters, to read sort of handwritten digits,

133
00:05:42,320 --> 00:05:44,060
that actually turns out to be amazingly difficult

134
00:05:44,380 --> 00:05:48,860
to write a piece of software to take this input,

135
00:05:49,040 --> 00:05:50,540
an image of something that I wrote

136
00:05:50,720 --> 00:05:52,510
and to figure out just what it is,

137
00:05:52,680 --> 00:05:55,210
to translate my cursive handwriting into —

138
00:05:58,140 --> 00:06:01,290
to extract the characters I wrote out in longhand.

139
00:06:02,780 --> 00:06:04,020
And other things:

140
00:06:04,230 --> 00:06:05,670
One thing that my students and I do

141
00:06:05,850 --> 00:06:06,830
is autonomous flight.

142
00:06:07,010 --> 00:06:08,410
It turns out to be extremely difficult

143
00:06:08,830 --> 00:06:12,280
to sit down and write a program to fly a helicopter.

144
00:06:13,740 --> 00:06:15,100
But in contrast,

145
00:06:15,290 --> 00:06:16,210
if you want to do things like to

146
00:06:16,390 --> 00:06:18,920
get software to fly a helicopter

147
00:06:19,100 --> 00:06:21,190
or have software recognize handwritten digits,

148
00:06:21,540 --> 00:06:24,700
one very successful approach is to

149
00:06:24,890 --> 00:06:27,100
use a learning algorithm and have a computer learn

150
00:06:27,280 --> 00:06:28,490
by itself how to, say,

151
00:06:28,660 --> 00:06:30,160
recognize your handwriting.

152
00:06:30,370 --> 00:06:32,150
And in fact, handwritten digit recognition,

153
00:06:32,370 --> 00:06:33,770
this is pretty much the only

154
00:06:33,970 --> 00:06:35,130
approach that works well.

155
00:06:35,320 --> 00:06:37,970
It uses applications that are hard to program by hand.

156
00:06:39,220 --> 00:06:42,730
Learning algorithms has also made I

157
00:06:42,900 --> 00:06:44,610
guess significant inroads in

158
00:06:44,830 --> 00:06:47,080
what's sometimes called database mining.

159
00:06:48,170 --> 00:06:49,110
So, for example,

160
00:06:49,280 --> 00:06:50,990
with the growth of IT and computers,

161
00:06:51,290 --> 00:06:54,090
increasingly many hospitals are keeping around

162
00:06:54,280 --> 00:06:55,330
medical records of

163
00:06:55,500 --> 00:06:57,750
what sort of patients, what problems they had,

164
00:06:57,960 --> 00:07:00,010
what their prognoses was, what the outcome was.

165
00:07:00,540 --> 00:07:02,540
And taking all of these medical records,

166
00:07:02,790 --> 00:07:06,370
which started to be digitized only about maybe 15 years,

167
00:07:06,600 --> 00:07:09,720
applying learning algorithms to them can

168
00:07:09,910 --> 00:07:13,140
turn raw medical records into what I might loosely call

169
00:07:13,340 --> 00:07:14,590
medical knowledge in which

170
00:07:14,790 --> 00:07:15,810
we start to detect

171
00:07:15,970 --> 00:07:17,000
trends in medical practice

172
00:07:17,190 --> 00:07:18,790
and even start to alter medical practice

173
00:07:18,970 --> 00:07:21,110
as a result of medical knowledge

174
00:07:21,310 --> 00:07:24,720
that's derived by applying earning algorithms to the sorts

175
00:07:26,640 --> 00:07:29,640
of medical records that hospitals have just been building

176
00:07:29,830 --> 00:07:33,400
over the last 15, 20 years in an electronic format.

177
00:07:35,970 --> 00:07:37,160
Turns out that most of you

178
00:07:37,370 --> 00:07:39,890
probably use learning algorithms — I don't know —

179
00:07:40,070 --> 00:07:41,540
I think half a dozen times a day or maybe

180
00:07:41,720 --> 00:07:42,970
a dozen times a day or more,

181
00:07:43,270 --> 00:07:45,640
and often without knowing it.

182
00:07:46,510 --> 00:07:47,950
So, for example, every time

183
00:07:48,140 --> 00:07:50,670
you send mail via the US Postal System,

184
00:07:51,370 --> 00:07:55,490
turns out there's an algorithm that tries to automatically

185
00:07:55,650 --> 00:07:58,780
read the zip code you wrote on your envelope,

186
00:07:59,130 --> 00:08:01,490
and that's done by a learning algorithm.

187
00:08:01,680 --> 00:08:03,060
So every time you send US mail,

188
00:08:03,250 --> 00:08:04,560
you are using a learning algorithm,

189
00:08:04,740 --> 00:08:07,570
perhaps without even being aware of it.

190
00:08:08,660 --> 00:08:10,320
Similarly, every time you write a check,

191
00:08:10,530 --> 00:08:11,870
I actually don't know the number for this,

192
00:08:12,060 --> 00:08:14,140
but a significant fraction of checks that you write

193
00:08:14,900 --> 00:08:17,460
are processed by a learning algorithm

194
00:08:17,650 --> 00:08:19,600
that's learned to read the digits,

195
00:08:19,790 --> 00:08:22,010
so the dollar amount that you wrote down on your check

196
00:08:22,210 --> 00:08:23,030
So every time you write a check,

197
00:08:23,220 --> 00:08:24,910
there's another learning algorithm that you're probably

198
00:08:25,080 --> 00:08:26,790
using without even being aware of it.

199
00:08:27,430 --> 00:08:30,200
If you use a credit card,

200
00:08:30,410 --> 00:08:32,680
or I know at least one phone company was doing this

201
00:08:32,980 --> 00:08:34,750
and lots of companies like eBay as well

202
00:08:34,960 --> 00:08:36,640
that do electronic transactions,

203
00:08:37,180 --> 00:08:38,960
there's a good chance that there's a learning

204
00:08:39,160 --> 00:08:41,060
algorithm in the background trying to figure out if,

205
00:08:41,260 --> 00:08:42,620
say, your credit card's been stolen

206
00:08:42,780 --> 00:08:45,540
or if someone's engaging in a fraudulent transaction.

207
00:08:46,370 --> 00:08:49,340
If you use a website like Amazon

208
00:08:49,530 --> 00:08:51,200
or Netflix that will

209
00:08:51,380 --> 00:08:53,440
often recommend books for you to buy

210
00:08:53,650 --> 00:08:55,310
or movies for you to rent or whatever,

211
00:08:55,520 --> 00:08:57,730
these are other examples of learning algorithms

212
00:08:58,030 --> 00:08:59,890
that have learned what sorts of things

213
00:09:00,080 --> 00:09:01,750
you like to buy or what sorts of movies you like

214
00:09:01,950 --> 00:09:03,330
to watch and can therefore

215
00:09:03,520 --> 00:09:05,560
give customized recommendations to you.

216
00:09:06,780 --> 00:09:09,380
Just about a week ago, I had my car serviced,

217
00:09:09,580 --> 00:09:10,470
and even there,

218
00:09:10,640 --> 00:09:12,650
my car mechanic was trying to explain to me

219
00:09:13,040 --> 00:09:15,760
some learning algorithm in the innards of my car

220
00:09:15,960 --> 00:09:17,470
that's sort of doing its best to optimize

221
00:09:17,650 --> 00:09:19,880
my driving performance for fuel efficiency or something.

222
00:09:20,080 --> 00:09:20,920
So, see,

223
00:09:21,110 --> 00:09:23,480
most of us use learning algorithms

224
00:09:23,660 --> 00:09:25,910
half a dozen, a dozen, maybe dozens of times

225
00:09:26,270 --> 00:09:27,890
without even knowing it.

226
00:09:28,380 --> 00:09:29,720
And of course, learning algorithms

227
00:09:29,930 --> 00:09:31,300
are also doing things like giving us

228
00:09:31,550 --> 00:09:33,990
a growing understanding of the human genome.

229
00:09:34,240 --> 00:09:36,780
So if someday we ever find a cure for cancer,

230
00:09:37,100 --> 00:09:38,420
I bet learning algorithms

231
00:09:38,620 --> 00:09:40,340
will have had a large role in that.

232
00:09:40,530 --> 00:09:42,080
That's sort of the thing that Tom works on, yes?

233
00:09:44,860 --> 00:09:49,160
So in teaching this class, I sort of have three goals.

234
00:09:49,670 --> 00:09:52,280
One of them is just to I hope

235
00:09:52,460 --> 00:09:53,350
convey some of my own

236
00:09:53,510 --> 00:09:55,530
excitement about machine learning to you.

237
00:09:55,740 --> 00:09:59,230
The second goal is by the end of this class,

238
00:09:59,440 --> 00:10:01,980
I hope all of you will be able to apply

239
00:10:02,230 --> 00:10:05,930
state-of the-art machine learning algorithms to

240
00:10:06,220 --> 00:10:08,100
whatever problems you're interested in.

241
00:10:08,310 --> 00:10:09,910
And if you ever need to build a

242
00:10:10,120 --> 00:10:11,870
system for reading zip codes,

243
00:10:12,200 --> 00:10:14,230
you'll know how to do that by the end of this class.

244
00:10:15,730 --> 00:10:18,560
And lastly, by the end of this class,

245
00:10:19,360 --> 00:10:21,790
I realize that only a subset of you are interested

246
00:10:22,010 --> 00:10:23,810
in doing research in machine learning,

247
00:10:24,120 --> 00:10:25,770
but by the conclusion of this class, I

248
00:10:25,960 --> 00:10:29,130
hope that all of you will actually be well qualified

249
00:10:29,390 --> 00:10:32,260
to start doing research in machine learning, okay?

250
00:10:34,830 --> 00:10:35,740
So let's say a few

251
00:10:35,930 --> 00:10:37,330
words about logistics.

252
00:10:37,520 --> 00:10:39,380
The prerequisites of this class are written

253
00:10:39,570 --> 00:10:43,650
on one of the handouts, are as follows:

254
00:10:45,280 --> 00:10:45,950
In this class,

255
00:10:46,130 --> 00:10:47,200
I'm going to assume that all of you

256
00:10:47,380 --> 00:10:48,290
have sort of

257
00:10:48,480 --> 00:10:51,130
basic knowledge of computer science and

258
00:10:51,340 --> 00:10:55,180
knowledge of the basic computer skills and principles

259
00:10:55,960 --> 00:10:56,890
So I assume all of you

260
00:10:57,100 --> 00:10:58,390
know what big?O notation,

261
00:10:58,570 --> 00:11:00,290
that all of you know about sort of data structures

262
00:11:00,470 --> 00:11:02,380
like queues, stacks, binary trees,

263
00:11:02,570 --> 00:11:04,600
and that all of you know enough programming skills to

264
00:11:04,760 --> 00:11:07,050
like, write a simple computer program.

265
00:11:07,560 --> 00:11:10,800
And it turns out that most of this class will not

266
00:11:11,030 --> 00:11:12,580
be very programming intensive,

267
00:11:12,800 --> 00:11:14,490
although we will do some programming,

268
00:11:14,710 --> 00:11:16,160
mostly in either MATLAB

269
00:11:16,350 --> 00:11:17,370
or Octave.

270
00:11:17,520 --> 00:11:19,190
I'll say a bit more about that later.

271
00:11:19,360 --> 00:11:21,310
I also assume familiarity with

272
00:11:21,500 --> 00:11:22,990
basic probability and statistics.

273
00:11:23,820 --> 00:11:26,130
So most undergraduate statistics class,

274
00:11:26,490 --> 00:11:28,530
like Stat 116 taught here at Stanford,

275
00:11:29,110 --> 00:11:30,380
will be more than enough.

276
00:11:30,580 --> 00:11:31,470
I'm gonna assume all of you

277
00:11:31,640 --> 00:11:32,950
know what random variables are,

278
00:11:33,130 --> 00:11:34,920
that all of you know what expectation is,

279
00:11:35,070 --> 00:11:36,660
what a variance or a random variable is.

280
00:11:37,740 --> 00:11:39,340
And in case of some of you,

281
00:11:39,510 --> 00:11:41,220
it's been a while since you've seen some of this material

282
00:11:41,490 --> 00:11:43,890
At some of the discussion sections,

283
00:11:44,470 --> 00:11:47,460
we'll actually go over some of the prerequisites,

284
00:11:47,750 --> 00:11:50,870
sort of as a refresher course under prerequisite class.

285
00:11:51,070 --> 00:11:52,930
I'll say a bit more about that later as well.

286
00:11:54,090 --> 00:11:57,660
Lastly, I also assume familiarity with

287
00:11:57,860 --> 00:11:59,320
basic linear algebra.

288
00:11:59,520 --> 00:12:01,260
And again, most undergraduate

289
00:12:01,520 --> 00:12:03,570
linear algebra courses are more than enough.

290
00:12:03,800 --> 00:12:06,750
So if you've taken courses like Math 51,

291
00:12:07,020 --> 00:12:10,970
103, Math 113 or CS205 at Stanford,

292
00:12:11,200 --> 00:12:12,800
that would be more than enough.

293
00:12:13,110 --> 00:12:14,380
Basically, I'm gonna assume that all of you

294
00:12:14,570 --> 00:12:16,160
know what matrixes and vectors are,

295
00:12:16,460 --> 00:12:17,380
that you know how to

296
00:12:17,580 --> 00:12:20,160
multiply matrices and vectors and multiply matrix

297
00:12:20,360 --> 00:12:21,480
and matrices,

298
00:12:21,650 --> 00:12:23,290
that you know what a matrix inverse is.

299
00:12:23,460 --> 00:12:25,210
If you know what an eigenvector of a matrix is,

300
00:12:25,390 --> 00:12:26,380
that'd be even better.

301
00:12:26,570 --> 00:12:28,970
But if you don't quite know or if you're not quite sure

302
00:12:29,170 --> 00:12:30,250
that's fine, too.

303
00:12:30,430 --> 00:12:32,230
We'll go over it in the review sections.

304
00:12:33,660 --> 00:12:44,320
So there are a couple more logistical things

305
00:12:44,520 --> 00:12:46,290
I should deal with in this class.

306
00:12:46,950 --> 00:12:50,800
One is that, as most of you know,

307
00:12:51,050 --> 00:12:53,770
CS229 is a televised class.

308
00:12:53,970 --> 00:12:55,010
And in fact, I guess many of you are

309
00:12:55,200 --> 00:12:58,160
probably watching this at home on TV,

310
00:12:58,340 --> 00:13:00,200
so I'm gonna say hi to our home viewers.

311
00:13:01,040 --> 00:13:03,830
So earlier this year, I approached SCPD,

312
00:13:04,030 --> 00:13:07,240
which televises these classes, about trying to

313
00:13:07,430 --> 00:13:09,320
make a small number of Stanford classes

314
00:13:09,550 --> 00:13:12,490
publicly available or posting the videos on the web.

315
00:13:13,060 --> 00:13:16,350
And so this year, Stanford is actually starting

316
00:13:16,560 --> 00:13:18,420
a small pilot program in which

317
00:13:18,600 --> 00:13:21,160
we'll post videos of a small number of classes online,

318
00:13:21,400 --> 00:13:23,400
so on the Internet in a way that

319
00:13:23,580 --> 00:13:25,340
makes it publicly accessible to everyone.

320
00:13:25,530 --> 00:13:27,110
I'm very excited about that

321
00:13:27,280 --> 00:13:29,030
because machine learning in school,

322
00:13:29,330 --> 00:13:30,520
let's get the word out there.

323
00:13:30,790 --> 00:13:33,540
One of the consequences of this is that —

324
00:13:33,780 --> 00:13:34,570
let's see —

325
00:13:34,770 --> 00:13:38,110
so videos or pictures of the students in this classroom

326
00:13:38,300 --> 00:13:39,690
will not be posted online,

327
00:13:39,890 --> 00:13:42,680
so your images — so don't worry about being

328
00:13:42,830 --> 00:13:45,270
by seeing your own face appear on YouTube one day

329
00:13:45,540 --> 00:13:48,940
But the microphones may pick up your voices,

330
00:13:51,410 --> 00:13:53,500
so I guess the consequence of that is that

331
00:13:53,690 --> 00:13:56,570
because microphones may pick up your voices,

332
00:13:56,770 --> 00:13:58,720
no matter how irritated you are at me,

333
00:13:58,910 --> 00:14:01,050
don't yell out swear words in the middle of class,

334
00:14:02,040 --> 00:14:03,950
but because there won't be video

335
00:14:04,130 --> 00:14:05,910
you can safely sit there and make faces at me,

336
00:14:06,090 --> 00:14:08,160
and that won't show, okay?

337
00:14:10,000 --> 00:14:12,200
Let's see. I also handed out this —

338
00:14:12,590 --> 00:14:15,210
there were two handouts I hope most of you have,

339
00:14:16,010 --> 00:14:17,830
course information handout.

340
00:14:18,080 --> 00:14:20,170
So let me just say a few words about parts of these.

341
00:14:21,880 --> 00:14:24,120
On the third page, there's a section

342
00:14:24,340 --> 00:14:26,060
that says Online Resources.

343
00:14:32,610 --> 00:14:35,650
Oh, okay. Louder?

344
00:14:36,570 --> 00:14:38,600
Actually, could you turn up the volume?

345
00:14:38,810 --> 00:14:40,340
Testing. Is this better?

346
00:14:41,310 --> 00:14:43,980
Testing, testing. Okay, cool. Thanks.

347
00:14:45,200 --> 00:14:48,910
So all right, online resources.

348
00:14:49,130 --> 00:14:51,280
The class has a home page, so it's in on the handouts.

349
00:14:51,470 --> 00:14:53,140
I won't write on the chalkboard

350
00:14:53,500 --> 00:14:56,650
http://cs229.stanford.edu.

351
00:14:57,190 --> 00:15:00,070
And so when there are homework assignments

352
00:15:00,330 --> 00:15:01,360
or things like that,

353
00:15:01,520 --> 00:15:03,310
we usually won't sort of —

354
00:15:03,460 --> 00:15:06,060
in the mission of saving trees,

355
00:15:06,360 --> 00:15:09,860
we will usually not give out many handouts in class.

356
00:15:10,470 --> 00:15:12,630
So homework assignments, homework solutions

357
00:15:13,070 --> 00:15:16,320
will be posted online at the course home page.

358
00:15:17,880 --> 00:15:19,790
As far as this class, I've also written,

359
00:15:19,980 --> 00:15:22,510
and I guess I've also revised every year

360
00:15:22,860 --> 00:15:24,780
a set of fairly detailed lecture notes

361
00:15:24,980 --> 00:15:27,140
that cover the technical content of this class.

362
00:15:27,580 --> 00:15:29,540
And so if you visit the course homepage,

363
00:15:29,770 --> 00:15:31,450
you'll also find the detailed lecture notes

364
00:15:31,650 --> 00:15:36,160
that go over in detail all the math and equations

365
00:15:36,360 --> 00:15:38,160
and so on that I'll be doing in class.

366
00:15:38,540 --> 00:15:40,120
There's also a newsgroup,

367
00:15:40,320 --> 00:15:42,820
su.class.cs229, also written on the handout.

368
00:15:43,500 --> 00:15:46,990
This is a newsgroup that's sort of a forum for people

369
00:15:47,200 --> 00:15:50,550
in the class to get to know each other and have whatever

370
00:15:50,730 --> 00:15:52,860
discussions you want to have amongst yourselves.

371
00:15:53,440 --> 00:15:55,120
So the class newsgroup

372
00:15:55,330 --> 00:15:57,360
will not be monitored by the TAs and me.

373
00:15:57,870 --> 00:16:00,350
But this is a place for you to form study groups

374
00:16:00,560 --> 00:16:02,140
or find project partners or discuss

375
00:16:02,330 --> 00:16:03,930
homework problemsand so on,

376
00:16:04,110 --> 00:16:05,760
and it's not monitored by the TAs and me.

377
00:16:05,970 --> 00:16:08,900
So feel free to talk trash about this class there.

378
00:16:09,140 --> 00:16:12,480
If you want to contact the teaching staff,

379
00:16:12,860 --> 00:16:15,460
please use the email address written down here,

380
00:16:15,690 --> 00:16:19,330
cs229-qa@cs.stanford.edu.

381
00:16:20,270 --> 00:16:22,060
This goes to an account that's

382
00:16:22,250 --> 00:16:23,660
read by all the TAs and me.

383
00:16:23,870 --> 00:16:25,990
So rather than sending us email individually,

384
00:16:26,370 --> 00:16:28,410
if you send email to this account,

385
00:16:28,680 --> 00:16:30,560
it will actually let us get back to you

386
00:16:31,100 --> 00:16:34,480
maximally quickly with answers to your questions.

387
00:16:36,310 --> 00:16:38,530
If you're asking questions about homework problems

388
00:16:38,750 --> 00:16:39,980
please say in the subject line

389
00:16:40,160 --> 00:16:43,390
which assignment and which question the email refers to,

390
00:16:43,610 --> 00:16:46,120
since that will also help us to route your question

391
00:16:46,310 --> 00:16:48,170
to the appropriate TA or to me appropriately

392
00:16:48,440 --> 00:16:50,950
and get the response back to you quickly.

393
00:16:51,460 --> 00:16:55,180
Let's see. Skipping ahead — let's see —

394
00:16:55,380 --> 00:16:57,750
for homework, one midterm, one open and term project

395
00:16:58,160 --> 00:17:00,130
Notice on the honor code.

396
00:17:01,090 --> 00:17:04,470
So one thing that I think will help you to succeed

397
00:17:04,720 --> 00:17:06,390
and do well in this class and even help you

398
00:17:06,580 --> 00:17:09,860
to enjoy this class more is if you form a study group.

399
00:17:10,540 --> 00:17:12,840
So start looking around where you're sitting now

400
00:17:13,070 --> 00:17:14,890
or at the end of class today,

401
00:17:15,140 --> 00:17:18,490
mingle a little bit and get to know your classmates.

402
00:17:18,790 --> 00:17:20,800
I strongly encourage you to form study groups

403
00:17:20,990 --> 00:17:23,460
and sort of have a group of people to study with

404
00:17:23,680 --> 00:17:26,560
and have a group of your fellow students

405
00:17:26,720 --> 00:17:28,570
to talk over these concepts with.

406
00:17:28,840 --> 00:17:31,310
You can also post on the class newsgroup

407
00:17:31,510 --> 00:17:33,840
if you want to use that to try to form a study group.

408
00:17:34,150 --> 00:17:36,080
But some of the problems sets

409
00:17:36,270 --> 00:17:39,150
in this class are reasonably difficult.

410
00:17:40,220 --> 00:17:41,290
People that have taken the class before

411
00:17:41,460 --> 00:17:42,810
may tell you they were very difficult.

412
00:17:43,010 --> 00:17:46,560
And just I bet it would be more fun for you,

413
00:17:46,740 --> 00:17:49,030
and you'd probably have a better learning experience

414
00:17:49,220 --> 00:17:52,090
if you form a study group of people to work with.

415
00:17:52,300 --> 00:17:53,810
So I definitely encourage you to do that.

416
00:17:54,630 --> 00:17:57,090
And just to say a word on the honor code,

417
00:17:57,340 --> 00:17:59,880
which is I definitely encourage you to form

418
00:18:00,110 --> 00:18:01,480
a study group and work together,

419
00:18:01,680 --> 00:18:03,380
discuss homework problems together.

420
00:18:04,040 --> 00:18:05,980
But if you discuss homework problems

421
00:18:06,330 --> 00:18:08,200
with other students,

422
00:18:08,480 --> 00:18:11,640
then I'll ask you to sort of go home

423
00:18:12,110 --> 00:18:14,620
and write down your own solutions independently

424
00:18:14,970 --> 00:18:16,520
without referring to notes that were

425
00:18:16,690 --> 00:18:18,870
taken in any of your joint study sessions.

426
00:18:19,570 --> 00:18:22,030
So in other words, when you turn in a

427
00:18:22,220 --> 00:18:24,230
homework problem, what you turn in should be

428
00:18:24,420 --> 00:18:26,330
something that was reconstructed

429
00:18:26,570 --> 00:18:28,490
independently by yourself

430
00:18:28,720 --> 00:18:29,870
and without referring to notes that

431
00:18:30,070 --> 00:18:32,170
you took during your study sessions with other people

432
00:18:32,430 --> 00:18:33,660
okay? And obviously,

433
00:18:33,950 --> 00:18:35,560
showing your solutions to others or

434
00:18:35,740 --> 00:18:37,830
copying other solutions directly is right out.

435
00:18:39,020 --> 00:18:41,300
We occasionally also reuse problem set questions

436
00:18:41,520 --> 00:18:42,760
problem set from previous years

437
00:18:42,970 --> 00:18:44,970
so that the problems are a bit more debugged

438
00:18:45,170 --> 00:18:46,970
and work more smoothly.

439
00:18:47,340 --> 00:18:50,100
And as a result of that, I also ask you not to look at

440
00:18:50,270 --> 00:18:52,560
solutions from previous years,

441
00:18:52,810 --> 00:18:55,270
and this includes both sort of official solutions

442
00:18:55,500 --> 00:18:57,380
that we've given out to previous generations

443
00:18:57,570 --> 00:19:01,190
of this class and previous solutions that people

444
00:19:01,380 --> 00:19:02,870
that have taken this class in previous

445
00:19:03,060 --> 00:19:04,980
years may have written out by themselves, okay?

446
00:19:05,340 --> 00:19:09,190
Sadly, in this class, there are usually —

447
00:19:09,760 --> 00:19:12,010
sadly, in previous years, there have often been

448
00:19:12,200 --> 00:19:13,970
a few honor code violations in this class.

449
00:19:14,210 --> 00:19:16,200
And last year, I think I prosecuted

450
00:19:16,410 --> 00:19:17,800
five honor code violations,

451
00:19:17,990 --> 00:19:20,020
which I think is a ridiculously large number.

452
00:19:20,390 --> 00:19:22,180
And so just don't work without solutions,

453
00:19:22,410 --> 00:19:25,330
and hopefully there'll be zero honor code

454
00:19:25,530 --> 00:19:27,580
violations this year. I'd love for that to happen.

455
00:19:29,900 --> 00:19:32,010
The section here on the late homework policy

456
00:19:32,220 --> 00:19:33,590
if you ever want to hand in a homework late,

457
00:19:33,780 --> 00:19:35,160
I'll leave you to read that yourself.

458
00:19:36,130 --> 00:19:37,380
We also have a midterm,

459
00:19:37,630 --> 00:19:41,040
which is scheduled for Thursday, 8th of

460
00:19:41,240 --> 00:19:42,490
November at 6:00 p.m.,

461
00:19:42,680 --> 00:19:46,020
so please keep that evening free.

462
00:19:50,930 --> 00:20:01,340
And let's see. And one more administrative

463
00:20:01,500 --> 00:20:03,650
thing I wanted to say is about the class project.

464
00:20:05,280 --> 00:20:07,890
So part of the goal of this class is to

465
00:20:08,070 --> 00:20:09,620
leave you well equipped to apply

466
00:20:09,830 --> 00:20:11,920
machine learning algorithms to a problem

467
00:20:12,220 --> 00:20:14,120
or to do research in machine learning.

468
00:20:14,860 --> 00:20:16,630
And so as part of this class,

469
00:20:16,870 --> 00:20:19,290
I'll ask you to execute a small research

470
00:20:19,480 --> 00:20:22,050
project sort of as a small term project.

471
00:20:22,630 --> 00:20:26,380
And what most students do for this is either

472
00:20:26,550 --> 00:20:27,910
apply machine learning to a problem

473
00:20:28,110 --> 00:20:29,880
that you find interesting

474
00:20:30,150 --> 00:20:32,770
or investigate some aspect of machine learning.

475
00:20:33,410 --> 00:20:36,100
So to those of you that are either already doing

476
00:20:36,290 --> 00:20:38,070
research or to those of you who are in industry,

477
00:20:38,290 --> 00:20:43,210
you're taking this from a company, one fantastic sort

478
00:20:43,420 --> 00:20:45,950
of way to do a class project would be if you

479
00:20:46,150 --> 00:20:48,280
apply machine learning algorithms to a problem

480
00:20:48,470 --> 00:20:50,620
that you're interested in, to a problem that you're already

481
00:20:50,800 --> 00:20:54,450
working on, whether it be a science research problem or

482
00:20:54,630 --> 00:20:56,990
sort of a problem in industry where you're trying to get a

483
00:20:57,180 --> 00:20:59,080
system to work using a learning algorithm.

484
00:20:59,750 --> 00:21:03,240
To those of you that are not currently doing research,

485
00:21:06,270 --> 00:21:08,560
one great way to do a project would be

486
00:21:08,810 --> 00:21:11,260
if you apply learning algorithms to just

487
00:21:11,460 --> 00:21:13,110
pick a problem that you care about.

488
00:21:13,300 --> 00:21:14,720
Pick a problem that you find interesting,

489
00:21:15,080 --> 00:21:16,630
and apply learning algorithms to that

490
00:21:16,820 --> 00:21:18,660
and play with the ideas and see what happens.

491
00:21:19,250 --> 00:21:28,010
And let's see. Oh, and the goal of the project

492
00:21:28,200 --> 00:21:30,000
should really be for you to do

493
00:21:30,210 --> 00:21:31,720
a publishable piece of research

494
00:21:31,910 --> 00:21:33,650
in machine learning, okay?

495
00:21:34,580 --> 00:21:36,140
And if you go to the course website,

496
00:21:36,620 --> 00:21:39,090
you'll actually find a list of the projects

497
00:21:39,350 --> 00:21:40,990
that students had done last year.

498
00:21:41,180 --> 00:21:42,530
And so I'm holding the list in my hand.

499
00:21:42,720 --> 00:21:45,120
You can go home later and take a look at it online.

500
00:21:45,580 --> 00:21:47,640
But reading down this list, I see that last year,

501
00:21:47,850 --> 00:21:50,390
there were students that applied learning algorithms

502
00:21:50,580 --> 00:21:51,820
to control a snake robot.

503
00:21:52,080 --> 00:21:54,490
There was a few projects on

504
00:21:54,670 --> 00:21:56,220
improving learning algorithms.

505
00:21:57,010 --> 00:22:00,520
There's a project on flying autonomous aircraft.

506
00:22:01,280 --> 00:22:03,370
There was a project actually done by our TA Paul

507
00:22:03,860 --> 00:22:06,820
on improving computer vision algorithms

508
00:22:07,020 --> 00:22:08,380
using machine learning.

509
00:22:08,600 --> 00:22:10,130
There are a couple of projects on Netflix

510
00:22:10,370 --> 00:22:12,600
rankings using learning algorithms;

511
00:22:13,310 --> 00:22:17,340
a few medical robots; ones on segmenting to segmenting

512
00:22:17,530 --> 00:22:20,100
pieces of the body using learning algorithms;

513
00:22:20,470 --> 00:22:22,200
one on musical instrument detection;

514
00:22:22,400 --> 00:22:24,450
another on irony sequence alignment;

515
00:22:25,970 --> 00:22:28,430
and a few algorithms on understanding

516
00:22:28,660 --> 00:22:30,460
the brain neuroscience,

517
00:22:30,670 --> 00:22:32,660
actually quite a few projects on neuroscience;

518
00:22:33,260 --> 00:22:34,790
a couple of projects on undescending

519
00:22:34,970 --> 00:22:38,680
fMRI data on brain scans, and so on;

520
00:22:38,920 --> 00:22:41,920
another project on market makings, the financial trading

521
00:22:42,620 --> 00:22:43,830
There was an interesting project on

522
00:22:44,340 --> 00:22:46,390
trying to use learning algorithms to

523
00:22:46,570 --> 00:22:47,670
decide what is it that makes a

524
00:22:47,870 --> 00:22:50,430
person's face physically attractive.

525
00:22:51,100 --> 00:22:52,090
There's a learning algorithm on

526
00:22:52,300 --> 00:22:53,390
optical illusions, and so on.

527
00:22:53,530 --> 00:22:55,450
And it goes on, so lots of fun projects.

528
00:22:55,610 --> 00:22:56,550
And take a look,

529
00:22:56,750 --> 00:22:58,990
then come up with your own ideas.

530
00:22:59,150 --> 00:23:01,250
But whatever you find cool and interesting,

531
00:23:01,500 --> 00:23:03,720
I hope you'll be able to make machine

532
00:23:03,880 --> 00:23:06,090
learning a project out of it. Yeah, question?

533
00:23:06,720 --> 00:23:07,870
Student : Are these group projects?

534
00:23:08,030 --> 00:23:09,110
Instructor (Andrew Ng): Oh, yes, thank you.

535
00:23:09,280 --> 00:23:10,440
Student : So how many people can be in a group?

536
00:23:10,630 --> 00:23:12,140
Instructor (Andrew Ng): Right. So projects can be

537
00:23:12,290 --> 00:23:14,080
done in groups of up to three people.

538
00:23:14,590 --> 00:23:16,810
So as part of forming study groups,

539
00:23:17,740 --> 00:23:20,180
later today as you get to know your classmates,

540
00:23:20,550 --> 00:23:23,320
I definitely also encourage you to grab two other people

541
00:23:23,580 --> 00:23:24,920
and form a group of up to

542
00:23:25,090 --> 00:23:26,680
three people for your project, okay?

543
00:23:27,010 --> 00:23:29,000
And just start brainstorming ideas

544
00:23:29,220 --> 00:23:30,940
for now amongst yourselves.

545
00:23:31,310 --> 00:23:33,050
You can also come and talk to me or the TAs

546
00:23:33,270 --> 00:23:35,360
if you want to brainstorm ideas with us.

547
00:23:38,370 --> 00:23:44,310
Okay. So one more organizational question.

548
00:23:45,230 --> 00:23:48,280
I'm curious, how many of you know MATLAB?

549
00:23:49,870 --> 00:23:52,880
Wow, cool, quite a lot. Okay.

550
00:23:53,260 --> 00:23:57,520
So as part of the — actually how many

551
00:23:57,690 --> 00:24:00,210
of you know Octave or have used Octave?

552
00:24:00,750 --> 00:24:02,710
Oh, okay, much smaller number.

553
00:24:02,920 --> 00:24:06,770
So as part of this class, especially in the homeworks,

554
00:24:06,960 --> 00:24:09,980
we'll ask you to implement a few programs, a few

555
00:24:10,180 --> 00:24:12,460
machine learning algorithms as part of the homeworks.

556
00:24:12,920 --> 00:24:15,640
And most of those homeworks will be

557
00:24:15,900 --> 00:24:18,550
done in either MATLAB or in Octave,

558
00:24:18,770 --> 00:24:20,450
which is sort of — I know some people

559
00:24:20,650 --> 00:24:22,480
call it a free version of MATLAB,

560
00:24:22,760 --> 00:24:24,620
which it sort of is, sort of isn't.

561
00:24:24,980 --> 00:24:26,620
So I guess for those of you that

562
00:24:26,820 --> 00:24:28,190
haven't seen MATLAB before,

563
00:24:28,580 --> 00:24:29,730
and I know most of you have,

564
00:24:29,920 --> 00:24:32,680
MATLAB is I guess part of the

565
00:24:32,850 --> 00:24:33,910
programming language that

566
00:24:34,150 --> 00:24:37,070
makes it very easy to write codes using matrices,

567
00:24:37,650 --> 00:24:40,720
to write code for numerical routines,

568
00:24:41,010 --> 00:24:43,020
to move data around, to plot data.

569
00:24:44,310 --> 00:24:47,480
And it's sort of an extremely easy to learn tool to use

570
00:24:48,180 --> 00:24:50,120
for implementing a lot of learning algorithms.

571
00:24:51,140 --> 00:24:54,790
And in case some of you want to work on

572
00:24:55,030 --> 00:24:56,470
your own home computer or something

573
00:24:56,670 --> 00:24:58,170
if you don't have a MATLAB license,

574
00:24:58,780 --> 00:25:00,320
for the purposes of this class,

575
00:25:00,820 --> 00:25:03,350
there's also write that down MATLAB —

576
00:25:04,800 --> 00:25:07,580
there's also a software package called Octave

577
00:25:07,840 --> 00:25:09,860
that you can download for free off the Internet.

578
00:25:10,100 --> 00:25:11,760
And it has somewhat fewer

579
00:25:11,930 --> 00:25:13,030
features than MATLAB,

580
00:25:13,200 --> 00:25:14,010
but it's free,

581
00:25:14,160 --> 00:25:15,320
and for the purposes of this class,

582
00:25:15,490 --> 00:25:17,420
it will work for just about everything.

583
00:25:19,610 --> 00:25:24,860
So actually I, well, so yeah, just a side comment for

584
00:25:25,030 --> 00:25:27,140
those of you that haven't seen MATLAB before I guess,

585
00:25:27,460 --> 00:25:30,900
once a colleague of mine at a different university,

586
00:25:31,110 --> 00:25:32,710
not at Stanford, actually teaches

587
00:25:33,260 --> 00:25:34,850
another machine learning course.

588
00:25:35,920 --> 00:25:37,290
He's taught it for many years.

589
00:25:37,810 --> 00:25:40,900
So one day, he was in his office,

590
00:25:41,180 --> 00:25:42,880
and an old student of his from, like,

591
00:25:43,070 --> 00:25:45,630
ten years ago came into his office and he said,

592
00:25:45,930 --> 00:25:46,810
"Oh, professor, professor,

593
00:25:47,060 --> 00:25:49,510
thank you so much for your machine learning class.

594
00:25:49,710 --> 00:25:50,860
I learned so much from it.

595
00:25:51,580 --> 00:25:53,390
There's this stuff that I learned in your class,

596
00:25:53,580 --> 00:25:54,650
and I now use every day.

597
00:25:55,180 --> 00:25:56,610
And it's helped me make lots of money,

598
00:25:56,820 --> 00:25:59,070
and here's a picture of my big house."

599
00:25:59,400 --> 00:26:01,470
So my friend was very excited.

600
00:26:01,660 --> 00:26:02,640
He said, "Wow. That's great.

601
00:26:02,810 --> 00:26:03,680
I'm glad to hear this machine

602
00:26:03,850 --> 00:26:05,170
learning stuff was actually useful.

603
00:26:05,340 --> 00:26:06,620
So what was it that you learned?

604
00:26:06,800 --> 00:26:10,910
Was it logistic regression? Was it the PCA?

605
00:26:11,140 --> 00:26:12,430
Was it the data networks?

606
00:26:12,600 --> 00:26:13,990
What was it that you learned that was so helpful?"

607
00:26:14,240 --> 00:26:16,250
And the student said, "Oh, it was the MATLAB."

608
00:26:19,460 --> 00:26:23,140
So for those of you that don't know MATLAB yet,

609
00:26:23,340 --> 00:26:25,360
I hope you do learn it.

610
00:26:25,550 --> 00:26:26,570
It's not hard,

611
00:26:26,790 --> 00:26:28,300
and we'll actually have a short

612
00:26:28,510 --> 00:26:30,570
MATLAB tutorial in one of the

613
00:26:30,820 --> 00:26:34,540
discussion sections for those of you that don't know it.

614
00:26:35,710 --> 00:26:39,990
Okay. The very last piece of logistical

615
00:26:40,230 --> 00:26:42,690
thing is the discussion sections.

616
00:26:43,800 --> 00:26:46,960
So discussion sections will be taught by the TAs,

617
00:26:47,260 --> 00:26:50,400
and attendance at discussion sections is optional,

618
00:26:50,940 --> 00:26:53,730
although they'll also be recorded and televised.

619
00:26:54,940 --> 00:26:56,880
And we'll use the discussion sections

620
00:26:57,080 --> 00:26:58,350
mainly for two things.

621
00:26:58,550 --> 00:26:59,980
For the next two or three weeks,

622
00:27:00,280 --> 00:27:01,600
we'll use the discussion sections to

623
00:27:01,810 --> 00:27:04,900
go over the prerequisites to this class

624
00:27:05,130 --> 00:27:07,310
or if some of you haven't seen probability

625
00:27:07,510 --> 00:27:09,410
or statistics for a while or maybe algebra,

626
00:27:10,550 --> 00:27:12,710
we'll go over those in the discussion sections

627
00:27:13,040 --> 00:27:15,400
as a refresher for those of you that want one.

628
00:27:16,250 --> 00:27:17,510
Later in this quarter,

629
00:27:17,740 --> 00:27:19,180
we'll also use the discussion sections

630
00:27:19,510 --> 00:27:22,460
to go over extensions for the material that

631
00:27:22,720 --> 00:27:24,380
I'm teaching in the main lectures.

632
00:27:24,600 --> 00:27:26,050
So machine learning is a huge field,

633
00:27:26,240 --> 00:27:28,400
and there are a few extensions that we really want

634
00:27:28,570 --> 00:27:30,740
to teach but didn't have time in the main lectures for.

635
00:27:31,110 --> 00:27:32,600
So later this quarter,

636
00:27:32,940 --> 00:27:34,490
we'll use the discussion sections to

637
00:27:34,670 --> 00:27:35,420
talk about things

638
00:27:35,590 --> 00:27:36,890
like convex optimization,

639
00:27:37,180 --> 00:27:39,050
to talk a little bit about hidden Markov models,

640
00:27:39,240 --> 00:27:40,330
which is a type of machine learning

641
00:27:40,520 --> 00:27:42,510
algorithm for modeling time series

642
00:27:42,700 --> 00:27:44,510
and a few other things, so extensions to the

643
00:27:44,690 --> 00:27:46,960
materials that I'll be covering in the main lectures.

644
00:27:47,530 --> 00:27:51,220
And attendance at the discussion sections is optional,

645
00:27:51,860 --> 00:27:53,020
okay?

646
00:27:53,550 --> 00:27:58,400
So that was all I had from logistics.

647
00:27:58,990 --> 00:28:02,120
Before we move on to start talking a bit

648
00:28:02,170 --> 00:28:03,110
about machine learning,

649
00:28:03,170 --> 00:28:14,360
let me check what questions you have. Yeah?

650
00:28:14,450 --> 00:28:19,100
Student : R or something like that?

651
00:28:19,990 --> 00:28:23,440
Instructor : Oh, yeah, let's see, right.

652
00:28:23,700 --> 00:28:29,940
So our policy has been that you're welcome to use R,

653
00:28:30,240 --> 00:28:32,710
but I would strongly advise against it,

654
00:28:32,900 --> 00:28:34,530
mainly because in the last problem set,

655
00:28:34,700 --> 00:28:35,820
we actually supply some code

656
00:28:36,000 --> 00:28:37,110
that will run in Octave but

657
00:28:37,330 --> 00:28:40,050
that would be somewhat painful for

658
00:28:40,190 --> 00:28:43,220
you to translate into R yourself.

659
00:28:43,800 --> 00:28:44,720
So for your other assignments,

660
00:28:44,880 --> 00:28:46,720
if you wanna submit a solution in R, that's fine.

661
00:28:47,010 --> 00:28:49,970
But I think MATLAB is actually totally worth learning.

662
00:28:50,130 --> 00:28:52,170
I know R and MATLAB,

663
00:28:52,350 --> 00:28:55,390
and I personally end up using MATLAB quite

664
00:28:55,560 --> 00:29:00,930
a bit more often for various reasons. Yeah?

665
00:29:01,910 --> 00:29:07,700
Student : For the project?

666
00:29:07,910 --> 00:29:08,820
Instructor : So for the term project,

667
00:29:08,980 --> 00:29:10,710
you're welcome to do it in smaller groups of three,

668
00:29:10,930 --> 00:29:12,020
or you're welcome to do it

669
00:29:12,240 --> 00:29:13,670
by yourself or in groups of two.

670
00:29:13,920 --> 00:29:16,770
Grading is the same regardless of the group size,

671
00:29:17,190 --> 00:29:19,040
so with a larger group, you probably —

672
00:29:19,290 --> 00:29:22,420
I recommend trying to form a team,

673
00:29:22,600 --> 00:29:24,080
but it's actually totally fine to do it in

674
00:29:24,240 --> 00:29:25,500
a smaller group if you want.

675
00:29:26,830 --> 00:29:34,020
Student : what language?

676
00:29:34,260 --> 00:29:38,170
Instructor : So let's see. There is no C programming

677
00:29:38,340 --> 00:29:39,960
in this class other than any that

678
00:29:40,200 --> 00:29:42,440
you may choose to do yourself in your project.

679
00:29:43,430 --> 00:29:46,140
So all the homeworks can be

680
00:29:46,320 --> 00:29:48,170
done in MATLAB or Octave,

681
00:29:48,720 --> 00:29:52,890
and let's see. And I guess the program prerequisites

682
00:29:53,110 --> 00:29:56,510
is more the ability to understand big?O notation

683
00:29:56,840 --> 00:29:59,360
and knowledge of what a data structure,

684
00:29:59,540 --> 00:30:02,690
like a linked list or a queue or binary treatments,

685
00:30:03,240 --> 00:30:06,190
more so than your knowledge of C or Java specifically.

686
00:30:10,410 --> 00:30:11,750
Student : Looking at the end

687
00:30:11,910 --> 00:30:12,950
semester project, I mean,

688
00:30:13,130 --> 00:30:13,980
what exactly will you be

689
00:30:14,120 --> 00:30:15,150
testing over there? [Inaudible]?

690
00:30:17,230 --> 00:30:18,450
Instructor: Of the project?

691
00:30:18,600 --> 00:30:19,320
Student: Yeah.

692
00:30:19,460 --> 00:30:20,460
Instructor: Yeah, let me answer that later.

693
00:30:20,610 --> 00:30:23,350
In a couple of weeks, I shall give out a handout

694
00:30:23,580 --> 00:30:25,270
with guidelines for the project.

695
00:30:25,740 --> 00:30:27,360
But for now, we should think of the goal

696
00:30:27,540 --> 00:30:30,340
as being to do a cool piece of machine

697
00:30:30,550 --> 00:30:32,580
learning work that will let you experience

698
00:30:33,240 --> 00:30:37,860
the joys of machine learning firsthand and really

699
00:30:38,030 --> 00:30:40,270
try to think about doing a publishable piece of work.

700
00:30:40,620 --> 00:30:42,060
So many students will try to

701
00:30:42,250 --> 00:30:43,940
build a cool machine learning application.

702
00:30:44,140 --> 00:30:45,990
That's probably the most common project.

703
00:30:46,200 --> 00:30:48,950
Some students will try to improve

704
00:30:49,130 --> 00:30:50,550
state-of-the-art machine learning.

705
00:30:50,770 --> 00:30:52,930
Some of those projects are also very successful.

706
00:30:53,140 --> 00:30:54,830
It's a little bit harder to do.

707
00:30:55,050 --> 00:30:56,990
And there's also a smaller minority of students

708
00:30:57,190 --> 00:30:58,640
that will sometimes try to prove —

709
00:30:58,820 --> 00:31:02,070
develop the theory of machine learning further

710
00:31:02,270 --> 00:31:04,470
or try to prove theorems about machine learning.

711
00:31:04,750 --> 00:31:08,250
So they're usually great projects of all of those types

712
00:31:08,460 --> 00:31:09,780
with applications and machine

713
00:31:09,970 --> 00:31:12,290
learning being the most common.

714
00:31:12,910 --> 00:31:18,280
Anything else? Okay, cool.

715
00:31:19,450 --> 00:31:22,840
So that was it for logistics.

716
00:31:24,110 --> 00:31:27,290
Let's talk about learning algorithms.

717
00:31:28,560 --> 00:31:30,930
So can I have the laptop

718
00:31:31,090 --> 00:31:33,720
display, please, or the projector?

719
00:31:45,230 --> 00:31:46,950
Actually, could you lower the big screen?

720
00:31:53,800 --> 00:31:58,060
Cool. This is amazing customer service.

721
00:32:07,630 --> 00:32:08,950
Thank you. I see.

722
00:32:09,520 --> 00:32:10,720
Big screen isn't working today,

723
00:32:10,910 --> 00:32:12,280
but I hope you can read things on the

724
00:32:12,430 --> 00:32:13,450
smaller screens out there.

725
00:32:13,580 --> 00:32:15,630
Actually, [inaudible] I think this room

726
00:32:15,800 --> 00:32:17,310
just got a new projector that —

727
00:32:19,160 --> 00:32:20,630
someone sent you an excited email —

728
00:32:21,460 --> 00:32:22,830
was it just on Friday?

729
00:32:23,160 --> 00:32:25,140
saying we just got a new projector and they

730
00:32:25,300 --> 00:32:28,820
said 4,000-to-1 something or other brightness ratio.

731
00:32:29,020 --> 00:32:30,830
I don't know. Someone was very excited

732
00:32:31,020 --> 00:32:32,360
about the new projector in this room,

733
00:32:32,530 --> 00:32:35,060
but I guess we'll see that in operation on Wednesday.

734
00:32:35,680 --> 00:32:40,190
So start by talking about what machine learning is.

735
00:32:41,230 --> 00:32:42,900
What is machine learning?

736
00:32:43,710 --> 00:32:46,970
Actually, can you read the text out there?

737
00:32:47,180 --> 00:32:48,840
Raise your hand if the text on

738
00:32:49,020 --> 00:32:50,650
the small screens is legible.

739
00:32:52,230 --> 00:32:54,140
Oh, okay, cool, mostly legible.

740
00:32:54,390 --> 00:32:55,790
Okay. So I'll just read it out.

741
00:32:56,600 --> 00:32:58,340
So what is machine learning?

742
00:32:59,560 --> 00:33:01,250
Way back in about 1959,

743
00:33:01,530 --> 00:33:04,560
Arthur Samuel defined

744
00:33:04,740 --> 00:33:06,780
machine learning informally as the

745
00:33:06,990 --> 00:33:09,270
that gives computers to learn —that gives computers

746
00:33:09,450 --> 00:33:15,460
the ability to learn without being explicitly programmed

747
00:33:17,070 --> 00:33:19,100
So Arthur Samuel, so way back

748
00:33:19,280 --> 00:33:20,510
in the history of machine learning,

749
00:33:20,680 --> 00:33:24,540
actually did something very cool,

750
00:33:24,720 --> 00:33:27,760
which was he wrote a checkers program,

751
00:33:28,390 --> 00:33:31,120
which would play games of checkers against itself.

752
00:33:31,730 --> 00:33:36,350
And so because a computer can play thousands

753
00:33:36,530 --> 00:33:39,070
of games against itself relatively quickly,

754
00:33:39,860 --> 00:33:42,080
Arthur Samuel had his program play

755
00:33:42,270 --> 00:33:43,790
thousands of games against itself,

756
00:33:44,070 --> 00:33:46,630
and over time it would start to learn to

757
00:33:46,820 --> 00:33:49,560
recognize patterns which led to wins

758
00:33:49,800 --> 00:33:51,570
and patterns which led to losses.

759
00:33:51,820 --> 00:33:53,150
So over time it learned things like that,

760
00:33:53,460 --> 00:33:57,330
"Gee, if I get a lot of pieces taken by the opponent,

761
00:33:57,590 --> 00:33:59,550
then I'm more likely to lose than win,"

762
00:33:59,750 --> 00:34:01,930
or, "Gee, if I get my pieces into a certain position,

763
00:34:02,330 --> 00:34:05,590
then I'm especially likely to win rather than lose."

764
00:34:06,150 --> 00:34:07,500
And so over time,

765
00:34:07,750 --> 00:34:09,790
Arthur Samuel had a checkers program

766
00:34:09,990 --> 00:34:12,370
that would actually learn to play checkers by learning

767
00:34:12,650 --> 00:34:15,570
what are the sort of board positions that tend to be

768
00:34:15,740 --> 00:34:17,550
associated with wins and what are the board

769
00:34:17,730 --> 00:34:19,730
positions that tend to be associated with losses.

770
00:34:21,080 --> 00:34:24,240
And way back around 1959,

771
00:34:24,790 --> 00:34:26,590
the amazing thing about this was that

772
00:34:27,360 --> 00:34:29,970
his program actually learned to play checkers

773
00:34:30,210 --> 00:34:32,760
much better than Arthur Samuel himself could.

774
00:34:33,650 --> 00:34:36,600
So even today, there are some people that say,

775
00:34:36,890 --> 00:34:38,800
well, computers can't do anything that

776
00:34:38,980 --> 00:34:40,660
they're not explicitly programmed to.

777
00:34:40,910 --> 00:34:42,580
And Arthur Samuel's checkers program

778
00:34:42,770 --> 00:34:46,020
was maybe the first I think really

779
00:34:46,190 --> 00:34:49,530
convincing refutation of this claim.

780
00:34:49,970 --> 00:34:52,660
Namely, Arthur Samuel managed to

781
00:34:52,840 --> 00:34:54,270
write a checkers program that could play

782
00:34:54,480 --> 00:34:56,550
checkers much better than he personally could,

783
00:34:56,810 --> 00:34:58,860
and this is an instance of maybe computers

784
00:34:59,050 --> 00:35:00,550
learning to do things that they were

785
00:35:00,720 --> 00:35:02,500
not programmed explicitly to do.

786
00:35:04,710 --> 00:35:08,350
Here's a more recent, a more modern, more formal

787
00:35:08,540 --> 00:35:10,050
definition of machine learning

788
00:35:10,260 --> 00:35:11,780
due to Tom Mitchell,

789
00:35:12,690 --> 00:35:14,610
who says that a well-posed learning

790
00:35:14,800 --> 00:35:17,300
problem is defined as follows:

791
00:35:18,010 --> 00:35:21,710
He says that a computer program is set to

792
00:35:21,900 --> 00:35:24,470
learn from an experience E with

793
00:35:24,630 --> 00:35:26,560
respect to some task T and some

794
00:35:26,730 --> 00:35:28,740
performance measure P if its

795
00:35:28,900 --> 00:35:30,350
performance on T as measured

796
00:35:30,520 --> 00:35:32,620
by P improves with experience E.

797
00:35:32,900 --> 00:35:34,490
Okay. So not only is it a definition,

798
00:35:34,700 --> 00:35:36,420
it even rhymes.

799
00:35:38,300 --> 00:35:43,080
So, for example, in the case of checkers, the experience

800
00:35:43,540 --> 00:35:48,180
E that a program has would be the experience of playing

801
00:35:48,370 --> 00:35:50,670
lots of games of checkers against itself, say.

802
00:35:50,920 --> 00:35:53,650
The task T is the task of playing checkers,

803
00:35:54,170 --> 00:35:56,110
and the performance measure P will be something

804
00:35:56,300 --> 00:35:58,600
like the fraction of games it wins against a certain

805
00:35:58,770 --> 00:36:00,220
set of human opponents.

806
00:36:00,570 --> 00:36:01,720
And by this definition,

807
00:36:01,900 --> 00:36:05,230
we'll say that Arthur Samuel's checkers program

808
00:36:05,410 --> 00:36:07,810
has learned to play checkers, okay?

809
00:36:08,000 --> 00:36:12,800
So as an overview of what we're going to do in this class,

810
00:36:13,830 --> 00:36:17,220
this class is sort of organized into four major sections.

811
00:36:17,740 --> 00:36:19,050
We're gonna talk about four

812
00:36:19,220 --> 00:36:20,380
major topics in this class,

813
00:36:20,530 --> 00:36:23,140
the first of which is supervised learning.

814
00:36:24,740 --> 00:36:26,550
So let me give you an example of that.

815
00:36:33,610 --> 00:36:39,010
So suppose you collect a data set of housing prices.

816
00:36:39,310 --> 00:36:41,020
And one of the TAs, Dan Ramage,

817
00:36:41,210 --> 00:36:43,210
actually collected a data set for me last week

818
00:36:43,600 --> 00:36:45,110
to use in the example later.

819
00:36:45,940 --> 00:36:53,040
But suppose that you go to collect statistics about

820
00:36:53,270 --> 00:36:56,340
how much houses cost in a certain geographic area.

821
00:36:56,870 --> 00:36:58,500
And Dan, the TA, collected data

822
00:36:58,720 --> 00:37:00,840
from housing prices in Portland, Oregon.

823
00:37:03,320 --> 00:37:08,440
So what you can do is let's say plot the

824
00:37:08,660 --> 00:37:11,350
square footage of the house against

825
00:37:11,840 --> 00:37:13,680
the list price of the house, right,

826
00:37:13,850 --> 00:37:18,900
so you collect data on a bunch of houses.

827
00:37:19,310 --> 00:37:22,100
And let's say you get a data set like this

828
00:37:22,300 --> 00:37:25,080
with houses of different sizes that are listed

829
00:37:25,230 --> 00:37:26,480
for different amounts of money.

830
00:37:29,070 --> 00:37:31,150
Now, let's say that I'm trying to sell

831
00:37:31,320 --> 00:37:34,630
a house in the same area as Portland, Oregon

832
00:37:34,820 --> 00:37:36,160
as where the data comes from.

833
00:37:37,050 --> 00:37:40,370
Let's say I have a house that's this size in square footage,

834
00:37:42,080 --> 00:37:44,510
and I want an algorithm to tell me about

835
00:37:44,660 --> 00:37:46,590
how much should I expect my house to sell for.

836
00:37:48,290 --> 00:37:49,710
So there are lots of ways to do this,

837
00:37:50,450 --> 00:37:52,630
and some of you may have seen elements

838
00:37:52,800 --> 00:37:54,630
of what I'm about to say before.

839
00:37:55,590 --> 00:37:57,130
So one thing you could do is look at this

840
00:37:57,280 --> 00:37:59,470
data and maybe put a straight line to it.

841
00:38:00,350 --> 00:38:02,870
And then if this is my house,

842
00:38:05,390 --> 00:38:08,280
you may then look at the straight line and predict that my

843
00:38:08,460 --> 00:38:10,770
house is gonna go for about that much money, right?

844
00:38:12,940 --> 00:38:14,450
There are other decisions that we can make,

845
00:38:14,640 --> 00:38:16,800
which we'll talk about later, which is, well,

846
00:38:16,990 --> 00:38:19,080
what if I don't wanna put a straight line?

847
00:38:19,260 --> 00:38:21,460
Maybe I should put a quadratic function to it.

848
00:38:21,770 --> 00:38:23,740
Maybe that fits the data a little bit better.

849
00:38:24,100 --> 00:38:25,080
You notice if you do that,

850
00:38:25,270 --> 00:38:27,500
the price of my house goes up a bit, so that'd be nice.

851
00:38:30,810 --> 00:38:33,030
And this sort of learning problem of learning

852
00:38:33,210 --> 00:38:35,410
to predict housing prices is an example of

853
00:38:35,690 --> 00:38:37,750
what's called a supervised learning problem.

854
00:38:38,720 --> 00:38:40,620
And the reason that it's called supervised learning

855
00:38:40,810 --> 00:38:45,720
isbecause we're providing the algorithm a data set of a

856
00:38:45,910 --> 00:38:48,400
bunch of square footages, a bunch of housing sizes,

857
00:38:48,590 --> 00:38:51,510
and as well as sort of the right answer of what

858
00:38:51,690 --> 00:38:55,530
the actual prices of a number of houses were, right?

859
00:38:55,850 --> 00:38:57,020
So we call this supervised learning

860
00:38:57,210 --> 00:38:58,900
because we're supervising the algorithm

861
00:38:59,060 --> 00:39:00,560
or, in other words, we're giving the algorithm

862
00:39:00,730 --> 00:39:03,650
the, quote, right answer

863
00:39:03,830 --> 00:39:05,300
for a number of houses.

864
00:39:05,450 --> 00:39:06,900
And then we want the algorithm to learn the

865
00:39:07,060 --> 00:39:08,960
association between the inputs and the outputs

866
00:39:09,130 --> 00:39:10,400
and to sort of give us more

867
00:39:10,580 --> 00:39:12,260
of the right answers, okay?

868
00:39:15,060 --> 00:39:16,870
It turns out this specific example that I drew here

869
00:39:17,080 --> 00:39:20,150
is an example of something called a regression problem.

870
00:39:22,310 --> 00:39:25,850
And the term regression sort of refers to the fact

871
00:39:26,030 --> 00:39:28,400
that the variable you're trying to predict is

872
00:39:28,570 --> 00:39:32,060
a continuous value and price.

873
00:39:37,900 --> 00:39:39,260
There's another class of supervised

874
00:39:39,470 --> 00:39:41,070
learning problems which we'll talk about,

875
00:39:41,700 --> 00:39:43,360
which are classification problems.

876
00:39:44,690 --> 00:39:50,810
And so, in a classification problem,

877
00:39:51,310 --> 00:39:52,840
the variable you're trying to predict is

878
00:39:53,030 --> 00:39:55,370
discreet rather than continuous.

879
00:39:55,760 --> 00:39:57,730
So as one specific example —

880
00:39:58,030 --> 00:40:00,720
so actually a standard data set

881
00:40:00,910 --> 00:40:01,930
you can download online

882
00:40:02,120 --> 00:40:04,650
that lots of machine learning people have played with.

883
00:40:05,450 --> 00:40:08,840
Let's say you collect a data set on breast cancer tumors,

884
00:40:09,650 --> 00:40:16,440
and you want to learn the algorithm to predict

885
00:40:16,610 --> 00:40:19,070
whether or not a certain tumor is malignant.

886
00:40:19,270 --> 00:40:21,090
Malignant is the opposite of benign, right,

887
00:40:21,270 --> 00:40:23,520
so malignancy is a sort of harmful, bad tumor.

888
00:40:24,220 --> 00:40:26,560
So we collect some number of features,

889
00:40:26,760 --> 00:40:28,610
some number of properties of these tumors,

890
00:40:28,860 --> 00:40:34,560
and for the sake of sort of having a simple explanation,

891
00:40:34,950 --> 00:40:36,580
let's just say that we're going to look at

892
00:40:37,640 --> 00:40:39,060
the size of the tumor

893
00:40:39,250 --> 00:40:40,560
and depending on the size of the tumor,

894
00:40:40,940 --> 00:40:42,330
we'll try to figure out whether or not

895
00:40:42,530 --> 00:40:44,760
the tumor is malignant or benign.

896
00:40:45,930 --> 00:40:47,720
So the tumor is either malignant or benign,

897
00:40:48,100 --> 00:40:51,860
and so the variable in the Y axis is either zero or 1,

898
00:40:53,180 --> 00:41:01,650
and so your data set may look something like that, right?

899
00:41:04,310 --> 00:41:09,030
And that's 1 and that's zero, okay?

900
00:41:09,250 --> 00:41:12,130
And so this is an example of a classification problem

901
00:41:18,500 --> 00:41:20,610
where the variable you're

902
00:41:20,770 --> 00:41:22,180
trying to predict is a discreet value.

903
00:41:22,350 --> 00:41:23,520
It's either zero or 1.

904
00:41:23,910 --> 00:41:25,570
And in fact, more generally,

905
00:41:27,760 --> 00:41:29,350
there will be many learning problems

906
00:41:29,520 --> 00:41:33,460
where we'll have more than one input variable,

907
00:41:33,660 --> 00:41:35,300
more than one input feature and

908
00:41:35,490 --> 00:41:37,140
use more than one variable to try to predict,

909
00:41:37,370 --> 00:41:39,550
say, whether a tumor is malignant or benign.

910
00:41:39,730 --> 00:41:43,930
So, for example, continuing with this,

911
00:41:46,570 --> 00:41:49,450
you may instead have a data set that looks like this.

912
00:41:49,640 --> 00:41:50,920
I'm gonna part this data set in a

913
00:41:51,090 --> 00:41:52,990
slightly different way now.

914
00:41:57,300 --> 00:41:59,090
And I'm making this data set look much

915
00:41:59,280 --> 00:42:07,150
cleaner than it really is in reality for illustration, okay?

916
00:42:10,230 --> 00:42:14,500
For example, maybe the crosses indicate malignant

917
00:42:14,690 --> 00:42:20,190
tumors and the "O"s may indicate benign tumors.

918
00:42:22,150 --> 00:42:29,040
And so you may have a data set comprising

919
00:42:30,170 --> 00:42:31,760
patients of different ages and who

920
00:42:31,940 --> 00:42:33,630
have different tumor sizes and

921
00:42:34,150 --> 00:42:37,150
where a cross indicates a malignant tumor,

922
00:42:37,440 --> 00:42:39,140
and an "O" indicates a benign tumor.

923
00:42:39,320 --> 00:42:42,420
And you may want an algorithm to learn to predict,

924
00:42:42,850 --> 00:42:43,810
given a new patient,

925
00:42:43,970 --> 00:42:45,530
whether their tumor is malignant or benign.

926
00:42:47,430 --> 00:42:48,540
So, for example,

927
00:42:48,830 --> 00:42:50,700
what a learning algorithm may do is maybe

928
00:42:50,860 --> 00:42:53,380
come in and decide that a straight line like that

929
00:42:53,650 --> 00:42:55,680
separates the two classes of tumors really well,

930
00:42:56,310 --> 00:42:58,710
and so if you have a new patient who's

931
00:43:00,830 --> 00:43:02,840
age and tumor size fall over there,

932
00:43:03,250 --> 00:43:05,570
then the algorithm may predict that the tumor

933
00:43:05,800 --> 00:43:08,990
is benign rather than malignant, okay?

934
00:43:09,480 --> 00:43:13,860
So this is just another example of another supervised

935
00:43:14,040 --> 00:43:17,540
learning problem and another classification problem.

936
00:43:17,970 --> 00:43:20,880
And so it turns out that one of the issues we'll talk

937
00:43:21,040 --> 00:43:24,300
about later in this class is in this specific example,

938
00:43:25,040 --> 00:43:26,960
we're going to try to predict whether a

939
00:43:27,140 --> 00:43:29,070
tumor is malignant or benign based on two

940
00:43:29,260 --> 00:43:30,850
features or based on two inputs,

941
00:43:31,170 --> 00:43:34,850
namely the age of the patient and the tumor size.

942
00:43:35,060 --> 00:43:37,370
It turns out that when you look at a real data set,

943
00:43:37,630 --> 00:43:40,690
you find that learning algorithms often use

944
00:43:40,870 --> 00:43:42,330
other sets of features.

945
00:43:42,520 --> 00:43:44,220
In the breast cancer data example,

946
00:43:44,450 --> 00:43:46,980
you also use properties of the tumors,

947
00:43:47,140 --> 00:43:50,190
like clump thickness, uniformity of cell size,

948
00:43:50,350 --> 00:43:53,390
uniformity of cell shape, adhesion and so on,

949
00:43:53,560 --> 00:43:55,920
so various other medical properties.

950
00:43:56,150 --> 00:43:59,300
And one of the most interesting things

951
00:43:59,470 --> 00:44:01,280
we'll talk about later this quarter is

952
00:44:01,740 --> 00:44:04,600
what if your data doesn't lie in a two-dimensional

953
00:44:04,770 --> 00:44:06,380
or three-dimensional or sort of even a

954
00:44:06,560 --> 00:44:08,980
finite dimensional space, but is it possible —

955
00:44:10,060 --> 00:44:11,460
what if your data actually lies in an

956
00:44:11,640 --> 00:44:13,430
infinite dimensional space?

957
00:44:13,610 --> 00:44:15,490
Our plots here are two-dimensional space.

958
00:44:16,030 --> 00:44:17,200
I can't plot you an infinite

959
00:44:17,380 --> 00:44:19,060
dimensional space, right?

960
00:44:19,230 --> 00:44:22,540
And so it turns out that one of the most

961
00:44:22,710 --> 00:44:24,670
successful classes of machine learning algorithms

962
00:44:24,840 --> 00:44:26,430
some may call support vector machines —

963
00:44:26,620 --> 00:44:31,150
actually takes data and maps data to

964
00:44:31,320 --> 00:44:32,770
an infinite dimensional space

965
00:44:32,950 --> 00:44:34,880
and then does classification using not

966
00:44:35,050 --> 00:44:36,650
two features like I've done here,

967
00:44:36,820 --> 00:44:38,530
but an infinite number of features.

968
00:44:38,700 --> 00:44:40,060
And that will actually be one

969
00:44:40,230 --> 00:44:41,190
of the most fascinating things

970
00:44:41,340 --> 00:44:42,310
we talk about when we go

971
00:44:42,450 --> 00:44:44,130
deeply into classification algorithms.

972
00:44:44,260 --> 00:44:45,570
And it's actually an interesting question, right,

973
00:44:45,710 --> 00:44:47,040
so think about how do you even represent

974
00:44:47,190 --> 00:44:51,380
an infinite dimensional vector in computer memory?

975
00:44:51,550 --> 00:44:53,200
You don't have an infinite amount of computers.

976
00:44:53,370 --> 00:44:55,210
How do you even represent a point that

977
00:44:55,350 --> 00:44:57,090
lies in an infinite dimensional space?

978
00:44:57,650 --> 00:44:59,050
We'll talk about that when we

979
00:44:59,200 --> 00:45:03,950
get to support vector machines, okay?

980
00:45:05,170 --> 00:45:21,640
So let's see. So that was supervised learning.

981
00:45:21,820 --> 00:45:26,690
The second of the four major topics of this class

982
00:45:26,860 --> 00:45:28,820
will be learning theory.

983
00:45:31,730 --> 00:45:34,290
So I have a friend who teaches

984
00:45:34,460 --> 00:45:37,300
math at a different university, not at Stanford,

985
00:45:38,020 --> 00:45:40,610
and when you talk to him about his work

986
00:45:40,820 --> 00:45:42,460
and what he's really out to do,

987
00:45:42,680 --> 00:45:45,970
this friend of mine will — he's a math professor, right?

988
00:45:46,140 --> 00:45:47,590
this friend of mine will sort of get the

989
00:45:47,750 --> 00:45:48,990
look of wonder in his eyes,

990
00:45:49,170 --> 00:45:51,930
and he'll tell you about how in his mathematical work,

991
00:45:52,410 --> 00:45:54,530
he feels like he's discovering truth

992
00:45:54,710 --> 00:45:56,170
and beauty in the universe.

993
00:45:56,350 --> 00:45:59,530
And he says it in sort of a really touching, sincere way,

994
00:45:59,790 --> 00:46:02,140
and then he has this — you can see it in his eyes —

995
00:46:02,320 --> 00:46:05,270
he has this deep appreciation of the truth and beauty

996
00:46:05,450 --> 00:46:08,380
in the universe as revealed to him by the math he does.

997
00:46:08,990 --> 00:46:12,390
In this class, I'm not gonna do any truth and beauty.

998
00:46:12,940 --> 00:46:17,220
In this class, I'm gonna talk about learning theory

999
00:46:17,590 --> 00:46:20,350
to try to convey to you an understanding of

1000
00:46:20,530 --> 00:46:22,730
how and why learning algorithms work

1001
00:46:23,040 --> 00:46:25,600
so that we can apply these learning

1002
00:46:25,780 --> 00:46:28,060
algorithms as effectively as possible.

1003
00:46:30,160 --> 00:46:32,900
So, for example, it turns out you can

1004
00:46:33,080 --> 00:46:35,130
prove surprisingly deep theorems

1005
00:46:35,470 --> 00:46:37,150
on when you can guarantee that a

1006
00:46:37,340 --> 00:46:38,920
learning algorithm will work, all right?

1007
00:46:39,100 --> 00:46:39,980
So think about a learning

1008
00:46:40,130 --> 00:46:41,550
algorithm for reading zip codes.

1009
00:46:41,700 --> 00:46:43,230
When can you prove a theorem

1010
00:46:43,390 --> 00:46:44,900
guaranteeing that a learning

1011
00:46:45,050 --> 00:46:46,570
algorithm will be at least 99.9

1012
00:46:46,720 --> 00:46:48,620
percent accurate on reading zip codes?

1013
00:46:48,750 --> 00:46:49,840
This is actually somewhat surprising.

1014
00:46:49,980 --> 00:46:51,310
We actually prove theorems showing

1015
00:46:51,470 --> 00:46:53,400
when you can expect that to hold.

1016
00:46:54,780 --> 00:46:56,570
We'll also sort of delve into learning

1017
00:46:56,710 --> 00:46:57,890
theory to try to understand

1018
00:46:58,050 --> 00:46:59,640
what algorithms can approximate

1019
00:46:59,770 --> 00:47:03,310
different functions well and also try to

1020
00:47:03,460 --> 00:47:05,030
understand things like how much

1021
00:47:05,190 --> 00:47:06,350
training data do you need?

1022
00:47:06,500 --> 00:47:08,460
So how many examples of houses do I need

1023
00:47:08,650 --> 00:47:10,400
in order for your learning algorithm

1024
00:47:10,570 --> 00:47:12,760
to recognize the pattern between the

1025
00:47:13,050 --> 00:47:15,970
square footage of a house and its housing price?

1026
00:47:16,120 --> 00:47:17,570
And this will help us answer questions like

1027
00:47:17,780 --> 00:47:20,480
if you're trying to design a learning algorithm,

1028
00:47:20,690 --> 00:47:22,040
should you be spending more time

1029
00:47:22,200 --> 00:47:24,390
collecting more data or is it a case

1030
00:47:24,570 --> 00:47:25,850
that you already have enough data;

1031
00:47:26,000 --> 00:47:26,830
it would be a waste of time

1032
00:47:26,970 --> 00:47:28,520
to try to collect more. Okay?

1033
00:47:30,180 --> 00:47:33,430
So I think learning algorithms are a very powerful tool

1034
00:47:33,630 --> 00:47:38,070
that as I walk around sort of industry

1035
00:47:38,230 --> 00:47:39,720
in Silicon Valley or as I work with

1036
00:47:39,870 --> 00:47:41,390
various businesses in CS

1037
00:47:41,550 --> 00:47:42,700
and outside CS,

1038
00:47:42,850 --> 00:47:45,180
I find that there's often a huge difference between

1039
00:47:45,480 --> 00:47:47,510
how well someone who really understands

1040
00:47:47,690 --> 00:47:50,390
this stuff can apply a learning algorithm versus

1041
00:47:50,590 --> 00:47:53,260
someone who sort of gets it but sort of doesn't.

1042
00:47:53,860 --> 00:47:58,180
The analogy I like to think of is imagine you were going

1043
00:47:58,390 --> 00:48:01,160
to a carpentry school instead of a machine learning class,

1044
00:48:01,320 --> 00:48:02,730
If you go to a carpentry school,

1045
00:48:02,960 --> 00:48:04,480
they can give you the tools of carpentry.

1046
00:48:04,660 --> 00:48:05,890
They'll give you a hammer, a bunch of nails,

1047
00:48:06,050 --> 00:48:07,630
a screwdriver or whatever.

1048
00:48:08,340 --> 00:48:09,970
But a master carpenter will be able to

1049
00:48:10,150 --> 00:48:11,850
use those tools far better than

1050
00:48:12,020 --> 00:48:13,130
most of us in this room.

1051
00:48:13,300 --> 00:48:15,350
I know a carpenter can do things with a hammer

1052
00:48:15,520 --> 00:48:17,110
and nail that I couldn't possibly.

1053
00:48:17,940 --> 00:48:19,970
And it's actually a little bit like that in machine learning

1054
00:48:20,130 --> 00:48:23,040
One thing that's sadly not taught in many courses

1055
00:48:23,220 --> 00:48:25,380
on machine learning is how to take the tools of

1056
00:48:25,510 --> 00:48:27,950
machine learning and really, really apply them well.

1057
00:48:29,430 --> 00:48:31,110
So in the same way, so the tools of

1058
00:48:31,280 --> 00:48:33,770
machine learning are I wanna say quite a bit

1059
00:48:33,960 --> 00:48:35,460
more advanced than the tools of carpentry.

1060
00:48:35,650 --> 00:48:36,810
Maybe a carpenter will disagree.

1061
00:48:36,950 --> 00:48:38,620
But a large part of this class will be

1062
00:48:40,240 --> 00:48:41,980
just giving you the raw tools of machine learning,

1063
00:48:42,210 --> 00:48:43,600
just the algorithms and so on.

1064
00:48:44,460 --> 00:48:47,330
But what I plan to do throughout this entire quarter,

1065
00:48:47,530 --> 00:48:49,180
not just in the segment of learning theory,

1066
00:48:49,340 --> 00:48:51,580
but actually as a theme running through everything

1067
00:48:51,750 --> 00:48:55,700
I do this quarter, will be to try to convey to you the

1068
00:48:55,870 --> 00:48:58,720
skills to really take the learning algorithm ideas

1069
00:48:58,910 --> 00:49:00,850
and really to get them to work on a problem.

1070
00:49:01,790 --> 00:49:04,630
It's sort of hard for me to stand here

1071
00:49:04,800 --> 00:49:07,460
and say how big a deal that is,

1072
00:49:07,640 --> 00:49:09,790
but when I walk around companies in Silicon Valley,

1073
00:49:10,270 --> 00:49:12,480
it's completely not uncommon for me to see

1074
00:49:12,670 --> 00:49:15,680
someone using some machine learning algorithm

1075
00:49:16,410 --> 00:49:17,910
and then explain to me what they've been

1076
00:49:18,080 --> 00:49:19,010
doing for the last six months,

1077
00:49:19,180 --> 00:49:20,320
and I go, oh, gee,

1078
00:49:20,500 --> 00:49:22,940
it should have been obvious from the start

1079
00:49:23,120 --> 00:49:24,110
that the last six months,

1080
00:49:24,260 --> 00:49:25,370
you've been wasting your time, right?

1081
00:49:25,510 --> 00:49:30,110
And so my goal in this class,

1082
00:49:30,550 --> 00:49:31,560
running through the entire quarter,

1083
00:49:31,750 --> 00:49:34,040
not just on learning theory, is actually not

1084
00:49:34,230 --> 00:49:35,700
only to give you the tools of machine learning,

1085
00:49:36,130 --> 00:49:38,160
but to teach you how to use them well.

1086
00:49:38,760 --> 00:49:40,920
And I've noticed this is something that

1087
00:49:41,120 --> 00:49:43,270
really not many other classes teach.

1088
00:49:43,460 --> 00:49:46,760
And this is something I'm really

1089
00:49:46,920 --> 00:49:48,390
convinced is a huge deal,

1090
00:49:48,570 --> 00:49:50,060
and so by the end of this class,

1091
00:49:50,260 --> 00:49:52,450
I hope all of you will be master carpenters.

1092
00:49:52,610 --> 00:49:54,800
I hope all of you will be really good at applying

1093
00:49:54,960 --> 00:49:57,060
these learning algorithms and getting them

1094
00:49:57,220 --> 00:50:00,090
to work amazingly well in many problems. Okay?

1095
00:50:02,600 --> 00:50:11,330
Let's see. So the board. After learning theory,

1096
00:50:13,330 --> 00:50:15,930
there's another class of learning algorithms

1097
00:50:16,170 --> 00:50:19,400
that I then want to teach you about,

1098
00:50:20,770 --> 00:50:22,330
and that's unsupervised learning.

1099
00:50:31,420 --> 00:50:32,590
So you recall, right,

1100
00:50:36,790 --> 00:50:40,610
a little earlier I drew an example like this, right,

1101
00:50:40,780 --> 00:50:42,770
where you have a couple of features,

1102
00:50:42,930 --> 00:50:44,120
a couple of input variables

1103
00:50:44,290 --> 00:50:45,390
and sort of malignant tumors and

1104
00:50:45,520 --> 00:50:46,780
benign tumors or whatever.

1105
00:50:46,940 --> 00:50:47,730
And that was an

1106
00:50:47,860 --> 00:50:49,200
example of a supervised learning problem

1107
00:50:49,360 --> 00:50:52,460
because the data you have gives you the

1108
00:50:52,590 --> 00:50:54,240
right answer for each of your patients.

1109
00:50:54,420 --> 00:50:55,710
The data tells you this patient

1110
00:50:55,850 --> 00:50:56,670
has a malignant tumor;

1111
00:50:56,800 --> 00:50:58,330
this patient has a benign tumor.

1112
00:50:58,500 --> 00:50:59,890
So it had the right answers,

1113
00:51:00,060 --> 00:51:01,250
and you wanted the algorithm to

1114
00:51:01,390 --> 00:51:02,810
just produce more of the same.

1115
00:51:04,830 --> 00:51:07,720
In contrast, in an unsupervised learning problem,

1116
00:51:09,280 --> 00:51:11,500
this is the sort of data you get, okay?

1117
00:51:19,580 --> 00:51:23,190
Where speaking loosely, you're given a data set,

1118
00:51:23,450 --> 00:51:24,670
and I'm not gonna tell you what the

1119
00:51:24,860 --> 00:51:26,890
right answer is on any of your data.

1120
00:51:27,080 --> 00:51:28,570
I'm just gonna give you a data set and I'm gonna say

1121
00:51:28,840 --> 00:51:30,160
"Would you please find interesting

1122
00:51:30,330 --> 00:51:32,130
structure in this data set?"

1123
00:51:32,280 --> 00:51:33,550
So that's the unsupervised learning problem

1124
00:51:33,710 --> 00:51:34,990
where you're sort of not given

1125
00:51:35,150 --> 00:51:36,830
the right answer for everything.

1126
00:51:38,130 --> 00:51:44,320
So, for example, an algorithm may find structure

1127
00:51:44,490 --> 00:51:46,740
in the data in the form of the data

1128
00:51:47,040 --> 00:51:49,470
being partitioned into two clusters,

1129
00:51:49,650 --> 00:51:51,930
or clustering is sort of one example

1130
00:51:53,480 --> 00:51:56,070
of an unsupervised learning problem.

1131
00:51:57,140 --> 00:51:59,550
So I hope you can see this.

1132
00:52:00,170 --> 00:52:02,360
It turns out that these sort of unsupervised

1133
00:52:02,500 --> 00:52:04,460
learning algorithms are also used in many problems.

1134
00:52:05,190 --> 00:52:06,210
This is a screen shot —

1135
00:52:06,360 --> 00:52:08,050
this is a picture I got from Sue Emvee,

1136
00:52:08,200 --> 00:52:09,700
who's a PhD student here,

1137
00:52:10,170 --> 00:52:11,590
who is applying unsupervised learning

1138
00:52:11,760 --> 00:52:13,670
algorithms to try to understand gene data,

1139
00:52:13,950 --> 00:52:17,100
so is trying to look at genes as individuals and

1140
00:52:17,260 --> 00:52:21,090
group them into clusters based on properties of

1141
00:52:21,240 --> 00:52:23,250
what genes they respond to — based on properties

1142
00:52:23,380 --> 00:52:25,390
of how the genes respond to different experiments.

1143
00:52:26,970 --> 00:52:30,780
Another interesting application of sorts

1144
00:52:30,960 --> 00:52:33,210
of clustering algorithms is actually image processing,

1145
00:52:33,700 --> 00:52:34,330
this which I got

1146
00:52:34,490 --> 00:52:35,290
from Steve Gules,

1147
00:52:35,430 --> 00:52:36,640
who's another PhD student.

1148
00:52:36,810 --> 00:52:39,440
It turns out what you can do is if you

1149
00:52:39,600 --> 00:52:42,520
give this sort of data, say an image,

1150
00:52:42,680 --> 00:52:44,940
to certain unsupervised learning algorithms,

1151
00:52:45,480 --> 00:52:48,190
they will then learn to group pixels together

1152
00:52:48,830 --> 00:52:52,230
and say, gee, this sort of pixel seems to belong together,

1153
00:52:52,430 --> 00:52:54,240
and that sort of pixel seems to belong together.

1154
00:52:54,800 --> 00:52:57,780
And so the images you see on the bottom —

1155
00:52:58,170 --> 00:52:59,990
I guess you can just barely see them on there —

1156
00:53:02,320 --> 00:53:06,430
so the images you see on the bottom are groupings —

1157
00:53:06,780 --> 00:53:08,370
are what the algorithm has done to

1158
00:53:08,550 --> 00:53:10,190
group certain pixels together.

1159
00:53:10,800 --> 00:53:12,320
On a small display, it might be easier to

1160
00:53:12,510 --> 00:53:14,400
just look at the image on the right.

1161
00:53:14,590 --> 00:53:17,510
The two images on the bottom are two sort

1162
00:53:17,680 --> 00:53:19,790
of identical visualizations of the same grouping

1163
00:53:20,240 --> 00:53:24,760
of the pixels into regions.

1164
00:53:26,950 --> 00:53:28,620
And so it turns out that this sort of clustering

1165
00:53:28,790 --> 00:53:30,920
algorithm or this sort of unsupervised learning algorithm,

1166
00:53:31,310 --> 00:53:33,400
which learns to group pixels together,

1167
00:53:33,860 --> 00:53:37,330
it turns out to be useful for many applications in vision,

1168
00:53:37,560 --> 00:53:39,130
in computer vision image processing.

1169
00:53:39,460 --> 00:53:41,150
I'll just show you one example,

1170
00:53:41,330 --> 00:53:43,110
and this is a rather cool one that two students,

1171
00:53:43,300 --> 00:53:45,280
Ashutosh Saxena and Min Sun here did,

1172
00:53:45,950 --> 00:53:47,780
which is given an image like this, right?

1173
00:53:48,000 --> 00:53:50,480
This is actually a picture taken of the Stanford campus.

1174
00:53:50,920 --> 00:53:53,210
You can apply that sort of clustering algorithm

1175
00:53:53,480 --> 00:53:56,380
and group the picture into regions.

1176
00:53:56,960 --> 00:54:01,350
Let me actually blow that up

1177
00:54:01,510 --> 00:54:03,210
so that you can see it more clearly.

1178
00:54:20,270 --> 00:54:21,820
Okay. So in the middle,

1179
00:54:21,970 --> 00:54:24,990
you see the lines sort of grouping the image together,

1180
00:54:25,170 --> 00:54:27,220
grouping the image into regions.

1181
00:54:27,940 --> 00:54:30,100
And what Ashutosh and Min did was they then

1182
00:54:30,260 --> 00:54:33,630
applied the learning algorithm to say can we take this

1183
00:54:33,800 --> 00:54:38,020
clustering and use it to build a 3D model of the world?

1184
00:54:41,930 --> 00:54:44,680
And so using the clustering,

1185
00:54:47,780 --> 00:54:50,290
they then had a learning algorithm try to learn

1186
00:54:51,640 --> 00:54:54,160
what the 3D structure of the world looks like

1187
00:54:56,720 --> 00:54:58,630
so that they could come up with a 3D model

1188
00:54:59,770 --> 00:55:04,620
that you can sort of fly through, okay?

1189
00:55:05,470 --> 00:55:07,270
Although many people used to think it's not possible

1190
00:55:07,560 --> 00:55:10,640
to take a single image and build a 3D model,

1191
00:55:13,230 --> 00:55:16,290
but using a learning algorithm

1192
00:55:16,480 --> 00:55:18,590
and that sort of clustering algorithm is the first step.

1193
00:55:24,070 --> 00:55:24,980
They were able to.

1194
00:55:25,150 --> 00:55:26,030
I'll just show you one more example.

1195
00:55:26,180 --> 00:55:27,740
I like this because it's a picture

1196
00:55:27,900 --> 00:55:29,830
of Stanford with our beautiful Stanford campus.

1197
00:55:31,110 --> 00:55:33,080
So again, taking the same sort of clustering algorithms,

1198
00:55:33,350 --> 00:55:36,130
taking the same sort of unsupervised learning algorithm,

1199
00:55:36,290 --> 00:55:38,300
you can group the pixels into different regions.

1200
00:55:38,890 --> 00:55:40,930
And using that as a pre-processing step,

1201
00:55:42,450 --> 00:55:45,440
they eventually built this sort of 3D model

1202
00:55:46,310 --> 00:55:49,090
of Stanford campus in a single picture.

1203
00:55:49,330 --> 00:55:51,140
You can sort of walk into the ceiling,

1204
00:55:51,430 --> 00:55:57,240
look around the campus. Okay?

1205
00:55:57,710 --> 00:55:59,020
This actually turned out to be a mix of

1206
00:55:59,180 --> 00:56:00,580
supervised and unsupervised learning,

1207
00:56:00,750 --> 00:56:01,990
but the unsupervised learning,

1208
00:56:02,140 --> 00:56:04,570
this sort of clustering was the first step.

1209
00:56:06,730 --> 00:56:12,960
So it turns out these sorts of unsupervised —

1210
00:56:13,110 --> 00:56:14,930
clustering algorithms are actually routinely

1211
00:56:15,080 --> 00:56:16,760
used for many different problems,

1212
00:56:17,400 --> 00:56:19,460
things like organizing computing clusters,

1213
00:56:19,950 --> 00:56:22,590
social network analysis, market segmentation,

1214
00:56:23,060 --> 00:56:24,560
so if you're a marketer

1215
00:56:24,720 --> 00:56:26,780
and you want to divide your market into

1216
00:56:26,940 --> 00:56:28,360
different segments or different groups of

1217
00:56:28,510 --> 00:56:30,000
people to market to them separately;

1218
00:56:30,210 --> 00:56:33,100
even for astronomical data analysis

1219
00:56:33,390 --> 00:56:35,780
and understanding how galaxies are formed.

1220
00:56:35,940 --> 00:56:37,770
These are just a sort of small sample of the

1221
00:56:37,940 --> 00:56:40,600
applications of unsupervised learning algorithms

1222
00:56:40,750 --> 00:56:42,140
and clustering algorithms that we'll

1223
00:56:42,310 --> 00:56:43,670
talk about later in this class.

1224
00:56:44,310 --> 00:56:47,770
Just one particularly cool example of an unsupervised

1225
00:56:47,920 --> 00:56:50,330
learning algorithm that I want to tell you about.

1226
00:56:50,610 --> 00:56:53,020
And to motivate that, I'm gonna tell you about

1227
00:56:53,160 --> 00:56:56,240
what's called the cocktail party problem,

1228
00:56:56,560 --> 00:56:59,430
which is imagine that you're at some cocktail party

1229
00:56:59,600 --> 00:57:01,310
and there are lots of people standing all over.

1230
00:57:01,500 --> 00:57:03,620
And you know how it is, right, if you're at a large party,

1231
00:57:03,800 --> 00:57:05,250
everyone's talking,

1232
00:57:05,780 --> 00:57:07,130
it can be sometimes very hard to hear

1233
00:57:07,270 --> 00:57:08,490
even the person in front of you.

1234
00:57:08,650 --> 00:57:10,880
So imagine a large cocktail party with lots of people.

1235
00:57:11,140 --> 00:57:13,640
So the problem is, is that all of these people talking,

1236
00:57:14,150 --> 00:57:18,300
can you separate out the voice of just the

1237
00:57:18,430 --> 00:57:20,080
person you're interested in talking to

1238
00:57:20,440 --> 00:57:22,250
with all this loud background noise?

1239
00:57:22,700 --> 00:57:26,230
So I'll show you a specific example in a second,

1240
00:57:26,550 --> 00:57:31,720
but here's a cocktail party that's I guess rather

1241
00:57:31,880 --> 00:57:34,260
sparsely attended by just two people.

1242
00:57:37,550 --> 00:57:38,990
But what we're gonna do is we'll put two

1243
00:57:39,150 --> 00:57:41,400
microphones in the room, okay?

1244
00:57:41,780 --> 00:57:45,030
And so because the microphones are just

1245
00:57:45,200 --> 00:57:47,450
at slightly different distances to the two people,

1246
00:57:47,620 --> 00:57:48,690
and the two people may speak in

1247
00:57:48,840 --> 00:57:50,050
slightly different volumes,

1248
00:57:50,200 --> 00:57:53,180
each microphone will pick up an overlapping

1249
00:57:53,340 --> 00:57:55,590
combination of these two people's voices,

1250
00:57:57,370 --> 00:57:59,280
so slightly different overlapping voices.

1251
00:57:59,480 --> 00:58:01,310
So Speaker 1's voice may be more loud

1252
00:58:01,500 --> 00:58:03,740
on Microphone 1, and Speaker 2's voice

1253
00:58:03,920 --> 00:58:05,510
may be louder on Microphone 2, whatever.

1254
00:58:06,450 --> 00:58:08,440
But the question is, given these microphone recordings,

1255
00:58:08,630 --> 00:58:09,910
can you separate out the

1256
00:58:10,090 --> 00:58:11,680
original speaker's voices?

1257
00:58:12,000 --> 00:58:14,820
So I'm gonna play some audio clips that

1258
00:58:14,980 --> 00:58:16,400
were collected by

1259
00:58:16,580 --> 00:58:18,580
Tai Yuan Lee at UCSD.

1260
00:58:31,130 --> 00:58:38,100
One, two, three, four, five, six, seven, eight, nine, ten.

1261
00:58:45,030 --> 00:58:47,340
Uno, dos, tres, cuatro, cinco,

1262
00:58:47,480 --> 00:58:51,030
seis, siete, ocho, nueve, diez.

1263
00:58:51,800 --> 00:58:54,180
Instructor : Okay. So in supervised learning,

1264
00:58:54,350 --> 00:58:55,860
we don't know what the right answer is, right?

1265
00:58:56,020 --> 00:58:58,540
So what we're going to do is take exactly the two

1266
00:58:58,720 --> 00:59:01,900
microphone recordings you just heard and give it to an

1267
00:59:02,080 --> 00:59:05,370
unsupervised learning algorithm and tell the algorithm

1268
00:59:05,740 --> 00:59:07,700
which of these discover structure in the

1269
00:59:07,860 --> 00:59:09,960
data  or what structure is there in this data?

1270
00:59:11,160 --> 00:59:11,930
And we actually don't know

1271
00:59:12,070 --> 00:59:14,090
what the right answer is offhand.

1272
00:59:14,240 --> 00:59:16,360
So give this data to an

1273
00:59:16,530 --> 00:59:18,170
unsupervised learning algorithm,

1274
00:59:18,500 --> 00:59:19,940
and what the algorithm does in this case,

1275
00:59:20,130 --> 00:59:22,630
it will discover that this data can

1276
00:59:22,800 --> 00:59:24,520
actually be explained by two

1277
00:59:24,930 --> 00:59:27,340
independent speakers speaking at the same time,

1278
00:59:27,590 --> 00:59:31,040
and it can further separate out the two speakers for you.

1279
00:59:31,250 --> 00:59:33,270
So here's Output 1 of the algorithm:

1280
00:59:33,550 --> 00:59:40,320
One, two, three, four, five, six, seven, eight, nine, ten.

1281
00:59:40,880 --> 00:59:41,850
And there's the second algorithm:

1282
00:59:42,000 --> 00:59:44,920
Uno, dos, tres, cuatro, cinco,

1283
00:59:45,060 --> 00:59:48,680
seis, siete, ocho, nueve, diez.

1284
00:59:49,260 --> 00:59:51,890
And so the algorithm discovers that, gee,

1285
00:59:52,030 --> 00:59:54,110
the structure underlying the data is

1286
00:59:54,260 --> 00:59:55,930
really that there are two sources of sound,

1287
00:59:56,160 --> 00:59:59,210
and here they are. I'll show you one more example.

1288
00:59:59,370 --> 01:00:04,290
This is a, well, this is a second sort

1289
01:00:04,420 --> 01:00:06,250
of different pair of microphone recordings:

1290
01:00:06,400 --> 01:00:12,550
One, two, three, four, five, six, seven, eight, nine, ten.

1291
01:00:12,880 --> 01:00:14,880
So the poor guy is not at a cocktail party.

1292
01:00:15,040 --> 01:00:20,020
He's talking to his radio. There's the second recording:

1293
01:00:20,350 --> 01:00:24,410
One, two, three, four, five, six, seven, eight, nine, ten.

1294
01:00:24,630 --> 01:00:26,230
Right. And we get this data.

1295
01:00:26,400 --> 01:00:28,700
It's the same unsupervised learning algorithm.

1296
01:00:28,900 --> 01:00:30,090
The algorithm is actually called

1297
01:00:30,240 --> 01:00:31,910
independent component analysis,

1298
01:00:32,070 --> 01:00:33,230
and later in this quarter, you'll see why.

1299
01:00:33,540 --> 01:00:35,350
And then output's the following:

1300
01:00:35,740 --> 01:00:41,850
One, two, three, four, five, six, seven, eight, nine, ten.

1301
01:00:42,470 --> 01:00:43,620
And that's the second one:

1302
01:00:43,790 --> 01:00:50,410
[Music playing.]

1303
01:00:50,890 --> 01:00:53,500
Okay. So it turns out that beyond

1304
01:00:53,660 --> 01:00:56,490
solving the cocktail party algorithm,

1305
01:00:56,700 --> 01:00:59,430
this specific class of unsupervised learning

1306
01:00:59,590 --> 01:01:02,330
algorithms are also applied to a bunch of other problems,

1307
01:01:02,520 --> 01:01:03,960
like in text processing

1308
01:01:04,250 --> 01:01:06,720
or understanding functional grading and machine data,

1309
01:01:06,920 --> 01:01:09,370
like the magneto-encephalogram would be an EEG data.

1310
01:01:09,570 --> 01:01:12,610
We'll talk about that more when we go and describe

1311
01:01:12,790 --> 01:01:15,730
ICA or independent component analysis algorithms,

1312
01:01:15,900 --> 01:01:17,500
which is what you just saw.

1313
01:01:18,810 --> 01:01:20,100
And as an aside,

1314
01:01:20,310 --> 01:01:23,490
this algorithm I just showed you, it seems like it

1315
01:01:23,620 --> 01:01:25,370
must be a pretty complicated algorithm, right,

1316
01:01:25,540 --> 01:01:27,000
to take this overlapping audio streams

1317
01:01:27,230 --> 01:01:28,220
and separate them out.

1318
01:01:28,370 --> 01:01:29,720
It sounds like a pretty complicated thing to do.

1319
01:01:29,880 --> 01:01:32,140
So you're gonna ask how complicated is it

1320
01:01:32,280 --> 01:01:34,440
really to implement an algorithm like this?

1321
01:01:35,390 --> 01:01:38,350
It turns out if you do it in MATLAB,

1322
01:01:38,540 --> 01:01:40,200
you can do it in one line of code.

1323
01:01:41,540 --> 01:01:43,310
So I got this from Samuel Wyse

1324
01:01:43,490 --> 01:01:45,040
at Toronto, U of Toronto,

1325
01:01:45,200 --> 01:01:47,430
and the example I showed you

1326
01:01:47,600 --> 01:01:48,720
actually used a more complicated

1327
01:01:48,880 --> 01:01:50,130
ICA algorithm than this.

1328
01:01:50,310 --> 01:01:53,750
But nonetheless, I guess this is why for this class

1329
01:01:53,930 --> 01:01:56,610
I'm going to ask you to do most of your programming

1330
01:01:56,770 --> 01:01:59,110
in MATLAB and Octave because

1331
01:01:59,290 --> 01:02:02,250
if you try to implement the same algorithm in C

1332
01:02:02,420 --> 01:02:03,370
or Java or something,

1333
01:02:03,550 --> 01:02:05,500
I can tell you from personal,

1334
01:02:05,660 --> 01:02:06,810
painful experience,

1335
01:02:06,970 --> 01:02:08,630
you end up writing pages and pages

1336
01:02:08,760 --> 01:02:09,680
of code rather than

1337
01:02:09,860 --> 01:02:11,300
relatively few lines of code.

1338
01:02:11,920 --> 01:02:13,070
I'll also mention that

1339
01:02:13,250 --> 01:02:15,580
it did take researchers many,

1340
01:02:15,750 --> 01:02:16,900
many years to come up with that

1341
01:02:17,080 --> 01:02:18,150
one line of code,

1342
01:02:18,330 --> 01:02:19,910
so this is not easy.

1343
01:02:23,130 --> 01:02:25,430
So that was unsupervised learning,

1344
01:02:25,650 --> 01:02:27,690
and then the last of the four major topics

1345
01:02:27,850 --> 01:02:30,640
I wanna tell you about is reinforcement learning.

1346
01:02:31,470 --> 01:02:35,560
And this refers to problems where you

1347
01:02:35,730 --> 01:02:37,960
don't do one-shot decision-making.

1348
01:02:38,120 --> 01:02:39,210
So, for example,

1349
01:02:39,580 --> 01:02:42,370
in the supervised learning cancer prediction problem,

1350
01:02:42,580 --> 01:02:44,550
you have a patient come in,

1351
01:02:44,760 --> 01:02:47,370
you predict that the cancer is malignant or benign.

1352
01:02:47,570 --> 01:02:48,860
And then based on your prediction,

1353
01:02:49,040 --> 01:02:51,230
maybe the patient lives or dies, and then that's it, right?

1354
01:02:51,400 --> 01:02:54,000
So you make a decision and then there's a consequence.

1355
01:02:54,160 --> 01:02:55,250
You either got it right or wrong.

1356
01:02:56,280 --> 01:02:58,340
In reinforcement learning problems,

1357
01:02:58,520 --> 01:03:00,470
you are usually asked to make a

1358
01:03:00,640 --> 01:03:02,180
equence of decisions over time.

1359
01:03:02,900 --> 01:03:05,730
So, for example, this is something that

1360
01:03:05,870 --> 01:03:07,080
my students and I work on.

1361
01:03:07,240 --> 01:03:10,590
If I give you the keys to an autonomous helicopter —

1362
01:03:11,370 --> 01:03:14,410
we actually have this helicopter here at Stanford, —

1363
01:03:15,470 --> 01:03:17,390
how do you write a program to make it fly, right?

1364
01:03:17,590 --> 01:03:18,300
You notice that

1365
01:03:18,440 --> 01:03:20,300
if you make a wrong decision on a helicopter,

1366
01:03:20,780 --> 01:03:22,810
the consequence of crashing it

1367
01:03:22,950 --> 01:03:24,770
may not happen until much later.

1368
01:03:24,920 --> 01:03:25,930
And in fact,

1369
01:03:26,040 --> 01:03:26,890
usually you need to make a whole

1370
01:03:27,030 --> 01:03:28,440
sequence of bad decisions to crash a helicopter.

1371
01:03:28,650 --> 01:03:29,980
But conversely,

1372
01:03:30,110 --> 01:03:31,130
you also need to make a whole sequence

1373
01:03:31,280 --> 01:03:34,110
of good decisions in order to fly a helicopter really well.

1374
01:03:34,720 --> 01:03:37,850
So I'm gonna show you some fun videos

1375
01:03:38,020 --> 01:03:40,940
of learning algorithms flying helicopters.

1376
01:03:42,520 --> 01:03:44,440
This is a video of our

1377
01:03:44,590 --> 01:03:47,830
helicopter at Stanford flying using a controller that

1378
01:03:47,980 --> 01:03:49,370
was learned using a reinforcement

1379
01:03:49,530 --> 01:03:51,340
learning algorithm.

1380
01:03:53,300 --> 01:03:55,380
So this was done on the Stanford football field,

1381
01:03:55,570 --> 01:03:58,370
and we'll zoom out the camera in a second.

1382
01:03:58,550 --> 01:04:01,690
You'll sort of see the trees planted in the sky.

1383
01:04:04,850 --> 01:04:07,320
So maybe this is one of the most

1384
01:04:07,510 --> 01:04:11,260
difficult aerobatic maneuvers flown on

1385
01:04:11,420 --> 01:04:14,200
any helicopter under computer control.

1386
01:04:14,600 --> 01:04:18,410
And this controller, which is very, very hard for a

1387
01:04:18,570 --> 01:04:20,410
human to sit down and write out, was learned

1388
01:04:20,570 --> 01:04:22,670
using one of these reinforcement learning algorithms.

1389
01:04:24,250 --> 01:04:26,150
Just a word about that: The basic idea

1390
01:04:26,320 --> 01:04:28,470
behind a reinforcement learning algorithm is

1391
01:04:28,620 --> 01:04:30,410
this idea of what's called a reward function.

1392
01:04:31,140 --> 01:04:33,230
What we have to think about is imagine

1393
01:04:33,430 --> 01:04:34,800
you're trying to train a dog.

1394
01:04:35,700 --> 01:04:37,700
So every time your dog does something good,

1395
01:04:37,850 --> 01:04:39,110
you say, "Good dog,"

1396
01:04:39,290 --> 01:04:40,720
and you reward the dog.

1397
01:04:40,870 --> 01:04:42,260
Every time your dog does something bad,

1398
01:04:42,410 --> 01:04:44,180
you go, "Bad dog," right?

1399
01:04:44,350 --> 01:04:46,710
And hopefully, over time, your dog will learn to do

1400
01:04:46,880 --> 01:04:49,100
the right things to get more of the positive rewards,

1401
01:04:49,250 --> 01:04:51,040
to get more of the "Good dogs"

1402
01:04:51,040 --> 01:04:52,040
and to get fewer of the "Bad dogs.”

1403
01:04:53,090 --> 01:04:54,430
So the way we teach a helicopter

1404
01:04:54,590 --> 01:04:56,670
to fly or any of these robots is sort of the same thing.

1405
01:04:56,820 --> 01:04:58,250
Every time the helicopter crashes,

1406
01:04:58,400 --> 01:05:00,110
we go, "Bad helicopter,"

1407
01:05:00,280 --> 01:05:02,620
and every time it does the right thing,

1408
01:05:02,770 --> 01:05:04,140
we go, "Good helicopter,"

1409
01:05:04,290 --> 01:05:05,950
and over time it learns how to control itself

1410
01:05:06,110 --> 01:05:08,090
so as to get more of these positive rewards.

1411
01:05:08,800 --> 01:05:09,750
So reinforcement learning is —

1412
01:05:09,880 --> 01:05:12,340
I think of it as a way for you to specify

1413
01:05:12,530 --> 01:05:13,620
what you want done,

1414
01:05:13,770 --> 01:05:16,110
so you have to specify what is a "good dog"

1415
01:05:16,280 --> 01:05:18,000
and what is a "bad dog" behavior.

1416
01:05:18,400 --> 01:05:19,890
And then it's up to the learning algorithm to

1417
01:05:20,060 --> 01:05:23,420
figure out how to maximize the "good dog"

1418
01:05:23,580 --> 01:05:26,170
reward signals and minimize the "bad dog" punishments.

1419
01:05:28,150 --> 01:05:32,120
So it turns out reinforcement learning is applied to

1420
01:05:32,420 --> 01:05:34,500
other problems in robotics.

1421
01:05:34,690 --> 01:05:36,730
It's applied to things in web crawling and so on.

1422
01:05:37,060 --> 01:05:38,700
But it's just cool to show videos,

1423
01:05:38,870 --> 01:05:40,460
so let me just show a bunch of them.

1424
01:05:41,330 --> 01:05:45,650
This learning algorithm was actually implemented by ou

1425
01:05:45,810 --> 01:05:50,240
head TA, Zico, of programming a four-legged dog.

1426
01:05:50,430 --> 01:05:53,290
I guess Sam Shriver in this class also worked on the

1427
01:05:53,450 --> 01:05:57,870
project and Peter Renfrew and Mike and a few others.

1428
01:06:01,420 --> 01:06:04,280
But I guess this really is a good dog/bad dog

1429
01:06:04,450 --> 01:06:06,980
since it's a robot dog.

1430
01:06:07,900 --> 01:06:09,200
The second video on the right,

1431
01:06:09,850 --> 01:06:11,800
some of the students,

1432
01:06:11,950 --> 01:06:14,610
I guess Peter, Zico, Tonca working on a robotic snake

1433
01:06:15,100 --> 01:06:16,970
again using learning algorithms

1434
01:06:17,130 --> 01:06:19,180
to teach a snake robot to climb over obstacles.

1435
01:06:21,160 --> 01:06:22,900
Below that, this is kind of a fun example.

1436
01:06:23,120 --> 01:06:26,000
Ashutosh Saxena and Jeff Michaels

1437
01:06:26,430 --> 01:06:28,580
used learning algorithms to teach a car how to drive

1438
01:06:28,790 --> 01:06:33,050
at reasonably high speeds off roads avoiding obstacles.

1439
01:06:36,810 --> 01:06:40,780
And on the lower right, that's a robot programmed

1440
01:06:40,960 --> 01:06:46,160
by PhD student Eva Roshen to teach a sort of somewhat

1441
01:06:46,330 --> 01:06:49,140
strangely configured robot how to get on

1442
01:06:49,330 --> 01:06:51,630
top of an obstacle, how to get over an obstacle.

1443
01:06:52,750 --> 01:06:54,260
Sorry. I know the video's kind of small.

1444
01:06:54,460 --> 01:06:56,710
I hope you can sort of see it.

1445
01:06:56,950 --> 01:07:00,560
So I think all of these are robots that I think

1446
01:07:00,740 --> 01:07:03,470
are very difficult to hand-code a controller

1447
01:07:03,640 --> 01:07:05,750
for by learning these sorts of learning algorithms.

1448
01:07:06,320 --> 01:07:11,700
You can in relatively short order get a robot

1449
01:07:11,900 --> 01:07:13,870
to do often pretty amazing things.

1450
01:07:24,610 --> 01:07:31,500
Okay. So that was most of what I wanted to say today.

1451
01:07:31,660 --> 01:07:33,070
Just a couple more last things,

1452
01:07:33,230 --> 01:07:36,120
but let me just check what questions you have right now

1453
01:07:46,410 --> 01:07:48,540
So if there are no questions,

1454
01:07:48,710 --> 01:07:50,720
I'll just close with two reminders,

1455
01:07:50,910 --> 01:07:54,700
which are after class today or as you

1456
01:07:54,890 --> 01:07:57,460
start to talk with other people in this class,

1457
01:07:57,630 --> 01:08:00,180
I just encourage you again to start to form

1458
01:08:00,360 --> 01:08:02,480
project partners, to try to find project

1459
01:08:02,650 --> 01:08:04,510
partners to do your project with.

1460
01:08:05,150 --> 01:08:07,680
And also, this is a good time to start

1461
01:08:07,850 --> 01:08:09,150
forming study groups,

1462
01:08:09,310 --> 01:08:10,300
so either talk to your friends or

1463
01:08:10,440 --> 01:08:11,490
post in the newsgroup,

1464
01:08:11,650 --> 01:08:13,440
but we just encourage you to try to

1465
01:08:13,590 --> 01:08:15,290
start to do both of those today, okay?

1466
01:08:15,450 --> 01:08:16,570
Form study groups,

1467
01:08:16,720 --> 01:08:18,870
and try to find two other project partners.

1468
01:08:20,100 --> 01:08:22,490
So thank you. I'm looking forward to teaching this class,

1469
01:08:22,640 --> 01:08:24,760
and I'll see you in a couple of days.

