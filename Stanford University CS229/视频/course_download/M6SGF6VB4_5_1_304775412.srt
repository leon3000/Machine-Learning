1
00:00:23,390 --> 00:00:26,430
今天我要讲

2
00:00:26,630 --> 00:00:28,620
一类新的学习算法

3
00:00:28,790 --> 00:00:29,820
具体地

4
00:00:29,970 --> 00:00:31,500
我们会开始讲生成学习算法

5
00:00:31,670 --> 00:00:33,740
以及一种特殊的算法

6
00:00:33,910 --> 00:00:35,590
被称之为高斯判别分析

7
00:00:35,790 --> 00:00:39,320
这里讲点题外话  关于高斯分布

8
00:00:39,490 --> 00:00:43,360
我会简要地对比一下

9
00:00:43,530 --> 00:00:45,030
生成学习算法与判别学习算法

10
00:00:45,190 --> 00:00:47,340
之后希望能够在课程结束之前

11
00:00:47,510 --> 00:00:50,260
讲完朴素贝叶斯方法与

12
00:00:50,430 --> 00:00:51,190
Laplace平滑

13
00:00:52,450 --> 00:00:55,190
以生成学习算法

14
00:00:55,380 --> 00:00:56,810
开始今天的课程

15
00:00:56,980 --> 00:00:58,400
作为对比

16
00:00:58,590 --> 00:01:00,330
我们已经讨论过

17
00:01:00,470 --> 00:01:01,520
分类算法源于

18
00:01:01,710 --> 00:01:03,820
我认为算法源于解决这样的问题

19
00:01:04,020 --> 00:01:05,570
给定一个训练集合

20
00:01:05,750 --> 00:01:12,890
如果你对它运行

21
00:01:13,000 --> 00:01:14,390
像logistic回归这样的算法

22
00:01:14,550 --> 00:01:17,510
我认为logistic回归的工作方式是

23
00:01:17,670 --> 00:01:19,680
观察这组数据

24
00:01:19,810 --> 00:01:21,310
并尝试找到一条直线

25
00:01:21,440 --> 00:01:23,910
能够将图中的X与O分开  对吗?

26
00:01:24,030 --> 00:01:25,460
所以  试着找到一条直线

27
00:01:25,620 --> 00:01:28,910
让我添加一些噪声数据

28
00:01:29,040 --> 00:01:30,480
试着找到一条直线

29
00:01:30,610 --> 00:01:35,270
能够把两类数据

30
00:01:35,400 --> 00:01:38,500
尽可能好地分隔开  对吗?

31
00:01:38,620 --> 00:01:40,510
实际上  可以在电脑上演示这一过程

32
00:01:40,630 --> 00:01:41,670
我们可以

33
00:01:41,790 --> 00:01:43,310
使用投影仪来演示一下

34
00:01:43,410 --> 00:01:47,750
实际上  你可以看到

35
00:01:47,890 --> 00:01:51,870
我们对一个数据集合使用logistic回归

36
00:01:52,010 --> 00:01:55,870
我对参数进行随机的初始化

37
00:01:55,990 --> 00:01:56,990
所以此时logistic回归

38
00:01:57,120 --> 00:01:58,950
在某种程度上来说

39
00:01:59,100 --> 00:02:02,550
输出的是在第0次迭代时的假设

40
00:02:02,680 --> 00:02:04,070
正如此时的这条直线所表示的那样

41
00:02:04,210 --> 00:02:06,940
所以基于梯度上升算法进行了一次迭代后

42
00:02:07,070 --> 00:02:08,370
直线的位置发生了一些改变

43
00:02:08,510 --> 00:02:11,400
两次 三次 四次

44
00:02:11,550 --> 00:02:14,350
知道最后logistic回归算法收敛

45
00:02:14,480 --> 00:02:16,700
并且得到了这样一条直线

46
00:02:16,810 --> 00:02:17,530
几乎

47
00:02:17,700 --> 00:02:18,870
将两类数据完全分隔开来

48
00:02:19,040 --> 00:02:20,410
明白吗?所以  你们可以认为

49
00:02:20,530 --> 00:02:22,070
在某种程度上来说  logistic回归的执行过程

50
00:02:22,190 --> 00:02:23,960
就是要搜索这样的一条直线

51
00:02:24,080 --> 00:02:25,100
能够将两类数据分隔开

52
00:02:25,270 --> 00:02:29,910
我今天要讲的算法

53
00:02:30,030 --> 00:02:31,490
和分类算法有些不同

54
00:02:31,650 --> 00:02:33,570
为了更好地理解

55
00:02:33,710 --> 00:02:35,690
让我们再次用到一个曾经用过的例子

56
00:02:35,830 --> 00:02:37,470
我们希望对恶性癌症和良性癌症

57
00:02:37,630 --> 00:02:39,560
进行分类  对吗?

58
00:02:39,640 --> 00:02:41,730
当一个病人身患癌症时

59
00:02:41,840 --> 00:02:42,980
你们希望了解它是一种恶性的

60
00:02:43,180 --> 00:02:44,370
有害的癌症

61
00:02:44,520 --> 00:02:46,650
还是一种良性的  不是那么有害的癌症

62
00:02:46,780 --> 00:02:50,100
除了寻找一条直线

63
00:02:50,220 --> 00:02:51,250
将两类数据分开之外

64
00:02:51,400 --> 00:02:52,480
我们还可以做一些其它的事情

65
00:02:52,660 --> 00:02:54,880
我们可以遍历我们的训练集合

66
00:02:55,020 --> 00:02:58,670
查看其中的所有恶性癌症的样本

67
00:02:58,790 --> 00:03:00,160
只需要

68
00:03:00,300 --> 00:03:01,420
从我们的训练集合中

69
00:03:01,530 --> 00:03:03,610
找到所有恶性癌症的样本

70
00:03:03,770 --> 00:03:05,180
之后我们可以基于这些样本

71
00:03:05,300 --> 00:03:07,230
直接对恶性癌症的特征进行建模

72
00:03:07,370 --> 00:03:10,300
之后我们会再次遍历训练集合

73
00:03:10,400 --> 00:03:12,810
并找到所有的良性癌症的样本

74
00:03:12,930 --> 00:03:14,200
之后再基于这些样本

75
00:03:14,330 --> 00:03:17,130
直接对良性癌症的特征进行建模  明白吗?

76
00:03:17,270 --> 00:03:21,280
当你需要对一个新的样本进行分类时

77
00:03:21,420 --> 00:03:22,330
当你有了一个新的病人时

78
00:03:22,460 --> 00:03:23,280
你需要确定

79
00:03:23,430 --> 00:03:25,120
这个病人所患的癌症是恶性还是良性

80
00:03:25,250 --> 00:03:26,830
之后你可以用这个样本

81
00:03:26,950 --> 00:03:30,380
分别匹配你对恶性癌症所建立的模型

82
00:03:30,520 --> 00:03:32,780
与你对良性癌症所建立的模型

83
00:03:32,900 --> 00:03:35,210
看一下那个模型匹配的更好

84
00:03:35,330 --> 00:03:36,630
然后根据

85
00:03:36,740 --> 00:03:38,020
它能更好地匹配哪个模型

86
00:03:38,120 --> 00:03:41,100
来预测新的癌症样本

87
00:03:41,210 --> 00:03:42,410
是恶性还是良性  明白吗?

88
00:03:42,540 --> 00:03:47,870
在我刚才所描述的

89
00:03:47,980 --> 00:03:50,300
这类方法中

90
00:03:50,420 --> 00:03:51,830
你需要对

91
00:03:51,940 --> 00:03:53,040
恶性癌症

92
00:03:53,160 --> 00:03:54,710
与良性癌症分别建立一个模型

93
00:03:54,850 --> 00:03:57,230
这类方法被称之为生成学习算法

94
00:03:57,370 --> 00:04:00,220
让我们来形式化地定义一下

95
00:04:00,340 --> 00:04:05,740
我们之前一直在讨论的

96
00:04:05,870 --> 00:04:06,900
那些模型

97
00:04:07,080 --> 00:04:08,240
实质上

98
00:04:08,350 --> 00:04:10,200
都属于判别学习算法

99
00:04:10,340 --> 00:04:15,460
形式化地描述的话

100
00:04:15,570 --> 00:04:19,140
判别学习算法

101
00:04:19,250 --> 00:04:23,900
或者直接学习p(y|x)

102
00:04:24,030 --> 00:04:31,170
或者学习得到一个假设

103
00:04:31,270 --> 00:04:43,470
直接输出0或1  明白吗?

104
00:04:43,620 --> 00:04:45,430
所以logistic回归

105
00:04:45,550 --> 00:04:49,030
是判别学习算法的一个例子

106
00:04:49,170 --> 00:04:51,840
相反地  一个生成学习算法

107
00:04:51,980 --> 00:04:59,920
用来对p(x|y)进行建模

108
00:05:00,080 --> 00:05:01,360
给定所属的类的情况下

109
00:05:01,360 --> 00:05:02,530
显示某种特定特征的概率

110
00:05:02,680 --> 00:05:07,750
处于技术上的考虑  它也会对p(y)进行建模

111
00:05:07,850 --> 00:05:09,090
但是这并不是很重要

112
00:05:09,260 --> 00:05:12,930
对于这个式子的解释是

113
00:05:13,060 --> 00:05:16,850
一个生成模型对样本特征

114
00:05:16,990 --> 00:05:18,790
建立概率模型

115
00:05:18,940 --> 00:05:28,610
在给定了样本所属的类的条件下 明白吗?

116
00:05:28,640 --> 00:05:29,610
换句话说

117
00:05:29,730 --> 00:05:31,570
假定一个癌症为恶性或良性

118
00:05:31,690 --> 00:05:34,310
它会对该条件下的癌症症状

119
00:05:34,410 --> 00:05:36,300
的概率分布进行建模

120
00:05:36,440 --> 00:05:38,690
建立了模型之后-

121
00:05:38,880 --> 00:05:41,830
如果对p(x|y)与p(y)建立了模型

122
00:05:41,900 --> 00:05:43,850
那么根据贝叶斯公式  很显然

123
00:05:43,890 --> 00:05:47,520
你们就可以计算p(y=1|x)

124
00:05:47,630 --> 00:05:57,980
这里应该是P(x|y=1)*p(x)/p(x) 

125
00:05:58,070 --> 00:06:00,230
如果有必要的话

126
00:06:00,350 --> 00:06:07,080
你可以这样计算分母

127
00:06:09,560 --> 00:06:22,510
对p(x|y)

128
00:06:22,590 --> 00:06:24,040
与p(y)进行建模之后

129
00:06:24,200 --> 00:06:25,410
你可以使用贝叶斯公式

130
00:06:25,500 --> 00:06:26,860
去计算p(y|x)

131
00:06:27,010 --> 00:06:29,130
但是一个生成模型

132
00:06:29,250 --> 00:06:31,180
生成学习算法

133
00:06:31,290 --> 00:06:34,450
一开始是对p(x|y)进行建模  而不是对p(y|x)  明白吗?

134
00:06:34,570 --> 00:06:36,040
我们稍后会讨论一些一些权衡

135
00:06:36,150 --> 00:06:38,720
考虑和判别模型相比

136
00:06:38,840 --> 00:06:39,900
这样的算法是一个更好的

137
00:06:39,900 --> 00:06:40,770
还是一个更坏的算法

138
00:06:40,920 --> 00:06:43,640
让我们来讲一个

139
00:06:43,740 --> 00:06:45,180
生成学习算法的例子

140
00:06:45,290 --> 00:06:49,830
对于这个特殊的例子

141
00:06:49,940 --> 00:06:55,240
我要假设  输入特征x

142
00:06:55,350 --> 00:07:04,800
属于R^n  并且是连续值

143
00:07:04,960 --> 00:07:06,970
在这个假设之下

144
00:07:07,100 --> 00:07:10,310
我会向你们描述一个特殊的算法

145
00:07:10,440 --> 00:07:12,990
被称为高斯判别分析

146
00:07:13,130 --> 00:07:27,500
我想  核心的假设应该是

147
00:07:27,630 --> 00:07:28,690
我们会假设

148
00:07:28,820 --> 00:07:30,560
在高斯判别分析模型中

149
00:07:30,670 --> 00:07:36,930
p(x|y)满足高斯分布

150
00:07:37,010 --> 00:07:40,810
举一下手

151
00:07:40,930 --> 00:07:41,920
让我看看你们中的多少人曾经见过

152
00:07:42,040 --> 00:07:43,580
多元高斯分布-

153
00:07:43,730 --> 00:07:44,700
不是一维的高斯分布

154
00:07:44,810 --> 00:07:46,060
而是更高维的

155
00:07:46,180 --> 00:07:47,860
好的  似乎有一半

156
00:07:47,940 --> 00:07:48,950
三分之二

157
00:07:49,030 --> 00:07:52,340
我来补充一点

158
00:07:52,490 --> 00:07:53,580
关于高斯分布的知识

159
00:07:53,580 --> 00:07:54,670
对于你们那些已经知道的

160
00:07:54,790 --> 00:07:56,960
可以一起来回顾一下

161
00:07:57,120 --> 00:07:58,320
当我们说

162
00:07:58,490 --> 00:08:00,570
一个随机变量z满足高斯分布

163
00:08:00,730 --> 00:08:02,450
多元高斯分布-

164
00:08:02,580 --> 00:08:06,480
这个字母N表示normal

165
00:08:06,610 --> 00:08:10,920
以均值μ与协方差∑为参数

166
00:08:11,110 --> 00:08:31,510
这样的话z的概率密度函数是这样的

167
00:08:31,690 --> 00:08:33,460
这是概率密度公式

168
00:08:33,570 --> 00:08:35,310
可以看成一维高斯分布的推广

169
00:08:35,460 --> 00:08:37,390
形状也是一种钟形曲线

170
00:08:38,440 --> 00:08:41,430
随机变量z是一个高维向量

171
00:08:41,560 --> 00:08:44,390
不需要过多地注意

172
00:08:44,510 --> 00:08:45,750
这个概率密度公式

173
00:08:45,920 --> 00:08:47,710
你很少会直接用到它

174
00:08:47,850 --> 00:08:49,740
但是需要注意这两个关键的量

175
00:08:49,870 --> 00:08:53,910
向量μ是高斯分布的均值

176
00:08:54,080 --> 00:08:58,150
矩阵∑是协方差矩阵-

177
00:08:58,290 --> 00:09:07,590
∑应该等于

178
00:09:07,720 --> 00:09:10,010
向量随机变量的协方差的定义应该是

179
00:09:10,150 --> 00:09:16,470
E[(x-μ) (x-μ)^T ]  明白吗?

180
00:09:16,590 --> 00:09:21,890
如果你觉得这个公式很陌生的话

181
00:09:22,040 --> 00:09:26,160
你也许需要重新看一下

182
00:09:26,280 --> 00:09:29,290
上周五助教上的习题课的内容

183
00:09:29,430 --> 00:09:31,870
或者来上这周的习题课

184
00:09:32,040 --> 00:09:34,510
来对概率知识进行一些回顾  好吗?

185
00:09:34,650 --> 00:09:39,400
所以多元高斯分布

186
00:09:39,510 --> 00:09:41,870
以一个均值与一个协方差为参数  让我-

187
00:09:42,040 --> 00:09:44,460
可以把我的笔记本显示出来吗?

188
00:09:44,620 --> 00:09:48,030
我会继续向你们展示

189
00:09:48,160 --> 00:09:49,680
通过图形化的方式

190
00:09:49,800 --> 00:09:52,750
展示改变

191
00:09:52,930 --> 00:09:55,430
多元高斯分布的参数的效果

192
00:09:55,630 --> 00:10:00,010
我们这里显示的是

193
00:10:00,130 --> 00:10:01,810
均值为0的高斯分布的密度函数

194
00:10:01,950 --> 00:10:04,290
协方差矩阵等于单位矩阵

195
00:10:04,430 --> 00:10:05,760
协方差矩阵

196
00:10:05,880 --> 00:10:07,360
显示在讲义的右上角

197
00:10:07,500 --> 00:10:10,330
这是熟悉的二维的

198
00:10:10,450 --> 00:10:11,340
钟形曲面

199
00:10:11,550 --> 00:10:14,850
如果我减小协方差矩阵中元素的值

200
00:10:15,030 --> 00:10:17,790
而不再采用原来的单位矩阵

201
00:10:17,910 --> 00:10:19,260
如果我减小原矩阵中元素的值

202
00:10:19,400 --> 00:10:21,290
那么高斯曲面会变地更加陡峭

203
00:10:21,430 --> 00:10:23,340
而如果我们放大矩阵中的元素的值

204
00:10:23,500 --> 00:10:27,010
比如都设为2  那么分布的曲面

205
00:10:27,120 --> 00:10:29,060
将会变得更加扁平

206
00:10:29,200 --> 00:10:32,160
这张图可以看成标准

207
00:10:32,360 --> 00:10:33,670
采用单位矩阵

208
00:10:33,800 --> 00:10:38,280
如果我增加矩阵的对角元素的值

209
00:10:38,400 --> 00:10:41,130
如果我让变量之间存在一定的相关性

210
00:10:41,280 --> 00:10:43,150
高斯曲面会沿x=y的方向

211
00:10:43,290 --> 00:10:44,920
趋于扁平

212
00:10:45,070 --> 00:10:48,200
进一步增加对角元素的值  那么我们的变量

213
00:10:48,360 --> 00:10:49,270
x与y

214
00:10:49,430 --> 00:10:52,430
对不起  这里是z1和z2

215
00:10:52,550 --> 00:10:55,470
两个水平轴代表的变量

216
00:10:55,600 --> 00:10:56,790
会变得更加相关

217
00:10:56,940 --> 00:10:58,660
同样的图形我们用轮廓再来展示一遍

218
00:10:58,790 --> 00:11:00,880
标准分布

219
00:11:01,030 --> 00:11:02,270
的轮廓-

220
00:11:02,400 --> 00:11:03,740
看起来类似于圆形

221
00:11:03,880 --> 00:11:05,550
由于屏幕高宽比的原因

222
00:11:05,670 --> 00:11:07,200
它们看起来像是椭圆

223
00:11:07,320 --> 00:11:08,550
它们实际上是圆

224
00:11:08,700 --> 00:11:11,110
如果增加协方差矩阵的

225
00:11:11,230 --> 00:11:12,680
对角线上的元素

226
00:11:12,790 --> 00:11:16,160
它们会沿45度角

227
00:11:16,280 --> 00:11:19,040
偏转成一个椭圆的形状

228
00:11:19,170 --> 00:11:21,750
这是一样的

229
00:11:21,870 --> 00:11:23,640
这是一个在高斯密度函数中出现

230
00:11:23,750 --> 00:11:25,010
负的协方差的例子

231
00:11:25,110 --> 00:11:29,240
所以现在相关发生在另一个方向上

232
00:11:29,480 --> 00:11:31,370
相关性很强

233
00:11:31,500 --> 00:11:32,890
这是以轮廓方式展示的相同的图形

234
00:11:33,030 --> 00:11:34,880
这是在高斯分布的对角线上

235
00:11:35,000 --> 00:11:35,770
出现负项

236
00:11:35,900 --> 00:11:38,440
和出现很大的项的例子

237
00:11:38,580 --> 00:11:42,400
另外的一个参数是

238
00:11:42,510 --> 00:11:44,270
均值参数

239
00:11:44,410 --> 00:11:45,370
如果μ

240
00:11:45,480 --> 00:11:46,880
的元素的值

241
00:11:46,990 --> 00:11:49,570
等于0.15

242
00:11:49,680 --> 00:11:51,850
那么告诉分布曲面

243
00:11:51,970 --> 00:11:53,190
的中心会发生移动

244
00:11:53,310 --> 00:11:57,950
好的  我们刚刚对高斯分布

245
00:11:58,080 --> 00:11:59,100
进行了一个初步的回顾

246
00:11:59,240 --> 00:12:04,060
这是一张

247
00:12:04,190 --> 00:12:05,050
需要记住的图

248
00:12:05,160 --> 00:12:06,110
当我们描述

249
00:12:06,240 --> 00:12:07,770
高斯判别分析算法时

250
00:12:07,880 --> 00:12:08,960
我们会这样做

251
00:12:09,120 --> 00:12:10,330
这是训练样本

252
00:12:10,470 --> 00:12:14,240
在高斯判别分析算法中

253
00:12:14,360 --> 00:12:15,620
我们要做的是

254
00:12:15,800 --> 00:12:17,880
我会先观察正样本

255
00:12:18,020 --> 00:12:19,040
也就是x

256
00:12:19,240 --> 00:12:21,320
只观察正样本

257
00:12:21,470 --> 00:12:22,610
之后我可以拟合出一个

258
00:12:22,740 --> 00:12:24,080
正样本数据的高斯分布

259
00:12:24,240 --> 00:12:26,120
所以也许我会得到一个

260
00:12:26,250 --> 00:12:27,960
像这样的高斯分布  明白吗?

261
00:12:28,080 --> 00:12:30,500
他表示了p(x|y=1)

262
00:12:30,620 --> 00:12:32,730
之后我们再观察负样本

263
00:12:32,900 --> 00:12:34,080
也就是图中的o

264
00:12:34,200 --> 00:12:35,600
对它拟合出高斯分布

265
00:12:35,720 --> 00:12:38,280
也许我得到的高斯分布的重心在这里

266
00:12:38,370 --> 00:12:39,910
这就是我得到的第二个高斯分布

267
00:12:40,020 --> 00:12:41,170
将它们放在一起-

268
00:12:41,320 --> 00:12:43,870
之后-

269
00:12:44,000 --> 00:12:47,000
这两个高斯分布的密度函数

270
00:12:47,160 --> 00:12:50,690
可以定义出两个类别的分隔器  明白吗?

271
00:12:50,820 --> 00:12:53,260
事实上  这个分隔器

272
00:12:53,390 --> 00:12:56,390
比我们之前使用logistic回归得到的直线

273
00:12:56,500 --> 00:12:57,960
要稍微复杂一些

274
00:12:58,100 --> 00:13:00,000
如果你运行logistic回归

275
00:13:00,170 --> 00:13:01,570
你实际上会得到

276
00:13:01,690 --> 00:13:02,990
绿线所示的分隔线

277
00:13:03,100 --> 00:13:04,560
而运行高斯判别分析

278
00:13:04,670 --> 00:13:06,260
会得到蓝色的分隔线  明白吗?

279
00:13:06,380 --> 00:13:09,110
请切换回黑板

280
00:13:21,960 --> 00:13:27,360
好的

281
00:13:27,480 --> 00:13:32,020
这就是高斯判别分析模型

282
00:13:32,190 --> 00:13:36,360
对于模型p(y)

283
00:13:36,480 --> 00:13:38,110
像往常一样y是一个服从

284
00:13:38,110 --> 00:13:39,590
伯努利分布的随机变量

285
00:13:39,710 --> 00:13:43,410
它是一个伯努利随机变量

286
00:13:43,520 --> 00:13:45,650
并且以Φ为参数

287
00:13:45,740 --> 00:13:46,990
你们之前已经见过了

288
00:13:47,110 --> 00:13:53,800
用高斯分布对p(x|y=0)进行建模

289
00:13:53,940 --> 00:13:58,980
哦  你们发现了吗?

290
00:13:59,110 --> 00:14:02,120
哦  使得  对不起

291
00:14:02,240 --> 00:14:06,050
我觉得这个式子看起来很奇怪

292
00:14:06,160 --> 00:14:12,480
这里应该有一个Σ

293
00:14:12,600 --> 00:14:14,340
行列式值的

294
00:14:14,420 --> 00:14:15,620
1/2次方

295
00:14:15,770 --> 00:14:17,040
不是什么大问题

296
00:14:17,230 --> 00:14:23,720
好的

297
00:14:23,870 --> 00:14:25,750
我之前黑板上的公式中

298
00:14:25,930 --> 00:14:27,560
少了个|Σ|^(1/2)

299
00:14:27,670 --> 00:14:28,800
抱歉

300
00:14:28,920 --> 00:14:43,230
好的  我将p(x|y=0)

301
00:14:43,330 --> 00:14:47,450
建模成均值为μ_0  协方差为Σ的高斯分布

302
00:14:47,540 --> 00:15:10,280
这里不是-1/2  而是-1  好的

303
00:15:10,410 --> 00:15:15,700
这个模型的参数包括

304
00:15:15,820 --> 00:15:22,170
Φ  μ_0  μ_1和Σ

305
00:15:22,280 --> 00:15:27,480
我现在可以写出这些参数的

306
00:15:27,600 --> 00:15:29,710
似然性公式-

307
00:15:30,040 --> 00:15:31,440
哦  对不起  实际上

308
00:15:31,560 --> 00:15:32,670
是这些参数的对数似然性

309
00:15:32,790 --> 00:15:43,970
是这个式子的对数  对吗?

310
00:15:44,100 --> 00:15:46,550
所以  换句话说

311
00:15:46,690 --> 00:15:47,910
如果给我这个训练集合

312
00:15:48,030 --> 00:15:50,070
那么就可以将参数的对数似然性

313
00:15:50,180 --> 00:15:52,550
写成这个式子的对数

314
00:15:52,670 --> 00:15:57,290
也就是这些p(x^((i) )  y^((i) ))的乘积的对数  对吗?

315
00:15:57,400 --> 00:16:12,790
它应该等于这个

316
00:16:12,950 --> 00:16:15,470
其中的这些项  p(x^((i) ) |y^((i) ))

317
00:16:15,570 --> 00:16:18,590
p(y^((i) ))

318
00:16:18,730 --> 00:16:22,890
由上面的三个公式给出  明白吗?

319
00:16:23,010 --> 00:16:27,920
我需要将这个模型

320
00:16:28,030 --> 00:16:30,330
与判别学习算法进行对比

321
00:16:30,430 --> 00:16:33,460
我想可以给这个式子取个名字

322
00:16:33,590 --> 00:16:34,830
实际上

323
00:16:34,940 --> 00:16:36,640
它有时被称为

324
00:16:36,750 --> 00:16:38,110
joint likelihood

325
00:16:38,230 --> 00:16:43,460
让我将这个模型

326
00:16:43,580 --> 00:16:44,650
与我们之前

327
00:16:44,790 --> 00:16:46,970
在将logistic回归时学到的模型进行一下对比

328
00:16:47,110 --> 00:16:49,810
我讲过

329
00:16:49,920 --> 00:16:53,910
参数θ的对数似然率应该等于

330
00:16:54,020 --> 00:16:55,640
log∏_(i=1)^m?〖p(y^((i) ) |x^((i) )  θ)〗

331
00:16:55,770 --> 00:17:04,140
log∏_(i=1)^m?〖p(y^((i) ) |x^((i) )  θ)〗   对吗?

332
00:17:04,380 --> 00:17:07,160
在logistic回归

333
00:17:07,290 --> 00:17:08,320
以及一些其它的判别学习模型中

334
00:17:08,440 --> 00:17:11,040
我们经常会对p(y^((i) ) |x^((i) );θ)

335
00:17:11,120 --> 00:17:12,420
进行建模

336
00:17:12,540 --> 00:17:22,560
它是conditional likelihood

337
00:17:22,680 --> 00:17:25,990
因为我们是在对p(y^((i) ) |x^((i) ))进行建模

338
00:17:26,090 --> 00:17:29,240
而在生成学习算法中

339
00:17:29,350 --> 00:17:31,290
我们考虑的是joint likelihood

340
00:17:31,400 --> 00:17:33,990
里面的项是p(x^((i) )  y^((i) ))  明白吗?

341
00:17:34,130 --> 00:17:45,730
让我看看

342
00:17:45,820 --> 00:17:47,760
给定一个训练集合

343
00:17:47,830 --> 00:17:50,500
使用高斯判别分析模型

344
00:17:51,620 --> 00:17:52,820
拟合出模型的参数

345
00:17:52,940 --> 00:17:54,920
我们像往常一样进行极大似然估计

346
00:17:55,030 --> 00:18:03,130
关于参数Φ  μ_0  μ_1  Σ

347
00:18:03,210 --> 00:18:06,860
使l最大化

348
00:18:06,980 --> 00:18:11,130
如果我们求出了

349
00:18:11,260 --> 00:18:12,780
这些参数的极大似然估计

350
00:18:12,780 --> 00:18:14,190
你们会发现Φ等于这个式子

351
00:18:14,340 --> 00:18:18,880
这个极大似然估计的结果

352
00:18:18,990 --> 00:18:19,840
并不令人惊讶

353
00:18:19,970 --> 00:18:21,090
我把它写下来

354
00:18:21,210 --> 00:18:24,070
主要用来练习一下指示符号

355
00:18:24,190 --> 00:18:25,770
Φ的极大似然估计的结果

356
00:18:25,870 --> 00:18:28,910
应该是(∑_i?y^((i) ) )/m

357
00:18:29,030 --> 00:18:34,510
也可以写成这样

358
00:18:34,640 --> 00:18:36,560
对于所有的训练样本

359
00:18:36,670 --> 00:18:43,050
对1求和  之后除以m  明白吗?

360
00:18:43,190 --> 00:18:43,850
换句话说

361
00:18:43,970 --> 00:18:45,790
参数Φ的极大似然估计的结果

362
00:18:45,910 --> 00:18:50,310
应该是训练样本中标签为1的训练样本

363
00:18:50,380 --> 00:18:53,050
所占的比例

364
00:18:53,170 --> 00:18:59,160
对于μ_0的极大似然估计

365
00:18:59,270 --> 00:19:20,850
是这样的  明白吗?

366
00:19:20,970 --> 00:19:23,250
你们应该看一会儿这个公式

367
00:19:23,370 --> 00:19:24,810
看看是不是能看明白

368
00:19:24,920 --> 00:19:27,840
在你们看公式的时候

369
00:19:27,930 --> 00:19:48,070
我会把μ_1的公式写出来  好吗?

370
00:19:48,210 --> 00:19:52,690
这个式子的分母

371
00:19:52,850 --> 00:19:55,690
是你的训练集合中y^((i) )=0的样本数之和

372
00:19:55,840 --> 00:20:00,380
所以对于每个y^((i) )=0的样本

373
00:20:00,510 --> 00:20:05,710
这个求和的结果会增加1  明白吗?

374
00:20:05,840 --> 00:20:08,650
所以分母就是

375
00:20:08,790 --> 00:20:19,510
标签为0的样本数目  明白吗?

376
00:20:19,640 --> 00:20:21,190
那么

377
00:20:21,530 --> 00:20:24,170
让我们来看一下分子

378
00:20:24,290 --> 00:20:25,340
对m个样本求和

379
00:20:25,460 --> 00:20:27,520
每当y^((i) )=0时

380
00:20:27,630 --> 00:20:29,730
这里就是1  否则

381
00:20:29,850 --> 00:20:31,090
这里就是0

382
00:20:31,210 --> 00:20:33,150
所以这个指示函数的含义是

383
00:20:33,260 --> 00:20:35,700
你只考虑那些

384
00:20:35,820 --> 00:20:37,170
y^((i) )=1

385
00:20:37,270 --> 00:20:39,210
对不起  只考虑那些y^((i) )=0的项

386
00:20:39,330 --> 00:20:41,250
因为对于所有的

387
00:20:41,330 --> 00:20:42,820
y^((i) )=1 的项

388
00:20:42,930 --> 00:20:46,980
这一项应该为0

389
00:20:47,070 --> 00:20:49,560
之后你又乘上了x^((i) )

390
00:20:49,670 --> 00:20:56,330
所以分子实际上

391
00:20:56,410 --> 00:21:03,050
是对标签为0的所有样本的

392
00:21:03,170 --> 00:21:07,030
x^((i) )的求和 明白吗?

393
00:21:07,100 --> 00:21:09,600
明白的请举手

394
00:21:09,700 --> 00:21:13,650
很好

395
00:21:13,770 --> 00:21:16,480
我们可以直观地理解一下这个公式

396
00:21:16,590 --> 00:21:19,090
这意味着你要遍历整个训练集合

397
00:21:19,180 --> 00:21:22,130
从中找到所有y=0的样本

398
00:21:22,220 --> 00:21:26,380
之后对于所有这些y=0的样本

399
00:21:26,490 --> 00:21:28,450
对它们的x做平均

400
00:21:28,530 --> 00:21:31,350
选取所有的负样本

401
00:21:31,470 --> 00:21:35,530
并对它们的x的值做平均  从而得到μ_0  明白吗?

402
00:21:35,630 --> 00:21:42,860
如果你仍然对这些符号感到陌生

403
00:21:42,930 --> 00:21:43,870
如果你仍然不确定

404
00:21:44,000 --> 00:21:46,680
为什么这些公式可以按我说的理解

405
00:21:46,790 --> 00:21:51,010
回去自己再复习一下

406
00:21:51,110 --> 00:21:52,250
直到搞清楚它

407
00:21:52,360 --> 00:21:54,430
得到这样的结果并不令人惊讶

408
00:21:54,550 --> 00:21:56,130
它的意思就是说  要想估测

409
00:21:56,240 --> 00:21:57,390
所有负样本的均值

410
00:21:57,500 --> 00:21:59,190
就选取所有的负样本  然后对它们求平均

411
00:21:59,300 --> 00:22:01,910
所以这个结论并不令人惊讶  但是把结论

412
00:22:02,020 --> 00:22:03,170
写出来是对指示符号的一个很好的练习

413
00:22:03,290 --> 00:22:08,660
对Σ的极大似然估计的推导

414
00:22:08,750 --> 00:22:10,660
我这里就不写了

415
00:22:10,760 --> 00:22:12,370
你们可以自己回去看讲义

416
00:22:12,490 --> 00:22:24,640
找到这些参数之后

417
00:22:24,770 --> 00:22:29,600
μ_0  μ_1  Σ

418
00:22:29,710 --> 00:22:32,480
你们现在需要进行预测

419
00:22:32,610 --> 00:22:37,880
当你得到一个新的x

420
00:22:38,000 --> 00:22:39,140
例如你们得到一个新的癌症样本

421
00:22:39,250 --> 00:22:40,840
你们需要预测它是恶性的还是良性的

422
00:22:40,960 --> 00:22:44,750
你们的预测应该是

423
00:22:44,850 --> 00:22:49,240
给定x的情况下最可能的y

424
00:22:49,830 --> 00:22:52,140
我这里应该写分号来表示参数

425
00:22:52,240 --> 00:22:53,240
之后我会得到

426
00:22:53,330 --> 00:23:05,250
根据贝叶斯公式  使这个式子最大的y  对吗?

427
00:23:05,350 --> 00:23:06,960
也就是等于这个式子

428
00:23:07,080 --> 00:23:17,190
因为p(x)是独立于y的

429
00:23:17,290 --> 00:23:24,290
所以它的值不会变的 如果p(y)是均匀分布

430
00:23:24,400 --> 00:23:27,830
换句话说

431
00:23:27,950 --> 00:23:34,030
如果取每种类型的概率都相同

432
00:23:34,150 --> 00:23:37,390
如果p(y)对于不同的y值取值都相同

433
00:23:37,510 --> 00:23:44,760
那么最后要求的就是使p(x|y)最大的那个y  明白吗?

434
00:23:44,870 --> 00:23:48,930
这种情形有时会出现  但是不是很常见

435
00:23:49,040 --> 00:23:51,040
所以通常情况下你会用到这个公式

436
00:23:51,130 --> 00:23:55,210
这里需要使用你的模型

437
00:23:55,310 --> 00:23:58,380
来计算p(x|y)与p(y)  明白吗?

438
00:23:58,510 --> 00:24:00,340
你能给我们argmax的定义吗?

439
00:24:00,470 --> 00:24:02,310
哦  让我看看

440
00:24:02,420 --> 00:24:09,740
如果你-让我来举个例子 最小的-

441
00:24:09,880 --> 00:24:11,470
argmax表示

442
00:24:11,550 --> 00:24:12,900
使这一项最大的那个y的值

443
00:24:13,010 --> 00:24:14,110
好的  我明白了

444
00:24:14,220 --> 00:24:15,940
举个例子

445
00:24:16,040 --> 00:24:19,800
min?(x-5)^2=0

446
00:24:19,910 --> 00:24:21,890
因为令x=5  它的值为0

447
00:24:22,010 --> 00:24:27,080
那么:argmin_x (x-5)^2=5

448
00:24:27,190 --> 00:24:30,040
因为5是使得这个式子最小的

449
00:24:30,140 --> 00:24:33,500
那个x的值  明白吗?很好

450
00:24:33,640 --> 00:24:38,490
谢谢你

451
00:24:42,000 --> 00:24:44,400
好的

452
00:24:44,500 --> 00:24:48,030
还有问题吗?什么?

453
00:24:48,140 --> 00:24:50,600
为什么要移除分配值

454
00:24:50,710 --> 00:24:52,430
为什么

455
00:24:52,550 --> 00:25:02,460
哦  我知道了

456
00:25:02,570 --> 00:25:05,420
我说分布是均匀的-我这里没有写

457
00:25:05,530 --> 00:25:10,090
我的意思是p(y=0)=p(y=1)

458
00:25:10,210 --> 00:25:11,860
或者说y是集合

459
00:25:11,950 --> 00:25:14,530
上的均匀分布

460
00:25:14,660 --> 00:25:15,700
哦

461
00:25:15,810 --> 00:25:18,430
我的意思是-

462
00:25:18,560 --> 00:25:23,530
是的  p(y=0)=p(y=1)

463
00:25:23,650 --> 00:25:28,540
我的意思是这样的  明白吗?还有问题吗?

464
00:25:28,650 --> 00:25:39,690
好的

465
00:25:39,790 --> 00:25:44,160
实际上高斯判别分析

466
00:25:44,300 --> 00:25:46,690
与logistic回归之间

467
00:25:46,690 --> 00:25:47,970
存在有趣的联系

468
00:25:48,100 --> 00:25:50,460
让我举例说明一下

469
00:25:50,570 --> 00:25:56,260
假设你有一个训练集合-

470
00:25:56,660 --> 00:26:00,200
让我们继续

471
00:26:00,280 --> 00:26:01,320
来画一个一维的训练集合

472
00:26:01,480 --> 00:26:13,760
这样就可以了  好的

473
00:26:13,880 --> 00:26:15,840
比如说我们有一个训练集合

474
00:26:15,930 --> 00:26:17,350
包含一些负样本和一些正样本

475
00:26:17,470 --> 00:26:20,800
假如现在我运行了高斯判别分析

476
00:26:20,920 --> 00:26:22,520
之后我会对每一类样本拟合出

477
00:26:22,640 --> 00:26:23,690
一个概率密度函数-

478
00:26:23,810 --> 00:26:25,220
对于两类中的每一类都会

479
00:26:25,220 --> 00:26:26,880
拟合出一个高斯概率密度函数

480
00:26:27,000 --> 00:26:29,280
包括我的正样本和负样本

481
00:26:29,400 --> 00:26:35,290
所以也许我的正样本  这些x点

482
00:26:35,400 --> 00:26:37,100
可能拟合出一条这样的高斯曲线

483
00:26:37,270 --> 00:26:46,780
对于我的负样本

484
00:26:46,890 --> 00:26:50,680
可能拟合出一条这样的高斯曲线

485
00:26:50,810 --> 00:27:02,030
现在

486
00:27:02,160 --> 00:27:05,650
让我们沿x轴进行变化

487
00:27:05,780 --> 00:27:08,190
我希望做的是

488
00:27:08,350 --> 00:27:11,770
我会在一些x轴上的点的正上方画一些点

489
00:27:11,860 --> 00:27:16,220
对于水平轴上的x  我会画出

490
00:27:16,350 --> 00:27:24,010
与其相应的表示p(y=1|x)

491
00:27:24,010 --> 00:27:28,600
的值的点 明白吗?

492
00:27:28,730 --> 00:27:32,710
我意识到我之前就应该这样做了

493
00:27:32,800 --> 00:27:34,940
我将x点称之为负样本

494
00:27:35,030 --> 00:27:36,230
我会

495
00:27:36,310 --> 00:27:37,470
将o点称之为正样本

496
00:27:37,560 --> 00:27:39,080
这样会方便我们这一部分的讲解

497
00:27:39,200 --> 00:27:41,670
我们选一个值相当小的x值

498
00:27:41,760 --> 00:27:44,660
比如说x是水平轴上的这个值

499
00:27:44,770 --> 00:27:46,510
那么这一点上

500
00:27:46,800 --> 00:27:48,190
p(y=1|x)的值是多少呢?

501
00:27:48,300 --> 00:27:49,970
计算这个值的方法是

502
00:27:50,070 --> 00:27:54,560
对于p(y=1|x)

503
00:27:54,670 --> 00:27:57,140
将之前的那些公式代入  对吗?

504
00:27:57,240 --> 00:27:58,600
这里是p(x|y=1)

505
00:27:58,680 --> 00:28:00,480
这是一个高斯密度函数

506
00:28:00,550 --> 00:28:04,430
乘以p(y=1)  你知道  这一项实质上是-

507
00:28:04,520 --> 00:28:07,040
这一项等于Φ

508
00:28:07,190 --> 00:28:10,520
之后除以  对  p(x)

509
00:28:10,630 --> 00:28:12,200
这一项告诉你应该怎样计算它

510
00:28:12,330 --> 00:28:14,480
根据这两个高斯分布

511
00:28:14,610 --> 00:28:16,780
以及p(y)分布中的Φ

512
00:28:16,910 --> 00:28:20,110
我实际上可以通过计算得到p(y=1|x)

513
00:28:20,250 --> 00:28:26,230
这个例子中  如果x这么小

514
00:28:26,380 --> 00:28:28,340
很显然它属于左边的这个高斯分布

515
00:28:28,490 --> 00:28:30,710
它不太可能属于正样本类

516
00:28:30,820 --> 00:28:32,420
它非常小

517
00:28:32,570 --> 00:28:34,650
非常接近0  明白吗?

518
00:28:34,800 --> 00:28:37,910
之后我们可以稍微增加x的值

519
00:28:38,040 --> 00:28:39,680
取一个新的值

520
00:28:39,820 --> 00:28:45,830
画出这一点对应的p(y=1|x)

521
00:28:45,950 --> 00:28:47,400
它仍然非常小

522
00:28:47,540 --> 00:28:51,180
让我们使用这一点  好吗?

523
00:28:51,330 --> 00:28:53,240
这一点上

524
00:28:53,390 --> 00:28:56,270
两个高斯分布会得到相同的点

525
00:28:56,390 --> 00:29:00,550
那么我要问  如果这是x的值

526
00:29:00,700 --> 00:29:01,910
就是由箭头所指的这个值

527
00:29:02,050 --> 00:29:03,120
p(y=1|x)

528
00:29:03,270 --> 00:29:05,060
应该等于多少?

529
00:29:05,180 --> 00:29:06,100
实际上你算不出来

530
00:29:06,220 --> 00:29:07,470
也许它是0.5  好吗?

531
00:29:07,610 --> 00:29:12,530
如果你填充更多的点

532
00:29:12,670 --> 00:29:14,520
你会得到一条这样的曲线

533
00:29:14,640 --> 00:29:17,680
之后你可以继续

534
00:29:17,770 --> 00:29:19,290
比如  对于这样的一个点

535
00:29:19,410 --> 00:29:21,240
你可以问y=1的概率是多少?

536
00:29:21,360 --> 00:29:23,520
如果它的位置如此靠外  那么很显然

537
00:29:23,650 --> 00:29:25,990
它属于右边的这个高斯分布

538
00:29:26,100 --> 00:29:28,430
所以这一点上y=1的概率

539
00:29:28,580 --> 00:29:29,370
将会非常高

540
00:29:29,540 --> 00:29:30,890
它几乎等于1

541
00:29:31,040 --> 00:29:34,430
所以  你可以重复这一过程

542
00:29:34,570 --> 00:29:37,310
得到很多新的点

543
00:29:37,410 --> 00:29:40,020
只需要对于每个点

544
00:29:40,140 --> 00:29:41,310
计算p(y=1|x)

545
00:29:41,440 --> 00:29:43,280
如果你将这些点连起来

546
00:29:43,390 --> 00:29:49,080
你会发现  你最终得到的这条曲线

547
00:29:49,280 --> 00:29:53,450
形状和sigmoid函数曲线的形状很相似 对吗?

548
00:29:53,590 --> 00:29:57,460
换句话说

549
00:29:57,600 --> 00:29:58,710
当你做出了这些假设

550
00:29:58,840 --> 00:30:04,090
使用高斯判别分析模型

551
00:30:04,210 --> 00:30:07,510
其中p(x|y)属于高斯分布

552
00:30:07,650 --> 00:30:11,190
当你回头计算p(y|x)时

553
00:30:11,260 --> 00:30:12,900
实际上你几乎得到了

554
00:30:13,120 --> 00:30:15,390
之前和我们在logistic回归中使用的

555
00:30:15,530 --> 00:30:18,380
sigmoid函数一样的函数

556
00:30:18,520 --> 00:30:22,300
但是实际上  它们之间存在本质区别

557
00:30:22,430 --> 00:30:24,920
高斯判别分析得到的曲线

558
00:30:25,060 --> 00:30:30,150
无论是位置和这个部分的陡峭程度

559
00:30:30,290 --> 00:30:33,040
都和logistic回归中的sigmoid函数不同

560
00:30:33,170 --> 00:30:34,490
有问题吗?

561
00:30:34,660 --> 00:30:37,420
我只是疑惑

562
00:30:37,570 --> 00:30:39,370
这个高斯分布

563
00:30:39,540 --> 00:30:40,780
不  让我们看看

564
00:30:40,870 --> 00:30:46,690
这个高斯分布表示的是p(x|y=1)

565
00:30:46,880 --> 00:30:51,450
这个高斯分布表示的是p(x|y=0)

566
00:30:51,600 --> 00:30:54,680
明白了吗?还有其他问题吗?

567
00:30:54,830 --> 00:30:58,300
好的

568
00:30:58,440 --> 00:31:00,090
什么问题?

569
00:31:00,240 --> 00:31:02,040
当你在画那些点时

570
00:31:02,200 --> 00:31:04,770
你怎样求出和x对应的y?

571
00:31:04,920 --> 00:31:07,350
能再说一遍吗?

572
00:31:07,500 --> 00:31:08,500
对不起

573
00:31:08,640 --> 00:31:09,650
你能重复一遍

574
00:31:09,800 --> 00:31:11,430
你是怎样画出每个点的吗?

575
00:31:11,610 --> 00:31:13,280
让我看看  好的

576
00:31:13,420 --> 00:31:17,490
计算的过程是这样的

577
00:31:17,620 --> 00:31:20,040
首先我得到一个训练集合

578
00:31:20,200 --> 00:31:21,580
给定了这组训练集合

579
00:31:21,720 --> 00:31:22,490
我会拟合出

580
00:31:22,630 --> 00:31:24,770
一个高斯判别分析模型

581
00:31:24,960 --> 00:31:26,360
这意味着

582
00:31:26,540 --> 00:31:30,050
我会为p(x|y=1)建立一个模型

583
00:31:30,190 --> 00:31:32,530
我会为p(x|y=0)建立一个模型

584
00:31:32,690 --> 00:31:35,390
我同样会拟合出p(y)的

585
00:31:35,390 --> 00:31:38,090
伯努利分布模型  对吗?

586
00:31:38,240 --> 00:31:40,340
换句话说  给定一个训练集合

587
00:31:40,480 --> 00:31:43,960
我会基于这组数据拟合出p(x|y)和p(y)

588
00:31:44,130 --> 00:31:45,910
现在我已经选定了模型的参数

589
00:31:46,040 --> 00:31:51,020
也就是μ_0  μ_1 和Σ  对吗?

590
00:31:51,150 --> 00:31:53,480
接下来我要做的是

591
00:31:53,610 --> 00:31:56,660
画出这些点  对吗?

592
00:31:56,790 --> 00:31:58,380
我从x轴上挑一个点

593
00:31:58,520 --> 00:32:04,320
之后相对于该点x  我会计算出p(y|x)

594
00:32:04,500 --> 00:32:07,290
计算出p(y=1|x)

595
00:32:07,430 --> 00:32:09,240
这将是一个0 1之间的值

596
00:32:09,410 --> 00:32:10,630
是一个实数

597
00:32:10,750 --> 00:32:11,920
不管这个实数是什么

598
00:32:12,030 --> 00:32:15,130
我都会将它画在纵轴上  对吗?

599
00:32:15,240 --> 00:32:17,210
我计算p(y=1|x)的时候

600
00:32:17,370 --> 00:32:22,760
需要用到这些量

601
00:32:22,900 --> 00:32:27,300
我会用到p(x|y)和p(y)

602
00:32:27,430 --> 00:32:28,760
将它们代入贝叶斯公式

603
00:32:28,870 --> 00:32:31,540
这样我就可以基于这三个量

604
00:32:31,680 --> 00:32:33,720
计算得到p(y|x)了

605
00:32:33,850 --> 00:32:34,980
明白了吗?

606
00:32:35,120 --> 00:32:36,510
明白了

607
00:32:36,630 --> 00:32:37,420
还有

608
00:32:37,600 --> 00:32:38,680
其它问题吗?

609
00:32:38,830 --> 00:32:41,390
你怎样对p(x)进行建模?

610
00:32:41,520 --> 00:32:43,700
哦  好的

611
00:32:43,840 --> 00:32:46,240
写在这里

612
00:32:46,330 --> 00:32:49,260
p(x)可以被写成

613
00:32:49,370 --> 00:32:58,440
p(x|y=0)p(y=0)+p(x|y=1)p(y=1)

614
00:32:58,570 --> 00:33:04,620
所有的这些量

615
00:33:04,740 --> 00:33:06,300
p(x|y)和p(y)

616
00:33:06,450 --> 00:33:09,040
我都可以直接地

617
00:33:09,190 --> 00:33:10,900
从我的高斯判别分析模型中得到

618
00:33:11,020 --> 00:33:13,340
所有的这些项

619
00:33:13,460 --> 00:33:15,240
都可以直接从模型中得到

620
00:33:15,360 --> 00:33:17,700
将这些项带入分母

621
00:33:17,810 --> 00:33:19,180
这样

622
00:33:19,300 --> 00:33:22,460
这是我计算p(y=1|x)的方法  明白了吗?

623
00:33:22,560 --> 00:33:24,180
谢谢

624
00:33:24,400 --> 00:33:26,120
很好

625
00:33:26,260 --> 00:33:56,150
接下来我们来讨论一些

626
00:33:56,280 --> 00:33:57,390
使用生成学习算法的

627
00:33:57,530 --> 00:33:59,700
好处和坏处

628
00:33:59,820 --> 00:34:01,900
对于这个

629
00:34:02,050 --> 00:34:03,700
高斯判别分析的特例

630
00:34:03,830 --> 00:34:08,900
我们假设x|y服从高斯分布

631
00:34:09,040 --> 00:34:15,130
我之前黑板上

632
00:34:15,260 --> 00:34:16,330
展示的那个论点

633
00:34:16,460 --> 00:34:17,510
我并没有形式化地证明

634
00:34:17,680 --> 00:34:19,200
但是你们可以回去

635
00:34:19,320 --> 00:34:20,860
自己证明

636
00:34:20,940 --> 00:34:23,290
如果你假设x|y服从高斯分布

637
00:34:23,440 --> 00:34:29,750
这意味着  如果你相对于x对p(y|x)画点

638
00:34:29,930 --> 00:34:32,030
你会发现

639
00:34:32,160 --> 00:34:40,010
让我们先把logistic后验分布写下来

640
00:34:40,130 --> 00:34:41,980
这是我之前写的一个论点

641
00:34:42,100 --> 00:34:43,130
我并没有证明

642
00:34:43,260 --> 00:34:44,420
你们可以回去自己证明

643
00:34:44,550 --> 00:34:46,710
如果假设x|y服从高斯分布

644
00:34:46,820 --> 00:34:50,530
这意味着p(y=1|x)

645
00:34:50,640 --> 00:34:56,540
的后验分布函数

646
00:34:56,680 --> 00:34:58,850
将会是一个logistic函数

647
00:34:58,980 --> 00:35:06,350
事实证明

648
00:35:06,500 --> 00:35:10,880
这个结论在反方向上并不成立  明白吗?

649
00:35:11,020 --> 00:35:13,810
事实证明

650
00:35:13,970 --> 00:35:16,270
这个结论有点酷

651
00:35:16,400 --> 00:35:19,600
事实证明  如果你假设

652
00:35:19,670 --> 00:35:27,760
x|y=1是泊松分布  参数为λ_1

653
00:35:27,910 --> 00:35:30,650
x|y=0

654
00:35:30,830 --> 00:35:38,240
也是泊松分布  参数为λ_0

655
00:35:38,390 --> 00:35:40,900
实际上  如果你进行了上述假设

656
00:35:41,070 --> 00:35:44,610
着同样意味着

657
00:35:44,800 --> 00:35:53,890
p(y=1|x)是一个logistic函数

658
00:35:54,020 --> 00:35:56,810
关于x|y还可以有很多其它的假设

659
00:35:56,960 --> 00:36:02,070
可以使p(y=1|x)是一个logistic函数

660
00:36:02,160 --> 00:36:07,310
因此

661
00:36:07,450 --> 00:36:09,860
所以  x|y服从高斯分布的假设

662
00:36:10,010 --> 00:36:14,030
与y|x服从logistic分布的假设相比

663
00:36:14,170 --> 00:36:15,610
是一个更强的假设

664
00:36:15,750 --> 00:36:17,870
因为这个假设蕴含这个假设  对吗?

665
00:36:18,010 --> 00:36:19,730
所以这意味着这是一个更强的假设

666
00:36:19,890 --> 00:36:23,170
因为

667
00:36:23,310 --> 00:36:24,980
只要当x|y服从高斯分布时

668
00:36:25,100 --> 00:36:26,460
p(y=1|x)就是一个logistic

669
00:36:26,460 --> 00:36:27,620
后验分布  但是反过来不一定成立.

670
00:36:27,810 --> 00:36:32,490
这样我们就可以

671
00:36:32,600 --> 00:36:34,370
在高斯判别分析与logistic回归之间

672
00:36:34,490 --> 00:36:36,820
进行一些权衡  对吗?

673
00:36:36,970 --> 00:36:38,320
高斯判别分析

674
00:36:38,430 --> 00:36:40,290
做出了一个更强的假设

675
00:36:40,430 --> 00:36:41,770
即:x|y服从高斯分布

676
00:36:41,880 --> 00:36:44,720
所以当这个假设正确

677
00:36:44,850 --> 00:36:46,900
或者近似正确时

678
00:36:47,040 --> 00:36:50,290
也就是说  当你在画点时  x|y

679
00:36:50,420 --> 00:36:51,600
大概服从高斯分布时

680
00:36:51,740 --> 00:36:53,860
此时如果你在算法中

681
00:36:53,970 --> 00:36:55,480
显式地做出了这个假设

682
00:36:55,600 --> 00:36:57,590
那么这个算法的表现将会更好

683
00:36:57,690 --> 00:36:59,850
因为这个算法

684
00:36:59,970 --> 00:37:01,860
利用了更多的关于数据的信息

685
00:37:01,970 --> 00:37:04,660
算法知道数据服从高斯分布  对吗?

686
00:37:04,760 --> 00:37:07,060
所以如果高斯假设

687
00:37:07,120 --> 00:37:08,910
成立或大概成立

688
00:37:09,030 --> 00:37:11,600
那么高斯判别分析

689
00:37:11,690 --> 00:37:12,910
将会优于logistic回归

690
00:37:13,030 --> 00:37:14,890
如果  相反地

691
00:37:14,960 --> 00:37:17,410
如果你不确定x|y的分布情况

692
00:37:17,540 --> 00:37:19,960
那么logistic回归

693
00:37:20,070 --> 00:37:23,050
一个判别算法  它的表现可能会更好

694
00:37:23,290 --> 00:37:24,800
举个例子

695
00:37:24,920 --> 00:37:26,770
也许你预先假设

696
00:37:26,880 --> 00:37:28,470
数据服从高斯分布

697
00:37:28,570 --> 00:37:30,910
但是实际上数据服从泊松分布

698
00:37:31,030 --> 00:37:33,790
那么logistic回归仍然会获得不错的效果

699
00:37:33,910 --> 00:37:36,870
因为如果数据服从泊松分布

700
00:37:36,970 --> 00:37:38,970
p(y=1|x)仍然是一个logistic函数

701
00:37:39,040 --> 00:37:40,520
所以会获得不错的效果

702
00:37:40,630 --> 00:37:42,330
但是如果你假设数据服从高斯分布

703
00:37:42,440 --> 00:37:44,170
那么算法的效果

704
00:37:44,230 --> 00:37:46,640
可能就不会那么好  明白吗?

705
00:37:46,730 --> 00:37:55,930
事实证明-

706
00:37:56,040 --> 00:37:57,720
这稍微有些不同

707
00:37:57,820 --> 00:38:03,090
事实证明

708
00:38:03,190 --> 00:38:04,760
使用生成学习算法的真正的好处通常在于

709
00:38:04,860 --> 00:38:08,380
它需要更少数据

710
00:38:08,460 --> 00:38:09,970
通常情况下数据并不是

711
00:38:09,970 --> 00:38:12,240
精确地服从高斯分布的  对吗?

712
00:38:12,370 --> 00:38:14,020
因为数据总是近似地服从高斯分布

713
00:38:14,140 --> 00:38:15,580
并非精确地服从高斯分布

714
00:38:15,690 --> 00:38:18,210
事实证明  生成学习算法的效果

715
00:38:18,290 --> 00:38:20,490
通常出奇地好

716
00:38:20,590 --> 00:38:22,800
即使模型假设不满足

717
00:38:22,930 --> 00:38:26,420
一个另外的权衡是

718
00:38:26,510 --> 00:38:31,190
通过对数据做更强的假设

719
00:38:31,300 --> 00:38:35,710
高斯判别分析为了拟合出一个还不错的模型

720
00:38:35,840 --> 00:38:37,950
通常需要更少的数据

721
00:38:38,060 --> 00:38:39,770
即使样本数量很小

722
00:38:39,890 --> 00:38:41,410
相比之下

723
00:38:41,550 --> 00:38:44,080
logistic回归的假设更少

724
00:38:44,220 --> 00:38:47,970
对模型的假设方面更为健壮

725
00:38:48,060 --> 00:38:49,580
因为你做了更弱的假设

726
00:38:49,690 --> 00:38:50,850
你做出了更少的假设

727
00:38:50,980 --> 00:38:52,970
但有时  与高斯判别分析相比

728
00:38:53,080 --> 00:38:56,330
为了拟合出模型  它需要多的样本

729
00:38:56,450 --> 00:38:57,630
有问题吗?

730
00:38:57,750 --> 00:39:00,280
为了实现任意

731
00:39:00,390 --> 00:39:02,290
关于数的假设

732
00:39:02,450 --> 00:39:05,260
我们假定PFY等于1

733
00:39:05,360 --> 00:39:10,860
等于两个数

734
00:39:10,970 --> 00:39:14,110
当样本数为边界时为真值

735
00:39:14,230 --> 00:39:16,890
好的  我们看一下

736
00:39:17,010 --> 00:39:21,400
你的问题是  它是否正确-

737
00:39:21,520 --> 00:39:22,450
是什么?

738
00:39:22,570 --> 00:39:24,350
让我换一种说法

739
00:39:24,470 --> 00:39:27,570
建模的假设是

740
00:39:27,680 --> 00:39:29,020
独立于你的训练集合的大小的

741
00:39:29,150 --> 00:39:31,990
例如  在最小二乘回归中

742
00:39:32,110 --> 00:39:34,460
在所有的这些模型中  我都在假设

743
00:39:34,570 --> 00:39:37,200
有一些随机变量

744
00:39:37,330 --> 00:39:38,940
服从某些概率分布  最后

745
00:39:39,050 --> 00:39:41,630
我利用一个训练样本

746
00:39:41,760 --> 00:39:44,330
来得到分布中的参数  对吗?

747
00:39:44,440 --> 00:39:49,330
y=1的概率是什么?

748
00:39:49,440 --> 00:39:52,840
y=1的概率?

749
00:39:52,990 --> 00:39:54,990
是的  你用到了-

750
00:39:55,230 --> 00:39:57,850
这有点类似于-

751
00:39:57,950 --> 00:40:00,830
让我们回到

752
00:40:00,940 --> 00:40:01,980
极大似然估计的哲学中

753
00:40:02,070 --> 00:40:05,630
我假设:

754
00:40:05,750 --> 00:40:07,660
p(y)等于

755
00:40:07,790 --> 00:40:11,000
Φ^y (1-Φ)^(1-y)

756
00:40:11,150 --> 00:40:11,990
所以  我假设了

757
00:40:12,090 --> 00:40:14,870
有一些y的真值生成了我们的全部数据

758
00:40:15,000 --> 00:40:20,730
当我写这个的时候  我想

759
00:40:20,860 --> 00:40:22,620
也许我应该写-

760
00:40:22,760 --> 00:40:26,520
当我写这个的时候

761
00:40:26,650 --> 00:40:29,350
我想Φ已经具有两个值了

762
00:40:29,440 --> 00:40:32,370
一个是Φ的真实值

763
00:40:32,480 --> 00:40:34,380
用来生成整个数据

764
00:40:34,520 --> 00:40:36,820
第二个是通过极大似然估计

765
00:40:36,920 --> 00:40:38,380
计算得到的Φ

766
00:40:38,520 --> 00:40:40,870
我之前写的那些公式

767
00:40:40,970 --> 00:40:44,010
那些关于Φ μ_0 μ_1的公式

768
00:40:44,120 --> 00:40:45,590
实际上都是

769
00:40:45,700 --> 00:40:47,240
对它们的极大似然估计

770
00:40:47,290 --> 00:40:50,100
它们和真正的参数值是不同的

771
00:40:50,200 --> 00:40:51,750
但是-

772
00:40:52,130 --> 00:40:53,480
【听不清】

773
00:40:53,620 --> 00:40:55,390
是的

774
00:40:55,520 --> 00:40:56,450
极大似然估计

775
00:40:56,550 --> 00:40:58,480
是通过数据得到的  除此之外

776
00:40:58,590 --> 00:41:00,630
还有一些隐藏的

777
00:41:00,740 --> 00:41:01,910
我尝试去预测的Φ的真实值

778
00:41:02,030 --> 00:41:03,120
极大似然估计的结果

779
00:41:03,300 --> 00:41:04,840
对真实值估计的一种尝试

780
00:41:04,960 --> 00:41:06,070
但是

781
00:41:06,230 --> 00:41:08,910
出于符号上的一些约定和方便的考虑

782
00:41:09,040 --> 00:41:11,190
我们通常不会区分

783
00:41:11,310 --> 00:41:13,310
极大似然估计得到的值

784
00:41:13,430 --> 00:41:14,590
与我们尝试去估计

785
00:41:14,680 --> 00:41:15,880
的真实值

786
00:41:16,000 --> 00:41:18,140
这是我想要估计的值

787
00:41:18,270 --> 00:41:24,510
实际上

788
00:41:24,630 --> 00:41:27,320
对于和极大似然估计有关的问题

789
00:41:27,430 --> 00:41:28,810
以及一些类似的问题

790
00:41:28,940 --> 00:41:32,970
我希望助教能够在周五的习题课上

791
00:41:33,090 --> 00:41:38,020
去解答这类

792
00:41:38,140 --> 00:41:39,920
和概率论中的定义有关的问题

793
00:41:40,070 --> 00:41:42,300
还有其它问题吗?

794
00:41:42,420 --> 00:41:50,360
没有了 很好

795
00:41:50,470 --> 00:42:01,790
好的

796
00:42:01,890 --> 00:42:03,780
再讲一些比较酷的东西

797
00:42:03,900 --> 00:42:10,740
我提到过  如果x|y服从泊松分布

798
00:42:10,850 --> 00:42:12,910
你仍然会得到logistic后验分布

799
00:42:13,030 --> 00:42:14,330
实际上

800
00:42:14,430 --> 00:42:15,840
还有更为一般化的结论

801
00:42:15,940 --> 00:42:18,180
如果你假设x|y=1

802
00:42:18,270 --> 00:42:25,950
属于指数分布族  以η_1为参数

803
00:42:26,080 --> 00:42:29,490
之后你可以假设x|y=0

804
00:42:29,610 --> 00:42:35,350
属于指数分布族  以η_0为参数

805
00:42:35,460 --> 00:42:41,170
这蕴含着p(y=1|x)

806
00:42:41,260 --> 00:42:43,350
是一个logistic函数  明白吗?

807
00:42:43,480 --> 00:42:44,890
这个结论很酷

808
00:42:45,010 --> 00:42:47,310
这意味着x|y可以是

809
00:42:47,460 --> 00:42:49,130
我不知道  一些很奇怪的东西

810
00:42:49,220 --> 00:42:50,440
可以是伽马分布

811
00:42:50,600 --> 00:42:52,190
我们已经用过了高斯分布

812
00:42:52,200 --> 00:42:53,300
那么之后还可以用-

813
00:42:53,410 --> 00:42:55,450
我不确定  或许可以用伽马指数分布

814
00:42:55,630 --> 00:42:58,150
实际上是β分布

815
00:42:58,270 --> 00:43:00,600
我在想

816
00:43:00,720 --> 00:43:01,760
有哪些指数分布族中的分布可以用

817
00:43:01,890 --> 00:43:03,060
可以是任何分布

818
00:43:03,200 --> 00:43:05,030
所以

819
00:43:05,100 --> 00:43:06,320
使用相同的指数分布族的分布

820
00:43:06,470 --> 00:43:07,740
对这两类数据进行建模

821
00:43:07,870 --> 00:43:09,230
采用不同的自然参数

822
00:43:09,390 --> 00:43:14,460
这样所得的后验分布p(y=1|x)

823
00:43:14,530 --> 00:43:15,410
是logistic后验分布

824
00:43:15,560 --> 00:43:17,520
所以这显示了

825
00:43:17,650 --> 00:43:20,170
logistic回归

826
00:43:20,290 --> 00:43:21,260
在建模假设选择方面的鲁棒性

827
00:43:21,380 --> 00:43:23,340
因为即使数据

828
00:43:23,460 --> 00:43:24,440
服从伽马分布

829
00:43:24,560 --> 00:43:26,780
结果仍然是logistic函数

830
00:43:26,880 --> 00:43:30,470
所以这就是logistic回归

831
00:43:30,580 --> 00:43:31,600
在建模假设方面的鲁棒性

832
00:43:31,750 --> 00:43:42,140
这就是概率密度函数

833
00:43:42,250 --> 00:43:45,280
我记得之前我提到过

834
00:43:45,390 --> 00:43:47,500
有两种方式

835
00:43:47,630 --> 00:43:48,730
可以导出logistic函数

836
00:43:48,820 --> 00:43:50,670
一个是我们上次课讲的

837
00:43:50,780 --> 00:43:52,210
我们可以从指数分布族中推导出来

838
00:43:52,300 --> 00:43:53,590
今天所讲的是第二个

839
00:43:53,700 --> 00:43:54,850
所有这些建模假设

840
00:43:54,980 --> 00:43:58,330
都会得到logistic函数 什么问题?

841
00:43:58,450 --> 00:43:59,530
【听不清】

842
00:43:59,660 --> 00:44:05,800
哦

843
00:44:05,930 --> 00:44:07,390
y=1这项是logistic函数

844
00:44:07,500 --> 00:44:08,780
这一项蕴含这一项

845
00:44:08,850 --> 00:44:10,340
这一项并不会蕴含这一项

846
00:44:10,460 --> 00:44:13,140
指数分布族的假设

847
00:44:13,370 --> 00:44:15,470
蕴含着y=1这项是logistic函数

848
00:44:15,570 --> 00:44:17,960
但是相反的假设不成立

849
00:44:18,100 --> 00:44:19,460
x还可以服从

850
00:44:19,540 --> 00:44:23,350
一些很奇怪的概率分布

851
00:44:23,440 --> 00:44:26,420
从而也可以得到logistic函数  明白吗?

852
00:44:26,540 --> 00:44:30,650
好的  我们来讲-

853
00:44:30,730 --> 00:44:33,180
上面的这些是第一个生成学习算法

854
00:44:33,290 --> 00:44:34,420
也许我会

855
00:44:34,540 --> 00:44:35,860
结合一个例子

856
00:44:35,980 --> 00:44:38,870
来讲一下第二个生成学习算法

857
00:44:38,970 --> 00:44:41,220
这个算法被称为朴素贝叶斯算法

858
00:44:41,350 --> 00:44:46,830
我要用到的例子是

859
00:44:46,920 --> 00:44:49,440
垃圾邮件分类

860
00:44:49,540 --> 00:44:51,220
好的 比如说

861
00:44:51,310 --> 00:44:52,630
你想实现一个垃圾邮件分类器

862
00:44:52,710 --> 00:44:53,960
以你的邮件的输入流作为输入

863
00:44:54,070 --> 00:45:00,460
确定邮件是否为垃圾邮件 让我看看

864
00:45:00,570 --> 00:45:08,880
y=0或1  1表示是垃圾邮件

865
00:45:08,980 --> 00:45:10,000
0表示不是垃圾邮件

866
00:45:10,120 --> 00:45:13,640
我们要做的第一个决策是

867
00:45:13,800 --> 00:45:15,360
给你一封邮件

868
00:45:15,460 --> 00:45:18,770
你怎样用一个特征向量x

869
00:45:18,860 --> 00:45:21,160
来表示这封邮件  对吗?

870
00:45:21,270 --> 00:45:22,890
电子邮件仅仅是一段文本  对吗?

871
00:45:23,000 --> 00:45:26,120
就像一个词列表

872
00:45:26,230 --> 00:45:27,540
或者一个ASCII字符的列表

873
00:45:27,660 --> 00:45:28,840
所以我可以将电子邮件

874
00:45:28,850 --> 00:45:30,030
表示为一个特征向量x

875
00:45:30,140 --> 00:45:33,440
我们可以有多种不同的表示方法

876
00:45:33,550 --> 00:45:35,490
但是我今天要用的是

877
00:45:35,610 --> 00:45:39,400
我们会这样创建特征向量x

878
00:45:39,540 --> 00:45:42,260
我会首先遍历我的词典

879
00:45:42,380 --> 00:45:44,900
并得到一个词典中词的列表

880
00:45:45,010 --> 00:45:46,420
第一个词是a

881
00:45:46,540 --> 00:45:48,350
第二个词是Aardvark

882
00:45:48,460 --> 00:45:54,350
ausworth

883
00:45:54,460 --> 00:45:56,770
在某个地方

884
00:45:56,880 --> 00:45:58,980
你在垃圾邮件中发现了"buy"

885
00:45:59,060 --> 00:46:00,140
告诉你一些购物的信息

886
00:46:00,260 --> 00:46:02,920
你应该怎样获得词语列表呢?

887
00:46:03,040 --> 00:46:06,980
你在其中找不到cs229

888
00:46:07,090 --> 00:46:08,170
也就是我们的课程号

889
00:46:08,280 --> 00:46:10,290
不过  你可以

890
00:46:10,400 --> 00:46:12,240
通过其它的你收到的邮件获得词列表

891
00:46:12,350 --> 00:46:14,440
或者从别的地方获得

892
00:46:14,520 --> 00:46:18,560
我的词典中的最后一个词是zymurgy

893
00:46:18,670 --> 00:46:24,110
它指的是一种技术化学

894
00:46:24,230 --> 00:46:26,420
用来处理酿酒过程中的发酵过程

895
00:46:26,530 --> 00:46:32,180
比如说我收到了一封邮件

896
00:46:32,280 --> 00:46:34,500
我要做的是

897
00:46:34,610 --> 00:46:36,360
遍历这个词列表

898
00:46:36,490 --> 00:46:38,980
一旦发现某个词

899
00:46:39,070 --> 00:46:40,740
出现在我的邮件中  就将相应位值设为1.

900
00:46:40,890 --> 00:46:43,060
如果有封邮件有一个词是a

901
00:46:43,200 --> 00:46:44,500
那么这里是1

902
00:46:44,640 --> 00:46:45,950
我的邮件中没有

903
00:46:46,050 --> 00:46:47,900
ausworth与aardvark  所以这两项为0

904
00:46:48,020 --> 00:46:50,380
这封邮件

905
00:46:50,510 --> 00:46:51,770
想让我买些东西

906
00:46:51,890 --> 00:46:54,300
cs229没有出现 诸如此类  看明白了吗?

907
00:46:54,410 --> 00:47:00,300
这是一种

908
00:47:00,420 --> 00:47:06,380
创建特征向量来表示邮件的方式

909
00:47:06,580 --> 00:47:12,150
现在让我们先不管生成模型

910
00:47:12,260 --> 00:47:19,900
让我们用这块黑板

911
00:47:20,000 --> 00:47:23,770
换句话说  我想对p(x|y)建模

912
00:47:23,880 --> 00:47:27,130
给定y=0或y=1  对吗?

913
00:47:27,240 --> 00:47:31,550
我的特征向量将是一个元素为0或1的

914
00:47:31,660 --> 00:47:32,680
n维向量

915
00:47:32,790 --> 00:47:34,080
这些向量

916
00:47:34,180 --> 00:47:35,210
是一些比特向量

917
00:47:35,320 --> 00:47:36,560
而且是n维向量

918
00:47:36,660 --> 00:47:41,480
n的数量可能是50000

919
00:47:41,600 --> 00:47:43,370
如果你的词典中有50000个词

920
00:47:43,480 --> 00:47:45,180
这是一种非典型的例子

921
00:47:45,290 --> 00:47:46,590
所以  大概-

922
00:47:46,700 --> 00:47:48,130
我不确定

923
00:47:48,250 --> 00:47:50,640
大概几百个到上万个

924
00:47:50,740 --> 00:47:53,260
对于这些问题来说都是非常普遍的

925
00:47:53,380 --> 00:47:54,920
因此

926
00:47:55,040 --> 00:47:59,760
对于x来说有2^50000种可能的值

927
00:47:59,880 --> 00:48:02,260
2^50000 种不同的位向量

928
00:48:02,370 --> 00:48:07,970
长度为50000  一种对其进行建模的方法

929
00:48:08,100 --> 00:48:10,000
是多项式分布

930
00:48:10,100 --> 00:48:12,500
但是由于对于x

931
00:48:12,620 --> 00:48:13,770
有2^50000种可能的值

932
00:48:13,890 --> 00:48:16,360
我应该需要

933
00:48:16,500 --> 00:48:20,710
2^50000-1 个参数  对吗?

934
00:48:20,830 --> 00:48:22,580
因为所有这些的和为1

935
00:48:22,700 --> 00:48:23,580
所以这里应该-1

936
00:48:23,740 --> 00:48:26,260
很显然  使用多项式分布

937
00:48:26,360 --> 00:48:29,840
对这2^50000种可能进行建模

938
00:48:29,950 --> 00:48:32,730
所需要的参数实在太多了

939
00:48:32,850 --> 00:48:37,580
在朴素贝叶斯算法中

940
00:48:37,710 --> 00:48:43,810
我们会对p(x|y)

941
00:48:43,930 --> 00:48:47,640
做一个非常强的假设

942
00:48:47,750 --> 00:48:49,170
我会假设-

943
00:48:49,290 --> 00:48:51,310
我先告诉你们这个假设叫什么

944
00:48:51,400 --> 00:48:52,690
之后再来解释它的含义

945
00:48:52,780 --> 00:48:53,770
我会假设

946
00:48:53,880 --> 00:49:07,980
给定y的时候  x_i是条件独立的

947
00:49:08,100 --> 00:49:09,140
让我解释一下它的含义

948
00:49:09,260 --> 00:49:15,200
我有

949
00:49:15,320 --> 00:49:19,800
p(x_1  x_2  …  x_50000  ┤|y)

950
00:49:19,920 --> 00:49:22,330
根据概率论的链式法则

951
00:49:22,440 --> 00:49:28,240
它应该等于p(x_1 |y)乘以p(x_2 |y  x_1)

952
00:49:28,360 --> 00:49:34,000
乘以

953
00:49:34,090 --> 00:49:41,400
p(x_3 |y  x_1  x_2)   一直乘到

954
00:49:41,560 --> 00:49:43,530
你们应该知道

955
00:49:43,630 --> 00:49:46,120
一直乘到p(x_50000)  明白吗?

956
00:49:46,240 --> 00:49:48,030
这里用到的是概率论的链式法则

957
00:49:48,150 --> 00:49:49,310
这个式子永远成立

958
00:49:49,430 --> 00:49:52,970
我现在并没有做任何假设

959
00:49:53,090 --> 00:49:55,420
我们接下来用到的假设

960
00:49:55,550 --> 00:49:56,880
称之为朴素贝叶斯假设

961
00:49:57,000 --> 00:49:58,610
也就是这个假设

962
00:49:58,720 --> 00:50:00,070
给定y时  x_i条件独立

963
00:50:00,180 --> 00:50:03,190
会假设-

964
00:50:03,310 --> 00:50:05,540
第一项没有变化

965
00:50:05,630 --> 00:50:08,500
这里我会假设

966
00:50:08,620 --> 00:50:12,380
p(x_2 ┤|y  x_1)=p(x_2 |y)

967
00:50:12,500 --> 00:50:14,220
这里我会假设

968
00:50:14,340 --> 00:50:17,690
p(x_3│y  x_1  x_2 )=p(x_3 |y)

969
00:50:17,810 --> 00:50:23,940
依次类推  一直到p(x_50000 |y)  明白吗?

970
00:50:24,070 --> 00:50:26,540
或者写成更简洁的形式

971
00:50:26,650 --> 00:50:36,410
这就意味这我们假设这个式子

972
00:50:36,550 --> 00:50:42,720
等于这个乘积式 明白吗?

973
00:50:42,840 --> 00:50:47,260
非正式地  这个式子的含义是

974
00:50:47,370 --> 00:50:48,630
我在假设-

975
00:50:48,740 --> 00:50:51,020
即使你知道标签y的值

976
00:50:51,160 --> 00:50:52,220
即使你知道

977
00:50:52,330 --> 00:50:53,360
它是垃圾邮件或者不是垃圾邮件

978
00:50:53,500 --> 00:50:56,720
并且即使你知道"a"

979
00:50:56,860 --> 00:51:00,740
是否出现在邮件中  这些都不会影响到

980
00:51:00,870 --> 00:51:04,910
"ausworth"出现在邮件中的概率

981
00:51:05,040 --> 00:51:08,020
明白吗?换句话说

982
00:51:08,100 --> 00:51:10,180
我们的假设是-

983
00:51:10,270 --> 00:51:11,910
你了解到一封邮件是否为垃圾邮件

984
00:51:12,040 --> 00:51:13,900
以及一些词

985
00:51:14,000 --> 00:51:16,140
是否出现在邮件中  这些并不会帮助你预测

986
00:51:16,250 --> 00:51:18,740
其它的词是否出现在邮件中  对吗?

987
00:51:18,840 --> 00:51:23,220
很显然这个假设是错的  对吗?

988
00:51:23,340 --> 00:51:24,940
这个假设不可能成立

989
00:51:25,070 --> 00:51:27,130
我的意思是  如果你在邮件中

990
00:51:27,250 --> 00:51:29,180
看到了"cs229"

991
00:51:29,280 --> 00:51:31,390
那么你们非常有可能在邮件中看到

992
00:51:31,500 --> 00:51:32,550
我的名字或者是助教的名字

993
00:51:32,550 --> 00:51:33,150
或者一些其它的词

994
00:51:33,260 --> 00:51:35,360
所以这个假设

995
00:51:35,490 --> 00:51:38,970
在英文语境下通常是错误的  对吗?

996
00:51:39,080 --> 00:51:40,630
对于正常书写的英语

997
00:51:40,760 --> 00:51:45,150
但是事实证明  虽然这个假设

998
00:51:45,250 --> 00:51:47,660
在字面意义上是错的

999
00:51:47,780 --> 00:51:51,540
但是朴素贝叶斯算法

1000
00:51:51,690 --> 00:51:53,980
仍然是一个非常有效的算法

1001
00:51:54,070 --> 00:51:55,360
可以对文本文档进行分类

1002
00:51:55,360 --> 00:51:56,760
例如判定它是否为垃圾邮件

1003
00:51:56,870 --> 00:51:59,840
或者自动地将你的电子邮件

1004
00:51:59,950 --> 00:52:01,590
分类到不同的类别目录下

1005
00:52:01,680 --> 00:52:02,630
或者可以对

1006
00:52:02,740 --> 00:52:04,680
网页进行分类

1007
00:52:04,790 --> 00:52:06,000
来检测网页是否在尝试销售一些东西

1008
00:52:06,130 --> 00:52:08,600
事实证明  当对文本文件进行分类时

1009
00:52:08,680 --> 00:52:10,030
这个假设的效果非常好

1010
00:52:10,170 --> 00:52:12,010
至于该算法的其他应用

1011
00:52:12,120 --> 00:52:13,380
我们之后会再讲

1012
00:52:15,600 --> 00:52:19,030
讲点题外话  可能只有你们中的

1013
00:52:19,030 --> 00:52:20,640
某些人才能明白

1014
00:52:20,770 --> 00:52:23,480
如果你们

1015
00:52:23,570 --> 00:52:26,280
对贝叶斯网络比较熟悉  也就是这个图模型

1016
00:52:26,370 --> 00:52:32,020
那么和这个模型对应的

1017
00:52:32,120 --> 00:52:33,630
贝叶斯网络看起来是这样的

1018
00:52:33,740 --> 00:52:35,490
你假设

1019
00:52:35,590 --> 00:52:37,020
这个随机变量y

1020
00:52:37,130 --> 00:52:41,090
生成了x_1  x_2  直到x_50000  明白吗?

1021
00:52:41,200 --> 00:52:43,130
如果你之前没见过贝叶斯网络

1022
00:52:43,250 --> 00:52:44,240
如果你不知道图模型

1023
00:52:44,360 --> 00:52:45,320
那么可以暂时忽略它

1024
00:52:45,420 --> 00:52:46,690
它对我们所讲的内容并不重要

1025
00:52:46,810 --> 00:52:48,420
如果你之前见过它

1026
00:52:48,530 --> 00:52:49,520
你会知道它看起来是这个样子

1027
00:52:49,600 --> 00:53:16,160
所以模型的参数是这样的

1028
00:53:16,270 --> 00:53:21,050
Φ_(i│y=1)

1029
00:53:21,160 --> 00:53:26,930
等于p(x_i=1|y=1)

1030
00:53:27,040 --> 00:53:39,480
Φ_(i│y=0)    以及Φ_y

1031
00:53:39,590 --> 00:53:44,080
这些是模型的参数

1032
00:53:44,260 --> 00:53:49,110
因此  为了拟合出模型的参数

1033
00:53:49,230 --> 00:53:55,980
你可以写出joint似然性

1034
00:53:56,110 --> 00:54:18,330
像往常一样  它应该等于这个式子  对吗?

1035
00:54:18,780 --> 00:54:20,140
所以给定训练集合

1036
00:54:20,280 --> 00:54:28,000
你们可以写出参数的

1037
00:54:28,160 --> 00:54:43,910
joint似然性

1038
00:54:44,020 --> 00:54:46,330
之后进行极大似然估计

1039
00:54:46,470 --> 00:54:48,870
你最终求得的

1040
00:54:48,980 --> 00:54:50,430
这些参数的极大似然估计的结果是-

1041
00:54:50,560 --> 00:54:52,580
这些结果大概就像你们所期望的那样

1042
00:54:52,740 --> 00:54:54,750
对于

1043
00:54:54,850 --> 00:55:00,210
Φ_(j│y=1) 的极大似然估计的结果应该是

1044
00:55:00,310 --> 00:55:17,750
(∑_(i=1)^m?〖1〗)/(∑_(i=1)^m?〖1〗)

1045
00:55:17,830 --> 00:55:22,550
简单地描述一下这个式子

1046
00:55:22,660 --> 00:55:24,300
分子的含义是

1047
00:55:24,390 --> 00:55:26,270
遍历整个训练集合

1048
00:55:26,390 --> 00:55:27,510
遍历这些训练样本

1049
00:55:27,630 --> 00:55:29,500
对于标签y=1的邮件

1050
00:55:29,610 --> 00:55:33,040
计算其中词语j

1051
00:55:33,150 --> 00:55:34,800
出现的邮件数目之和

1052
00:55:34,910 --> 00:55:36,210
换句话说

1053
00:55:36,320 --> 00:55:37,490
就是说遍历所有的垃圾邮件

1054
00:55:37,590 --> 00:55:39,200
统计这些垃圾邮件中

1055
00:55:39,320 --> 00:55:41,950
包含词语j的邮件数目

1056
00:55:42,080 --> 00:55:44,390
分母是

1057
00:55:44,510 --> 00:55:47,150
对i从1到m求和  最后得到垃圾邮件的总数

1058
00:55:47,230 --> 00:55:48,980
分母就是训练集合中

1059
00:55:49,090 --> 00:55:50,400
垃圾邮件的数目

1060
00:55:50,530 --> 00:55:55,620
所以这个比例的含义是

1061
00:55:55,790 --> 00:55:57,080
在你训练集合中的所有垃圾邮件中

1062
00:55:57,180 --> 00:55:59,200
包含词语j

1063
00:55:59,340 --> 00:56:03,550
也就是你的词典中的

1064
00:56:03,730 --> 00:56:04,590
第j个词语

1065
00:56:04,690 --> 00:56:06,090
的邮件的比例

1066
00:56:06,210 --> 00:56:07,780
所以这个极大似然估计的结果

1067
00:56:07,900 --> 00:56:11,150
表示的是你在垃圾邮件中

1068
00:56:11,250 --> 00:56:16,340
见到词语j的概率  明白吗?

1069
00:56:16,470 --> 00:56:22,570
类似地

1070
00:56:22,690 --> 00:56:25,830
你对Φ_y进行极大似然估计所得的结果

1071
00:56:25,940 --> 00:56:36,890
大概也是你预料的那样  对吗?

1072
00:56:37,010 --> 00:56:44,690
在得到这些参数的估计值以后

1073
00:56:44,820 --> 00:56:49,590
给你一封新邮件

1074
00:56:49,710 --> 00:56:51,600
你之后就可以

1075
00:56:51,700 --> 00:56:55,470
根据贝叶斯公式计算p(y|x)  对吗?

1076
00:56:55,600 --> 00:56:57,040
和往常一样

1077
00:56:57,160 --> 00:56:59,450
因为这些参数

1078
00:56:59,550 --> 00:57:03,260
已经给出了p(x|y)与p(y)的 模型

1079
00:57:03,360 --> 00:57:06,530
使用贝叶斯公式  结合这两项

1080
00:57:06,670 --> 00:57:09,980
你就可以计算出p(x|y)了

1081
00:57:10,080 --> 00:57:13,630
这就是你的垃圾邮件分类器  明白吗?

1082
00:57:13,780 --> 00:57:16,320
事实上

1083
00:57:16,430 --> 00:57:17,440
我们对于这个问题还需要

1084
00:57:17,440 --> 00:57:18,260
进行一些额外的阐述

1085
00:57:18,370 --> 00:57:19,370
但是让我先看看

1086
00:57:19,470 --> 00:57:20,870
到现在为止你们有哪些问题

1087
00:57:21,020 --> 00:57:26,620
这个模型依赖于

1088
00:57:26,710 --> 00:57:27,890
输入的数量吗?

1089
00:57:27,980 --> 00:57:30,020
你是什么意思?

1090
00:57:30,130 --> 00:57:32,010
是输入的数量还是特征的数量?

1091
00:57:32,130 --> 00:57:34,740
样本的数量

1092
00:57:34,830 --> 00:57:35,790
好的

1093
00:57:35,870 --> 00:57:37,150
m是样本数量

1094
00:57:37,250 --> 00:57:39,560
所以给定m个训练样本

1095
00:57:39,640 --> 00:57:41,230
这个公式

1096
00:57:41,320 --> 00:57:43,600
是对参数的极大似然估计  对吗?

1097
00:57:43,670 --> 00:57:49,110
还有其它问题吗?明白了吗?

1098
00:57:49,220 --> 00:57:50,700
m是训练样本数量

1099
00:57:50,790 --> 00:57:52,190
当你有m个训练样本时

1100
00:57:52,280 --> 00:57:53,830
你可以将它们带入到这个公式中

1101
00:57:53,920 --> 00:57:55,200
这是你计算

1102
00:57:55,280 --> 00:57:56,300
极大似然估计的方法

1103
00:57:56,400 --> 00:57:59,480
关于训练样本

1104
00:57:59,580 --> 00:58:01,210
你的意思是说  m是邮件的数量?

1105
00:58:01,310 --> 00:58:02,760
是的

1106
00:58:02,890 --> 00:58:05,030
你的训练样本可以这样得到

1107
00:58:05,120 --> 00:58:06,650
我会查看

1108
00:58:06,760 --> 00:58:08,270
我过去两个月中收到的所有邮件

1109
00:58:08,380 --> 00:58:10,720
并将它们标识为"垃圾"  或"非垃圾"

1110
00:58:10,820 --> 00:58:14,310
所以你可能会有

1111
00:58:14,360 --> 00:58:17,910
几百封标识了"垃圾"或"非垃圾"的邮件

1112
00:58:18,020 --> 00:58:21,380
这些邮件会组成你的训练集合

1113
00:58:21,450 --> 00:58:28,340
从(x^((1) )  y^((1) ))  到(x^((m) )  y^((m) ))

1114
00:58:28,480 --> 00:58:30,980
x是一个向量

1115
00:58:31,100 --> 00:58:32,600
用来标识词语是否出现在邮件中

1116
00:58:32,670 --> 00:58:34,490
y是0或1

1117
00:58:34,580 --> 00:58:35,230
这取决于它的标签是"垃圾"

1118
00:58:35,230 --> 00:58:35,880
还是"非垃圾"  明白吗?

1119
00:58:36,090 --> 00:58:41,720
你的意思是

1120
00:58:41,810 --> 00:58:44,510
模型依赖于样本数量

1121
00:58:44,600 --> 00:58:48,100
但是最后一个模型

1122
00:58:48,180 --> 00:58:49,660
并不依赖于模型

1123
00:58:49,780 --> 00:58:53,110
但是你的Φ对于每个模型都一样

1124
00:58:53,230 --> 00:58:54,500
它们是不同的

1125
00:58:54,580 --> 00:58:56,270
这个模型

1126
00:58:56,380 --> 00:59:01,340
并没有用到任何假设

1127
00:59:01,420 --> 00:59:02,260
我的假设是

1128
00:59:02,380 --> 00:59:03,680
朴素贝叶斯假设

1129
00:59:03,820 --> 00:59:07,470
所以这个概率模型

1130
00:59:07,580 --> 00:59:09,770
依赖于对(x  y)分布的joint假设

1131
00:59:09,850 --> 00:59:12,660
这才是我们的模型

1132
00:59:12,770 --> 00:59:14,530
之后  我有了一个固定数目的训练样本集合

1133
00:59:14,620 --> 00:59:17,020
给我m个训练样本

1134
00:59:17,160 --> 00:59:19,070
在给我整个训练集合之后

1135
00:59:19,180 --> 00:59:20,610
我之后会写出

1136
00:59:20,690 --> 00:59:22,310
参数的极大似然估计  对吗?

1137
00:59:22,430 --> 00:59:24,220
所以

1138
00:59:24,290 --> 00:59:26,620
也许我们可以课下讨论一下这个问题

1139
00:59:26,730 --> 00:59:29,390
有问题吗?

1140
00:59:30,070 --> 00:59:31,540
如何这个方法不行

1141
00:59:31,590 --> 00:59:32,800
你会怎么做

1142
00:59:32,920 --> 00:59:34,640
你能再说一遍吗?

1143
00:59:34,730 --> 00:59:36,760
你是怎么做到的?

1144
00:59:36,870 --> 00:59:37,990
比如说  你是怎么找到这5000个词的?

1145
00:59:38,090 --> 00:59:39,160
噢 好吧

1146
00:59:39,260 --> 00:59:40,600
我怎样找到这50000个词的  好的

1147
00:59:40,690 --> 00:59:43,310
这是一个

1148
00:59:43,380 --> 00:59:44,520
非常现实的问题

1149
00:59:44,640 --> 00:59:45,760
我是怎样得到这个词语列表的?

1150
00:59:45,850 --> 00:59:47,130
一个常用的方法是

1151
00:59:47,240 --> 00:59:51,040
用某种方式来找到这个词语列表

1152
00:59:51,120 --> 00:59:52,420
例如遍历你的所有邮件

1153
00:59:52,560 --> 00:59:53,770
遍历所有的-

1154
00:59:53,890 --> 00:59:56,530
实际上  一个常用的做法是

1155
00:59:56,650 --> 00:59:58,150
选取训练集合中

1156
00:59:58,250 --> 00:59:59,440
出现的所有词

1157
00:59:59,540 --> 01:00:00,930
还有一个更为常用的方法

1158
01:00:01,050 --> 01:00:04,480
如果你不这样做  词语的数目会有很多

1159
01:00:04,590 --> 01:00:06,090
你可以选取所有

1160
01:00:06,180 --> 01:00:08,950
在你的训练集合中出现过三次以上的词

1161
01:00:09,010 --> 01:00:12,040
在过去两个月你收到的所有邮件中

1162
01:00:12,130 --> 01:00:15,230
出现次数不足三次的词

1163
01:00:15,350 --> 01:00:16,710
你可以忽略掉它们

1164
01:00:16,820 --> 01:00:18,720
我讲的是为了生成一个词典

1165
01:00:18,810 --> 01:00:20,140
怎样的方法比较好

1166
01:00:20,240 --> 01:00:21,180
实际上

1167
01:00:21,320 --> 01:00:23,090
你只需要遍历整个训练集合

1168
01:00:23,200 --> 01:00:24,330
对其中所有出现的词

1169
01:00:24,430 --> 01:00:25,520
求并集

1170
01:00:25,640 --> 01:00:28,300
顺便说一下  之前我提到过

1171
01:00:28,400 --> 01:00:29,400
特征的选取

1172
01:00:29,520 --> 01:00:30,280
你可以将创建词典的过程

1173
01:00:30,400 --> 01:00:33,200
看成一个创建特征向量的过程

1174
01:00:33,320 --> 01:00:36,480
其中的值只能是0或1  明白吗?

1175
01:00:36,590 --> 01:00:42,850
继续 有问题吗?

1176
01:00:42,960 --> 01:00:44,730
我有些疑惑

1177
01:00:44,840 --> 01:00:46,790
你是怎样计算所有这些参数的?

1178
01:00:46,880 --> 01:00:49,590
我是怎样

1179
01:00:49,680 --> 01:00:50,710
得到这些参数的?

1180
01:00:50,820 --> 01:00:51,840
是的

1181
01:00:51,940 --> 01:00:53,630
让我看看

1182
01:00:53,760 --> 01:00:55,650
对于朴素贝叶斯方法  我需要做的是-

1183
01:00:55,780 --> 01:00:56,840
问题是

1184
01:00:56,930 --> 01:00:58,050
我是怎样得到这些参数的  是吗?

1185
01:00:58,160 --> 01:01:00,460
在朴素贝叶斯算法中  我需要

1186
01:01:00,590 --> 01:01:05,680
分别为p(x|y)与p(y)建立一个模型  对吗?

1187
01:01:05,820 --> 01:01:07,260
我的意思是

1188
01:01:07,370 --> 01:01:08,590
在生成学习算法中

1189
01:01:08,680 --> 01:01:10,430
我需要建立这些模型

1190
01:01:10,550 --> 01:01:12,340
我怎样对p(y)进行建模?

1191
01:01:12,490 --> 01:01:13,840
我选择用

1192
01:01:13,950 --> 01:01:15,560
伯努利分布对其进行建模

1193
01:01:15,840 --> 01:01:21,010
所以p(y)以它为参数  对吗?

1194
01:01:21,120 --> 01:01:22,020
对

1195
01:01:22,130 --> 01:01:22,960
那么

1196
01:01:23,070 --> 01:01:24,500
我怎样对p(x|y)进行建模?

1197
01:01:24,610 --> 01:01:27,570
好的  让我换块黑板

1198
01:01:27,680 --> 01:01:30,050
在朴素贝叶斯假设下

1199
01:01:30,180 --> 01:01:31,960
我的p(x|y)的模型

1200
01:01:32,070 --> 01:01:34,210
我假设p(x|y)

1201
01:01:34,330 --> 01:01:36,150
等于这些概率的乘积

1202
01:01:36,280 --> 01:01:40,300
所以我需要一些参数来告诉我

1203
01:01:40,440 --> 01:01:42,560
每个词出现的概率

1204
01:01:42,670 --> 01:01:44,450
你知道

1205
01:01:44,530 --> 01:01:45,590
每个词出现或不出现的概率

1206
01:01:45,690 --> 01:01:47,760
以邮件

1207
01:01:47,880 --> 01:01:50,830
是否为垃圾邮件为条件  明白吗?

1208
01:01:50,940 --> 01:01:54,620
为什么要用伯努利分布?

1209
01:01:54,740 --> 01:01:55,380
哦

1210
01:01:55,530 --> 01:01:58,290
因为x是0或1  对吗?

1211
01:01:58,400 --> 01:01:59,760
我之前定义的特征向量中

1212
01:01:59,870 --> 01:02:02,490
x_i只能取0或1

1213
01:02:02,590 --> 01:02:03,520
这取决于这个词

1214
01:02:03,650 --> 01:02:05,460
是否出现在邮件中  对吗?

1215
01:02:05,570 --> 01:02:07,250
在我对特征向量的定义中

1216
01:02:07,370 --> 01:02:13,090
x_i不是0就是1

1217
01:02:13,190 --> 01:02:16,260
根据定义  如果x_i

1218
01:02:16,360 --> 01:02:17,960
只能取0或1  那么

1219
01:02:18,020 --> 01:02:19,780
它一定是一个伯努利分布  对吗?

1220
01:02:19,890 --> 01:02:20,910
如果x_i是一个连续变量

1221
01:02:21,030 --> 01:02:23,640
那么你可以用高斯分布对它建模

1222
01:02:23,750 --> 01:02:25,620
所以你最后可能会用到

1223
01:02:25,710 --> 01:02:26,760
类似于高斯判别分析的算法

1224
01:02:26,870 --> 01:02:27,880
这是我建立

1225
01:02:27,980 --> 01:02:29,160
邮件特征的方式

1226
01:02:29,250 --> 01:02:30,680
x_i是一个二元值

1227
01:02:30,780 --> 01:02:32,600
所以你这里用到的是一个

1228
01:02:32,600 --> 01:02:34,420
伯努利分布  明白吗?

1229
01:02:34,540 --> 01:02:36,330
好的  我应该继续讲了

1230
01:02:36,420 --> 01:02:44,940
事实证明  这个算法可以工作地很好

1231
01:02:45,070 --> 01:02:47,070
现在  这里仍然存在一个问题

1232
01:02:47,190 --> 01:02:50,070
比如说  当你上完课之后

1233
01:02:50,190 --> 01:02:52,830
你开始完成课程项目

1234
01:02:52,950 --> 01:02:55,910
你不停地去做课程项目

1235
01:02:56,020 --> 01:02:57,270
最后结果非常好

1236
01:02:57,390 --> 01:02:58,740
之后你将项目成果

1237
01:02:58,850 --> 01:03:00,030
发表在会议上

1238
01:03:00,110 --> 01:03:01,930
你知道  大概

1239
01:03:02,010 --> 01:03:04,230
在每年的六月份

1240
01:03:04,570 --> 01:03:07,070
都是对下次会议的截止时间

1241
01:03:07,160 --> 01:03:09,110
会议的名字

1242
01:03:09,220 --> 01:03:10,460
通常会是一个缩写词

1243
01:03:10,550 --> 01:03:13,920
可能你会给你的项目伙伴

1244
01:03:14,470 --> 01:03:15,740
或者高年级的朋友发邮件说

1245
01:03:15,830 --> 01:03:16,910
"嘿  我们将项目的成果

1246
01:03:17,010 --> 01:03:18,280
发表在NIPS会议上吧 "

1247
01:03:18,380 --> 01:03:19,690
所以这样你就得到了一些

1248
01:03:19,790 --> 01:03:21,460
包含"NIPS"的邮件

1249
01:03:21,540 --> 01:03:22,950
这个词你之前从来从未见过

1250
01:03:23,060 --> 01:03:28,860
这样的一封邮件

1251
01:03:28,940 --> 01:03:30,800
来自于你的项目伙伴

1252
01:03:30,880 --> 01:03:31,920
所以你会回复

1253
01:03:32,020 --> 01:03:34,470
"让我们在NIPS上发表一篇论文吧 "

1254
01:03:34,580 --> 01:03:39,430
之后你的垃圾邮件分类器会-

1255
01:03:39,580 --> 01:03:42,510
比如说NIPS是词典中的

1256
01:03:42,630 --> 01:03:43,980
第30000个词  好吗?

1257
01:03:44,090 --> 01:03:48,190
所以p(x_30000=1|y=1)

1258
01:03:48,310 --> 01:03:53,830
应该等于0

1259
01:03:53,910 --> 01:03:55,700
这是它的极大似然估计  对吗?

1260
01:03:55,790 --> 01:03:57,190
因为你之前在你的训练集合中

1261
01:03:57,300 --> 01:03:58,160
你从来没有见过NIPS这个词

1262
01:03:58,270 --> 01:03:59,920
这个参数的含义是邮件中出现NIPS的概率

1263
01:04:00,070 --> 01:04:01,700
所以它的极大似然估计值应该等于0

1264
01:04:01,820 --> 01:04:10,120
类似的  在非垃圾邮件中

1265
01:04:10,200 --> 01:04:11,510
出现NIPS的概率

1266
01:04:11,650 --> 01:04:15,490
也会被估计为0

1267
01:04:15,600 --> 01:04:36,770
所以当你的垃圾邮件分类器

1268
01:04:36,880 --> 01:04:38,850
开始计算p(y=1|x)时

1269
01:04:38,960 --> 01:04:47,420
它会这样计算

1270
01:04:47,530 --> 01:05:09,100
好的

1271
01:05:09,210 --> 01:05:12,980
来看一下这些项

1272
01:05:13,090 --> 01:05:17,010
比如说这一项应该是对于i=1到50000

1273
01:05:17,130 --> 01:05:23,520
对p(x_i |y)求乘积  这些概率中的其中一个

1274
01:05:23,630 --> 01:05:24,850
等于0

1275
01:05:24,930 --> 01:05:32,000
因为:p(x_30000=1|y=1)等于0

1276
01:05:32,090 --> 01:05:33,930
所以这个乘积式中的一项为0

1277
01:05:34,050 --> 01:05:35,450
所以分子为0

1278
01:05:35,590 --> 01:05:37,730
类似地

1279
01:05:37,830 --> 01:05:39,980
实际上分母也是0

1280
01:05:40,080 --> 01:05:41,750
所以你得到了-

1281
01:05:41,840 --> 01:05:44,550
实际上所有这些项都是0

1282
01:05:44,660 --> 01:05:49,040
所以你最后得到了0/(0+0)

1283
01:05:49,130 --> 01:05:50,200
结果是未定义的

1284
01:05:50,310 --> 01:05:52,730
其中存在的问题是

1285
01:05:52,850 --> 01:05:58,230
在统计意义上  认为p(x_30000│y)=0

1286
01:05:58,380 --> 01:06:03,970
并不是一个很好的主意

1287
01:06:04,090 --> 01:06:05,660
仅仅因为在过去两个月的邮件中

1288
01:06:05,750 --> 01:06:07,610
你没有见过NIPS

1289
01:06:07,690 --> 01:06:10,220
在统计意义上

1290
01:06:10,330 --> 01:06:11,730
认为这个词出现的概率为0

1291
01:06:11,810 --> 01:06:14,250
听起来并不合理  对吗?

1292
01:06:14,370 --> 01:06:24,810
如果仅仅因为

1293
01:06:24,910 --> 01:06:26,220
你之前没有见过一些事件

1294
01:06:26,330 --> 01:06:29,650
你可以认为这些事件不太可能发生

1295
01:06:29,760 --> 01:06:31,460
但是这并不表示这些事件不可能发生

1296
01:06:31,600 --> 01:06:32,620
这就是说

1297
01:06:32,750 --> 01:06:34,160
如果你之前从未见过NIPS这个词

1298
01:06:34,290 --> 01:06:36,220
那么你在之后的邮件中

1299
01:06:36,320 --> 01:06:38,290
也不可能再见到它  几率为0

1300
01:06:38,410 --> 01:06:41,530
我们要改进这个问题

1301
01:06:41,650 --> 01:06:52,080
为了讲解修正方法

1302
01:06:52,200 --> 01:06:56,430
让我们举个例子  比如说

1303
01:06:56,530 --> 01:06:59,800
你跟随着斯坦福篮球队

1304
01:06:59,920 --> 01:07:02,560
参加了他们所有的客场比赛

1305
01:07:02,650 --> 01:07:05,510
之后统计了他们所有的胜场和负场数目

1306
01:07:05,620 --> 01:07:07,060
基于这些统计

1307
01:07:07,160 --> 01:07:08,390
你可以为他们开出一个赔率

1308
01:07:08,510 --> 01:07:09,450
表示他们下一场有多大的几率

1309
01:07:09,460 --> 01:07:10,400
会胜利或者失败  明白吗?

1310
01:07:10,500 --> 01:07:19,640
这里是一些统计

1311
01:07:19,730 --> 01:07:26,010
比如  上个赛季2月8号

1312
01:07:26,090 --> 01:07:30,190
对抗华盛顿州  输了

1313
01:07:30,290 --> 01:07:37,380
2月11号

1314
01:07:37,490 --> 01:07:45,090
对抗华盛顿  22号对抗USC

1315
01:07:45,210 --> 01:07:56,160
对抗UCLA  对抗USC

1316
01:07:56,270 --> 01:08:06,300
现在你想估计

1317
01:08:06,390 --> 01:08:08,810
他们对抗Louisville时胜利或失败的概率  对吗?

1318
01:08:08,910 --> 01:08:13,210
我们发现这些家伙在去年的连续5场

1319
01:08:13,290 --> 01:08:14,820
客场比赛中表现不佳

1320
01:08:14,930 --> 01:08:16,420
但是仅仅凭借这些

1321
01:08:16,520 --> 01:08:20,250
就说它们最后一场的胜率为0

1322
01:08:21,130 --> 01:08:22,630
有点太苛刻了

1323
01:08:22,700 --> 01:08:39,190
所以接下来我们要将Laplace平滑的概念

1324
01:08:39,240 --> 01:08:46,000
我们需要估计

1325
01:08:46,120 --> 01:08:47,870
p(y=1)的值  对吗?

1326
01:08:48,020 --> 01:08:52,150
通常情况下根据极大似然估计

1327
01:08:52,320 --> 01:08:58,270
我们得到的结果应该是1的数目

1328
01:08:58,340 --> 01:09:04,570
除以0的数目加上1的数目  对吗?

1329
01:09:04,680 --> 01:09:07,380
希望你们能够明白这种非正式的符号表示

1330
01:09:07,470 --> 01:09:08,970
极大似然估计的结果

1331
01:09:09,060 --> 01:09:10,200
例如

1332
01:09:10,310 --> 01:09:11,740
对于服从伯努利分布的取胜几率的估计

1333
01:09:11,860 --> 01:09:17,050
就是你看到的所有的"1"的数目

1334
01:09:17,180 --> 01:09:18,670
除以样本的总数

1335
01:09:18,790 --> 01:09:20,420
也就是你观察到的"0"的数量

1336
01:09:20,500 --> 01:09:21,510
加上"1"的数量

1337
01:09:21,630 --> 01:09:24,840
在Laplace平滑中

1338
01:09:24,940 --> 01:09:27,380
我们会用到这些项

1339
01:09:27,470 --> 01:09:29,760
并对"1"的数目

1340
01:09:29,860 --> 01:09:30,590
加上1

1341
01:09:30,710 --> 01:09:32,140
对"0"的数目加上1

1342
01:09:32,270 --> 01:09:33,660
对"1"的数目加上1

1343
01:09:33,770 --> 01:09:35,390
所以在我们的例子中

1344
01:09:35,500 --> 01:09:39,330
相对于用

1345
01:09:39,440 --> 01:09:44,270
0/(5+0)来计算取胜的几率


1346
01:09:44,380 --> 01:09:47,020
我们会将所有的数字加上1

1347
01:09:47,130 --> 01:09:52,100
所以他们下场获胜的概率是

1348
01:09:52,250 --> 01:09:54,790
1/7  明白吗?

1349
01:09:54,900 --> 01:09:57,040
也就是说

1350
01:09:57,100 --> 01:09:58,200
看到他们连续输掉了5场客场比赛之后

1351
01:09:58,310 --> 01:09:59,560
我们-

1352
01:09:59,670 --> 01:10:00,820
我们并不认为他们很有可能

1353
01:10:00,930 --> 01:10:02,130
赢得下场比赛

1354
01:10:02,250 --> 01:10:03,740
但是我们并没有说这是不可能的

1355
01:10:03,880 --> 01:10:06,980
在历史上

1356
01:10:07,110 --> 01:10:09,730
是Laplace发明的这个方法

1357
01:10:09,840 --> 01:10:11,370
所以这个方法被称为Laplace平滑

1358
01:10:11,480 --> 01:10:20,190
他尝试估计

1359
01:10:20,340 --> 01:10:21,730
太阳明天照常升起的概率

1360
01:10:21,810 --> 01:10:24,100
他的基本原理是  虽然连续许多天

1361
01:10:24,210 --> 01:10:25,110
我们都会看到太阳升起

1362
01:10:25,230 --> 01:10:27,070
但是这并不表示我们确定

1363
01:10:27,180 --> 01:10:28,480
太阳明天一定会升起

1364
01:10:28,570 --> 01:10:30,620
他发明了这个方法来计算

1365
01:10:30,730 --> 01:10:31,720
太阳明天升起的概率

1366
01:10:31,820 --> 01:10:33,290
这很酷

1367
01:10:33,400 --> 01:10:37,590
一般地

1368
01:10:37,690 --> 01:10:54,690
如果y可以取k种可能的值

1369
01:10:54,730 --> 01:10:57,220
如果你尝试估计多项式分布的参数

1370
01:10:57,330 --> 01:11:03,040
如果你要估计p(y=1)

1371
01:11:03,120 --> 01:11:06,200
让我看看

1372
01:11:06,320 --> 01:11:07,730
所以极大似然估计的结果

1373
01:11:07,840 --> 01:11:10,170
应该是

1374
01:11:10,270 --> 01:11:19,550
(∑_(j=1)^m?〖1〗)/m   对吗?


1375
01:11:19,650 --> 01:11:21,020
这是对多项式分布中

1376
01:11:21,130 --> 01:11:25,060
也就是y=

1377
01:11:25,170 --> 01:11:29,290
哦  对不起  y=j  好的

1378
01:11:29,370 --> 01:11:30,730
这是对p(y=j)

1379
01:11:30,840 --> 01:11:31,930
的极大似然估计

1380
01:11:32,040 --> 01:11:35,120
对它应用Laplace平滑

1381
01:11:37,150 --> 01:11:38,790
在分子上加1

1382
01:11:38,890 --> 01:11:41,750
在分母上加k

1383
01:11:41,790 --> 01:11:46,680
如果y可以取k个可能的值  对吗?

1384
01:11:46,810 --> 01:12:09,840
所以对于朴素贝叶斯算法  得到的结果是-

1385
01:12:09,980 --> 01:12:39,180
对吗?

1386
01:12:39,290 --> 01:12:40,630
这是极大似然估计的结果

1387
01:12:40,710 --> 01:12:41,880
所以最后得到的结果

1388
01:12:41,960 --> 01:12:45,290
要在分子上加1

1389
01:12:45,370 --> 01:12:46,980
在分母上加2

1390
01:12:47,060 --> 01:12:49,050
这样我们就解决了

1391
01:12:49,150 --> 01:12:50,540
0概率的问题

1392
01:12:50,650 --> 01:12:52,250
所以当你的朋友给你发

1393
01:12:52,360 --> 01:12:53,500
关于NIPS会议的邮件时

1394
01:12:53,610 --> 01:12:59,690
你的垃圾邮件过滤器仍然可以

1395
01:12:59,810 --> 01:13:04,080
做出有意义的预测  明白吗?

1396
01:13:04,210 --> 01:13:14,060
有问题吗?

1397
01:13:14,140 --> 01:13:15,290
这样是不是没有意义?

1398
01:13:15,390 --> 01:13:16,940
比如

1399
01:13:17,020 --> 01:13:23,590
按照右边的比赛结果

1400
01:13:23,700 --> 01:13:25,950
表面上的假设是

1401
01:13:26,090 --> 01:13:31,420
胜利的概率非常接近于0

1402
01:13:31,530 --> 01:13:32,520
所以  我的意思是

1403
01:13:32,610 --> 01:13:35,810
预测应该等于0

1404
01:13:35,910 --> 01:13:36,680
好的

1405
01:13:36,770 --> 01:13:37,430
我要说的是

1406
01:13:37,540 --> 01:13:38,480
这个例子中预测值为1/7  对吗?

1407
01:13:38,570 --> 01:13:39,620
我们并没有很多的-

1408
01:13:39,740 --> 01:13:41,220
如果你看到某支队伍连负五场

1409
01:13:41,310 --> 01:13:43,510
你一定对他们没有什么信心

1410
01:13:43,610 --> 01:13:45,730
但是作为一个极端的例子

1411
01:13:45,850 --> 01:13:48,810
假设你看到他们只输了一场  对吗?

1412
01:13:48,920 --> 01:13:50,350
这时你如果说他们下场比赛

1413
01:13:50,450 --> 01:13:52,180
胜利的概率为0  将会非常不合理

1414
01:13:52,290 --> 01:13:54,510
但是极大似然估计就会这样认为

1415
01:13:54,620 --> 01:13:55,440
是的

1416
01:13:55,540 --> 01:13:56,770
那么-

1417
01:13:56,870 --> 01:13:58,590
在这种情况下

1418
01:13:58,740 --> 01:14:03,640
学习算法


1419
01:14:03,690 --> 01:14:07,220
一些问题

1420
01:14:07,340 --> 01:14:09,350
仅仅给你5个训练样本

1421
01:14:09,430 --> 01:14:11,530
对于下一场取胜的概率

1422
01:14:11,650 --> 01:14:15,380
怎样的估计是合理的  我认为1/7

1423
01:14:15,490 --> 01:14:16,460
实际上是一个相当合理的估计

1424
01:14:16,570 --> 01:14:17,930
例如  它比1/5要小

1425
01:14:18,050 --> 01:14:19,160
我们说

1426
01:14:19,250 --> 01:14:20,890
下场比赛获胜的概率比1/5要小

1427
01:14:20,990 --> 01:14:23,730
实际上  在满足一些假设的情况下

1428
01:14:23,830 --> 01:14:24,590
我不会讲这些假设

1429
01:14:24,700 --> 01:14:26,120
在满足一些贝叶斯假设的情况下

1430
01:14:26,210 --> 01:14:27,410
关于先验分布和后验分布的假设

1431
01:14:27,500 --> 01:14:30,240
使用Laplace平滑方法

1432
01:14:30,330 --> 01:14:31,610
实际上会得到最优的估计

1433
01:14:31,720 --> 01:14:33,650
我不会去计算

1434
01:14:33,780 --> 01:14:35,750
下场比赛获胜的概率

1435
01:14:35,870 --> 01:14:37,270
关于参数所做的一些和

1436
01:14:37,370 --> 01:14:40,180
贝叶斯后验分布有关的假设

1437
01:14:40,300 --> 01:14:41,670
我不知道

1438
01:14:41,730 --> 01:14:43,090
至少在我看来

1439
01:14:43,190 --> 01:14:44,370
这个假设相当合理

1440
01:14:44,480 --> 01:14:47,380
我应该说明  事实上-

1441
01:14:47,500 --> 01:14:50,920
不 我只是在吹毛求疵

1442
01:14:51,030 --> 01:14:53,060
我们有一支很好的篮球队

1443
01:14:53,180 --> 01:14:54,510
但是我只是选择了一段他们连败的时期

1444
01:14:54,610 --> 01:14:59,800
因为这样的例子比较有趣 让我看看

1445
01:14:59,900 --> 01:15:00,930
还有人

1446
01:15:01,040 --> 01:15:03,330
有问题吗?

1447
01:15:03,440 --> 01:15:10,010
没有了  好的 我还会讲一些

1448
01:15:10,100 --> 01:15:11,770
关于朴素贝叶斯的内容

1449
01:15:11,850 --> 01:15:14,210
但是我们会在下节课讲

1450
01:15:14,330 --> 01:15:16,390
今天就到这里吧

