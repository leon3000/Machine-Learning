1
00:00:15,790 --> 00:00:25,300
So welcome back,

2
00:00:25,370 --> 00:00:27,050
and what I want to do  today

3
00:00:27,120 --> 00:00:30,270
is continue our discussions

4
00:00:30,350 --> 00:00:31,910
of the EM Algorithm,

5
00:00:32,010 --> 00:00:33,400
and in particular,

6
00:00:33,450 --> 00:00:36,300
I want to talk  about the EM formulation

7
00:00:36,370 --> 00:00:38,150
that we derived in the previous lecture

8
00:00:38,190 --> 00:00:40,070
and apply it to the  mixture

9
00:00:40,130 --> 00:00:40,890
of Gaussians model,

10
00:00:40,970 --> 00:00:42,650
apply it to a different model

11
00:00:42,730 --> 00:00:44,010
and a mixture of naive Bayes  model,

12
00:00:44,100 --> 00:00:45,820
and then the launch part

13
00:00:45,900 --> 00:00:47,960
of today's lecture will be

14
00:00:48,030 --> 00:00:49,120
on the factor analysis  algorithm,

15
00:00:49,170 --> 00:00:50,780
which will also use the EM.

16
00:00:50,810 --> 00:00:52,680
And as part of that,

17
00:00:52,780 --> 00:00:54,070
we'll actually take a brief  digression

18
00:00:54,110 --> 00:00:56,220
to talk a little bit about sort of useful

19
00:00:56,320 --> 00:00:57,500
properties of Gaussian distributions.

20
00:00:57,650 --> 00:00:59,010
So just to recap where we are.

21
00:00:59,120 --> 00:01:04,570
In the previous lecture,

22
00:01:04,680 --> 00:01:06,400
I started to talk about

23
00:01:06,510 --> 00:01:07,740
unsupervised  learning,

24
00:01:07,870 --> 00:01:10,810
which was machine-learning problems,

25
00:01:10,930 --> 00:01:11,670
where you're given

26
00:01:11,790 --> 00:01:13,840
an unlabeled  training

27
00:01:13,950 --> 00:01:17,410
set comprising m examples here, right?

28
00:01:17,490 --> 00:01:19,570
And then

29
00:01:19,660 --> 00:01:20,700
so the fact that there are no  labels;

30
00:01:20,830 --> 00:01:21,820
that's what makes this

31
00:01:21,980 --> 00:01:23,650
unsupervised or anything.

32
00:01:23,780 --> 00:01:26,070
So one problem that

33
00:01:26,260 --> 00:01:27,600
I talked  about last time was

34
00:01:27,700 --> 00:01:31,580
what if you're given a data set

35
00:01:31,630 --> 00:01:32,880
that looks like this and

36
00:01:32,990 --> 00:01:37,730
you want to  model the density PFX from

37
00:01:37,840 --> 00:01:39,420
which you think the data had been drawn,

38
00:01:39,490 --> 00:01:43,580
and so with a data  set like this,

39
00:01:43,640 --> 00:01:44,410
maybe you think

40
00:01:44,500 --> 00:01:46,530
was a mixture of two Gaussians

41
00:01:46,640 --> 00:01:49,540
and start to talk about an  algorithm

42
00:01:49,640 --> 00:01:52,310
for fitting a mixture of Gaussians model,

43
00:01:52,430 --> 00:01:53,410
all right?

44
00:01:53,500 --> 00:01:55,090
And so we said that

45
00:01:55,160 --> 00:01:56,660
we  would model

46
00:01:56,760 --> 00:01:58,560
the density of XP of X

47
00:01:58,680 --> 00:02:02,900
as sum over Z PFX

48
00:02:03,000 --> 00:02:07,190
given Z times P of Z where

49
00:02:07,280 --> 00:02:09,600
this  later random variable meaning

50
00:02:09,700 --> 00:02:10,790
this hidden random variable Z

51
00:02:10,880 --> 00:02:13,600
indicates which of the two  Gaussian

52
00:02:13,740 --> 00:02:14,880
distributions each

53
00:02:14,950 --> 00:02:15,870
of your data points came from

54
00:02:15,970 --> 00:02:19,490
and so we have, you know,

55
00:02:19,560 --> 00:02:24,720
Z  was not a nomial with parameter

56
00:02:24,850 --> 00:02:29,520
phi and X conditions on a coming from

57
00:02:29,600 --> 00:02:37,380
the JAFE  Gaussian was given by Gaussian

58
00:02:37,500 --> 00:02:41,410
of mean mu J and covariant sigma J,

59
00:02:41,500 --> 00:02:42,930
all right?

60
00:02:43,000 --> 00:02:45,230
So, like I said at the beginning

61
00:02:45,300 --> 00:02:45,900
of the previous lecture,

62
00:02:45,950 --> 00:02:46,580
I just talked about

63
00:02:46,710 --> 00:02:47,870
a very specific  algorithm

64
00:02:47,960 --> 00:02:49,720
that I sort of pulled out of the air

65
00:02:49,810 --> 00:02:52,020
for fitting the parameters of this model

66
00:02:52,130 --> 00:02:54,720
for  finian, Francis, phi, mu and sigma,

67
00:02:54,820 --> 00:02:58,480
but then in the second half of the previous lecture

68
00:02:58,580 --> 00:02:59,380
I  talked about

69
00:02:59,470 --> 00:03:00,360
what's called the EM Algorithm

70
00:03:00,470 --> 00:03:05,090
in which our goal is that

71
00:03:05,170 --> 00:03:06,640
it's a  estimation of parameters.

72
00:03:06,770 --> 00:03:08,020
So we want to maximize

73
00:03:08,150 --> 00:03:08,730
in terms of theta,

74
00:03:08,830 --> 00:03:10,820
you know, the, sort  of,

75
00:03:10,940 --> 00:03:14,820
usual right matter of log likelihood

76
00:03:14,930 --> 00:03:23,000
well, parameterized by theta.

77
00:03:23,100 --> 00:03:25,050
And because we  have

78
00:03:25,190 --> 00:03:26,660
a later random variable Z

79
00:03:26,780 --> 00:03:28,460
this is really maximizing

80
00:03:37,520 --> 00:03:40,410
Okay?

81
00:03:40,520 --> 00:03:48,880
So using Jensen's inequality last  time

82
00:03:48,980 --> 00:03:51,790
we worked out the EM Algorithm

83
00:03:51,920 --> 00:03:55,000
in which in the E step we would

84
00:03:55,090 --> 00:03:57,800
chose these  probability distributions QI

85
00:03:57,950 --> 00:04:05,090
to the l posterior on Z given X and

86
00:04:05,180 --> 00:04:06,200
parameterized by theta

87
00:04:06,320 --> 00:04:10,430
and in the M step

88
00:04:10,490 --> 00:04:17,510
we would set theta to be the value

89
00:04:17,600 --> 00:04:22,230
that maximizes this.

90
00:04:22,340 --> 00:04:36,510
Okay?

91
00:04:36,600 --> 00:04:38,470
So these  are the ones

92
00:04:38,580 --> 00:04:39,890
we worked out last time and

93
00:04:39,960 --> 00:04:43,460
the cartoon that I drew was that

94
00:04:43,560 --> 00:04:45,050
you have this  long likelihood function

95
00:04:45,180 --> 00:04:46,910
L of theta

96
00:04:47,040 --> 00:04:48,150
that's often hard to maximize

97
00:04:48,260 --> 00:04:50,580
and what the E step does

98
00:04:50,660 --> 00:04:52,860
is choose these probability

99
00:04:52,960 --> 00:04:54,080
distribution production QI's.

100
00:04:54,180 --> 00:04:55,190
And in the cartoon,

101
00:04:55,260 --> 00:04:57,060
I drew what  that corresponded to

102
00:04:57,130 --> 00:04:59,260
was finding a lower bounds

103
00:04:59,370 --> 00:05:01,880
for the log likelihood.

104
00:05:01,980 --> 00:05:06,270
And then  horizontal access data

105
00:05:06,330 --> 00:05:07,690
and then the M step you maximize

106
00:05:07,770 --> 00:05:08,630
the lower boundary,

107
00:05:08,690 --> 00:05:09,390
right?

108
00:05:09,460 --> 00:05:10,660
So  maybe you were here previously

109
00:05:10,790 --> 00:05:12,880
and so you jumped to the new point,

110
00:05:12,960 --> 00:05:16,410
the new maximum  of this lower bound.

111
00:05:16,500 --> 00:05:19,310
Okay? And so this little curve here,

112
00:05:19,400 --> 00:05:21,040
right? This lower bound function  here

113
00:05:21,130 --> 00:05:27,090
that's really the right-hand side

114
00:05:27,240 --> 00:05:28,560
of that augments.

115
00:05:28,690 --> 00:05:29,410
Okay?

116
00:05:29,510 --> 00:05:32,440
So this whole thing in the  augments.

117
00:05:32,580 --> 00:05:33,560
If you view this thing

118
00:05:33,670 --> 00:05:34,980
as a function of theta,

119
00:05:35,110 --> 00:05:38,440
this function of theta is

120
00:05:38,520 --> 00:05:40,370
a lower bounds for the log likelihood

121
00:05:40,460 --> 00:05:42,320
of theta and so

122
00:05:42,390 --> 00:05:43,640
the M step we maximize this lower bound

123
00:05:43,750 --> 00:05:45,910
and that corresponds to jumping to

124
00:05:46,020 --> 00:05:49,910
this new maximum to lower bound.

125
00:05:49,990 --> 00:05:56,210
So it turns out that in the EM Algorithm

126
00:05:56,240 --> 00:05:58,810
so why do you evolve with

127
00:05:58,900 --> 00:05:59,690
the EM algorithm?

128
00:05:59,760 --> 00:06:00,640
It turns out that very often,

129
00:06:00,720 --> 00:06:02,770
and this will be true

130
00:06:02,890 --> 00:06:04,600
for all the examples we see today,

131
00:06:04,730 --> 00:06:06,130
it turns  out that very often

132
00:06:06,210 --> 00:06:11,510
in the EM Algorithm

133
00:06:11,590 --> 00:06:13,420
maximizing the M Step,

134
00:06:13,530 --> 00:06:15,480
so performing the  maximization the M

135
00:06:15,600 --> 00:06:16,570
Step, will be tractable and can often

136
00:06:16,650 --> 00:06:18,680
be done analytically in the  closed form.

137
00:06:18,780 --> 00:06:23,780
Whereas if you were trying to maximize

138
00:06:23,870 --> 00:06:24,620
this objective

139
00:06:24,710 --> 00:06:26,750
we try to take this  formula on the right

140
00:06:26,850 --> 00:06:28,520
and this maximum likely object,

141
00:06:28,640 --> 00:06:30,910
everyone, is to take this all

142
00:06:31,030 --> 00:06:31,860
on the  right and set its derivatives

143
00:06:31,930 --> 00:06:33,250
to zero and try to solve

144
00:06:33,350 --> 00:06:34,010
and you'll find that

145
00:06:34,110 --> 00:06:35,870
you're unable to obtain a solution

146
00:06:35,970 --> 00:06:36,880
to this in closed form this maximization.

147
00:06:36,990 --> 00:06:39,870
Okay?  And so to give you an example

148
00:06:39,980 --> 00:06:42,300
of that is that you remember

149
00:06:42,390 --> 00:06:43,070
our discussion

150
00:06:43,150 --> 00:06:45,050
on  exponential family marbles, right?

151
00:06:45,140 --> 00:06:50,700
It turns out that if X and Z is jointly,

152
00:06:50,790 --> 00:06:52,350
I guess, a line in  exponential

153
00:06:52,420 --> 00:06:53,140
families.

154
00:06:53,240 --> 00:06:55,960
So if P of X, Z prioritized by theta

155
00:06:56,080 --> 00:06:57,110
there's an explanation family

156
00:06:57,240 --> 00:06:57,860
distribution,

157
00:06:57,960 --> 00:06:59,750
which it turns out to be true for the

158
00:06:59,880 --> 00:07:01,120
mixture of Gaussians distribution.

159
00:07:01,240 --> 00:07:02,540
Then  turns out that

160
00:07:02,620 --> 00:07:04,490
the M step here will be tractable

161
00:07:04,610 --> 00:07:06,780
and the E step will also be tractable

162
00:07:06,870 --> 00:07:07,690
and so  you can do

163
00:07:07,780 --> 00:07:08,770
each of these steps very easily.

164
00:07:08,860 --> 00:07:11,070
Whereas performing

165
00:07:11,180 --> 00:07:13,020
trying to perform this

166
00:07:13,110 --> 00:07:14,230
original maximum likelihood

167
00:07:14,350 --> 00:07:18,510
estimation problem on this one, right?

168
00:07:18,530 --> 00:07:19,690
Will be  computationally very difficult.

169
00:07:19,770 --> 00:07:20,820
You're going to set

170
00:07:20,920 --> 00:07:21,930
the derivatives to zero

171
00:07:22,010 --> 00:07:22,640
and try to solve  for that.

172
00:07:22,660 --> 00:07:23,520
Analytically you won't be able to find

173
00:07:23,570 --> 00:07:24,510
an analytic solution to this.

174
00:07:24,580 --> 00:07:25,420
Okay?

175
00:07:25,490 --> 00:07:33,560
So what I want to do in a second

176
00:07:33,660 --> 00:07:34,830
is actually take this view

177
00:07:34,890 --> 00:07:37,790
of the EM Algorithm and  apply it

178
00:07:37,820 --> 00:07:39,470
to the mixture of Gaussians models.

179
00:07:39,540 --> 00:07:41,450
I want to take these

180
00:07:41,550 --> 00:07:42,560
E steps and M Steps and  work them out

181
00:07:42,630 --> 00:07:43,700
for the mixture of Gaussians model,

182
00:07:43,780 --> 00:07:45,570
but before I do that,

183
00:07:45,680 --> 00:07:46,770
I just want to say  one more thing about

184
00:07:46,870 --> 00:07:48,140
this other view of the EM Algorithm.

185
00:07:48,230 --> 00:07:52,300
It turns out there's one other  way

186
00:07:52,380 --> 00:07:54,350
of thinking about the EM Algorithm,

187
00:07:54,470 --> 00:07:55,860
which is the following:

188
00:07:55,940 --> 00:08:00,810
I can define an  optimization objective

189
00:08:00,920 --> 00:08:07,510
J of theta, Q are defined it to be this.

190
00:08:07,630 --> 00:08:12,920
This is just a thing in the  augments

191
00:08:13,040 --> 00:08:15,660
in the M step.

192
00:08:15,800 --> 00:08:22,980
Okay?

193
00:08:23,120 --> 00:08:27,470
And so what we proved

194
00:08:27,590 --> 00:08:31,750
using Jensen's inequality is that

195
00:08:31,890 --> 00:08:36,900
the log likelihood of theta

196
00:08:36,980 --> 00:08:39,210
is greater and equal to J of theta Q.

197
00:08:39,330 --> 00:08:40,400
So in other words,

198
00:08:40,480 --> 00:08:41,600
we  proved last time that

199
00:08:41,670 --> 00:08:43,410
for any value of theta and Q

200
00:08:43,510 --> 00:08:45,040
the log likelihood upper bounds

201
00:08:45,130 --> 00:08:45,880
J of  theta and Q.

202
00:08:45,980 --> 00:08:49,550
And so just to relate this back to,

203
00:08:49,680 --> 00:08:50,910
sort of, yet more things

204
00:08:50,980 --> 00:08:51,720
that you all ready  know,

205
00:08:51,770 --> 00:08:54,920
you can also think of covariant cause

206
00:08:55,020 --> 00:08:55,620
in a sense, right?

207
00:08:55,730 --> 00:09:00,420
However, our discussion  awhile back

208
00:09:00,520 --> 00:09:01,820
on the coordinate ascent optimization

209
00:09:01,900 --> 00:09:04,740
algorithm. So we can show,

210
00:09:04,880 --> 00:09:06,240
and I  won't actually show this view

211
00:09:06,320 --> 00:09:07,490
so just take our word for it

212
00:09:07,550 --> 00:09:08,760
and look for that at home if you  want,

213
00:09:08,840 --> 00:09:14,120
that EM is just coordinate

214
00:09:14,220 --> 00:09:17,940
in a set on the function J.

215
00:09:18,030 --> 00:09:19,310
So in the E step

216
00:09:19,360 --> 00:09:20,490
you maximize  with respect to Q

217
00:09:20,570 --> 00:09:24,810
and then the M step

218
00:09:24,900 --> 00:09:32,410
you maximize with respect to theta.

219
00:09:32,510 --> 00:09:34,090
Okay?

220
00:09:34,130 --> 00:09:36,480
So this  is another view

221
00:09:36,610 --> 00:09:37,440
of the EM Algorithm

222
00:09:37,520 --> 00:09:40,210
that shows why it has to converge,

223
00:09:40,210 --> 00:09:41,210
for example. If  you can –

224
00:09:44,180 --> 00:09:46,180
I've used in a sense of J of theta, Q

225
00:09:46,310 --> 00:09:48,580
having to monotonically increase

226
00:09:48,650 --> 00:09:49,470
on every  iteration.

227
00:09:49,550 --> 00:09:50,870
Okay?

228
00:09:50,930 --> 00:09:59,790
So what I want to do next

229
00:09:59,860 --> 00:10:00,710
is actually take

230
00:10:00,800 --> 00:10:02,980
this general EM machinery

231
00:10:03,060 --> 00:10:04,520
that we worked up  and apply it to

232
00:10:04,580 --> 00:10:08,580
a mixture Gaussians model.

233
00:10:08,690 --> 00:10:11,640
Before I do that,

234
00:10:11,740 --> 00:10:25,020
let me just check

235
00:10:25,150 --> 00:10:26,580
if there are  questions about

236
00:10:26,680 --> 00:10:28,610
the EM Algorithm as a whole?

237
00:10:28,710 --> 00:10:30,930
Okay, cool.

238
00:10:31,120 --> 00:10:33,710
So let's go ahead and work on

239
00:10:33,830 --> 00:10:40,790
the mixture of Gaussian's EM, all right?

240
00:10:40,890 --> 00:10:41,870
MOG, and that's  my abbreviation

241
00:10:41,980 --> 00:10:42,670
for Mixture of Gaussian's.

242
00:10:42,740 --> 00:10:43,660
So the E step

243
00:10:43,760 --> 00:10:44,830
were called those Q  distributions,

244
00:10:44,990 --> 00:10:45,940
right?

245
00:10:46,030 --> 00:10:46,910
In particular,

246
00:10:46,960 --> 00:10:48,080
I want to work out

247
00:10:48,150 --> 00:10:50,210
so Q is the probability distribution

248
00:10:50,330 --> 00:10:52,370
over the late and random variable Z

249
00:10:52,480 --> 00:10:54,890
and so the E step I'm gonna figure out

250
00:10:54,890 --> 00:10:55,890
what is these  compute –

251
00:10:56,110 --> 00:10:58,140
what is Q of ZI equals J.

252
00:10:58,220 --> 00:10:59,350
And you can think of this

253
00:10:59,410 --> 00:11:02,070
as my writing P of ZI  equals J,

254
00:11:02,200 --> 00:11:04,310
right? Under the Q distribution.

255
00:11:04,420 --> 00:11:05,140
That's what this notation means.

256
00:11:05,210 --> 00:11:10,090
And so the EM  Algorithm tells us that,

257
00:11:10,210 --> 00:11:15,870
let's see,

258
00:11:16,000 --> 00:11:17,900
Q of J is the likelihood probability

259
00:11:18,030 --> 00:11:19,490
of Z being the value J  and given XI

260
00:11:19,630 --> 00:11:31,520
and all your parameters. And so, well,

261
00:11:31,620 --> 00:11:32,410
the way you compute this

262
00:11:32,480 --> 00:11:34,210
is by  Dave's rule,right?

263
00:11:34,350 --> 00:11:36,250
So that is going to be equal to

264
00:11:42,330 --> 00:11:57,840
divided by right?

265
00:11:57,930 --> 00:11:59,670
That's all the by Dave's rule.

266
00:11:59,800 --> 00:12:04,510
And so this you know

267
00:12:04,570 --> 00:12:10,890
because XI  given ZI equals J.

268
00:12:10,990 --> 00:12:15,890
This was a Gaussian with mean mu J and

269
00:12:15,970 --> 00:12:16,790
covariant sigma J.

270
00:12:16,840 --> 00:12:18,530
And so to  compute this first term

271
00:12:18,590 --> 00:12:19,380
you plug in the formula

272
00:12:19,460 --> 00:12:21,260
for the Gaussian density there

273
00:12:21,350 --> 00:12:23,520
with  parameters mu J and sigma J

274
00:12:23,590 --> 00:12:25,610
and this you'd know

275
00:12:25,650 --> 00:12:32,160
because Z was not a nomial, right?

276
00:12:32,270 --> 00:12:37,460
Where parameters given by phi and so

277
00:12:37,590 --> 00:12:39,710
the problem of ZI being with J is just

278
00:12:39,810 --> 00:12:40,590
phi J

279
00:12:40,680 --> 00:12:45,180
and so  you can substitute

280
00:12:45,270 --> 00:12:45,830
these terms in.

281
00:12:45,960 --> 00:12:47,000
Similarly do the same thing

282
00:12:47,090 --> 00:12:47,780
for the denominator and

283
00:12:47,880 --> 00:12:48,870
that's how you work out what Q is.

284
00:12:48,970 --> 00:12:50,040
Okay?

285
00:12:50,130 --> 00:12:53,380
And so in the previous lecture

286
00:12:53,510 --> 00:12:54,390
this value the  probability that

287
00:12:54,480 --> 00:12:57,890
ZI equals J under the Q distribution

288
00:12:57,990 --> 00:13:00,640
that was why I denoted that as WIJ.

289
00:13:00,740 --> 00:13:02,270
So that would be the E step

290
00:13:02,330 --> 00:13:11,550
and then in the M step

291
00:13:11,620 --> 00:13:15,190
we maximize with respect

292
00:13:15,320 --> 00:13:16,170
to all of our  parameters.

293
00:13:16,260 --> 00:13:22,350
This, well I seem to be writing

294
00:13:22,460 --> 00:13:23,690
the same formula down a lot today.

295
00:13:23,770 --> 00:13:29,680
All right.

296
00:13:29,780 --> 00:13:49,290
And just so we're completely

297
00:13:49,420 --> 00:13:50,580
concrete about how you do that,

298
00:13:50,690 --> 00:13:52,420
right? So if you do that

299
00:13:52,520 --> 00:13:53,390
you end up with

300
00:13:53,470 --> 00:13:57,570
so plugging in the quantities

301
00:13:57,660 --> 00:14:01,990
that you know that becomes this,

302
00:14:02,110 --> 00:14:38,230
let's see.  Right.

303
00:14:38,370 --> 00:14:40,840
And so that we're completely

304
00:14:40,920 --> 00:14:42,460
concrete about what the M step is doing.

305
00:14:42,560 --> 00:14:45,010
So in the M  step that was,

306
00:14:45,110 --> 00:14:46,060
I guess,

307
00:14:46,190 --> 00:14:46,940
QI over Z, I being over J.

308
00:14:47,040 --> 00:14:48,170
Just in the summation,

309
00:14:48,270 --> 00:14:49,460
sum over J is the  sum over

310
00:14:49,560 --> 00:14:50,910
all the possible values of ZI

311
00:14:51,020 --> 00:14:52,950
and then

312
00:14:53,050 --> 00:14:54,880
this thing here is my Gaussian density.

313
00:14:54,880 --> 00:14:55,880
Sorry, guys, this thing –

314
00:14:57,670 --> 00:14:59,840
well, this first term here, right?

315
00:14:59,920 --> 00:15:06,910
Is my P of XI given ZI

316
00:15:07,040 --> 00:15:11,120
and that's  P of ZI.

317
00:15:11,220 --> 00:15:13,030
Okay?

318
00:15:13,170 --> 00:15:18,620
And so to maximize this with respect to

319
00:15:18,700 --> 00:15:20,330
say you want to maximize this

320
00:15:20,390 --> 00:15:21,680
with respect to all of your parameters

321
00:15:21,790 --> 00:15:23,490
phi, mu and sigma.

322
00:15:23,550 --> 00:15:25,820
So to maximize with respect to  the

323
00:15:25,930 --> 00:15:26,880
parameter mu,

324
00:15:26,950 --> 00:15:29,360
say, you would take the derivative

325
00:15:29,440 --> 00:15:30,280
for respect to mu

326
00:15:30,340 --> 00:15:33,300
and set that to  zero

327
00:15:33,440 --> 00:15:36,710
and you would and if you actually do

328
00:15:36,850 --> 00:15:37,730
that computation you would get,

329
00:15:37,820 --> 00:15:48,940
for  instance, that that becomes your

330
00:15:49,040 --> 00:15:50,030
update to mu J.

331
00:15:50,130 --> 00:15:51,600
Okay?

332
00:15:51,680 --> 00:15:53,820
Just so I want to

333
00:15:53,870 --> 00:15:55,380
the equation is  unimportant.

334
00:15:55,480 --> 00:15:57,760
All of these equations are written down

335
00:15:57,880 --> 00:15:58,530
in the lecture notes.

336
00:15:58,630 --> 00:15:59,250
I'm writing  these down

337
00:15:59,320 --> 00:16:01,470
just to be completely concrete about

338
00:16:01,540 --> 00:16:02,330
what the M step means.

339
00:16:02,440 --> 00:16:03,680
And so write  down that formula,

340
00:16:03,790 --> 00:16:04,850
plug in the densities you know,

341
00:16:04,920 --> 00:16:06,360
take the derivative set to zero,

342
00:16:06,440 --> 00:16:07,090
solve  for mu J

343
00:16:07,190 --> 00:16:08,860
and in the same way

344
00:16:09,020 --> 00:16:10,320
you set the derivatives equal to zero

345
00:16:10,440 --> 00:16:12,250
and solve for your  updates

346
00:16:12,400 --> 00:16:14,200
for your other parameters phi

347
00:16:14,300 --> 00:16:16,430
and sigma as well.

348
00:16:16,530 --> 00:16:17,670
Okay?

349
00:16:17,770 --> 00:16:19,540
Well,

350
00:16:19,700 --> 00:16:24,540
just point out just one little

351
00:16:24,640 --> 00:16:25,590
tricky bit for this

352
00:16:25,680 --> 00:16:27,650
that you haven't seen before

353
00:16:27,730 --> 00:16:29,110
that most  of you

354
00:16:29,200 --> 00:16:33,710
probably all ready now,

355
00:16:33,800 --> 00:16:34,930
but I'll just mention is that

356
00:16:34,990 --> 00:16:35,770
since phi here is

357
00:16:35,850 --> 00:16:36,960
a multinomial  distribution

358
00:16:37,060 --> 00:16:38,370
when you take this formula

359
00:16:38,460 --> 00:16:41,870
and you maximize it with respect to phi

360
00:16:41,950 --> 00:16:43,180
you  actually have

361
00:16:43,280 --> 00:16:44,500
an additional constraint, right?

362
00:16:44,560 --> 00:16:48,340
That the sum of I let's see,

363
00:16:48,440 --> 00:16:51,530
sum over J, phi  J must be equal to one.

364
00:16:51,620 --> 00:16:52,470
All right?

365
00:16:52,560 --> 00:16:54,190
So, again,in the M step

366
00:16:54,280 --> 00:16:55,030
I want to take this thing

367
00:16:55,130 --> 00:16:56,010
and  maximize it with respect

368
00:16:56,090 --> 00:16:56,820
to all the parameters

369
00:16:56,890 --> 00:16:59,110
and when you maximize this respect

370
00:16:59,180 --> 00:17:00,290
to the  parameters phi J

371
00:17:00,380 --> 00:17:02,280
you need to respect the constraint

372
00:17:02,380 --> 00:17:04,490
that sum of J phi J must be

373
00:17:04,560 --> 00:17:05,170
equal to  one.

374
00:17:05,250 --> 00:17:08,550
And so, well, you all ready know

375
00:17:08,640 --> 00:17:09,420
how to do constraint maximization,

376
00:17:09,480 --> 00:17:10,140
right?

377
00:17:10,190 --> 00:17:11,020
So I'll  throw out the method

378
00:17:11,070 --> 00:17:11,640
of the granjay multipliers

379
00:17:11,680 --> 00:17:12,210
and generalize the granjay when you

380
00:17:12,270 --> 00:17:14,290
talk  about the support of X machines.

381
00:17:14,340 --> 00:17:16,090
And so to actually perform the

382
00:17:16,180 --> 00:17:17,570
maximization in terms of phi J

383
00:17:17,570 --> 00:17:18,570
you construct to the granjay, which is –

384
00:17:21,460 --> 00:17:24,570
all right?

385
00:17:24,650 --> 00:17:25,950
So that's the equation from  above

386
00:17:26,010 --> 00:17:27,560
and we'll denote in the dot dot dot

387
00:17:27,680 --> 00:17:29,340
plus theta times that,

388
00:17:29,440 --> 00:17:34,970
where this is sort of

389
00:17:35,050 --> 00:17:36,290
the  granjay multiplier

390
00:17:36,380 --> 00:17:44,960
and this is your optimization objective.

391
00:17:47,970 --> 00:17:49,160
And so to actually solve

392
00:17:49,250 --> 00:17:51,940
the  parameters phi J

393
00:17:52,100 --> 00:17:53,240
you set the parameters of this

394
00:17:53,330 --> 00:17:58,380
so that the granjay is zero and solve.

395
00:17:58,500 --> 00:17:59,200
Okay?

396
00:17:59,260 --> 00:18:01,830
And if you then work through the math

397
00:18:01,890 --> 00:18:04,120
you get the appropriate value

398
00:18:04,200 --> 00:18:05,410
to update the  phi J's too

399
00:18:05,500 --> 00:18:07,990
which I won't do,

400
00:18:08,050 --> 00:18:10,710
but I'll be all the full directions

401
00:18:10,810 --> 00:18:11,730
are in the lecture notes.

402
00:18:11,820 --> 00:18:12,640
I  won't do that here.

403
00:18:12,750 --> 00:18:16,070
Okay.

404
00:18:16,160 --> 00:18:20,110
And so if you actually perform

405
00:18:20,190 --> 00:18:20,870
all these computations

406
00:18:20,960 --> 00:18:21,740
you can also verify that.

407
00:18:21,860 --> 00:18:23,660
So I  just wrote down

408
00:18:23,770 --> 00:18:25,710
a bunch of formulas for the EM Algorithm.

409
00:18:25,770 --> 00:18:28,380
At the beginning of the last  lecture

410
00:18:28,480 --> 00:18:29,100
I said for

411
00:18:29,100 --> 00:18:30,100
the mixture of Gaussian's model –

412
00:18:29,950 --> 00:18:32,010
I said for the EM here's the formula

413
00:18:32,090 --> 00:18:33,280
for computing the WIJ's

414
00:18:33,370 --> 00:18:34,310
and here's a formula for

415
00:18:34,380 --> 00:18:35,470
computing the mud's

416
00:18:35,560 --> 00:18:36,250
and so on,

417
00:18:36,310 --> 00:18:37,550
and  this variation is where

418
00:18:37,680 --> 00:18:38,940
all of those formulas actually come from.

419
00:18:39,020 --> 00:18:44,090
Okay? Questions about  this?

420
00:18:44,200 --> 00:18:46,540
Yeah?

421
00:18:46,620 --> 00:18:48,410
Student:

422
00:18:48,490 --> 00:18:52,320
Instructor (Andrew Ng):[Inaudible]

423
00:18:52,420 --> 00:18:55,420
Oh, I see. So it turns out that, yes,

424
00:18:55,500 --> 00:18:56,380
there's also constrained to

425
00:18:56,450 --> 00:18:58,430
the phi J this must be greater than zero.

426
00:18:58,500 --> 00:19:03,110
It turns out that if you want

427
00:19:03,230 --> 00:19:05,870
you could actually  write down

428
00:19:05,920 --> 00:19:07,560
then generalize the granjayn

429
00:19:07,660 --> 00:19:08,750
incorporating all of these constraints

430
00:19:08,840 --> 00:19:09,510
as well

431
00:19:09,590 --> 00:19:10,910
and  you can solve to [inaudible]

432
00:19:10,980 --> 00:19:11,580
these constraints.

433
00:19:11,670 --> 00:19:12,990
It turns out that

434
00:19:13,040 --> 00:19:14,750
in this particular derivation

435
00:19:14,760 --> 00:19:16,140
actually it turns out that very often

436
00:19:16,210 --> 00:19:17,800
we find maximum likely estimate for

437
00:19:17,890 --> 00:19:18,950
multinomial  distributions probabilities.

438
00:19:19,020 --> 00:19:20,660
It turns out that

439
00:19:20,730 --> 00:19:21,710
if you ignore these constraints

440
00:19:21,750 --> 00:19:23,020
and you just  maximize the formula

441
00:19:23,110 --> 00:19:26,820
luckily you end up with values that

442
00:19:26,890 --> 00:19:29,610
actually are greater than

443
00:19:29,660 --> 00:19:30,350
or  equal to zero

444
00:19:30,440 --> 00:19:33,040
and so if even ignoring those constraint

445
00:19:33,120 --> 00:19:34,770
you end up with parameters

446
00:19:34,870 --> 00:19:36,070
that are  greater than or equal to zero

447
00:19:36,170 --> 00:19:37,000
that shows that that must be

448
00:19:37,200 --> 00:19:37,900
the correct solution

449
00:19:37,990 --> 00:19:39,730
because  adding that constraint

450
00:19:39,850 --> 00:19:40,570
won't change anything.

451
00:19:40,570 --> 00:19:41,570
So this constraint it is then caused –

452
00:19:44,120 --> 00:19:45,780
it turns  out that if you ignore this

453
00:19:45,850 --> 00:19:46,990
and just do what I've wrote down

454
00:19:47,070 --> 00:19:48,140
you actually get the right  answer.

455
00:19:48,210 --> 00:19:56,840
Okay? Great.

456
00:19:56,920 --> 00:20:01,900
So let me just very quickly talk about

457
00:20:01,970 --> 00:20:03,600
one more example of a mixture model.

458
00:20:03,720 --> 00:20:06,640
And the  perfect example for this

459
00:20:06,690 --> 00:20:07,950
is imagine you want to do

460
00:20:08,030 --> 00:20:09,220
text clustering, right?

461
00:20:09,270 --> 00:20:10,710
So someone  gives you a large set of

462
00:20:10,800 --> 00:20:13,380
documents and you want to cluster them

463
00:20:13,470 --> 00:20:15,590
together into cohesive  topics.

464
00:20:15,700 --> 00:20:18,580
I think I mentioned

465
00:20:18,650 --> 00:20:19,450
the news website news.google.com.

466
00:20:19,530 --> 00:20:20,470
That's one application

467
00:20:20,560 --> 00:20:21,990
of  text clustering where you might

468
00:20:22,090 --> 00:20:24,090
want to look at all of the news

469
00:20:24,220 --> 00:20:25,880
stories about today,

470
00:20:26,010 --> 00:20:28,760
all the  news stories

471
00:20:28,850 --> 00:20:29,580
written by everyone,

472
00:20:29,640 --> 00:20:30,740
written by all the online news websites

473
00:20:30,790 --> 00:20:31,760
about whatever  happened yesterday

474
00:20:31,820 --> 00:20:32,710
and there will be

475
00:20:32,790 --> 00:20:34,770
many, many different stories

476
00:20:34,850 --> 00:20:35,670
on the same thing,

477
00:20:35,730 --> 00:20:36,320
right?

478
00:20:36,400 --> 00:20:38,130
And by running a text-clustering

479
00:20:38,200 --> 00:20:39,450
algorithm you can

480
00:20:39,530 --> 00:20:40,680
group related documents  together.

481
00:20:40,730 --> 00:20:41,980
Okay?

482
00:20:42,070 --> 00:20:44,690
So how do you apply the EM Algorithm

483
00:20:44,760 --> 00:20:45,450
to text clustering?

484
00:20:45,540 --> 00:20:54,480
I want to do this to illustrate

485
00:20:54,600 --> 00:20:56,660
an example in which

486
00:20:56,770 --> 00:20:58,620
you run the EM Algorithm

487
00:20:58,720 --> 00:21:00,450
on discreet valued inputs

488
00:21:00,570 --> 00:21:02,200
where the input

489
00:21:02,320 --> 00:21:03,870
where the training examples XI

490
00:21:03,960 --> 00:21:05,050
are discreet values.

491
00:21:05,130 --> 00:21:07,540
So what I want to talk about

492
00:21:07,610 --> 00:21:08,360
specifically is

493
00:21:08,600 --> 00:21:10,750
the mixture of na.ve Bayes model

494
00:21:10,830 --> 00:21:18,570
and depending on how much you  remember

495
00:21:18,640 --> 00:21:21,270
about na.ve Bayes

496
00:21:21,340 --> 00:21:22,040
I talked about two event models.

497
00:21:22,100 --> 00:21:22,970
One was the

498
00:21:23,040 --> 00:21:24,130
multivariant  vanuvy event model.

499
00:21:24,190 --> 00:21:26,310
One was the multinomial event model.

500
00:21:26,400 --> 00:21:28,400
Today I'm gonna use

501
00:21:28,490 --> 00:21:30,050
the   multivariant vanuvy event model.

502
00:21:30,130 --> 00:21:31,460
If you don't remember

503
00:21:31,550 --> 00:21:33,000
what those terms mean  anymore

504
00:21:33,060 --> 00:21:33,770
don't worry about it.

505
00:21:33,830 --> 00:21:35,630
I think the equation will

506
00:21:35,700 --> 00:21:36,290
still make sense.

507
00:21:36,390 --> 00:21:38,170
But in this  setting

508
00:21:38,240 --> 00:21:44,920
we're given a training set X1 for XM.

509
00:21:45,020 --> 00:21:47,750
So we're given M text documents

510
00:21:47,850 --> 00:21:57,290
where  each XI is zero one to the end.

511
00:21:57,380 --> 00:21:59,670
So each of our training examples

512
00:21:59,730 --> 00:22:03,330
is an indimensional bit  of vector,

513
00:22:03,440 --> 00:22:05,530
right? S this was the representation

514
00:22:05,630 --> 00:22:10,000
where XIJ was it indicates

515
00:22:10,120 --> 00:22:16,600
whether word  J appears in document I,

516
00:22:16,680 --> 00:22:25,190
right? And let's say that

517
00:22:25,280 --> 00:22:29,490
we're going to model ZI the

518
00:22:29,570 --> 00:22:32,600
our latent  random variable

519
00:22:32,710 --> 00:22:33,670
meaning our hidden random variable ZI

520
00:22:33,750 --> 00:22:34,680
will take on two values zero one

521
00:22:34,790 --> 00:22:37,330
so this means I'm just gonna find

522
00:22:37,420 --> 00:22:38,310
two clusters

523
00:22:38,360 --> 00:22:39,850
and you can generalize the clusters

524
00:22:39,930 --> 00:22:40,570
that  you want.

525
00:22:40,630 --> 00:22:48,210
So in the mixture of na.ve Bayes model

526
00:22:48,300 --> 00:22:52,890
we assume that ZI is distributed

527
00:22:52,970 --> 00:22:58,960
and mu E with  some value of phi

528
00:22:59,050 --> 00:23:01,650
so there's some probability of

529
00:23:01,690 --> 00:23:04,070
each document coming from cluster one

530
00:23:04,160 --> 00:23:04,940
or from cluster two.

531
00:23:05,050 --> 00:23:06,360
We assume that

532
00:23:06,450 --> 00:23:16,310
the probability of XI given ZI, right?

533
00:23:16,390 --> 00:23:23,170
Will make the  same na.ve Bayes

534
00:23:23,290 --> 00:23:24,540
assumption as we did before.

535
00:23:24,620 --> 00:23:33,410
Okay?

536
00:23:33,410 --> 00:23:34,410
And more specifically –

537
00:23:37,750 --> 00:24:01,440
well,  excuse me, right. Okay.

538
00:24:01,530 --> 00:24:03,670
And so most of us [inaudible] cycles

539
00:24:03,720 --> 00:24:06,980
one given ZI equals say  zero will be

540
00:24:07,070 --> 00:24:10,810
given by a parameter phi substitute J

541
00:24:10,900 --> 00:24:13,160
given Z equals zero.

542
00:24:13,280 --> 00:24:20,400
So if you take this  chalkboard

543
00:24:20,520 --> 00:24:22,070
and if you take all instances of

544
00:24:22,160 --> 00:24:25,450
the alphabet Z and replace it with Y

545
00:24:25,530 --> 00:24:28,920
then you  end up with exactly

546
00:24:29,020 --> 00:24:30,830
the same equation as I've written down

547
00:24:30,930 --> 00:24:32,910
for na.ve Bayes like a long  time ago.

548
00:24:33,020 --> 00:24:36,330
Okay?

549
00:24:36,390 --> 00:24:42,110
And I'm not actually going to work out

550
00:24:42,190 --> 00:24:43,700
the mechanics deriving the EM Algorithm,

551
00:24:43,720 --> 00:24:46,270
but it  turns out that

552
00:24:46,340 --> 00:24:47,770
if you take this joint distribution

553
00:24:47,850 --> 00:24:51,220
of X and Z and if you work out

554
00:24:51,320 --> 00:24:52,210
what the  equations are

555
00:24:52,310 --> 00:24:52,930
for the EM algorithm for

556
00:24:53,010 --> 00:24:54,540
maximum likelihood estimation

557
00:24:54,640 --> 00:24:55,850
of parameters

558
00:24:55,950 --> 00:25:02,270
you  find that in the E step you compute,

559
00:25:02,270 --> 00:25:03,270
you know, let's say these parameters –

560
00:25:07,090 --> 00:25:09,330
these weights  WI which are going to be

561
00:25:09,430 --> 00:25:11,380
equal to your perceived distribution of Z

562
00:25:11,480 --> 00:25:13,260
being equal one  conditioned on XI

563
00:25:13,360 --> 00:25:16,650
parameterized by your phi's

564
00:25:16,710 --> 00:25:20,880
and given your parameters

565
00:25:20,990 --> 00:25:26,330
and in the M  step.

566
00:25:26,410 --> 00:25:39,350
Okay?

567
00:25:39,410 --> 00:26:19,130
And that's the equation you get

568
00:26:19,230 --> 00:26:20,330
in the M step.

569
00:26:20,450 --> 00:26:23,330
I mean, again, the equations  themselves

570
00:26:23,390 --> 00:26:24,240
aren't too important.

571
00:26:24,240 --> 00:26:25,240
Just sort of convey –

572
00:26:26,660 --> 00:26:29,100
I'll give you a second

573
00:26:29,200 --> 00:26:30,390
to finish  writing, I guess.

574
00:26:30,450 --> 00:26:32,220
And when you're done

575
00:26:32,320 --> 00:26:33,150
or finished writing take a look

576
00:26:33,240 --> 00:26:33,970
at these equations

577
00:26:34,040 --> 00:26:35,180
and see if they make intuitive sense

578
00:26:35,240 --> 00:26:36,330
to you why these three equations,

579
00:26:36,400 --> 00:26:39,180
sort of, sound like  they might be

580
00:26:39,260 --> 00:26:42,160
the right thing to do. Yeah?

581
00:26:42,270 --> 00:26:46,410
Student:[Inaudible]

582
00:26:46,520 --> 00:26:48,250
Instructor (Andrew Ng):Say that again.

583
00:26:48,360 --> 00:26:49,730
Student:Y

584
00:26:49,820 --> 00:26:58,270
Instructor (Andrew Ng):Oh, yes, thank you. Right. Sorry, just,

585
00:26:58,370 --> 00:27:00,620
for everywhere over Y I  meant Z.

586
00:27:11,670 --> 00:27:14,730
Yeah?

587
00:27:14,820 --> 00:27:16,340
Student:[Inaudible] in the first place?

588
00:27:18,290 --> 00:27:21,050
Instructor (Andrew Ng):No.

589
00:27:24,520 --> 00:27:27,850
So what is it? Normally

590
00:27:27,930 --> 00:27:29,350
you initialize phi's

591
00:27:29,430 --> 00:27:30,820
to be  something else, say randomly.

592
00:27:35,250 --> 00:27:36,840
So just like in na.ve Bayes

593
00:27:36,950 --> 00:27:38,750
we saw zero probabilities

594
00:27:38,800 --> 00:27:40,720
as a  bad thing so the same reason

595
00:27:40,800 --> 00:27:41,710
you try to avoid zero probabilities,

596
00:27:41,800 --> 00:27:50,070
yeah. Okay? And so just the intuition

597
00:27:50,150 --> 00:27:51,300
behind these equations is in the E step

598
00:27:51,360 --> 00:27:58,010
WI's is you're gonna take

599
00:27:58,060 --> 00:27:59,430
your best  guess for whether the document

600
00:27:59,520 --> 00:28:01,290
came from cluster one or cluster zero,

601
00:28:01,400 --> 00:28:03,920
all right? This is  very similar

602
00:28:04,020 --> 00:28:07,170
to the intuitions behind the EM Algorithm

603
00:28:07,270 --> 00:28:07,980
that we talked about

604
00:28:08,090 --> 00:28:08,630
in a previous  lecture.

605
00:28:08,690 --> 00:28:09,700
So in the E step

606
00:28:09,750 --> 00:28:12,080
we're going to compute these weights

607
00:28:12,160 --> 00:28:12,810
that tell us

608
00:28:12,880 --> 00:28:13,970
do I think this  document

609
00:28:14,050 --> 00:28:15,070
came from cluster one or cluster zero.

610
00:28:15,230 --> 00:28:20,260
And then in the M step

611
00:28:20,340 --> 00:28:24,100
I'm gonna say  does this numerator

612
00:28:24,190 --> 00:28:26,240
is the sum over all the elements

613
00:28:26,330 --> 00:28:27,460
of my training set of

614
00:28:27,540 --> 00:28:29,870
so then  informally, right?

615
00:28:29,920 --> 00:28:31,430
WI is one there,

616
00:28:31,490 --> 00:28:33,690
but I think the document

617
00:28:33,760 --> 00:28:37,480
came from cluster one and

618
00:28:37,580 --> 00:28:39,460
so  this will essentially sum up

619
00:28:39,540 --> 00:28:40,940
all the times I saw words J

620
00:28:41,030 --> 00:28:44,450
in documents that I think are in

621
00:28:44,560 --> 00:28:45,660
cluster one.

622
00:28:45,720 --> 00:28:47,250
And these are sort of weighted

623
00:28:47,330 --> 00:28:48,230
by the actual probability.

624
00:28:48,300 --> 00:28:49,130
I think it came from  cluster one

625
00:28:49,210 --> 00:28:51,620
and then I'll divide by again,

626
00:28:51,770 --> 00:28:52,990
if all of these were ones and zeros

627
00:28:53,030 --> 00:28:55,300
then I'd be  dividing by the actual

628
00:28:55,370 --> 00:28:56,810
number of documents

629
00:28:56,860 --> 00:28:57,580
I had in cluster one.

630
00:28:57,660 --> 00:28:58,880
So if all the WI's were  either

631
00:28:58,940 --> 00:29:00,810
onesor zeroes

632
00:29:00,880 --> 00:29:02,300
then this would be exactly

633
00:29:02,390 --> 00:29:04,060
the fraction of documents that

634
00:29:04,150 --> 00:29:07,130
I saw in  cluster one in which

635
00:29:07,220 --> 00:29:08,680
I also saw were at J.

636
00:29:08,740 --> 00:29:09,480
Okay?

637
00:29:09,530 --> 00:29:10,440
But in the EM Algorithm

638
00:29:10,530 --> 00:29:11,740
you don't  make a hard

639
00:29:11,810 --> 00:29:13,170
assignment decision about

640
00:29:13,230 --> 00:29:14,280
is this in cluster one or

641
00:29:14,360 --> 00:29:15,530
is this in cluster zero.

642
00:29:15,570 --> 00:29:17,890
You  instead represent your uncertainty

643
00:29:17,990 --> 00:29:18,910
about cluster membership with the

644
00:29:18,990 --> 00:29:19,810
parameters WI.

645
00:29:19,940 --> 00:29:22,230
Okay?

646
00:29:22,330 --> 00:29:24,810
It actually turns out that

647
00:29:24,880 --> 00:29:27,420
when we actually implement

648
00:29:27,490 --> 00:29:28,220
this particular model

649
00:29:28,300 --> 00:29:29,150
it actually  turns out that

650
00:29:29,260 --> 00:29:30,020
by the nature of this computation

651
00:29:30,080 --> 00:29:31,250
all the values of WI's

652
00:29:31,330 --> 00:29:34,310
will be very close to  either one or zero

653
00:29:34,380 --> 00:29:35,960
so they'll be numerically

654
00:29:36,020 --> 00:29:37,530
almost indistinguishable

655
00:29:37,620 --> 00:29:39,140
from one's and  zeroes.

656
00:29:39,190 --> 00:29:39,820
This is a property of na.ve Bayes.

657
00:29:39,890 --> 00:29:40,760
If you actually compute this probability

658
00:29:40,820 --> 00:29:42,740
from  all those documents

659
00:29:42,810 --> 00:29:45,970
you find that WI is either 0.0001

660
00:29:46,050 --> 00:29:46,900
or 0.999.

661
00:29:46,940 --> 00:29:48,690
It'll be amazingly close to

662
00:29:48,750 --> 00:29:49,530
either zero or one

663
00:29:49,610 --> 00:29:50,790
and so the M step

664
00:29:50,860 --> 00:29:51,890
and so this is pretty much guessing

665
00:29:51,970 --> 00:29:53,640
whether each  document is

666
00:29:53,730 --> 00:29:54,530
in cluster one or cluster zero

667
00:29:54,620 --> 00:29:57,960
and then using formulas

668
00:29:58,060 --> 00:29:59,380
they're very similar to  maximum

669
00:29:59,470 --> 00:30:01,580
likely estimation for na.ve Bayes.

670
00:30:01,680 --> 00:30:06,720
Okay? Cool.

671
00:30:06,820 --> 00:30:10,860
Are there and if some of

672
00:30:10,940 --> 00:30:12,460
these equations don't look

673
00:30:12,530 --> 00:30:14,270
that familiar to you anymore,

674
00:30:14,370 --> 00:30:15,620
sort of, go back and

675
00:30:15,720 --> 00:30:20,350
take another  look at what you saw

676
00:30:20,410 --> 00:30:22,510
in na.ve Bayes

677
00:30:22,590 --> 00:30:23,760
and hopefully you can see the links

678
00:30:23,840 --> 00:30:25,530
there as well.

679
00:30:25,620 --> 00:30:27,330
Questions about this before I move on?

680
00:30:27,430 --> 00:30:31,140
Right, okay.

681
00:30:31,220 --> 00:30:34,940
Of course the way I got these equations

682
00:30:35,050 --> 00:30:36,680
was by turning through the machinery

683
00:30:36,770 --> 00:30:37,760
of the EM  Algorithm,

684
00:30:37,850 --> 00:30:39,080
right? I didn't just write these

685
00:30:39,210 --> 00:30:40,480
out of thin air.

686
00:30:40,600 --> 00:30:41,320
The way you do this is

687
00:30:41,410 --> 00:30:42,710
by  writing down the E step

688
00:30:42,810 --> 00:30:43,780
and the M step for this model

689
00:30:43,880 --> 00:30:45,930
and then the M step same  derivatives

690
00:30:46,020 --> 00:30:46,990
equal to zero

691
00:30:47,090 --> 00:30:47,860
and solving from that

692
00:30:48,000 --> 00:30:49,130
so that's how you get the

693
00:30:49,220 --> 00:30:50,070
M step and the E  step.

694
00:30:50,160 --> 00:31:23,410
So the last thing I want to do today

695
00:31:23,510 --> 00:31:25,240
is talk about the factor analysis model

696
00:31:25,370 --> 00:31:37,690
and the reason  I want to do this

697
00:31:37,800 --> 00:31:39,610
is sort of two reasons

698
00:31:39,750 --> 00:31:40,570
because one is factor analysis

699
00:31:40,660 --> 00:31:41,960
is kind of a useful  model.

700
00:31:42,090 --> 00:31:44,660
It's not as widely used as

701
00:31:44,750 --> 00:31:45,360
mixtures of Gaussian's

702
00:31:45,430 --> 00:31:47,420
and mixtures of na.ve Bayes  maybe,

703
00:31:47,510 --> 00:31:50,570
but it's sort of useful.

704
00:31:50,670 --> 00:31:52,110
But the other reason

705
00:31:52,200 --> 00:31:53,620
I want to derive this model is that

706
00:31:53,740 --> 00:31:56,030
there  are a few steps in the math

707
00:31:56,120 --> 00:31:57,250
that are more generally useful.

708
00:31:57,310 --> 00:31:58,600
In particular,

709
00:31:58,700 --> 00:32:00,010
where this is for  factor

710
00:32:00,080 --> 00:32:01,040
analysis this would be an example

711
00:32:01,150 --> 00:32:03,350
in which we'll do EM

712
00:32:03,410 --> 00:32:06,140
where the late and  random variable

713
00:32:06,260 --> 00:32:08,040
where the hidden random variable Z is

714
00:32:08,170 --> 00:32:10,110
going to be continued as  valued.

715
00:32:10,250 --> 00:32:12,780
And so some of the math we'll see

716
00:32:12,860 --> 00:32:14,340
in deriving factor analysis

717
00:32:14,410 --> 00:32:15,230
will be a little bit  different

718
00:32:15,270 --> 00:32:15,850
than what you saw before

719
00:32:15,850 --> 00:32:16,850
and they're just a –

720
00:32:17,040 --> 00:32:19,350
it turns out the full derivation

721
00:32:19,420 --> 00:32:20,470
for  EM for factor analysis

722
00:32:20,520 --> 00:32:21,760
is sort of extremely long and complicated

723
00:32:21,810 --> 00:32:22,750
and so I won't inflect  that

724
00:32:22,790 --> 00:32:25,340
on you in lecture today,

725
00:32:25,420 --> 00:32:28,160
but I will still be writing

726
00:32:28,160 --> 00:32:29,160
more equations than is –

727
00:32:29,510 --> 00:32:30,610
than you'll    see me do

728
00:32:30,670 --> 00:32:31,670
in other lectures because there are,

729
00:32:31,720 --> 00:32:33,360
sort of, just a few steps

730
00:32:33,450 --> 00:32:34,610
in the factor  analysis derivation

731
00:32:34,680 --> 00:32:37,020
so I'll physically illustrate it.

732
00:32:37,100 --> 00:32:42,970
So it's actually [inaudible] the model

733
00:32:43,060 --> 00:32:45,090
and it's really contrast to

734
00:32:45,190 --> 00:32:45,800
the mixture of Gaussians  model,

735
00:32:45,880 --> 00:32:46,700
all right?

736
00:32:46,770 --> 00:32:48,010
So for the mixture of Gaussians model,

737
00:32:48,090 --> 00:32:49,750
which is our first model we had,

738
00:32:49,820 --> 00:32:52,230
that well I actually motivated it

739
00:32:52,270 --> 00:32:56,530
by drawing the data set like this,

740
00:32:56,620 --> 00:32:57,900
right?

741
00:32:57,950 --> 00:32:59,030
That one of you  has a data set

742
00:32:59,100 --> 00:32:59,770
that looks like this,

743
00:32:59,830 --> 00:33:05,580
right? So this was a problem

744
00:33:05,640 --> 00:33:08,240
where n is twodimensional

745
00:33:08,280 --> 00:33:10,640
and you have, I don't know,

746
00:33:10,710 --> 00:33:11,610
maybe 50 or 100 training examples,

747
00:33:11,670 --> 00:33:12,810
whatever,  right?

748
00:33:12,850 --> 00:33:16,860
And I said maybe you want to give

749
00:33:16,930 --> 00:33:18,410
a label training set like this.

750
00:33:18,440 --> 00:33:19,540
Maybe you want to  model this

751
00:33:19,610 --> 00:33:22,140
as a mixture of two Gaussians.

752
00:33:22,210 --> 00:33:23,990
Okay?

753
00:33:24,080 --> 00:33:25,530
And so a mixture of Gaussian models

754
00:33:25,620 --> 00:33:28,150
tend to be applicable

755
00:33:28,270 --> 00:33:33,260
where m is larger, and often much

756
00:33:33,310 --> 00:33:34,300
larger, than n where the number  of

757
00:33:34,410 --> 00:33:36,290
training examples you have

758
00:33:36,340 --> 00:33:38,770
is at least as large as,

759
00:33:38,860 --> 00:33:39,780
and is usually much larger than,

760
00:33:39,850 --> 00:33:41,830
the  dimension of the data.

761
00:33:41,890 --> 00:33:47,180
What I want to do is talk about

762
00:33:47,230 --> 00:33:48,090
a different problem

763
00:33:48,150 --> 00:33:50,210
where I want you to imagine

764
00:33:50,320 --> 00:33:52,970
what  happens if either the dimension

765
00:33:53,050 --> 00:33:54,700
of your data is roughly equal

766
00:33:54,770 --> 00:33:56,880
to the number of examples  you have or

767
00:33:56,940 --> 00:34:02,430
maybe the dimension of your data is maybe

768
00:34:02,560 --> 00:34:05,270
even much larger than the  number of

769
00:34:05,380 --> 00:34:06,930
training examples you have. Okay?

770
00:34:07,090 --> 00:34:11,360
So how do you model such

771
00:34:11,410 --> 00:34:12,140
a very high  dimensional data?

772
00:34:12,200 --> 00:34:15,100
Watch and you will see sometimes, right?

773
00:34:15,180 --> 00:34:16,950
If you run a plant or  something,

774
00:34:17,020 --> 00:34:17,670
you run a factory,

775
00:34:17,740 --> 00:34:20,400
maybe you have a thousand measurements

776
00:34:20,500 --> 00:34:21,250
all through your  plants,

777
00:34:21,380 --> 00:34:22,250
but you only have five

778
00:34:22,340 --> 00:34:23,380
you only have 20 days of data.

779
00:34:23,460 --> 00:34:26,350
So you can have 1,000  dimensional data,

780
00:34:26,460 --> 00:34:28,220
but 20 examples of it all ready.

781
00:34:28,320 --> 00:34:32,760
So given data that has this property

782
00:34:32,820 --> 00:34:36,070
in  the beginning that

783
00:34:36,200 --> 00:34:38,860
we've given a training

784
00:34:38,960 --> 00:34:40,050
set of m examples. Well,

785
00:34:40,120 --> 00:34:41,040
what can you do to try  to model

786
00:34:41,130 --> 00:34:42,410
the density of X?

787
00:34:42,500 --> 00:34:44,890
So one thing you can do

788
00:34:44,990 --> 00:34:46,080
is try to model it just as

789
00:34:46,220 --> 00:34:47,790
a single  Gaussian, right?

790
00:34:47,910 --> 00:34:48,700
So in my mixtures of Gaussian

791
00:34:48,790 --> 00:34:49,980
this is how you try model as

792
00:34:50,030 --> 00:34:50,810
a single  Gaussian and say

793
00:34:50,900 --> 00:34:53,780
X is intuitive with mean mu

794
00:34:53,870 --> 00:35:00,730
and parameter sigma where sigma

795
00:35:00,840 --> 00:35:02,650
is going  to be done n by n matrix

796
00:35:02,760 --> 00:35:06,860
and so if you work out

797
00:35:06,910 --> 00:35:07,740
the maximum likelihood estimate

798
00:35:07,820 --> 00:35:08,630
of the  parameters

799
00:35:08,700 --> 00:35:10,420
you find that

800
00:35:10,540 --> 00:35:11,490
the maximum likelihood estimate

801
00:35:11,560 --> 00:35:12,220
for the mean is just the  empirical

802
00:35:12,290 --> 00:35:14,920
mean of your training set,

803
00:35:14,990 --> 00:35:21,570
right. So that makes sense. And the

804
00:35:21,610 --> 00:35:22,350
maximum  likelihood of

805
00:35:22,410 --> 00:35:24,080
the covariance matrix sigma will be this,

806
00:35:24,130 --> 00:35:39,960
all right? But it turns out that

807
00:35:40,030 --> 00:35:42,530
in  this regime where the data

808
00:35:42,600 --> 00:35:43,810
is much higher dimensional

809
00:35:43,870 --> 00:35:45,990
excuse me, where the data's  dimension

810
00:35:46,070 --> 00:35:47,780
is much larger than the training examples

811
00:35:47,870 --> 00:35:51,660
you have if you compute the  maximum

812
00:35:51,750 --> 00:35:52,730
likely estimate of

813
00:35:52,850 --> 00:35:54,130
the covariance matrix sigma

814
00:35:54,230 --> 00:35:56,400
you find that this matrix is  singular.

815
00:35:56,500 --> 00:36:00,640
Okay? By singular, I mean that it

816
00:36:00,730 --> 00:36:01,450
doesn't have four vanq

817
00:36:01,520 --> 00:36:02,540
or it has zero eigen  value

818
00:36:02,610 --> 00:36:04,080
so it doesn't have

819
00:36:04,160 --> 00:36:06,080
I hope one of those terms makes sense.

820
00:36:06,160 --> 00:36:13,660
And there's another  saying that

821
00:36:13,740 --> 00:36:16,080
the matrix sigma will be non-invertible.

822
00:36:16,130 --> 00:36:23,130
And just in pictures,

823
00:36:23,270 --> 00:36:28,180
one complete  example is if D is

824
00:36:28,280 --> 00:36:31,060
if N equals M equals two

825
00:36:31,170 --> 00:36:32,320
if you have two-dimensional data

826
00:36:32,390 --> 00:36:34,550
and you  have two examples.

827
00:36:34,610 --> 00:36:36,700
So I'd have two training examples

828
00:36:36,750 --> 00:36:39,720
in two-dimen this is X1 and X2.

829
00:36:39,830 --> 00:36:42,820
This is my unlabeled data.

830
00:36:42,900 --> 00:36:44,510
If you fit a Gaussian

831
00:36:44,600 --> 00:36:45,310
to this data set

832
00:36:45,350 --> 00:36:46,190
you find that

833
00:36:46,220 --> 00:36:50,010
well you  remember I used to

834
00:36:50,050 --> 00:36:51,310
draw constables of Gaussians as ellipses,

835
00:36:51,390 --> 00:36:54,040
right? So these are  examples

836
00:36:54,150 --> 00:36:55,440
of different constables of Gaussians.

837
00:36:55,510 --> 00:36:56,620
You find that

838
00:36:56,710 --> 00:37:00,030
the maximum likely  estimate Gaussian

839
00:37:00,150 --> 00:37:00,960
for this responds to Gaussian

840
00:37:01,020 --> 00:37:04,720
where the contours are sort of

841
00:37:04,800 --> 00:37:05,830
infinitely  thin and infinitely long

842
00:37:05,920 --> 00:37:06,760
in that direction.

843
00:37:06,860 --> 00:37:09,550
Okay? So in terms so the contours

844
00:37:09,620 --> 00:37:13,380
will sort of  be infinitely thin, right?

845
00:37:13,450 --> 00:37:14,460
And stretch infinitely long

846
00:37:14,620 --> 00:37:15,400
in that direction.

847
00:37:15,480 --> 00:37:20,080
And another way of  saying it is that

848
00:37:20,150 --> 00:37:24,180
if you actually plug in the formula

849
00:37:24,280 --> 00:37:25,010
for the density of the Gaussian,

850
00:37:25,100 --> 00:37:32,840
which  is this,

851
00:37:32,900 --> 00:37:38,090
you won't actually get a nice answer

852
00:37:38,160 --> 00:37:40,380
because the matrix sigma is

853
00:37:40,440 --> 00:37:41,130
non-invertible

854
00:37:41,190 --> 00:37:42,480
so sigma inverse is not defined

855
00:37:42,540 --> 00:37:44,240
and this is zero.

856
00:37:44,280 --> 00:37:46,840
So you also have one over zero times

857
00:37:46,950 --> 00:37:49,390
E to  the sum inversive

858
00:37:49,450 --> 00:37:53,590
and non-inversive matrix

859
00:37:53,670 --> 00:37:54,590
so not a good model.

860
00:37:54,660 --> 00:37:59,650
So let's do even better,  right?

861
00:37:59,750 --> 00:38:01,080
So given this sort of data

862
00:38:01,160 --> 00:38:03,120
how do you model P of X?

863
00:38:03,190 --> 00:38:29,300
Well, one thing you could do

864
00:38:29,400 --> 00:38:32,080
is constrain sigma to be diagonal.

865
00:38:32,170 --> 00:38:44,240
So you have a covariance  matrix X is

866
00:38:44,310 --> 00:38:47,580
okay? So in other words

867
00:38:47,650 --> 00:38:48,310
you get a constraint sigma

868
00:38:48,360 --> 00:38:50,320
to be this matrix, all  right?

869
00:38:50,390 --> 00:38:51,860
With zeroes on the off diagonals.

870
00:38:51,910 --> 00:38:58,140
I hope this makes sense.

871
00:38:58,210 --> 00:39:02,110
These zeroes I've  written down here

872
00:39:02,200 --> 00:39:04,190
denote that everything after diagonal

873
00:39:04,290 --> 00:39:06,310
of this matrix is a zero.

874
00:39:06,370 --> 00:39:10,600
So the  massive likely estimate

875
00:39:10,650 --> 00:39:13,090
of the parameters will be pretty

876
00:39:13,200 --> 00:39:17,610
much what you'll expect, right?

877
00:39:17,670 --> 00:39:23,530
And in pictures what this means

878
00:39:23,590 --> 00:39:26,760
is that the [inaudible] the distribution

879
00:39:26,830 --> 00:39:29,910
with Gaussians  whose controls

880
00:39:29,970 --> 00:39:30,960
are axis aligned.

881
00:39:31,000 --> 00:39:33,470
So that's one example of a Gaussian

882
00:39:33,490 --> 00:39:36,630
where the  covariance is diagonal.

883
00:39:36,690 --> 00:39:39,940
And here's another example

884
00:39:40,020 --> 00:39:43,960
and so here's a third example.

885
00:39:44,050 --> 00:39:46,370
But  often I've used the examples

886
00:39:46,460 --> 00:39:48,850
of Gaussians where the covariance matrix

887
00:39:48,960 --> 00:39:50,750
is off diagonal.  Okay?

888
00:39:50,820 --> 00:39:54,770
And, I don't know, you could do this

889
00:39:54,880 --> 00:39:55,630
in model P of X,

890
00:39:55,700 --> 00:39:56,390
but this isn't very nice

891
00:39:56,470 --> 00:39:57,450
because you've now thrown away

892
00:39:57,540 --> 00:39:58,580
all the correlations

893
00:39:58,670 --> 00:40:01,050
between the different variables

894
00:40:01,130 --> 00:40:02,960
so  the axis are X1 and X2, right?

895
00:40:03,060 --> 00:40:05,770
So you've thrown away you're

896
00:40:05,830 --> 00:40:08,140
failing to capture any of  the

897
00:40:08,210 --> 00:40:10,020
correlations or the relationships between

898
00:40:10,130 --> 00:40:12,690
any pair of variables in your data.

899
00:40:12,780 --> 00:40:13,800
Yeah?

900
00:40:13,880 --> 00:40:15,770
Student:Is it could you say again

901
00:40:15,910 --> 00:40:17,020
what does that do for the diagonal?

902
00:40:17,120 --> 00:40:17,840
Instructor (Andrew Ng):Say again.

903
00:40:17,930 --> 00:40:18,890
Student:The covariance matrix

904
00:40:18,990 --> 00:40:19,920
the diagonal,  what does that again?

905
00:40:20,000 --> 00:40:21,570
I didn't quite  understand  what the examples mean.

906
00:40:21,630 --> 00:40:22,670
Instructor (Andrew Ng):Okay.

907
00:40:22,740 --> 00:40:25,120
So these are the contours of

908
00:40:25,250 --> 00:40:26,420
the Gaussian density that  I'm drawing,

909
00:40:26,470 --> 00:40:29,110
right? So let's see so post

910
00:40:29,190 --> 00:40:31,420
covariance issues with diagonal

911
00:40:31,520 --> 00:40:33,570
then you can  ask what is P of X

912
00:40:33,650 --> 00:40:36,060
parameterized by mu and sigma,

913
00:40:36,190 --> 00:40:39,050
right?

914
00:40:39,110 --> 00:40:40,910
If sigma is diagonal and

915
00:40:41,020 --> 00:40:43,290
so this  will be some Gaussian dump,

916
00:40:43,390 --> 00:40:44,880
right? So not in

917
00:40:44,960 --> 00:40:47,050
oh, boy. My drawing's really bad,

918
00:40:47,100 --> 00:40:49,950
but in  two-D the density for Gaussian

919
00:40:50,000 --> 00:40:52,360
is like this bump shaped thing, right?

920
00:40:52,480 --> 00:40:55,740
So this is the  density of the Gaussian

921
00:40:55,840 --> 00:40:57,740
wow, and this is a really bad drawing.

922
00:40:57,810 --> 00:41:00,840
With those, your axis X1  and X2

923
00:41:00,980 --> 00:41:02,870
and the height of this is P of X

924
00:41:03,350 --> 00:41:05,400
and so those figures over there

925
00:41:05,520 --> 00:41:08,470
are the contours of  the density

926
00:41:09,420 --> 00:41:11,250
of the Gaussian. So those are the

927
00:41:11,340 --> 00:41:12,310
contours of this shape.

928
00:41:12,410 --> 00:41:13,850
Student:No, I don't mean the contour.

929
00:41:13,900 --> 00:41:15,070
What's special about these types?

930
00:41:15,170 --> 00:41:15,930
What makes  them different than

931
00:41:15,960 --> 00:41:17,930
instead of general covariance matrix?

932
00:41:17,970 --> 00:41:18,920
Instructor (Andrew Ng):Oh, I see. Oh,

933
00:41:19,020 --> 00:41:19,990
okay, sorry.

934
00:41:20,040 --> 00:41:23,220
They're axis aligned so the main

935
00:41:23,320 --> 00:41:25,750
these, let's see. So I'm not drawing

936
00:41:25,830 --> 00:41:26,680
a contour like this, right?

937
00:41:26,800 --> 00:41:29,760
Because the main axes of  these

938
00:41:29,830 --> 00:41:33,300
are not aligned with the X1 and X-axis

939
00:41:33,350 --> 00:41:36,400
so this occurs found to Gaussian

940
00:41:36,480 --> 00:41:38,460
where the  off-diagonals are non-zero,

941
00:41:38,560 --> 00:41:39,920
right?

942
00:41:39,990 --> 00:41:45,180
Cool. Okay.

943
00:41:45,250 --> 00:41:48,380
You could do this, this is sort of work.

944
00:41:48,440 --> 00:41:49,140
It  turns out that

945
00:41:49,210 --> 00:41:50,480
what our best view is two training

946
00:41:50,530 --> 00:41:52,470
examples you can learn

947
00:41:52,590 --> 00:41:54,110
in non-singular  covariance matrix,

948
00:41:54,240 --> 00:41:56,660
but you've thrown away

949
00:41:56,720 --> 00:41:57,670
all of the correlation in the data

950
00:41:57,750 --> 00:41:59,130
so this is not  a great model.

951
00:41:59,130 --> 00:42:00,130
It turns out you can do something –

952
00:42:07,400 --> 00:42:08,760
well, actually,

953
00:42:08,850 --> 00:42:09,550
we'll come back

954
00:42:09,640 --> 00:42:10,390
and use this property  later.

955
00:42:10,470 --> 00:42:12,010
But it turns out you can do something

956
00:42:12,050 --> 00:42:12,660
even more restrictive,

957
00:42:12,730 --> 00:42:19,840
which is you can  constrain sigma

958
00:42:19,930 --> 00:42:24,200
to equal to sigma squared times

959
00:42:24,270 --> 00:42:25,170
the identity matrix.

960
00:42:25,240 --> 00:42:25,960
So in other words,

961
00:42:26,040 --> 00:42:26,860
you can constrain it

962
00:42:26,920 --> 00:42:27,810
to be diagonal matrix and moreover

963
00:42:27,910 --> 00:42:33,310
all the diagonal entries must be

964
00:42:33,360 --> 00:42:35,580
the same and so the cartoon

965
00:42:35,640 --> 00:42:38,200
for that is that you're constraining

966
00:42:38,270 --> 00:42:40,880
the contours of your  Gaussian density

967
00:42:41,030 --> 00:42:42,180
to be circular.

968
00:42:42,240 --> 00:42:43,160
Okay?

969
00:42:43,230 --> 00:42:44,860
This is a sort of even harsher constraint

970
00:42:44,940 --> 00:42:45,770
to place in  your model.

971
00:42:45,840 --> 00:42:53,090
So either of these versions,

972
00:42:53,180 --> 00:42:55,960
diagonal sigma or sigma being the,

973
00:42:55,990 --> 00:42:56,730
sort of,  constant value diagonal

974
00:42:56,780 --> 00:42:59,280
are the all ready strong assumptions,

975
00:42:59,360 --> 00:43:01,270
all right? So if you have  enough data

976
00:43:01,340 --> 00:43:03,130
maybe write a model just a little bit

977
00:43:03,200 --> 00:43:04,700
of a correlation between

978
00:43:04,750 --> 00:43:05,540
your different  variables.

979
00:43:05,590 --> 00:43:08,990
So the factor analysis model

980
00:43:09,050 --> 00:43:12,340
is one way to attempt to do that.

981
00:43:12,400 --> 00:43:14,770
So here's the  idea.

982
00:43:14,860 --> 00:43:37,380
So this is how the factor analysis model

983
00:43:37,510 --> 00:43:39,700
models your data.

984
00:43:39,760 --> 00:43:42,470
We're going to assume  that

985
00:43:42,530 --> 00:43:44,940
there is a latent random variable,

986
00:43:45,060 --> 00:43:45,910
okay?

987
00:43:46,000 --> 00:43:47,610
Which just means random variable Z.

988
00:43:47,680 --> 00:43:51,900
So Z is  distributed Gaussian

989
00:43:51,970 --> 00:43:53,880
with mean zero and covariance

990
00:43:53,970 --> 00:43:58,090
identity where Z will be

991
00:43:58,150 --> 00:43:59,060
a Ddimensional  vector now

992
00:43:59,130 --> 00:44:04,370
and D will be chosen

993
00:44:04,440 --> 00:44:06,740
so that it is lower than the dimension

994
00:44:06,800 --> 00:44:08,300
of  your X's.

995
00:44:08,370 --> 00:44:09,050
Okay?

996
00:44:09,130 --> 00:44:14,060
And now I'm going to assume that

997
00:44:14,170 --> 00:44:15,590
X is given by

998
00:44:15,680 --> 00:44:20,250
well let me write  this.

999
00:44:20,330 --> 00:44:25,750
Each XI is distributed actually,

1000
00:44:25,820 --> 00:44:27,080
sorry, I'm just.

1001
00:44:27,170 --> 00:44:34,850
We have to assume that

1002
00:44:34,900 --> 00:44:38,850
conditions  on the value of Z, X

1003
00:44:38,860 --> 00:44:41,450
is given by another Gaussian with mean

1004
00:44:41,500 --> 00:44:47,350
given by mu plus lambda Z

1005
00:44:47,410 --> 00:44:52,560
and covariance given by matrix si.

1006
00:44:52,630 --> 00:44:55,520
So just to say the second line

1007
00:44:55,600 --> 00:44:57,820
in an equivalent form,

1008
00:44:57,900 --> 00:45:02,550
equivalently I'm going to model X

1009
00:45:02,620 --> 00:45:07,130
as mu plus lambda Z plus

1010
00:45:07,200 --> 00:45:10,070
a noise term epsilon

1011
00:45:10,190 --> 00:45:15,040
where  epsilon is Gaussian with mean zero

1012
00:45:15,130 --> 00:45:23,310
and covariant si.

1013
00:45:23,380 --> 00:45:26,110
And so the parameters of this model

1014
00:45:26,200 --> 00:45:31,600
are going to be a vector mu

1015
00:45:31,670 --> 00:45:35,510
with its n-dimensional

1016
00:45:35,590 --> 00:45:39,040
and matrix lambda,

1017
00:45:39,120 --> 00:45:44,260
which is n by D  and

1018
00:45:44,340 --> 00:45:47,750
a covariance matrix si, which is n by n,

1019
00:45:47,840 --> 00:45:56,850
and I'm going to impose

1020
00:45:56,890 --> 00:45:57,450
an additional  constraint on si.

1021
00:45:57,510 --> 00:45:59,910
I'm going to impose a constraint

1022
00:46:00,000 --> 00:46:01,790
that si is diagonal. Okay?

1023
00:46:01,890 --> 00:46:06,210
So that was a  form of definition

1024
00:46:06,300 --> 00:46:08,550
let me actually, sort of,

1025
00:46:08,630 --> 00:46:09,350
give a couple of examples

1026
00:46:09,420 --> 00:46:10,820
to make this more  complete.

1027
00:46:10,880 --> 00:46:12,610
So let's give a kind of example,

1028
00:46:12,690 --> 00:46:34,820
suppose Z is one-dimensional

1029
00:46:34,870 --> 00:46:39,790
and X is twodimensional

1030
00:46:39,790 --> 00:46:40,790
so let's see what this model –

1031
00:46:42,110 --> 00:46:50,450
let's see a, sort of,

1032
00:46:50,480 --> 00:46:51,220
specific instance of

1033
00:46:51,310 --> 00:46:52,050
the  factor analysis model

1034
00:46:52,050 --> 00:46:53,050
and how we're modeling the joint –

1035
00:46:53,840 --> 00:46:55,220
the distribution over X of

1036
00:46:55,290 --> 00:46:56,380
what this gives us in terms of

1037
00:46:56,470 --> 00:47:01,450
a model over P of X, all right?

1038
00:47:01,520 --> 00:47:03,660
So let's see.

1039
00:47:03,780 --> 00:47:05,650
From this model to let me assume that

1040
00:47:05,720 --> 00:47:13,870
lambda is 2, 1 and si,

1041
00:47:13,920 --> 00:47:14,760
which has to be diagonal matrix,

1042
00:47:14,820 --> 00:47:15,980
remember, is this.

1043
00:47:16,080 --> 00:47:18,100
Okay? So Z is one-dimensional

1044
00:47:18,190 --> 00:47:24,060
so let me just draw a  typical sample

1045
00:47:24,140 --> 00:47:25,650
for Z, all right?

1046
00:47:25,780 --> 00:47:34,730
So if I draw ZI from a Gaussian

1047
00:47:34,840 --> 00:47:37,570
so that's a typical sample  for

1048
00:47:37,760 --> 00:47:39,050
what Z might look like

1049
00:47:39,110 --> 00:47:41,130
and so I'm gonna at any rate

1050
00:47:41,190 --> 00:47:42,130
I'm gonna call this

1051
00:47:42,200 --> 00:47:46,200
Z1, Z2, Z3

1052
00:47:46,280 --> 00:47:46,920
and so on.

1053
00:47:47,010 --> 00:47:48,830
If this really were a typical sample

1054
00:47:48,920 --> 00:47:50,390
the order of the Z's

1055
00:47:50,480 --> 00:47:51,410
would be jumbled up,

1056
00:47:51,490 --> 00:47:52,920
but I'm just ordering them like this

1057
00:47:53,010 --> 00:47:55,620
just to make the example easier.

1058
00:47:55,720 --> 00:47:57,080
So, yes, typical  sample

1059
00:47:57,200 --> 00:47:58,790
of random variable Z from

1060
00:47:58,850 --> 00:48:00,470
a Gaussian distribution with

1061
00:48:00,540 --> 00:48:01,150
mean of covariance one.

1062
00:48:01,240 --> 00:48:09,650
So and with this example

1063
00:48:09,710 --> 00:48:11,160
let me just set mu equals zero.

1064
00:48:11,260 --> 00:48:14,150
It's to write the just that

1065
00:48:14,170 --> 00:48:15,130
it's  easier to talk about.

1066
00:48:15,180 --> 00:48:22,710
So lambda times Z, right?

1067
00:48:22,770 --> 00:48:23,920
We'll take each of these numbers

1068
00:48:23,950 --> 00:48:25,150
and  multiply them by lambda.

1069
00:48:25,220 --> 00:48:32,040
And so you find that all of the values

1070
00:48:32,090 --> 00:48:37,510
for lambda times Z will  lie on

1071
00:48:37,560 --> 00:48:39,090
a straight line, all right?

1072
00:48:39,160 --> 00:48:40,280
So, for example,

1073
00:48:40,420 --> 00:48:43,080
this one here would be one, two, three,

1074
00:48:43,150 --> 00:48:44,420
four, five, six, seven, I guess.

1075
00:48:44,480 --> 00:48:46,890
So if this was Z7 then this one here

1076
00:48:47,020 --> 00:48:49,690
would be lambda times  Z7

1077
00:48:49,790 --> 00:48:54,090
and now that's the number in R2,

1078
00:48:54,190 --> 00:48:55,470
because lambda's a two by one matrix.

1079
00:48:55,560 --> 00:48:58,270
And so what  I've drawn here

1080
00:48:58,330 --> 00:48:59,170
is like a typical sample

1081
00:48:59,200 --> 00:49:00,820
for lambda times Z and the final step

1082
00:49:00,910 --> 00:49:09,130
for this is

1083
00:49:09,200 --> 00:49:09,960
what a typical sample for X looks like.

1084
00:49:10,060 --> 00:49:14,350
Well X is mu plus lambda Z plus epsilon

1085
00:49:14,440 --> 00:49:16,810
where  epsilon is Gaussian with

1086
00:49:16,860 --> 00:49:21,490
mean nu and covariance given by si,

1087
00:49:21,580 --> 00:49:23,710
right?

1088
00:49:23,810 --> 00:49:25,470
And so the last step

1089
00:49:25,600 --> 00:49:26,610
to draw a typical sample

1090
00:49:26,680 --> 00:49:27,840
for the random variables X

1091
00:49:27,950 --> 00:49:30,480
I'm gonna take these non

1092
00:49:30,590 --> 00:49:32,620
these are    really same as

1093
00:49:32,670 --> 00:49:33,810
mu plus lambda Z because mu is zero

1094
00:49:33,850 --> 00:49:35,190
in this example and around this  point

1095
00:49:35,290 --> 00:49:39,300
I'm going to place

1096
00:49:39,390 --> 00:49:43,520
an axis aligned ellipse.

1097
00:49:43,620 --> 00:49:44,790
Or in other words,

1098
00:49:44,890 --> 00:49:46,140
I'm going to create

1099
00:49:46,220 --> 00:49:48,370
a  Gaussian distribution centered

1100
00:49:48,490 --> 00:49:51,500
on this point and this

1101
00:49:51,590 --> 00:49:53,510
I've drawn corresponds

1102
00:49:53,580 --> 00:49:55,120
to one of the  contours of

1103
00:49:55,200 --> 00:49:57,240
my density for epsilon,

1104
00:49:57,330 --> 00:49:58,170
right?

1105
00:49:58,240 --> 00:50:00,730
And so you can imagine placing

1106
00:50:00,790 --> 00:50:02,060
a little  Gaussian bump here.

1107
00:50:02,130 --> 00:50:06,310
And so I'll draw an example

1108
00:50:06,420 --> 00:50:07,620
from this little Gaussian

1109
00:50:07,760 --> 00:50:09,500
and let's say I  get that point going,

1110
00:50:09,580 --> 00:50:27,780
I do the same here and so on.

1111
00:50:27,870 --> 00:50:29,830
So I draw a bunch of examples

1112
00:50:29,830 --> 00:50:30,830
from  these Gaussians and the –

1113
00:50:33,350 --> 00:50:34,230
the orange points I drew

1114
00:50:34,230 --> 00:50:35,230
whatever they call it –

1115
00:50:34,320 --> 00:50:35,170
will comprise a  typical sample

1116
00:50:35,250 --> 00:50:36,450
for whether distribution of X

1117
00:50:36,550 --> 00:50:37,320
is under this model.

1118
00:50:37,440 --> 00:50:45,380
Okay? Yeah?

1119
00:50:45,450 --> 00:50:46,470
Student:Would you add, like, mean?

1120
00:50:46,540 --> 00:50:47,930
Instructor:   Oh, say that again.

1121
00:50:47,990 --> 00:50:48,680
Student:Do you add mean into that?

1122
00:50:48,770 --> 00:50:49,880
Instructor (Andrew Ng):Oh, yes, you do.

1123
00:50:49,970 --> 00:50:50,560
And in this example,

1124
00:50:50,640 --> 00:50:51,170
I said you do a zero zero

1125
00:50:51,240 --> 00:50:51,940
just to make it easier.

1126
00:50:52,010 --> 00:50:52,530
If mu were something else

1127
00:50:52,600 --> 00:50:53,170
you'd take the whole picture

1128
00:50:53,240 --> 00:50:53,860
and you'd  sort of shift it

1129
00:50:53,940 --> 00:50:54,960
to whatever value of mu is. Yeah?

1130
00:50:58,100 --> 00:50:59,790
horizontal line right there,

1131
00:50:59,900 --> 00:51:02,000
which was Z.What did the X's,

1132
00:51:02,100 --> 00:51:06,030
of course,  what does that Y-axis

1133
00:51:06,090 --> 00:51:06,880
corresponds to?

1134
00:51:08,160 --> 00:51:08,760
so this is Z is one-dimensional

1135
00:51:08,820 --> 00:51:12,060
so here I'm plotting the  typical sample

1136
00:51:12,130 --> 00:51:15,010
for Z

1137
00:51:15,100 --> 00:51:17,020
so this is like zero.

1138
00:51:17,110 --> 00:51:18,540
So this is just the Z Axis, right.

1139
00:51:18,650 --> 00:51:20,460
So Z is onedimensional  data.

1140
00:51:20,570 --> 00:51:22,160
So this line here is like a plot of

1141
00:51:22,250 --> 00:51:25,860
a typical sample of values for Z.

1142
00:51:25,910 --> 00:51:26,870
Okay? Yeah?

1143
00:51:27,030 --> 00:51:32,250
Student:You have by axis, right?

1144
00:51:32,340 --> 00:51:34,240
And the axis data pertains samples.

1145
00:51:34,270 --> 00:51:35,410
Instructor (Andrew Ng):Oh, yes, right.

1146
00:51:35,470 --> 00:51:38,260
Student:So sort of projecting them

1147
00:51:38,370 --> 00:51:39,450
into that?

1148
00:51:39,500 --> 00:51:40,480
Instructor (Andrew Ng):Let's not

1149
00:51:40,550 --> 00:51:41,140
talk about projections yet,

1150
00:51:41,240 --> 00:51:41,910
but, yeah, right.

1151
00:51:41,910 --> 00:51:42,910
So these  beige points –

1152
00:51:43,250 --> 00:51:44,480
so that's like X1 and that's X2

1153
00:51:44,560 --> 00:51:47,690
and so on, right?

1154
00:51:47,760 --> 00:51:48,680
So the beige points are  what I see.

1155
00:51:48,740 --> 00:51:53,110
And so in reality all you ever get

1156
00:51:53,180 --> 00:51:54,310
to see are the X's,

1157
00:51:54,380 --> 00:51:57,040
but just like in the mixture

1158
00:51:57,100 --> 00:51:58,630
of Gaussians model I tell a story

1159
00:51:58,740 --> 00:52:00,700
about what I would imagine the Gau

1160
00:52:00,770 --> 00:52:03,920
—the data came  from two Gaussian's

1161
00:52:04,000 --> 00:52:05,610
was is had a random variable Z

1162
00:52:05,680 --> 00:52:06,450
that led to the generation of X's

1163
00:52:06,520 --> 00:52:07,070
from two Gaussians.

1164
00:52:07,140 --> 00:52:09,430
So the same way I'm sort of telling

1165
00:52:09,520 --> 00:52:10,150
the story here,

1166
00:52:10,230 --> 00:52:10,740
which all the  algorithm actually

1167
00:52:10,820 --> 00:52:11,850
sees are the orange points,

1168
00:52:11,960 --> 00:52:12,680
but we're gonna tell a story about

1169
00:52:12,800 --> 00:52:17,650
how the  data came about and

1170
00:52:17,780 --> 00:52:19,840
that story is what comprises

1171
00:52:19,950 --> 00:52:20,910
the factor analysis model.

1172
00:52:21,030 --> 00:52:25,710
Okay? So one  of the ways to see

1173
00:52:25,780 --> 00:52:26,650
the intrusion of this model

1174
00:52:26,730 --> 00:52:27,510
is that we're going to think of

1175
00:52:27,620 --> 00:52:31,220
the model as  one way just informally,

1176
00:52:31,300 --> 00:52:32,270
not formally,

1177
00:52:32,400 --> 00:52:33,750
but one way to think about this model

1178
00:52:33,850 --> 00:52:35,210
is you can  think of this

1179
00:52:35,340 --> 00:52:36,610
factor analysis model as modeling

1180
00:52:36,770 --> 00:52:39,540
the data from coming from

1181
00:52:39,680 --> 00:52:41,240
a lower  dimensional subspace

1182
00:52:41,330 --> 00:52:44,520
more or less so the data X here Y

1183
00:52:44,640 --> 00:52:46,160
is approximately on one D line

1184
00:52:46,160 --> 00:52:47,160
and then plus a little bit of noise –

1185
00:52:48,880 --> 00:52:51,260
plus a little bit of random noise

1186
00:52:51,350 --> 00:52:52,350
so the X isn't exactly

1187
00:52:52,440 --> 00:52:53,190
on this one D line.

1188
00:52:53,330 --> 00:52:54,760
That's one informal way of

1189
00:52:54,850 --> 00:52:55,810
thinking about factor analysis.

1190
00:52:59,480 --> 00:53:10,280
We're not doing great on time. Well,

1191
00:53:10,380 --> 00:53:18,690
let's do this.

1192
00:53:18,770 --> 00:53:22,030
So let me just do one more quick

1193
00:53:22,090 --> 00:53:22,970
example,

1194
00:53:23,060 --> 00:53:26,430
which is, in this example,

1195
00:53:26,490 --> 00:53:29,710
let's say Z is in R2

1196
00:53:29,780 --> 00:53:30,820
and X is in R3, right?

1197
00:53:30,900 --> 00:53:41,910
And so in  this example Z,

1198
00:53:41,980 --> 00:53:44,180
your data Z now lies in 2-D

1199
00:53:44,280 --> 00:53:46,690
and so let me draw this

1200
00:53:46,780 --> 00:53:47,590
on a sheet of paper.  Okay?

1201
00:53:47,710 --> 00:53:51,880
So let's say the axis of my paper

1202
00:53:51,920 --> 00:53:52,970
are the Z1 and Z2 axis

1203
00:53:53,030 --> 00:53:57,440
and so here is a typical  sample

1204
00:53:57,510 --> 00:53:58,980
of point Z,

1205
00:53:59,100 --> 00:54:00,530
right?

1206
00:54:00,600 --> 00:54:04,730
And so we'll then take the sample Z

1207
00:54:04,840 --> 00:54:07,290
well, actually let me draw  this here

1208
00:54:07,390 --> 00:54:12,200
as well. All right.

1209
00:54:12,260 --> 00:54:13,470
So this is a typical sample for Z

1210
00:54:13,540 --> 00:54:14,910
going on the Z1 and Z2 axis

1211
00:54:15,020 --> 00:54:16,470
and I guess the origin would be here.

1212
00:54:16,510 --> 00:54:17,840
So center around zero.

1213
00:54:17,900 --> 00:54:21,720
And then we'll take those  and map it

1214
00:54:21,780 --> 00:54:25,410
to mu plus lambda Z

1215
00:54:25,490 --> 00:54:27,140
and what that means is

1216
00:54:27,220 --> 00:54:28,500
if you imagine the free space

1217
00:54:28,540 --> 00:54:29,830
of  this classroom is R3.

1218
00:54:29,910 --> 00:54:31,510
What that means is

1219
00:54:31,560 --> 00:54:33,140
we'll take this sample of Z's

1220
00:54:33,210 --> 00:54:34,610
and we'll map it to  position

1221
00:54:34,700 --> 00:54:37,270
in free space.

1222
00:54:37,370 --> 00:54:38,490
So we'll take this sheet of paper

1223
00:54:38,620 --> 00:54:39,360
and move it somewhere

1224
00:54:39,470 --> 00:54:40,130
and some  orientation in 3-D space.

1225
00:54:40,200 --> 00:54:43,010
And the last step is you have

1226
00:54:43,100 --> 00:54:47,180
X equals mu plus lambda Z plus  epsilon

1227
00:54:47,290 --> 00:54:50,220
and so you would take

1228
00:54:50,290 --> 00:54:51,430
the set of the points which align

1229
00:54:51,520 --> 00:54:53,530
in some plane in our 3-D  space

1230
00:54:53,630 --> 00:54:55,170
the variable of noise of these

1231
00:54:55,260 --> 00:54:57,460
and the noise will, sort of,

1232
00:54:57,510 --> 00:54:59,060
come from Gaussians

1233
00:54:59,160 --> 00:55:01,020
to  the axis aligned. Okay?

1234
00:55:01,080 --> 00:55:02,480
So you end up with a data

1235
00:55:02,560 --> 00:55:03,910
set that's sort of like

1236
00:55:04,010 --> 00:55:06,360
a fat pancake or a  little bit of fuzz

1237
00:55:06,440 --> 00:55:07,170
off your pancake.

1238
00:55:07,250 --> 00:55:08,310
So that's a model

1239
00:55:08,480 --> 00:55:16,130
let's actually talk about

1240
00:55:16,200 --> 00:55:18,000
how to fit  the parameters of the model.

1241
00:55:18,080 --> 00:55:31,990
Okay?   In order to describe

1242
00:55:32,040 --> 00:55:32,960
how to fit the model

1243
00:55:33,010 --> 00:55:36,420
I'm sure we need to re-write

1244
00:55:36,460 --> 00:55:37,540
Gaussians and this  is

1245
00:55:37,580 --> 00:55:38,710
in a very slightly different way.

1246
00:55:38,780 --> 00:55:41,360
So, in particular,

1247
00:55:41,410 --> 00:55:43,670
let's say I have a vector X

1248
00:55:43,730 --> 00:55:46,750
and I'm  gonna use this notation

1249
00:55:46,840 --> 00:55:48,280
to denote partition vectors, right?

1250
00:55:48,340 --> 00:55:54,090
X1, X2 where if X1 is

1251
00:55:54,170 --> 00:55:55,530
say an rdimensional  vector

1252
00:55:55,570 --> 00:55:59,780
then X2 is an estimational vector

1253
00:55:59,880 --> 00:56:04,700
and X is an R plus S dimensional  vector.

1254
00:56:04,840 --> 00:56:07,020
Okay? So I'm gonna use this notation

1255
00:56:07,130 --> 00:56:11,550
to denote just the taking of vector and,

1256
00:56:11,640 --> 00:56:12,400
sort  of, partitioning the vector

1257
00:56:12,470 --> 00:56:13,140
into two halves.

1258
00:56:13,230 --> 00:56:14,750
The first R elements followed

1259
00:56:14,820 --> 00:56:17,750
by the last S  elements.

1260
00:56:17,840 --> 00:56:26,940
So let's say you have X

1261
00:56:27,020 --> 00:56:30,330
coming from a Gaussian distribution

1262
00:56:30,370 --> 00:56:31,320
with mean mu

1263
00:56:31,390 --> 00:56:32,320
and covariance sigma where mu

1264
00:56:32,410 --> 00:56:40,400
is itself a partition vector.

1265
00:56:40,460 --> 00:56:43,170
So break mu up into two  pieces mu1

1266
00:56:43,230 --> 00:56:43,860
and mu2

1267
00:56:43,930 --> 00:56:48,100
and the covariance matrix sigma

1268
00:56:48,190 --> 00:56:50,750
is now a partitioned matrix. Okay?

1269
00:56:50,870 --> 00:56:56,470
So what this means is that

1270
00:56:56,550 --> 00:56:57,910
you take the covariance matrix sigma

1271
00:56:58,000 --> 00:56:59,570
and I'm going to break it  up

1272
00:56:59,640 --> 00:57:00,500
into four blocks, right?

1273
00:57:00,560 --> 00:57:03,900
And so the dimension of this is there

1274
00:57:04,010 --> 00:57:05,130
will be R elements here

1275
00:57:05,230 --> 00:57:07,250
and there will be S elements here

1276
00:57:07,380 --> 00:57:09,160
and there will be R elements here.

1277
00:57:09,240 --> 00:57:12,540
So, for example,

1278
00:57:12,650 --> 00:57:16,190
sigma 1, 2 will be an R

1279
00:57:16,270 --> 00:57:17,270
by S matrix.

1280
00:57:17,350 --> 00:57:20,720
It's R elements tall

1281
00:57:20,780 --> 00:57:21,850
and S elements wide.

1282
00:57:21,970 --> 00:57:33,710
So this Gaussian over to down

1283
00:57:33,760 --> 00:57:35,320
is really a joint distribution

1284
00:57:35,380 --> 00:57:36,570
of a loss of variables,

1285
00:57:36,630 --> 00:57:37,880
right?

1286
00:57:37,980 --> 00:57:38,740
So  X is a vector so XY

1287
00:57:38,820 --> 00:57:40,830
is a joint distribution over X1

1288
00:57:40,920 --> 00:57:42,510
through X of

1289
00:57:42,610 --> 00:57:44,830
over XN or over X of R  plus S.

1290
00:57:44,900 --> 00:57:49,090
We can then ask what are the marginal

1291
00:57:49,190 --> 00:57:50,360
and conditional distributions

1292
00:57:50,440 --> 00:57:51,310
of this  Gaussian?

1293
00:57:51,380 --> 00:57:53,210
So, for example,

1294
00:57:53,310 --> 00:57:54,670
with my Gaussian,

1295
00:57:54,770 --> 00:57:55,740
I know what P of X is,

1296
00:57:55,810 --> 00:57:58,300
but can I compute  the modular

1297
00:57:58,380 --> 00:58:00,770
distribution of X1, right.

1298
00:58:00,860 --> 00:58:02,340
And so P of X1 is just equal to,

1299
00:58:02,420 --> 00:58:04,290
of course, integrate  our X2,

1300
00:58:04,410 --> 00:58:09,610
P of X1 comma X2 DX2.

1301
00:58:09,700 --> 00:58:11,920
And if you actually perform

1302
00:58:12,000 --> 00:58:13,330
that distribution

1303
00:58:13,380 --> 00:58:14,670
that  computation you find that

1304
00:58:14,770 --> 00:58:16,040
P of X1, I guess,

1305
00:58:16,090 --> 00:58:21,940
is Gaussian with mean given by mu1

1306
00:58:21,990 --> 00:58:24,760
and  sigma 1, 1. All right.

1307
00:58:24,830 --> 00:58:26,120
So this is sort of no surprise.

1308
00:58:26,190 --> 00:58:30,030
The marginal distribution

1309
00:58:30,120 --> 00:58:31,770
of a  Gaussian is itself the Gaussian

1310
00:58:31,830 --> 00:58:32,690
and you just take out

1311
00:58:32,760 --> 00:58:37,050
the relevant sub-blocks

1312
00:58:37,150 --> 00:58:38,380
of the  covariance matrix

1313
00:58:38,470 --> 00:58:39,910
and the relevant sub-vector

1314
00:58:40,000 --> 00:58:43,240
of the mu vector E in vector mu.

1315
00:58:43,330 --> 00:58:48,230
You can  also compute conditionals.

1316
00:58:48,330 --> 00:58:50,570
You can also what does P of X1

1317
00:58:50,680 --> 00:58:56,580
given a specific value for  X2, right?

1318
00:58:56,700 --> 00:58:58,790
And so the way you compute that is,

1319
00:58:58,870 --> 00:59:01,090
well, the usual way P of X1

1320
00:59:01,210 --> 00:59:07,640
comma X2    divided by P of X2, right?

1321
00:59:07,750 --> 00:59:10,510
And so you know what

1322
00:59:10,600 --> 00:59:11,890
both of these formulas are,

1323
00:59:11,960 --> 00:59:12,620
right?

1324
00:59:12,690 --> 00:59:14,190
The  numerator well,

1325
00:59:14,280 --> 00:59:15,080
this is just a usual Gaussian

1326
00:59:15,180 --> 00:59:18,560
that your joint distribution over X1,

1327
00:59:18,640 --> 00:59:21,360
X2 is a  Gaussian with mean mu

1328
00:59:21,460 --> 00:59:23,300
and covariance sigma

1329
00:59:23,390 --> 00:59:27,860
and this by that marginalization

1330
00:59:27,930 --> 00:59:34,490
operation  I talked about is that.

1331
00:59:34,610 --> 00:59:37,040
So if you actually plug in

1332
00:59:37,130 --> 00:59:37,690
the formulas for these two Gaussians

1333
00:59:37,780 --> 00:59:38,370
and  if you simplify

1334
00:59:38,490 --> 00:59:39,210
the simplification step is actually

1335
00:59:39,260 --> 00:59:40,070
fairly non-trivial.

1336
00:59:40,130 --> 00:59:41,240
If you haven't seen it  before

1337
00:59:41,240 --> 00:59:42,240
this will actually be –

1338
00:59:42,360 --> 00:59:42,860
this will actually be somewhat

1339
00:59:42,930 --> 00:59:43,650
difficult to do.

1340
00:59:43,720 --> 00:59:47,030
But if you  plug this in

1341
00:59:47,070 --> 00:59:47,880
for Gaussian

1342
00:59:47,950 --> 00:59:50,270
and simplify that expression

1343
00:59:50,350 --> 00:59:53,380
you find that conditioned

1344
00:59:53,490 --> 00:59:54,590
on the  value of X2, X1 is

1345
00:59:54,710 --> 00:59:59,420
the distribution of X1 conditioned

1346
00:59:59,470 --> 01:00:02,590
on X2 is itself going to be  Gaussian

1347
01:00:02,700 --> 01:00:09,280
and it will have mean mu of 1 given 2

1348
01:00:09,370 --> 01:00:10,800
and covariant sigma of 1 given 2

1349
01:00:10,890 --> 01:00:16,270
where  well, so about

1350
01:00:16,370 --> 01:00:17,440
the simplification and derivation

1351
01:00:17,520 --> 01:00:19,040
I'm not gonna show the formula

1352
01:00:19,120 --> 01:00:21,390
for mu  given of mu of one given 2

1353
01:00:21,470 --> 01:00:23,010
is given by this and I think the sigma

1354
01:00:23,090 --> 01:00:45,150
of 1 given 2 is given  by that.

1355
01:00:45,240 --> 01:00:46,780
Okay?

1356
01:00:46,870 --> 01:00:49,590
So these are just

1357
01:00:49,670 --> 01:00:53,490
useful formulas to know for

1358
01:00:53,540 --> 01:00:55,550
how to find the conditional

1359
01:00:55,650 --> 01:00:56,750
distributions of  the Gaussian

1360
01:00:56,820 --> 01:00:57,670
and the marginal distributions

1361
01:00:57,730 --> 01:00:58,770
of a Gaussian.

1362
01:00:58,820 --> 01:00:59,830
I won't actually show

1363
01:00:59,900 --> 01:01:00,580
the  derivation for this.

1364
01:01:00,670 --> 01:01:13,640
Student:Could you repeat

1365
01:01:13,720 --> 01:01:14,260
the [inaudible]?

1366
01:01:14,300 --> 01:01:14,980
Instructor (Andrew Ng):Sure.

1367
01:01:15,050 --> 01:01:15,880
So this one on the left mu

1368
01:01:15,940 --> 01:01:16,770
of 1 given 2 equals mu1

1369
01:01:16,830 --> 01:01:19,790
plus  sigma 1,2, sigma 2,

1370
01:01:19,880 --> 01:01:22,870
2 inverse times X2 minus mu2

1371
01:01:22,980 --> 01:01:24,920
and this is sigma 1 given 2 equals

1372
01:01:25,020 --> 01:01:26,860
sigma 1,1 minus sigma 1,2 sigma 2,

1373
01:01:26,960 --> 01:01:34,510
2 inverse sigma 2,1. Okay?

1374
01:01:34,590 --> 01:01:36,600
These are also in the  lecture notes.

1375
01:01:36,660 --> 01:01:52,580
Shoot. Nothing as

1376
01:01:52,710 --> 01:01:55,230
where I was hoping to on time. Well,

1377
01:01:55,350 --> 01:02:01,430
actually it is.  Okay?

1378
01:02:01,490 --> 01:02:21,900
So it turns out I think I'll skip

1379
01:02:22,010 --> 01:02:23,050
this in the interest of time.

1380
01:02:23,110 --> 01:02:24,130
So it turns out that well, so  let's

1381
01:02:24,220 --> 01:02:28,350
go back and use these in the factor

1382
01:02:28,420 --> 01:02:30,330
analysis model, right?

1383
01:02:30,400 --> 01:02:32,680
It turns out that you can go  back

1384
01:02:32,770 --> 01:02:39,340
and oh, do I want to do this?

1385
01:02:39,450 --> 01:02:47,340
I kind of need this though.

1386
01:02:47,450 --> 01:02:50,070
So let's go back and  figure out

1387
01:02:50,150 --> 01:02:53,210
just what the joint

1388
01:02:53,290 --> 01:02:55,920
distribution factor analysis

1389
01:02:56,070 --> 01:02:57,420
assumes on Z and X's. Okay?

1390
01:02:57,490 --> 01:03:01,760
So  under the factor analysis model Z

1391
01:03:01,870 --> 01:03:04,980
and X, the random variables Z

1392
01:03:05,030 --> 01:03:08,860
and X have some joint  distribution

1393
01:03:08,890 --> 01:03:10,980
given by I'll write

1394
01:03:11,040 --> 01:03:13,290
this vector as mu ZX

1395
01:03:13,380 --> 01:03:15,440
in some covariance matrix sigma.

1396
01:03:15,580 --> 01:03:18,550
So let's go back and figure out

1397
01:03:18,650 --> 01:03:20,420
what mu ZX is and what sigma is

1398
01:03:20,490 --> 01:03:21,660
and I'll do this

1399
01:03:21,770 --> 01:03:23,910
so that  we'll get a little bit more

1400
01:03:24,000 --> 01:03:25,680
practice with partition vectors

1401
01:03:25,750 --> 01:03:26,870
and partition matrixes.

1402
01:03:26,930 --> 01:03:30,120
So just to  remind you, right?

1403
01:03:30,190 --> 01:03:32,570
You have to have Z as Gaussian

1404
01:03:32,650 --> 01:03:33,340
with mean zero

1405
01:03:33,400 --> 01:03:35,590
and covariance  identity and X

1406
01:03:35,680 --> 01:03:37,540
is mu plus lambda Z plus epsilon

1407
01:03:37,620 --> 01:03:41,240
where epsilon is Gaussian with mean

1408
01:03:41,330 --> 01:03:43,110
zero covariant si.

1409
01:03:43,170 --> 01:03:44,440
So I have the

1410
01:03:44,560 --> 01:03:45,690
I'm just writing out

1411
01:03:45,800 --> 01:03:47,330
the same equations again.

1412
01:03:47,400 --> 01:03:48,250
So let's  first figure out

1413
01:03:48,320 --> 01:03:49,920
what this vector mu ZX is.

1414
01:03:49,990 --> 01:03:55,050
Well, the expected value of Z

1415
01:03:55,150 --> 01:03:58,290
is zero and,  again,

1416
01:03:58,350 --> 01:04:00,630
as usual I'll often drop

1417
01:04:00,690 --> 01:04:06,630
the square backers around here

1418
01:04:06,820 --> 01:04:07,350
And the expected value of  X is

1419
01:04:07,440 --> 01:04:08,290
well, the expected value

1420
01:04:08,400 --> 01:04:10,130
of mu plus lambda Z plus epsilon.

1421
01:04:10,210 --> 01:04:15,800
So these two terms  have zero

1422
01:04:15,880 --> 01:04:16,830
expectation

1423
01:04:16,910 --> 01:04:18,700
and so the expected value of X

1424
01:04:18,760 --> 01:04:26,720
is just mu and so that vector mu

1425
01:04:26,830 --> 01:04:29,110
ZX, right, in my parameter

1426
01:04:29,190 --> 01:04:31,800
for the Gaussian this is going to be

1427
01:04:31,930 --> 01:04:34,280
the expected value of this

1428
01:04:34,420 --> 01:04:35,640
partition vector given

1429
01:04:35,750 --> 01:04:38,740
by this partition Z and X

1430
01:04:38,900 --> 01:04:43,170
and so that would just be zero followed

1431
01:04:43,270 --> 01:04:46,340
by  mu. Okay?

1432
01:04:46,560 --> 01:04:47,830
And so that's a d-dimensional zero

1433
01:04:47,920 --> 01:04:53,530
followed by an indimensional mu.

1434
01:04:53,630 --> 01:04:58,160
That's  not gonna work out

1435
01:04:58,230 --> 01:04:59,590
what the covariance matrix sigma is.

1436
01:04:59,680 --> 01:05:22,620
So the covariance matrix sigma

1437
01:05:22,730 --> 01:05:28,030
if you work out definition

1438
01:05:28,130 --> 01:05:28,940
of a partition.

1439
01:05:29,020 --> 01:05:49,180
So this is into your partition matrix.

1440
01:05:49,200 --> 01:06:06,920
Okay?  Will be

1441
01:06:07,000 --> 01:06:08,000
so the covariance matrix sigma

1442
01:06:08,050 --> 01:06:08,910
will comprise four blocks like that

1443
01:06:08,980 --> 01:06:12,040
and so the   upper left most block,

1444
01:06:12,140 --> 01:06:13,430
which I write as sigma 1,1

1445
01:06:13,520 --> 01:06:16,140
well, that uppermost left block

1446
01:06:16,220 --> 01:06:19,910
is just  the covariance matrix of Z,

1447
01:06:19,970 --> 01:06:22,510
which we know is the identity.

1448
01:06:22,530 --> 01:06:25,690
I was gonna show you briefly

1449
01:06:25,790 --> 01:06:27,060
how to derive some of the other blocks,

1450
01:06:27,110 --> 01:06:28,790
right, so sigma 1,2 that's the upper

1451
01:06:28,890 --> 01:06:43,450
oh, actually,  excuse me.

1452
01:06:43,450 --> 01:06:44,670
Sigma 2,1

1453
01:06:44,760 --> 01:06:46,370
which is the lower left block

1454
01:06:46,430 --> 01:06:51,330
that's E of X minus EX times Z

1455
01:06:51,410 --> 01:06:53,280
minus EZ.

1456
01:06:53,350 --> 01:06:59,900
So X is equal to mu plus lambda Z

1457
01:07:00,000 --> 01:07:04,600
plus epsilon and then minus EX

1458
01:07:04,690 --> 01:07:08,250
is minus  mu and then times Z

1459
01:07:08,350 --> 01:07:12,850
because the expected value of Z is zero,

1460
01:07:12,950 --> 01:07:15,420
right, so that's equal to  zero.

1461
01:07:15,500 --> 01:07:20,980
And so if you simplify

1462
01:07:21,100 --> 01:07:22,720
or if you expand this out

1463
01:07:22,800 --> 01:07:24,960
plus mu minus mu cancel out

1464
01:07:25,070 --> 01:07:29,170
and  so you have the expected value

1465
01:07:29,280 --> 01:07:33,770
of lambda oh, excuse me.

1466
01:07:33,890 --> 01:07:40,630
ZZ transpose minus the  expected

1467
01:07:40,750 --> 01:07:53,580
value of epsilon Z is equal to that,

1468
01:07:53,680 --> 01:08:00,830
which is just equal to lambda times

1469
01:08:00,930 --> 01:08:03,670
the  identity matrix. Okay?

1470
01:08:03,770 --> 01:08:10,040
Does that make sense?

1471
01:08:10,130 --> 01:08:14,140
Cause this term is equal to zero.

1472
01:08:14,210 --> 01:08:16,180
Both  epsilon and Z are independent

1473
01:08:16,310 --> 01:08:17,400
and have zero expectation

1474
01:08:17,510 --> 01:08:18,490
so the second terms are zero.

1475
01:08:18,600 --> 01:08:50,270
Well, so the final block is sigma 2,2

1476
01:08:50,350 --> 01:08:52,530
which is equal to the expected value

1477
01:08:52,640 --> 01:08:55,200
of mu plus  lambda Z plus epsilon

1478
01:08:55,320 --> 01:08:59,910
minus mu times, right?

1479
01:08:59,990 --> 01:09:10,260
Is equal to and I won't do this,

1480
01:09:10,330 --> 01:09:11,670
but this  simplifies to lambda

1481
01:09:11,750 --> 01:09:15,100
lambda transpose plus si. Okay?

1482
01:09:15,200 --> 01:09:22,350
So putting all this together this tells

1483
01:09:22,430 --> 01:09:23,700
us that the joint distribution of this

1484
01:09:23,780 --> 01:09:29,690
vector ZX is going to be Gaussian

1485
01:09:29,770 --> 01:09:35,440
with mean vector  given by that,

1486
01:09:35,550 --> 01:09:37,350
which we worked out previously.

1487
01:09:37,450 --> 01:09:40,500
So this is the new ZX that

1488
01:09:40,580 --> 01:09:41,620
we worked out  previously,

1489
01:09:41,710 --> 01:09:43,260
and covariance matrix given by that.

1490
01:10:06,110 --> 01:10:09,100
let's see, so the  parameters

1491
01:10:09,100 --> 01:10:10,100
Okay? So in principle –

1492
01:10:09,150 --> 01:10:10,610
of our model are mu, lambda,

1493
01:10:10,690 --> 01:10:16,370
and si. And so in order to find

1494
01:10:16,460 --> 01:10:17,880
the parameters of  this model

1495
01:10:17,990 --> 01:10:19,770
we're given a training

1496
01:10:19,910 --> 01:10:21,210
set of m examples

1497
01:10:21,300 --> 01:10:25,860
and so we like to do a massive likely

1498
01:10:25,950 --> 01:10:27,470
estimation of the parameters.

1499
01:10:27,570 --> 01:10:30,350
And so in principle one thing

1500
01:10:30,430 --> 01:10:31,740
you could do is you can  actually

1501
01:10:31,840 --> 01:10:36,490
write down what P of XI is and,

1502
01:10:36,580 --> 01:10:41,840
right, so P of XI XI is actually

1503
01:10:41,980 --> 01:10:44,380
the distribution  of X, right?

1504
01:10:44,410 --> 01:10:45,410
If, again,

1505
01:10:45,520 --> 01:10:48,110
you can marginalize this Gaussian

1506
01:10:48,300 --> 01:10:50,800
and so the distribution of X,

1507
01:10:50,900 --> 01:10:52,350
which is the lower half of

1508
01:10:52,450 --> 01:10:53,870
this partition vector is going to

1509
01:10:53,980 --> 01:10:59,710
have mean mu and covariance  given by

1510
01:10:59,800 --> 01:11:01,250
lambda lambda transpose plus si.

1511
01:11:01,340 --> 01:11:03,770
Right?

1512
01:11:03,860 --> 01:11:06,050
So that's the distribution

1513
01:11:06,110 --> 01:11:09,650
that we're  using to model P of X.

1514
01:11:09,730 --> 01:11:14,070
And so in principle one thing

1515
01:11:14,170 --> 01:11:15,380
you could do is actually write down

1516
01:11:15,460 --> 01:11:19,270
the log likelihood of  your parameters,

1517
01:11:19,320 --> 01:11:20,520
right?

1518
01:11:20,570 --> 01:11:22,520
Which is just the product over of

1519
01:11:22,580 --> 01:11:25,900
it is the sum over I log P

1520
01:11:26,000 --> 01:11:31,340
of XI  where P of XI will be

1521
01:11:31,410 --> 01:11:35,200
given by this Gaussian density, right.

1522
01:11:35,270 --> 01:11:37,800
And I'm using theta as a  shorthand

1523
01:11:37,890 --> 01:11:39,010
to denote all of my parameters.

1524
01:11:39,090 --> 01:11:41,340
And so you actually know

1525
01:11:41,410 --> 01:11:42,720
what the density for  Gaussian is

1526
01:11:42,800 --> 01:11:47,340
and so you can say P of XI is

1527
01:11:47,410 --> 01:11:48,480
this Gaussian with E mu in covariance

1528
01:11:48,540 --> 01:11:50,080
given by  this

1529
01:11:50,160 --> 01:11:51,530
lambda lambda transpose plus si.

1530
01:11:51,580 --> 01:11:53,430
So in case you write down

1531
01:11:53,510 --> 01:11:55,240
the log likelihood of  your parameters

1532
01:11:55,330 --> 01:11:58,210
as follows and you can try

1533
01:11:58,280 --> 01:11:59,730
to take derivatives

1534
01:11:59,820 --> 01:12:00,800
of your log likelihood with  respect

1535
01:12:00,860 --> 01:12:02,720
to your parameters and maximize

1536
01:12:02,860 --> 01:12:04,990
the log likelihood, all right.

1537
01:12:05,070 --> 01:12:06,270
It turns out that if  you do that

1538
01:12:06,330 --> 01:12:08,750
you end up with sort of

1539
01:12:09,070 --> 01:12:09,660
an intractable atomization problem

1540
01:12:23,610 --> 01:12:25,020
you won't be able to find

1541
01:12:25,100 --> 01:12:27,210
the  massive likely estimate of

1542
01:12:27,280 --> 01:12:28,240
the parameters inclosed form.

1543
01:12:28,320 --> 01:12:31,740
So what I would have liked to  do

1544
01:12:33,170 --> 01:12:44,260
so in order to fit parameters

1545
01:12:44,260 --> 01:12:45,260
is– well,

1546
01:12:44,360 --> 01:12:45,340
to this model

1547
01:12:45,440 --> 01:12:47,710
what we'll actually do is use the  EM

1548
01:12:47,820 --> 01:13:00,080
Algorithm in with the E step, right?

1549
01:13:00,170 --> 01:13:07,370
We'll compute that and this formula

1550
01:13:07,410 --> 01:13:12,700
looks the  same except

1551
01:13:12,790 --> 01:13:15,320
that one difference is that now Z

1552
01:13:15,430 --> 01:13:16,470
is a continuous random variable

1553
01:13:16,590 --> 01:13:20,780
and so in  the E step we actually

1554
01:13:20,840 --> 01:13:22,540
have to find the density QI of ZI

1555
01:13:22,610 --> 01:13:23,610
where it's the, sort of,

1556
01:13:23,720 --> 01:13:24,950
E step  actually requires that

1557
01:13:25,030 --> 01:13:27,550
we find the posterior distribution

1558
01:13:27,610 --> 01:13:30,830
that so the density to the random

1559
01:13:30,890 --> 01:13:34,510
variable ZI

1560
01:13:34,580 --> 01:13:35,500
and then the M step

1561
01:13:35,600 --> 01:13:38,580
will then perform the following

1562
01:13:38,650 --> 01:13:42,540
maximization where,again,

1563
01:13:42,630 --> 01:13:45,530
because Z is now continuous

1564
01:13:45,590 --> 01:13:59,280
we now need to integrate over Z. Okay?

1565
01:13:59,400 --> 01:14:00,310
Where in the  M step now

1566
01:14:00,400 --> 01:14:01,710
because ZI was continuous we now have

1567
01:14:01,870 --> 01:14:02,880
an integral over Z

1568
01:14:02,950 --> 01:14:03,840
rather than a  sum.

1569
01:14:03,930 --> 01:14:04,750
Okay?

1570
01:14:04,810 --> 01:14:06,240
I was hoping to go a little bit further

1571
01:14:06,340 --> 01:14:08,130
r in deriving these things,

1572
01:14:08,170 --> 01:14:09,100
but I don't have time today

1573
01:14:09,150 --> 01:14:10,270
so we'll wrap that up

1574
01:14:10,320 --> 01:14:12,560
in the next lecture,

1575
01:14:12,630 --> 01:14:13,890
but before I close let's check

1576
01:14:14,010 --> 01:14:14,710
if there are  questions about

1577
01:14:14,800 --> 01:14:17,380
the whole factor analysis model.

1578
01:14:17,500 --> 01:14:26,490
Okay.

1579
01:14:26,600 --> 01:14:29,320
So we'll come back

1580
01:14:29,420 --> 01:14:30,250
in the next  lecture;

1581
01:14:30,390 --> 01:14:32,570
I will wrap up this model and

1582
01:14:32,660 --> 01:14:34,410
because I want to go a little bit deeper

1583
01:14:34,470 --> 01:14:35,550
into the E  and M steps,

1584
01:14:35,630 --> 01:14:37,250
as there's some tricky parts

1585
01:14:37,320 --> 01:14:38,020
for the factor analysis model

1586
01:14:38,090 --> 01:14:39,610
specifically. Okay.

1587
01:14:39,700 --> 01:14:41,520
I'll see you in a couple of days.

