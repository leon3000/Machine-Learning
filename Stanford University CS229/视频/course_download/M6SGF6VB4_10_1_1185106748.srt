1
00:00:24,230 --> 00:00:26,370
我今天要讲的还是

2
00:00:26,510 --> 00:00:30,390
关于学习理论的内容

3
00:00:30,570 --> 00:00:32,220
具体地  我首先会讲VC维

4
00:00:32,970 --> 00:00:37,730
它与偏差方差分析以及

5
00:00:37,900 --> 00:00:39,660
欠拟合和过拟合问题有关

6
00:00:39,840 --> 00:00:42,030
这些问题我们上一讲已经见过了

7
00:00:42,190 --> 00:00:43,380
现在我们继续关注这些问题

8
00:00:43,600 --> 00:00:45,710
我之后会讲模型选择算法

9
00:00:45,860 --> 00:00:49,110
可以自动进行

10
00:00:49,280 --> 00:00:52,890
偏差方差权衡

11
00:00:53,040 --> 00:00:54,840
我们上一讲提到过

12
00:00:55,060 --> 00:00:56,970
根据我们剩余的时间

13
00:00:57,160 --> 00:00:57,910
我会决定这一讲是否会讲到

14
00:00:58,060 --> 00:00:59,330
贝叶斯统计与规范化

15
00:00:59,470 --> 00:01:01,220
如果今天讲不了

16
00:01:01,410 --> 00:01:03,910
我们会在下周的课上讲

17
00:01:07,300 --> 00:01:10,540
回顾一下上一讲的内容

18
00:01:10,770 --> 00:01:11,920
上一讲我们已经证明了

19
00:01:12,100 --> 00:01:15,940
如果你有一个有限假设类--

20
00:01:16,120 --> 00:01:18,410
H由k个假设构成

21
00:01:24,130 --> 00:01:26,820
假设你固定某些参数

22
00:01:26,890 --> 00:01:41,630
为了保证下列结论至少在的概率下成立

23
00:01:41,790 --> 00:01:49,990
m必须满足下列条件

24
00:01:51,230 --> 00:02:02,970
应该大于等于这个式子  明白吗?

25
00:02:04,300 --> 00:02:06,940
使用大O表示法

26
00:02:07,630 --> 00:02:08,770
我们可以去掉常量

27
00:02:08,990 --> 00:02:14,090
将其写成这种形式  明白吗?

28
00:02:14,490 --> 00:02:16,140
快速地回忆一下

29
00:02:16,330 --> 00:02:17,660
这些符号的含义

30
00:02:18,180 --> 00:02:20,420
我们讲了经验风险最小化(ERM)

31
00:02:21,200 --> 00:02:23,260
它是一个简化的机器学习的算法模型

32
00:02:23,450 --> 00:02:27,620
对于一个假设类H

33
00:02:28,180 --> 00:02:30,530
ERM算法做的是从

34
00:02:30,720 --> 00:02:33,220
假设类中选择

35
00:02:33,530 --> 00:02:37,050
具有最小训练误差的假设

36
00:02:37,680 --> 00:02:40,210
这个符号

37
00:02:40,390 --> 00:02:42,460
表示一般误差  对吗?

38
00:02:42,630 --> 00:02:45,620
它表示一个假设h

39
00:02:45,800 --> 00:02:47,250
对于由和训练集合相同的

40
00:02:47,430 --> 00:02:49,030
分布生成的一个新的

41
00:02:49,190 --> 00:02:50,630
样本错误分类的概率

42
00:02:50,970 --> 00:02:57,840
为了保证ERM生成的

43
00:03:03,280 --> 00:03:07,820
假设的一般误差--

44
00:03:08,780 --> 00:03:10,760
小于等于假设类中

45
00:03:10,910 --> 00:03:13,080
最佳的一般误差

46
00:03:13,730 --> 00:03:17,820
加上  两倍的

47
00:03:18,050 --> 00:03:19,660
误差阈值

48
00:03:19,990 --> 00:03:21,410
我们需要保证这个结论

49
00:03:21,570 --> 00:03:23,430
至少在的概率下是成立的

50
00:03:24,310 --> 00:03:28,520
我们证明了训练样本大小m

51
00:03:29,070 --> 00:03:31,100
应该大于等于这个式子  明白吗?

52
00:03:31,300 --> 00:03:34,160
应该大于等于

53
00:03:34,350 --> 00:03:36,790
k是假设类的大小

54
00:03:37,270 --> 00:03:41,130
这是一个和样本复杂度有关的结论

55
00:03:41,340 --> 00:03:43,120
因为它给出了一个

56
00:03:43,290 --> 00:03:45,400
为了满足误差率的要求

57
00:03:45,640 --> 00:03:49,280
所需的训练样本数目的界  明白吗?

58
00:03:49,480 --> 00:03:51,600
所以这是一个和样本复杂度有关的结论

59
00:03:51,880 --> 00:03:55,710
我接下来要做的是

60
00:03:55,930 --> 00:03:58,510
将这个结论推广到

61
00:03:58,730 --> 00:04:02,140
无限假设类的情形

62
00:04:02,300 --> 00:04:05,100
我们这里的假设类H

63
00:04:05,360 --> 00:04:07,780
仅包含k个有限的函数

64
00:04:08,090 --> 00:04:09,950
当你们使用像logistic

65
00:04:10,140 --> 00:04:11,320
回归这样的模型时

66
00:04:11,500 --> 00:04:13,480
它实际上是以实数为参数的

67
00:04:13,790 --> 00:04:16,180
接下来我会先给出一个

68
00:04:16,340 --> 00:04:18,150
不是那么严格的论点--

69
00:04:18,390 --> 00:04:20,080
理论上说它并不严格

70
00:04:20,320 --> 00:04:21,800
但是它表达了一种有用的直观含义

71
00:04:22,120 --> 00:04:24,310
之后我们会给出一个更为正确的命题

72
00:04:24,590 --> 00:04:26,000
但是不会证明它

73
00:04:26,150 --> 00:04:27,680
但是证明方法会大概涉及一些

74
00:04:27,850 --> 00:04:29,590
让我们看看这个不严格的论述

75
00:04:29,820 --> 00:04:31,430
现在我要将这个结果

76
00:04:31,640 --> 00:04:33,590
应用到logistic回归中

77
00:04:35,280 --> 00:04:40,630
比如说你的假设类是

78
00:04:40,840 --> 00:04:43,520
由线性决策边界构成的

79
00:04:43,710 --> 00:04:58,480
所以H以d个实数为参数  明白吗?

80
00:04:58,730 --> 00:05:01,120
例如  如果你用logistic

81
00:05:01,300 --> 00:05:03,640
回归解决包含n个特征的问题

82
00:05:03,810 --> 00:05:04,880
那么d应该等于n+1

83
00:05:05,080 --> 00:05:06,330
所以logistic回归

84
00:05:06,480 --> 00:05:07,960
会找到一个线性决策边界

85
00:05:08,450 --> 00:05:11,070
以n+1个实数为参数

86
00:05:11,840 --> 00:05:15,290
你可以想象一下你的假设类

87
00:05:15,460 --> 00:05:17,230
是如何在计算机中表示的--

88
00:05:17,410 --> 00:05:20,970
计算机用若干0-1位表示实数

89
00:05:21,310 --> 00:05:25,900
如果你使用一台标准计算机

90
00:05:26,200 --> 00:05:27,580
那么它通常会用

91
00:05:27,790 --> 00:05:30,390
双精度浮点数来表示实数

92
00:05:30,650 --> 00:05:33,320
这意味着每个实数都是

93
00:05:33,500 --> 00:05:37,030
用64位来表示的  对吗?

94
00:05:37,240 --> 00:05:40,490
你们都知道计算机中浮点数的含义

95
00:05:40,690 --> 00:05:42,090
我们通常使用的

96
00:05:42,300 --> 00:05:44,230
就是64位浮点数

97
00:05:44,460 --> 00:05:46,580
如果假设类以d个实数为参数

98
00:05:46,810 --> 00:05:53,250
那么我们需要64d个位来表示这些参数

99
00:05:54,170 --> 00:05:55,840
计算机并不能真正地表示实数

100
00:05:56,030 --> 00:05:58,220
它只能通过位来近似地表示

101
00:05:58,420 --> 00:06:03,710
所以你们的假设类

102
00:06:03,900 --> 00:06:05,860
在计算机中的表示--

103
00:06:06,200 --> 00:06:10,050
因为有64d个位可以翻转

104
00:06:10,430 --> 00:06:14,110
所以总共可能

105
00:06:14,290 --> 00:06:19,950
具有的状态是个  明白吗?

106
00:06:21,080 --> 00:06:22,150
因为你总共有64d个位

107
00:06:22,330 --> 00:06:24,790
可以自由翻转

108
00:06:25,970 --> 00:06:30,620
所以这就是为什么

109
00:06:30,780 --> 00:06:32,380
这里的log k很关键的原因了

110
00:06:32,560 --> 00:06:36,000
这里k应该等于

111
00:06:36,240 --> 00:06:39,140
如果我其代入

112
00:06:40,290 --> 00:06:44,960
那么你会发现  为了使这个结论成立

113
00:06:50,020 --> 00:06:57,010
m应该大于等于这个--

114
00:06:57,620 --> 00:07:00,470
这个复杂度--

115
00:07:00,800 --> 00:07:11,600
这里是  应该等于这个  对吗?

116
00:07:13,570 --> 00:07:14,980
明确一下

117
00:07:15,300 --> 00:07:19,110
现在我们用的是新的结论

118
00:07:19,260 --> 00:07:21,460
而不是原来的老结论


119
00:07:21,620 --> 00:07:25,290
假设你需要

120
00:07:25,490 --> 00:07:28,510
保证ERM

121
00:07:28,850 --> 00:07:31,850
生成的假设和最好的假设

122
00:07:32,030 --> 00:07:34,360
之间的差异不超过

123
00:07:35,030 --> 00:07:37,290
这个结果意味着

124
00:07:37,820 --> 00:07:38,680
为了保证满足

125
00:07:38,860 --> 00:07:40,450
规定的误差界

126
00:07:40,850 --> 00:07:43,340
m应该大于等于这个

127
00:07:43,510 --> 00:07:44,580
换句话说

128
00:07:44,750 --> 00:07:47,020
你需要的样本数量

129
00:07:47,210 --> 00:07:50,560
必须是这个数量级:明白吗?

130
00:07:52,550 --> 00:07:55,460
这种直观理解

131
00:07:55,620 --> 00:07:56,920
大致是正确的

132
00:07:57,090 --> 00:07:58,000
这个结论表明

133
00:07:58,160 --> 00:07:59,210
你所需的训练样本的数目

134
00:07:59,350 --> 00:08:02,000
大致和你的假设类的

135
00:08:02,180 --> 00:08:04,110
参数数目呈线性关系

136
00:08:04,480 --> 00:08:06,530
m大致和d呈线性关系

137
00:08:06,720 --> 00:08:08,630
这种直观理解大致是正确的

138
00:08:09,330 --> 00:08:10,820
我稍后会讲到

139
00:08:22,290 --> 00:08:24,280
这个结论显然并不严格

140
00:08:24,440 --> 00:08:26,090
因为它依赖于对浮点数的

141
00:08:26,260 --> 00:08:28,360
64位表示这一假设

142
00:08:28,640 --> 00:08:31,840
所以让我们继续介绍一种

143
00:08:32,200 --> 00:08:35,150
更为正式的正确的论述方式

144
00:08:35,330 --> 00:08:37,220
事实证明这种

145
00:08:37,400 --> 00:08:39,830
正确的方式更为正式

146
00:08:40,280 --> 00:08:44,480
因为它包含大量的证明

147
00:08:44,660 --> 00:08:46,180
所以我仅仅陈述一下结果

148
00:08:46,390 --> 00:08:47,460
而略去证明

149
00:08:47,660 --> 00:08:49,470
我们不会详细证明关于

150
00:08:49,660 --> 00:08:51,640
无限假设类的情形

151
00:08:52,030 --> 00:08:55,600
首先给出一个定义--

152
00:08:56,500 --> 00:08:59,470
给定一个由d个点构成的集合

153
00:09:18,540 --> 00:09:29,920
我们说一个假设类H能够分散集合s

154
00:09:30,650 --> 00:09:35,680
如果H可以实现该

155
00:09:35,860 --> 00:09:47,050
集合的任意一种标记方式  明白吗?

156
00:09:47,530 --> 00:09:49,510
我所谓的实现标记方式

157
00:09:49,860 --> 00:09:51,660
你可以非正式地这样理解:

158
00:09:53,150 --> 00:09:55,230
如果一个假设类能够分散集合s

159
00:09:55,400 --> 00:09:57,350
那么这意味着我对于这d个点

160
00:09:58,380 --> 00:10:00,780
可以任意地为

161
00:10:00,950 --> 00:10:03,840
它们进行标记  明白吗?

162
00:10:04,150 --> 00:10:07,640
如果假设类H能够分散s的话

163
00:10:09,940 --> 00:10:12,460
那么对于s的任意一种标记方式

164
00:10:12,710 --> 00:10:15,580
都可以从H中找到一个假设h能够对

165
00:10:16,110 --> 00:10:20,140
s中的d个样本进行完美预测  明白吗?

166
00:10:20,310 --> 00:10:21,400
这就是分散这个概念的含义

167
00:10:21,580 --> 00:10:23,730
让我用一个例子来说明

168
00:10:23,920 --> 00:10:25,380
假设H是一个包含了所有

169
00:10:25,560 --> 00:10:32,330
二维线性分类器的假设类

170
00:10:35,490 --> 00:10:39,980
而s是一个包含了

171
00:10:41,010 --> 00:10:43,890
这样两个点的集合  明白吗?

172
00:10:44,080 --> 00:10:47,020
所以对于这两个点

173
00:10:47,180 --> 00:10:48,560
有四种可能的标记方式

174
00:10:48,730 --> 00:10:50,920
你可以选择将其都标记为正

175
00:10:51,170 --> 00:10:55,040
或者一正一负  或者一负一正

176
00:10:56,350 --> 00:10:58,480
或者将其都标记为负

177
00:10:59,090 --> 00:11:02,440
因为假设类H包含了

178
00:11:02,610 --> 00:11:04,670
所有的二维线性分类器

179
00:11:04,890 --> 00:11:06,350
所以对于每一种标记方式

180
00:11:06,530 --> 00:11:10,100
我都可以找到一个线性分类器

181
00:11:10,380 --> 00:11:13,990
使得其训练误差为0

182
00:11:14,230 --> 00:11:15,720
所以我们能够对所有的

183
00:11:15,880 --> 00:11:17,800
四种标记方式进行完美分类

184
00:11:17,980 --> 00:11:21,660
所以我们说假设类H能够

185
00:11:21,820 --> 00:11:27,240
分散这个由两个点组成的集合s 明白吗?

186
00:11:27,480 --> 00:11:39,860
再举一个例子--这是一个更大的训练集合

187
00:11:40,390 --> 00:11:43,790
假设集合s由三个点组成

188
00:11:43,970 --> 00:11:45,990
现在我们对于s有

189
00:11:46,190 --> 00:12:03,250
八种不同的标记方式  明白吗?

190
00:12:11,180 --> 00:12:12,470
对于这三个点

191
00:12:12,650 --> 00:12:14,340
我们有八种可能的标记方式

192
00:12:14,620 --> 00:12:18,090
对于每一种标记方式

193
00:12:18,270 --> 00:12:20,830
我都可以从假设类中找到

194
00:12:21,020 --> 00:12:24,520
一个假设能够对这些样本进行完美分类

195
00:12:25,560 --> 00:12:29,020
所以  根据定义

196
00:12:29,190 --> 00:12:33,280
我们的假设类H能够分散集合s

197
00:12:33,940 --> 00:12:35,770
我们之前提到的术语--

198
00:12:35,960 --> 00:12:37,710
H可以实现任何s上的标记方式

199
00:12:37,920 --> 00:12:39,240
它的意思是

200
00:12:39,460 --> 00:12:41,240
对于任何一种s上的标记方式

201
00:12:41,550 --> 00:12:42,640
你都可以从假设类H中

202
00:12:42,820 --> 00:12:44,160
找到一个假设能够将

203
00:12:44,350 --> 00:12:47,000
正负样本完美地分隔开 明白吗?

204
00:12:47,290 --> 00:12:52,310
这个集合怎么样?

205
00:12:57,440 --> 00:13:00,380
假设s现在是由四个点组成的集合

206
00:13:01,030 --> 00:13:02,350
可能有很多标记方式

207
00:13:02,730 --> 00:13:04,580
最多可能有16种  对吗?

208
00:13:04,800 --> 00:13:07,440
所以我可以实现一些标记方式

209
00:13:08,470 --> 00:13:12,270
但是没有线性决策边界

210
00:13:12,470 --> 00:13:15,770
能够实现这种标记方式

211
00:13:15,960 --> 00:13:19,170
所以H不能分散这个集合s  明白吗?

212
00:13:20,120 --> 00:13:22,870
我这里不打算证明

213
00:13:23,030 --> 00:13:26,340
但是事实上你可以证明在二维的情况下

214
00:13:26,690 --> 00:13:28,630
任意的线性分类器都不能

215
00:13:28,890 --> 00:13:33,380
分散四个点构成的集合  明白吗?

216
00:13:36,770 --> 00:13:47,090
这里是另外一个定义

217
00:13:48,720 --> 00:13:51,420
当我说--这个概念被称之为VC维

218
00:13:51,690 --> 00:13:53,190
VC代表两个人名的缩写

219
00:13:53,380 --> 00:13:55,580
Vapnik和Chervonenkis

220
00:14:06,750 --> 00:14:08,210
给定一个假设类H

221
00:14:10,120 --> 00:14:12,710
其VC维被表示为VC(H)

222
00:14:14,180 --> 00:14:18,190
表示能够被H分散的

223
00:14:18,400 --> 00:14:38,840
最大集合的大小

224
00:14:39,940 --> 00:14:42,530
如果一个假设类能够

225
00:14:42,770 --> 00:14:44,070
分散任意大的集合

226
00:14:44,270 --> 00:14:45,870
那么它的VC维为无穷大

227
00:14:47,180 --> 00:14:49,190
来看一个具体的例子

228
00:14:49,710 --> 00:14:58,200
如果H是所有二维分类器构成的假设类

229
00:14:59,310 --> 00:15:04,990
那么它的VC维应该等于3

230
00:15:05,360 --> 00:15:07,990
因为刚刚我们看到它能够

231
00:15:08,480 --> 00:15:11,980
分散一个包含了三个点的集合s

232
00:15:12,190 --> 00:15:14,200
但是实际上它不能分散

233
00:15:14,470 --> 00:15:15,420
任意一个包含四个点的集合

234
00:15:15,590 --> 00:15:17,760
这里我就不证明了

235
00:15:18,520 --> 00:15:22,570
因此  这个假设类的VC维等于3 什么问题?

236
00:15:23,070 --> 00:15:26,340
S:但是会有大小为3的集合

237
00:15:26,580 --> 00:15:28,370
不能被分散对吗?

238
00:15:28,540 --> 00:15:29,890
I:一点也没错

239
00:15:30,060 --> 00:15:34,010
实际上如果我选取

240
00:15:34,250 --> 00:15:35,750
这样的一个集合s

241
00:15:35,960 --> 00:15:38,300
这样的标记方式就不能被实现

242
00:15:38,470 --> 00:15:40,720
所以  h不能分散这个集合

243
00:15:41,120 --> 00:15:42,800
但是这没有关系

244
00:15:42,950 --> 00:15:44,530
因为只要存在大小为3

245
00:15:44,690 --> 00:15:46,320
的集合可以被分散

246
00:15:46,510 --> 00:15:48,040
那么VC维就是3

247
00:15:48,230 --> 00:15:50,400
但是肯定没有大小为4

248
00:15:50,600 --> 00:15:52,510
的集合可以被分散 什么问题?

249
00:15:56,220 --> 00:15:57,560
I:不是根据定义

250
00:15:57,760 --> 00:16:00,440
我再举个例子

251
00:16:00,750 --> 00:16:04,310
比如说  我可以这样选取集合s

252
00:16:04,660 --> 00:16:07,280
使其包含三个重合的点

253
00:16:07,630 --> 00:16:09,810
也就是说三个点的位置都相同

254
00:16:09,990 --> 00:16:11,800
显然我无法分散这个集合

255
00:16:11,990 --> 00:16:12,960
但是这没关系

256
00:16:13,140 --> 00:16:15,040
我也不能分散这个集合

257
00:16:15,310 --> 00:16:16,440
但是存在这其它大小为3

258
00:16:16,630 --> 00:16:19,690
而且可以分散的集合

259
00:16:23,990 --> 00:16:27,900
实际上这个结论可以

260
00:16:28,410 --> 00:16:36,060
推广到更一般的情形  对于任意维度

261
00:16:39,970 --> 00:16:44,950
由n维线性分类器构成的

262
00:16:49,740 --> 00:16:54,310
假设类的VC维等于n+1 明白吗?

263
00:16:54,510 --> 00:16:55,980
所以这里是二维的情形

264
00:16:56,150 --> 00:16:57,350
如果你的

265
00:16:57,530 --> 00:16:59,070
线性分类器是n维的

266
00:16:59,270 --> 00:17:00,790
也就是说对于n维的假设类

267
00:17:01,000 --> 00:17:03,710
VC维应该等于n+1

268
00:17:04,490 --> 00:17:13,470
我们可以写出这个结论

269
00:17:13,660 --> 00:17:17,730
我认为它无可争议地

270
00:17:17,940 --> 00:17:26,940
是学习理论中最为著名的结论

271
00:17:29,110 --> 00:17:31,880
给定一个假设类H

272
00:17:37,730 --> 00:17:42,020
令VC(H)=d

273
00:17:45,760 --> 00:17:48,610
那么至少在的概率下

274
00:17:50,350 --> 00:17:53,140
我们有下面的结论成立

275
00:18:30,950 --> 00:18:33,540
公式的右边看起来有些复杂

276
00:18:33,760 --> 00:18:34,560
但是不要担心

277
00:18:34,730 --> 00:18:36,880
我之后会来分析它

278
00:18:37,060 --> 00:18:38,470
这个结论的关键部分是

279
00:18:38,690 --> 00:18:41,690
如果你有一个VC维等于d的假设类

280
00:18:42,010 --> 00:18:44,280
它可以是无限假设类

281
00:18:44,890 --> 00:18:50,270
Vapnik和Chervonenkis证明了

282
00:18:50,800 --> 00:18:53,060
至少在的概率下

283
00:18:53,290 --> 00:18:57,860
会出现一致收敛 明白吗?

284
00:18:58,080 --> 00:19:00,480
对于所有的假设h--

285
00:19:02,390 --> 00:19:07,960
对于假设类中的所有假设h

286
00:19:08,170 --> 00:19:10,510
h的一般误差

287
00:19:10,710 --> 00:19:15,030
和训练误差

288
00:19:15,450 --> 00:19:17,110
之间的差异由右边这个

289
00:19:17,280 --> 00:19:19,620
看起来比较复杂的

290
00:19:19,810 --> 00:19:22,150
式子给出了限定  明白吗?

291
00:19:22,370 --> 00:19:31,230
因此  在1-Δ的情况下

292
00:19:31,450 --> 00:19:36,540
我们有这样的结论成立--

293
00:19:45,980 --> 00:19:51,020
这里是一样的  明白吗?

294
00:19:56,670 --> 00:19:59,230
我们从这一步推出了这一步  明白吗?

295
00:19:59,420 --> 00:20:02,040
从这一步到这一步的证明方法

296
00:20:02,210 --> 00:20:03,600
你们已经在

297
00:20:03,790 --> 00:20:05,820
上节课见到过了

298
00:20:05,980 --> 00:20:07,330
你们记得  在上一讲中

299
00:20:07,520 --> 00:20:08,530
我们证明了如果

300
00:20:08,680 --> 00:20:10,180
有一致收敛结果

301
00:20:10,660 --> 00:20:12,320
那么这意味着--

302
00:20:12,540 --> 00:20:15,540
我们已经证明了

303
00:20:15,700 --> 00:20:16,830
如果一般误差和

304
00:20:17,020 --> 00:20:18,460
训练误差相差不大

305
00:20:18,680 --> 00:20:20,370
在以内的话

306
00:20:20,650 --> 00:20:22,160
那么你选择的假设的

307
00:20:22,340 --> 00:20:23,760
一般误差和最好的

308
00:20:23,920 --> 00:20:27,390
一般误差之间的差异最多是

309
00:20:28,130 --> 00:20:31,070
这个结论实际上说的是

310
00:20:31,250 --> 00:20:33,050
的一般误差小于等于

311
00:20:33,240 --> 00:20:35,010
最小的一般误差加上

312
00:20:35,180 --> 00:20:36,710
前面的两个常量

313
00:20:36,880 --> 00:20:39,600
我们用大O符号省略掉了

314
00:20:43,870 --> 00:20:46,240
这个公式稍微有些复杂

315
00:20:46,650 --> 00:20:49,310
让我将它写成一个引理

316
00:20:49,520 --> 00:20:59,630
为了保证这个结论

317
00:21:04,400 --> 00:21:06,690
至少在的概率下成立

318
00:21:08,520 --> 00:21:09,880
应该满足这个结论

319
00:21:10,060 --> 00:21:18,030
我将m用大O符号

320
00:21:18,250 --> 00:21:23,230
表示成和d

321
00:21:23,410 --> 00:21:28,880
存在线性关系

322
00:21:29,120 --> 00:21:32,890
这里我将和写成下标 让我看看

323
00:21:33,100 --> 00:21:35,250
如果我将和视为常量

324
00:21:35,460 --> 00:21:36,900
这样我就可以用

325
00:21:37,110 --> 00:21:38,210
大O符号将其省略

326
00:21:38,400 --> 00:21:40,250
看成复杂度对

327
00:21:40,470 --> 00:21:43,160
这两个量的一种依赖

328
00:21:43,410 --> 00:21:45,500
为了保证这个结论成立

329
00:21:45,710 --> 00:21:48,120
m的阶必须和假设类的

330
00:21:48,670 --> 00:21:51,470
VC维相同  明白吗?

331
00:21:53,210 --> 00:21:56,880
让我们看看

332
00:21:58,890 --> 00:22:00,970
我们可以得出结论

333
00:22:01,780 --> 00:22:04,310
如果你的学习算法尝试--

334
00:22:04,560 --> 00:22:06,360
对于ERM算法来说--

335
00:22:06,530 --> 00:22:08,040
换句话说  这不是非常严格

336
00:22:08,360 --> 00:22:09,450
对于学习算法

337
00:22:09,630 --> 00:22:11,420
它们尝试最小化训练误差

338
00:22:11,690 --> 00:22:14,190
一种直观理解是

339
00:22:14,420 --> 00:22:16,400
你需要的训练样本数大概

340
00:22:16,630 --> 00:22:20,390
和假设类的VC维

341
00:22:20,740 --> 00:22:22,270
呈线性关系

342
00:22:22,890 --> 00:22:25,130
更为正式地

343
00:22:25,340 --> 00:22:28,140
这表明样本复杂度的

344
00:22:28,320 --> 00:22:30,540
上界是由VC维给定的

345
00:22:30,730 --> 00:22:35,010
明白吗?事实证明

346
00:22:35,250 --> 00:22:37,550
对于大多数的合理的假设类

347
00:22:37,760 --> 00:22:43,350
实际上VC维都是大概差不多的

348
00:22:43,630 --> 00:22:46,760
我想  大概都和你模型的参数数量差不多

349
00:22:47,160 --> 00:22:48,780
例如

350
00:22:49,010 --> 00:22:50,540
你要用模型进行logistic回归

351
00:22:50,760 --> 00:22:52,150
进行线性分类

352
00:22:52,340 --> 00:22:54,660
维度为n n维的logistic

353
00:22:54,860 --> 00:22:57,050
回归需要n+1个参数

354
00:22:57,320 --> 00:22:58,810
而n维线性分类器的

355
00:22:58,990 --> 00:23:01,970
假设类的VC维是n+1

356
00:23:02,240 --> 00:23:03,240
所以实际上

357
00:23:03,430 --> 00:23:07,030
对于多数合理的假设类

358
00:23:07,280 --> 00:23:09,070
VC维总是和

359
00:23:09,300 --> 00:23:11,620
模型参数的数量成正比

360
00:23:11,800 --> 00:23:14,020
因此  多数情况下

361
00:23:14,240 --> 00:23:16,580
总是模型参数的数量

362
00:23:16,830 --> 00:23:21,180
因此  直观上  你为了

363
00:23:21,360 --> 00:23:23,130
拟合出模型所需要的

364
00:23:23,300 --> 00:23:26,130
训练样本的数目大致和

365
00:23:26,310 --> 00:23:29,700
模型的参数数量呈线性关系  明白吗?

366
00:23:29,860 --> 00:23:31,050
确实有一些奇怪的例子

367
00:23:31,230 --> 00:23:32,450
使得我刚才说的那些都不成立

368
00:23:32,620 --> 00:23:33,970
有一些奇怪的例子

369
00:23:34,150 --> 00:23:35,210
模型参数很少

370
00:23:35,390 --> 00:23:37,030
但是VC维非常大

371
00:23:37,510 --> 00:23:38,630
但是据我所知

372
00:23:38,820 --> 00:23:39,770
所有的这些例子

373
00:23:39,980 --> 00:23:41,310
都比较

374
00:23:41,480 --> 00:23:43,420
奇怪和非主流

375
00:23:43,620 --> 00:23:44,780
它们并不常见

376
00:23:44,930 --> 00:23:46,030
并不属于那些你们

377
00:23:46,180 --> 00:23:48,370
经常会用到的学习算法

378
00:23:50,520 --> 00:23:52,840
再说一点

379
00:23:53,110 --> 00:23:54,550
事实证明--

380
00:23:55,100 --> 00:23:56,770
这个结论表明样本复杂度

381
00:23:56,950 --> 00:23:58,590
是以VC维为上界的

382
00:23:58,920 --> 00:24:00,890
但是如果你们的

383
00:24:01,050 --> 00:24:02,940
训练样本的数目的阶和VC维相同

384
00:24:03,120 --> 00:24:04,470
那么你会发现--

385
00:24:04,790 --> 00:24:06,460
事实证明最坏情况下

386
00:24:06,970 --> 00:24:14,970
样本复杂度的下界也是VC维

387
00:24:15,590 --> 00:24:17,500
这意味着如果你们

388
00:24:17,770 --> 00:24:20,390
有一个非常差的学习问题

389
00:24:20,580 --> 00:24:24,580
如果你们有的训练样本数

390
00:24:24,750 --> 00:24:26,810
小于VC维的阶

391
00:24:27,290 --> 00:24:31,540
那么就不可能证明出这个界

392
00:24:31,780 --> 00:24:33,030
所以我想在最坏情况下

393
00:24:33,270 --> 00:24:34,830
样本复杂度

394
00:24:35,020 --> 00:24:36,400
即你所需的训练样本的

395
00:24:36,590 --> 00:24:38,670
数量级的上界和下界都是由VC维限定的

396
00:24:41,670 --> 00:24:48,890
让我们看看 关于这些还有问题吗?

397
00:24:52,030 --> 00:24:54,290
S:证明过程中

398
00:24:54,560 --> 00:24:56,790
是否假设了

399
00:24:56,940 --> 00:24:59,910
假设类是有限的?

400
00:25:00,090 --> 00:25:01,270
I:让我看看

401
00:25:01,430 --> 00:25:02,570
证明过程并没有用到这样的假设

402
00:25:02,750 --> 00:25:05,660
实际上我说的这个定理是正确的

403
00:25:05,850 --> 00:25:08,320
事实证明

404
00:25:08,580 --> 00:25:11,050
在证明过程中

405
00:25:11,200 --> 00:25:13,180
我们重新构造了-net

406
00:25:13,340 --> 00:25:14,310
这是一种非常巧妙的技巧

407
00:25:14,490 --> 00:25:16,350
不用考虑证明

408
00:25:16,490 --> 00:25:20,230
你并不需要这样的假设

409
00:25:20,390 --> 00:25:23,170
这样的证明用到了一个

410
00:25:23,390 --> 00:25:24,770
非常巧妙的技巧

411
00:25:24,980 --> 00:25:28,250
但是我们不需要知道

412
00:25:28,530 --> 00:25:31,420
而且也没有这样的假设

413
00:25:32,410 --> 00:25:33,710
我需要说一下

414
00:25:33,890 --> 00:25:35,270
当我还在读PHD的时候

415
00:25:35,480 --> 00:25:36,710
我就在看这个证明

416
00:25:36,880 --> 00:25:39,240
它花了我一周时间

417
00:25:39,530 --> 00:25:40,940
每天我醒来  早晨九点到办公室

418
00:25:41,120 --> 00:25:44,040
之后看这个定理的证明过程

419
00:25:44,230 --> 00:25:46,930
每天我都从上午九点读到下午六点

420
00:25:47,150 --> 00:25:48,480
之后我会回家  第二天还是如此

421
00:25:48,670 --> 00:25:49,620
继续从昨天看到的地方开始看

422
00:25:49,830 --> 00:25:51,390
我就这样花了整整一周时间

423
00:25:51,590 --> 00:25:53,470
才理解证明过程

424
00:25:53,640 --> 00:25:55,340
所以我觉得你们也这样做的话会非常痛苦

425
00:25:59,060 --> 00:26:05,100
我们需要将一些零散的知识点串起来:

426
00:26:07,580 --> 00:26:08,480
我要做的是

427
00:26:08,660 --> 00:26:13,200
我要提到一些看起来

428
00:26:13,380 --> 00:26:14,920
比较零散的知识点

429
00:26:15,150 --> 00:26:17,140
但是我要将这些知识点串起来

430
00:26:17,380 --> 00:26:21,820
让我们看看

431
00:26:24,060 --> 00:26:25,850
事实证明--

432
00:26:26,880 --> 00:26:28,790
这一点你们已经很清楚了--

433
00:26:28,970 --> 00:26:31,310
这个界是基于ERM算法证明的

434
00:26:31,510 --> 00:26:33,950
它的目标是使取值在0 1

435
00:26:34,210 --> 00:26:37,280
之间的训练误差最小化

436
00:26:45,090 --> 00:26:50,040
所以你们可能会问一个问题

437
00:26:53,270 --> 00:26:54,660
SVM是这样的吗?

438
00:26:54,840 --> 00:26:56,590
为什么SVM不会过拟合?

439
00:26:57,090 --> 00:26:59,600
记得我们讨论SVM的时候说过

440
00:26:59,760 --> 00:27:02,810
你们会用核将特征

441
00:27:03,030 --> 00:27:04,780
映射到无限维的特征空间

442
00:27:04,950 --> 00:27:06,890
所以看起来VC维应该是无穷大

443
00:27:07,080 --> 00:27:09,650
因为它等于n+1

444
00:27:10,110 --> 00:27:12,330
而n是无穷大

445
00:27:12,520 --> 00:27:15,100
事实证明

446
00:27:15,370 --> 00:27:17,260
具有较大间隔的线性分类器

447
00:27:17,480 --> 00:27:19,830
的假设类都有比较低的VC维

448
00:27:20,140 --> 00:27:22,490
我会非常快速  非正式地解释一下

449
00:27:22,670 --> 00:27:24,760
这不是很重要

450
00:27:24,920 --> 00:27:26,370
你们不需要了解细节

451
00:27:26,890 --> 00:27:29,350
我会非正式地说一下

452
00:27:29,530 --> 00:27:31,630
我会给定一些点

453
00:27:31,820 --> 00:27:35,010
我需要你们

454
00:27:35,170 --> 00:27:37,340
考虑这样的假设类

455
00:27:37,550 --> 00:27:38,980
只包含间隔很大的线性分类器

456
00:27:39,170 --> 00:27:41,300
所以我的假设类将会仅包含那些

457
00:27:41,470 --> 00:27:46,130
能够以较大的间隔

458
00:27:46,310 --> 00:27:48,160
分隔点集合的边界

459
00:27:48,350 --> 00:27:51,980
比如说  我们规定间隔至少应该是

460
00:27:52,560 --> 00:27:55,390
我不要那些离点非常近的线

461
00:27:56,410 --> 00:27:57,800
像这条 我不要这条线

462
00:27:58,000 --> 00:28:00,080
因为它离这个点太近了

463
00:28:00,410 --> 00:28:11,400
实际上如果我仅考虑

464
00:28:11,600 --> 00:28:17,780
半径R以内的点

465
00:28:18,630 --> 00:28:20,790
和那些间隔至少为的

466
00:28:20,980 --> 00:28:23,780
线性分类器

467
00:28:24,180 --> 00:28:26,220
构成的假设类

468
00:28:26,950 --> 00:28:30,870
那么这个假设类的

469
00:28:31,110 --> 00:28:37,170
VC维小于等于  明白吗?

470
00:28:37,560 --> 00:28:42,450
这个有趣的符号表示上取整

471
00:28:42,680 --> 00:28:44,650
这是一个上取整符号

472
00:28:44,860 --> 00:28:46,710
表示向上取到最接近的整数

473
00:28:47,040 --> 00:28:49,320
实际上你可以证明--

474
00:28:49,530 --> 00:28:51,480
关于这个结论还有

475
00:28:51,670 --> 00:28:54,100
很多奇怪的性质  我不打算介绍了--

476
00:28:54,300 --> 00:28:57,300
你们可以证明仅包含较大间隔线性

477
00:28:57,510 --> 00:29:00,310
分类器的假设类的VC维是有上界的

478
00:29:01,230 --> 00:29:02,490
这个结论的惊人之处在于

479
00:29:02,780 --> 00:29:04,580
它表明VC维的上界

480
00:29:04,770 --> 00:29:07,650
并不依赖于x的维度

481
00:29:08,050 --> 00:29:09,470
换句话说

482
00:29:09,670 --> 00:29:12,120
虽然你的数据点位于无限维的空间中

483
00:29:12,430 --> 00:29:14,470
但是只要你们只考虑

484
00:29:14,680 --> 00:29:16,710
具有较大间隔的

485
00:29:16,940 --> 00:29:18,230
分类器所组成的假设类

486
00:29:18,440 --> 00:29:20,010
那么VC维就存在上界

487
00:29:20,230 --> 00:29:23,440
所以为了寻找一个

488
00:29:23,630 --> 00:29:25,270
较大间隔的分类器

489
00:29:25,710 --> 00:29:27,900
为了能够找到一个分类器

490
00:29:28,080 --> 00:29:29,230
能够将你的正负样本

491
00:29:29,430 --> 00:29:31,140
以较大间隔分隔开

492
00:29:32,640 --> 00:29:34,180
事实证明

493
00:29:35,300 --> 00:29:37,360
SVM会自动地尝试

494
00:29:37,550 --> 00:29:41,310
找到一个具有较小VC维的假设类

495
00:29:41,660 --> 00:29:50,100
所以它不会过拟合 Alex?

496
00:29:51,500 --> 00:29:52,700
S:无限维向量的模是怎样定义的?

497
00:29:52,880 --> 00:29:54,460
I:它和有限维的

498
00:29:54,630 --> 00:29:56,580
向量的定义是相同的

499
00:29:56,750 --> 00:29:58,500
假设你有一个无限--

500
00:29:58,700 --> 00:30:00,960
实际上  这些是固定的无限维向量

501
00:30:01,110 --> 00:30:07,270
而不是非固定的

502
00:30:07,700 --> 00:30:09,890
通常情况下  有限维向量的模

503
00:30:10,070 --> 00:30:11,880
应该等于这个式子

504
00:30:12,030 --> 00:30:13,840
如果x是无限维的

505
00:30:14,000 --> 00:30:19,080
它应该等于这个式子

506
00:30:20,100 --> 00:30:21,820
I:能再说一遍吗?

507
00:30:23,230 --> 00:30:25,210
I:是的 我假设这些点

508
00:30:25,310 --> 00:30:26,160
都在半径R的范围内

509
00:30:26,220 --> 00:30:27,070
S:哦

510
00:30:27,110 --> 00:30:27,640
I:所以这样就

511
00:30:27,700 --> 00:30:28,510
保证了收敛性

512
00:30:33,500 --> 00:30:36,360
有些人会对这个问题产生疑问

513
00:30:37,160 --> 00:30:41,700
最后  我要将ERM联系到

514
00:30:42,000 --> 00:30:44,060
我们之前讲过的

515
00:30:44,250 --> 00:30:45,870
学习算法上

516
00:30:46,890 --> 00:31:11,340
实际上--我们目前

517
00:31:11,570 --> 00:31:13,840
所讲的理论都是基于ERM的

518
00:31:14,050 --> 00:31:20,210
我们现在只考虑一个样本

519
00:31:21,690 --> 00:31:24,800
让我画一个函数  这里是0

520
00:31:25,280 --> 00:31:26,820
这里跳到1  它看起来是这样的

521
00:31:27,880 --> 00:31:31,120
对于一个特定的训练样本

522
00:31:31,880 --> 00:31:45,280
这是一个指示函数

523
00:31:45,580 --> 00:31:49,360
其横坐标为z=θTx  明白吗?

524
00:31:49,520 --> 00:31:51,220
对于一个训练样本来说

525
00:31:51,490 --> 00:31:54,720
它要么是正样本  要么是负样本

526
00:31:54,980 --> 00:31:58,520
根据的值  你的预测

527
00:31:58,710 --> 00:32:00,040
或者是正确的  或者是错误的

528
00:32:00,230 --> 00:32:02,060
所以你知道

529
00:32:02,290 --> 00:32:03,610
如果你的训练样本

530
00:32:03,810 --> 00:32:05,620
是个正样本

531
00:32:05,830 --> 00:32:10,600
那么当z大于0时  你的预测是正确的

532
00:32:18,500 --> 00:32:19,970
假设你有一个负样本

533
00:32:20,140 --> 00:32:22,160
那么y=0对吗?

534
00:32:22,670 --> 00:32:24,980
对于z  如果

535
00:32:25,250 --> 00:32:28,910
那么你对该样本的预测是错误的

536
00:32:29,710 --> 00:32:32,770
如果z小于0  那么你对该样本的预测是正确的

537
00:32:33,040 --> 00:32:35,210
这是因为这里是一个

538
00:32:35,420 --> 00:32:38,870
关于的指示函数 对吗?

539
00:32:39,190 --> 00:32:48,190
它应该等于  对吗?

540
00:32:49,480 --> 00:32:53,110
实际上

541
00:32:53,350 --> 00:32:55,720
你要做的是选取参数

542
00:32:55,930 --> 00:32:59,130
使得这个阶梯函数最小  对吗?

543
00:32:59,800 --> 00:33:01,150
你希望选择

544
00:33:01,330 --> 00:33:05,610
使得你能够

545
00:33:05,800 --> 00:33:07,150
对你的训练样本正确分类

546
00:33:07,350 --> 00:33:10,380
所以你不希望成立

547
00:33:10,910 --> 00:33:12,890
所以你希望这个指示函数的值为0

548
00:33:13,840 --> 00:33:16,020
事实证明

549
00:33:16,200 --> 00:33:18,210
这个阶梯函数明显不是一个凸函数

550
00:33:18,470 --> 00:33:21,010
事实证明线性分类器使

551
00:33:21,380 --> 00:33:24,280
训练误差最小是一个NP难问题

552
00:33:24,910 --> 00:33:26,960
实际上logistic回归和

553
00:33:27,140 --> 00:33:28,910
SVM都可以看做对

554
00:33:29,170 --> 00:33:32,380
这个问题的一种凸性近似

555
00:33:32,690 --> 00:33:37,520
具体地  函数看起来是这样的

556
00:33:41,780 --> 00:33:51,210
实际上logistic回归

557
00:33:51,380 --> 00:33:53,090
尝试最大化似然性

558
00:33:53,410 --> 00:33:54,740
所以它尝试令负的

559
00:33:54,930 --> 00:33:56,560
对数似然性最小

560
00:33:56,780 --> 00:33:57,810
如果你将负的对数

561
00:33:58,020 --> 00:33:58,960
似然性的图形画出来

562
00:33:59,150 --> 00:34:01,090
你会发现它看起来是这样的一个函数

563
00:34:01,280 --> 00:34:04,400
我画的这条线你可以看成

564
00:34:04,570 --> 00:34:05,570
是对于这个

565
00:34:05,760 --> 00:34:07,290
阶梯函数的近似

566
00:34:07,470 --> 00:34:08,950
这是你真正最小化的

567
00:34:09,140 --> 00:34:10,650
实际上就是近似地在最小化训练误差

568
00:34:10,850 --> 00:34:12,170
你可以认为logistic回归

569
00:34:12,390 --> 00:34:15,390
是一种ERM的近似

570
00:34:16,140 --> 00:34:17,390
它没有使用这个阶梯函数

571
00:34:17,640 --> 00:34:18,820
这个函数不是凸性的

572
00:34:19,100 --> 00:34:20,390
这使得优化问题的求解非常困难

573
00:34:20,680 --> 00:34:23,650
它使用上面的这条曲线

574
00:34:23,830 --> 00:34:25,180
通过这种近似

575
00:34:25,390 --> 00:34:27,560
你就得到了一个凸优化问题

576
00:34:28,290 --> 00:34:30,400
可以找到参数使得logistic

577
00:34:30,590 --> 00:34:32,590
回归的似然性最大

578
00:34:33,510 --> 00:34:34,860
实际上

579
00:34:35,040 --> 00:34:36,220
SVM也可以

580
00:34:36,440 --> 00:34:38,260
看成是一种近似

581
00:34:38,530 --> 00:34:39,970
但是有点不同

582
00:34:40,220 --> 00:34:42,600
让我们看看  SVM实际上

583
00:34:42,880 --> 00:34:44,520
可以被看成是尝试用

584
00:34:44,700 --> 00:34:46,750
两段不同的

585
00:34:46,930 --> 00:34:49,690
线性函数进行近似

586
00:34:49,910 --> 00:34:52,450
这里是一个线性函数

587
00:34:52,660 --> 00:34:55,010
一开始方向是水平的

588
00:34:55,520 --> 00:34:57,660
到了这里开始上升

589
00:34:57,870 --> 00:34:59,910
它看起来像是铰链的形状

590
00:35:00,180 --> 00:35:01,690
所以你可以认为logistic回归

591
00:35:01,850 --> 00:35:02,750
和SVM是两种不同的近似

592
00:35:02,920 --> 00:35:05,550
都是为了近似

593
00:35:05,810 --> 00:35:09,400
最小化这个阶梯函数  明白吗?

594
00:35:10,780 --> 00:35:15,150
我们推出的这些理论--

595
00:35:15,430 --> 00:35:18,150
SVM和logistic回归

596
00:35:18,330 --> 00:35:20,670
都不是直接的ERM算法

597
00:35:20,930 --> 00:35:24,890
我们推出的这些理论给出了

598
00:35:25,080 --> 00:35:27,290
这两种算法的直观含义

599
00:35:27,460 --> 00:35:31,380
的一种合理解释

600
00:35:32,870 --> 00:35:35,030
所以这是最后一个零散的知识点

601
00:35:35,250 --> 00:35:37,540
如果你们不是很明白  不要太在意

602
00:35:38,190 --> 00:35:39,400
这部分属于比较高级的知识

603
00:35:39,610 --> 00:35:41,790
它只是为了说明SVM和

604
00:35:42,010 --> 00:35:44,240
logistic回归都可以看成是

605
00:35:44,520 --> 00:35:46,690
对ERM算法的近似

606
00:35:46,930 --> 00:35:48,710
我接下来要继续

607
00:35:48,940 --> 00:35:50,630
讲模型选择的内容

608
00:35:50,860 --> 00:35:52,070
在那之前  让我看看

609
00:35:52,230 --> 00:36:02,970
你们有什么问题 很好

610
00:36:33,410 --> 00:36:50,700
好的  上一讲我们

611
00:36:50,940 --> 00:36:52,840
讲到了学习理论

612
00:36:53,030 --> 00:36:54,210
关于这部分最后

613
00:36:54,410 --> 00:36:56,290
我们讲了VC维

614
00:36:57,440 --> 00:36:57,960
从这部分的讲解中

615
00:36:58,300 --> 00:36:59,110
我们了解到

616
00:36:59,330 --> 00:37:00,700
偏差和方差之间存在这权衡

617
00:37:00,930 --> 00:37:03,810
具体地  我们不应该选择

618
00:37:04,100 --> 00:37:08,270
过于简单或过于复杂的模型

619
00:37:08,500 --> 00:37:11,380
如果你的数据中包含二次结构

620
00:37:11,720 --> 00:37:14,680
但你用线性函数

621
00:37:14,880 --> 00:37:16,710
尝试近似它  那么会导致欠拟合

622
00:37:17,060 --> 00:37:19,060
所以你的假设的偏差很高

623
00:37:20,140 --> 00:37:21,260
相反地  如果选择

624
00:37:21,450 --> 00:37:22,860
过于复杂的模型

625
00:37:23,110 --> 00:37:24,740
那么方差会很高

626
00:37:24,900 --> 00:37:26,900
从而拟合失败

627
00:37:27,080 --> 00:37:28,400
可能会导致过拟合

628
00:37:28,600 --> 00:37:30,240
你的模型的一般适用性不会很好

629
00:37:31,800 --> 00:37:35,530
所以模型选择算法

630
00:37:35,930 --> 00:37:38,720
提供了一类方法

631
00:37:38,950 --> 00:37:40,430
可以自动地在偏差

632
00:37:40,610 --> 00:37:42,090
和方差之间进行权衡

633
00:37:42,290 --> 00:37:43,900
记得我上节课画的关于

634
00:37:44,080 --> 00:37:46,480
一般误差的图吗?

635
00:37:54,410 --> 00:37:55,670
我上节课画了这个图

636
00:37:55,880 --> 00:37:59,550
x轴表示模型复杂度

637
00:38:01,590 --> 00:38:03,200
你可以认为它表示--

638
00:38:03,500 --> 00:38:05,450
多项式的次数

639
00:38:05,670 --> 00:38:08,320
或者参数的个数等等

640
00:38:08,570 --> 00:38:10,060
如果模型过于简单

641
00:38:10,270 --> 00:38:11,480
那么你会得到一个较高的一般误差

642
00:38:11,790 --> 00:38:12,710
这实际上就是欠拟合

643
00:38:12,940 --> 00:38:14,860
如果模型过于复杂

644
00:38:15,310 --> 00:38:18,410
例如14或15次的多项式来拟合几个点

645
00:38:18,680 --> 00:38:20,580
那么你也会得到较高的一般误差

646
00:38:20,800 --> 00:38:22,360
这实际上就是过拟合

647
00:38:22,690 --> 00:38:27,340
我要做的是抽象地

648
00:38:27,520 --> 00:38:29,140
进行模型选择  明白吗?

649
00:38:29,360 --> 00:38:32,280
一些模型选择的例子可能是--


650
00:38:32,980 --> 00:38:34,720
比如  我会举这个例子--

651
00:38:34,950 --> 00:38:37,890
比如你尝试选择

652
00:38:38,390 --> 00:38:44,380
多项式的次数  对吗?

653
00:38:45,790 --> 00:38:47,620
你会选择几次的多项式?

654
00:38:47,840 --> 00:38:51,150
另一个关于模型选择的例子是

655
00:38:51,390 --> 00:38:53,410
尝试选择带宽参数

656
00:38:53,940 --> 00:38:56,100
也就是局部加权线性回归

657
00:38:57,680 --> 00:39:02,780
或其他局部加权

658
00:39:04,860 --> 00:39:06,670
回归问题中的带宽参数

659
00:39:06,910 --> 00:39:10,600
另外一个模型选择问题是

660
00:39:10,850 --> 00:39:12,630
你尝试选择SVM

661
00:39:12,850 --> 00:39:16,960
中的参数C 对吗?

662
00:39:17,700 --> 00:39:20,890
对于l1 norm

663
00:39:21,110 --> 00:39:23,390
软间隔SVM

664
00:39:23,600 --> 00:39:28,150
我们的优化目标是这样的  对吗?

665
00:39:28,640 --> 00:39:32,080
参数C控制了一种权衡关系:

666
00:39:32,320 --> 00:39:35,090
你希望你的样本会以多大的间隔被分隔?

667
00:39:35,360 --> 00:39:36,990
对于那些错误分类的

668
00:39:37,160 --> 00:39:40,540
样本惩罚力度有多大?

669
00:39:40,720 --> 00:39:43,210
这是三个模型

670
00:39:43,470 --> 00:39:46,530
选择问题的具体例子

671
00:39:47,610 --> 00:39:48,860
让我们提出一个能够

672
00:39:49,110 --> 00:39:57,000
自动选择模型的方法

673
00:40:13,890 --> 00:40:16,110
假设你有一个包含有限个模型的集合

674
00:40:16,470 --> 00:40:19,310
将这些模型表示为等等

675
00:40:19,780 --> 00:40:21,810
例如  这个可能是

676
00:40:22,310 --> 00:40:24,770
一个线性分类器

677
00:40:25,000 --> 00:40:26,760
这个可能是一个二次分类器

678
00:40:28,130 --> 00:40:30,990
等等  明白吗?

679
00:40:31,530 --> 00:40:34,620
这个集合也可以表示

680
00:40:34,820 --> 00:40:36,600
不同的带宽参数

681
00:40:36,860 --> 00:40:39,500
我们将它离散成若干不同的值

682
00:40:39,820 --> 00:40:41,410
你之后从中选取

683
00:40:41,610 --> 00:40:43,210
一个具体的值

684
00:40:43,540 --> 00:40:44,710
让我们来讨论应该

685
00:40:44,890 --> 00:40:47,060
如何选择合适的模型

686
00:40:47,320 --> 00:40:49,370
你可以做的一件事是

687
00:40:49,980 --> 00:40:51,480
对于所有的模型

688
00:40:51,720 --> 00:40:53,890
用训练集合训练它们

689
00:40:54,110 --> 00:40:56,550
之后看看哪个模型具有最小的训练误差

690
00:40:57,240 --> 00:40:59,170
这是一个很糟糕的主意

691
00:40:59,450 --> 00:41:00,750
你们知道为什么吗?

692
00:41:05,870 --> 00:41:07,100
I:很好

693
00:41:07,280 --> 00:41:09,060
因为存在过拟合

694
00:41:09,240 --> 00:41:11,110
你们中有些人已经在笑我了

695
00:41:11,280 --> 00:41:13,300
这是一个很糟糕的主意

696
00:41:13,490 --> 00:41:15,460
因为通过训练误差来选取模型的话

697
00:41:15,630 --> 00:41:16,920
很显然你会选取

698
00:41:17,100 --> 00:41:19,070
最复杂的模型  对吗?

699
00:41:19,220 --> 00:41:20,390
你可能会选择一个10次多项式

700
00:41:20,540 --> 00:41:21,820
因为它能够最好地拟合训练集合

701
00:41:25,430 --> 00:41:27,330
当我们进行模型选择是

702
00:41:27,600 --> 00:41:30,510
有很多标准的方法

703
00:41:30,800 --> 00:41:32,780
一种称之为保留交叉验证

704
00:41:38,800 --> 00:41:40,610
对于这种方法

705
00:41:40,890 --> 00:41:42,140
我们对于一个给定的训练集合

706
00:41:42,760 --> 00:41:48,140
将其随机地划分成两个自己

707
00:41:48,700 --> 00:41:51,810
我们称子集为--

708
00:41:52,150 --> 00:41:53,720
将所有的数据

709
00:41:53,910 --> 00:41:56,080
随机分成两个子集

710
00:41:56,300 --> 00:41:57,830
一个我们称之为训练子集

711
00:41:58,100 --> 00:42:02,490
另外一个称之为保留交叉验证子集

712
00:42:03,100 --> 00:42:08,530
之后我们仅仅用

713
00:42:12,040 --> 00:42:14,680
训练子集训练模型

714
00:42:15,200 --> 00:42:20,510
之后用保留交叉验证子集进行测试

715
00:42:22,550 --> 00:42:26,330
之后选择具有最小测试

716
00:42:33,270 --> 00:42:35,690
误差的模型作为结果  明白吗?

717
00:42:35,890 --> 00:42:38,280
这个方法相当直观

718
00:42:38,470 --> 00:42:40,390
通常情况下我们用

719
00:42:40,570 --> 00:42:42,240
70%的数据进行训练

720
00:42:42,420 --> 00:42:43,320
用剩下的30%数据进

721
00:42:43,530 --> 00:42:45,020
行交叉验证  选

722
00:42:45,190 --> 00:42:46,610
取具有最小误差的

723
00:42:46,850 --> 00:42:49,400
模型作为结果

724
00:42:51,150 --> 00:42:52,550
在这之后你们就可以做出选择了

725
00:42:52,770 --> 00:42:54,080
你们可以用70%的

726
00:42:54,380 --> 00:42:55,750
数据训练假设模型

727
00:42:55,910 --> 00:42:58,320
之后用保留交叉验证子集

728
00:42:58,760 --> 00:43:00,910
进行测试并选择

729
00:43:01,120 --> 00:43:02,800
具有最小误差的模型

730
00:43:02,960 --> 00:43:05,940
在这之后你还可以选择

731
00:43:06,110 --> 00:43:09,100
用100%的数据对选出的

732
00:43:09,370 --> 00:43:10,810
模型进行重新训练

733
00:43:11,020 --> 00:43:13,570
明白吗?

734
00:43:13,740 --> 00:43:15,870
每种版本都很常用

735
00:43:16,030 --> 00:43:17,280
你可以用70%

736
00:43:17,440 --> 00:43:19,320
的数据进行训练

737
00:43:19,960 --> 00:43:21,270
之后直接将选择的

738
00:43:21,440 --> 00:43:22,640
模型作为最终选择

739
00:43:22,820 --> 00:43:24,270
你也可以--比如说

740
00:43:24,470 --> 00:43:26,400
已经选择了

741
00:43:26,560 --> 00:43:27,440
多项式的次数以后

742
00:43:27,610 --> 00:43:28,520
重新回去用100%的数据

743
00:43:28,690 --> 00:43:31,120
再训练一遍模型

744
00:43:31,440 --> 00:43:32,440
每一种方法都很常用

745
00:43:50,700 --> 00:43:53,500
虽然这种方法非常直观

746
00:43:54,070 --> 00:43:56,610
但是有时在

747
00:43:56,800 --> 00:44:01,380
一些实际应用中

748
00:44:01,640 --> 00:44:04,850
对于许多机器学习的

749
00:44:05,070 --> 00:44:07,120
应用我们的数据非常少

750
00:44:07,420 --> 00:44:09,360
每个你得到的训练样本

751
00:44:09,610 --> 00:44:12,550
都需要很大的代价来获得

752
00:44:12,830 --> 00:44:17,180
有些时候你的数据是通过医学实验获得的

753
00:44:17,510 --> 00:44:20,020
每个样本都代表着一个

754
00:44:20,180 --> 00:44:22,010
病人的病痛或一些不好的经历

755
00:44:22,230 --> 00:44:24,450
如果我们说

756
00:44:24,640 --> 00:44:26,660
"我要留出30%的数据

757
00:44:26,940 --> 00:44:28,320
仅仅为了进行模型选择"

758
00:44:28,530 --> 00:44:31,170
这通常会让那些人很不愉快

759
00:44:31,720 --> 00:44:33,050
所以也许你会想

760
00:44:33,300 --> 00:44:35,250
可不可以不用那么多

761
00:44:35,480 --> 00:44:37,440
的数据进行模型选择

762
00:44:37,810 --> 00:44:40,630
有几种保留

763
00:44:40,800 --> 00:44:43,460
交叉验证的变种

764
00:44:43,660 --> 00:44:45,310
可以更为高效地利用数据

765
00:44:45,860 --> 00:44:50,840
其中一种称为k重交叉验证

766
00:44:51,580 --> 00:44:53,000
算法思想是这样的:

767
00:44:54,220 --> 00:44:55,750
我会将数据集合s

768
00:44:56,140 --> 00:44:58,900
想象一下

769
00:44:59,100 --> 00:45:01,090
我用这个方块

770
00:45:01,290 --> 00:45:03,790
表示全部数据s

771
00:45:04,100 --> 00:45:08,900
之后我会将它分成k部分

772
00:45:09,180 --> 00:45:11,460
这里我画了5部分

773
00:45:11,750 --> 00:45:14,300
我们之后会重复地用

774
00:45:14,490 --> 00:45:19,080
k-1个部分进行训练

775
00:45:23,430 --> 00:45:27,880
用剩下的一个部分

776
00:45:28,090 --> 00:45:30,710
进行测试 明白吗?

777
00:45:30,920 --> 00:45:37,450
最后我们将对这k个结果求平均

778
00:45:37,710 --> 00:45:44,320
换句话说  我们会留出--

779
00:45:44,580 --> 00:45:49,850
我会留出  比如说  1/5的数据

780
00:45:50,480 --> 00:45:52,440
只用用剩下的4/5进行训练

781
00:45:52,620 --> 00:45:53,620
用第一个1/5进行测试

782
00:45:53,830 --> 00:45:57,160
之后我会保留第二个1/5

783
00:45:57,330 --> 00:45:59,270
之后用剩下的4/5进行训练

784
00:45:59,470 --> 00:46:00,620
用1/5进行测试

785
00:46:00,800 --> 00:46:02,690
之后留出第三个1/5

786
00:46:02,900 --> 00:46:03,890
用4/5进行测试

787
00:46:04,060 --> 00:46:05,500
我会重复5次这样的过程

788
00:46:05,850 --> 00:46:08,780
之后我们会得到5个误差

789
00:46:08,970 --> 00:46:10,430
之后对它们求平均

790
00:46:10,690 --> 00:46:12,950
从而得到一个对于模型的

791
00:46:13,140 --> 00:46:15,860
一般误差的估计  明白吗?

792
00:46:16,050 --> 00:46:18,810
当你进行k重交叉验证时

793
00:46:19,050 --> 00:46:21,130
你经常需要重新用

794
00:46:21,330 --> 00:46:24,200
100%的数据对选出的

795
00:46:24,390 --> 00:46:26,400
模型进行重新训练

796
00:46:27,740 --> 00:46:29,990
这里我画了5个部分

797
00:46:30,190 --> 00:46:32,610
因为这样比较容易画

798
00:46:32,910 --> 00:46:38,690
但是通常情况下k等于10比较常见  明白吗?

799
00:46:39,400 --> 00:46:43,290
k取10的情形是很常见的

800
00:46:43,790 --> 00:46:45,980
这时我们进行的是10重交叉验证

801
00:46:46,240 --> 00:46:49,930
这种保留交叉验证的好处是

802
00:46:50,310 --> 00:46:51,790
你将数据分成了10个部分

803
00:46:52,120 --> 00:46:55,210
每次你都只需要留出1/10的数据

804
00:46:55,520 --> 00:46:57,900
而不是30%的数据

805
00:46:59,570 --> 00:47:00,760
我必须说一下  在标准的--

806
00:47:00,950 --> 00:47:02,870
在简单的保留交叉验证方法中

807
00:47:03,140 --> 00:47:05,140
30%对70%的划分是很常用的

808
00:47:05,430 --> 00:47:07,690
有时2/3-1/3或者

809
00:47:07,920 --> 00:47:09,510
70%-30%的划分是很常见的

810
00:47:09,810 --> 00:47:11,180
如果你用的是k重交叉验证

811
00:47:11,400 --> 00:47:13,860
k=5或者k=10

812
00:47:14,140 --> 00:47:15,420
都是最常见的选择

813
00:47:15,650 --> 00:47:18,350
k重交叉验证的缺点是

814
00:47:18,540 --> 00:47:20,540
它需要大量的计算

815
00:47:20,880 --> 00:47:24,040
具体地  为了验证你的模型

816
00:47:24,280 --> 00:47:26,080
你需要训练10次

817
00:47:26,320 --> 00:47:27,440
而不是仅仅训练1次

818
00:47:27,610 --> 00:47:28,790
所以对于logistic回归

819
00:47:29,000 --> 00:47:30,000
每种模型都需要

820
00:47:30,180 --> 00:47:31,300
进行10次训练

821
00:47:31,510 --> 00:47:32,400
而不是一次

822
00:47:32,540 --> 00:47:34,120
所以这种方法需要进行大量计算

823
00:47:34,430 --> 00:47:35,770
通常情况下k=10效果就很好了

824
00:47:37,550 --> 00:47:44,400
最后  这个版本

825
00:47:45,330 --> 00:47:47,550
还有一个特例

826
00:47:47,900 --> 00:47:51,050
就是你可以令k等于样本数m

827
00:47:51,810 --> 00:47:53,550
也就是说对于一个训练样本集合

828
00:47:53,920 --> 00:47:56,360
你的划分数目和

829
00:47:56,570 --> 00:47:58,310
样本数目一样多

830
00:47:59,080 --> 00:48:00,370
这个方法通常称为:

831
00:48:00,610 --> 00:48:01,900
留1交叉验证

832
00:48:06,350 --> 00:48:08,340
你需要做的是

833
00:48:09,140 --> 00:48:10,570
留出第一个样本

834
00:48:10,840 --> 00:48:11,920
用剩下的进行训练

835
00:48:12,120 --> 00:48:13,540
之后用第一个样本进行测试

836
00:48:13,780 --> 00:48:15,130
之后你留出第二个样本

837
00:48:16,760 --> 00:48:17,420
用剩下的进行训练

838
00:48:17,620 --> 00:48:18,900
再用第二个样本进行测试

839
00:48:19,100 --> 00:48:20,260
之后你留出第三个样本

840
00:48:20,460 --> 00:48:22,000
用剩下的进行训练

841
00:48:22,180 --> 00:48:23,650
再用第三个样本进行测试  以此类推

842
00:48:23,970 --> 00:48:27,090
因为你现在划分了更多的部分

843
00:48:27,300 --> 00:48:30,080
所以对数据的利用效率

844
00:48:30,270 --> 00:48:31,950
可能比k重交叉验证更高

845
00:48:32,240 --> 00:48:33,610
但是每次留出一个样本

846
00:48:33,780 --> 00:48:36,210
所需的计算量将会分长达

847
00:48:36,540 --> 00:48:38,440
因为你每次都

848
00:48:38,680 --> 00:48:40,030
需要留出一个样本

849
00:48:40,250 --> 00:48:41,290
之后用剩下的m-1个

850
00:48:41,470 --> 00:48:44,780
训练样本对模型进行训练

851
00:48:44,940 --> 00:48:46,600
你需要训练很多次

852
00:48:46,760 --> 00:48:48,830
所以需要大量的计算

853
00:48:49,010 --> 00:48:50,130
通常情况下

854
00:48:50,300 --> 00:48:52,720
仅当你的数据非常少时才会用这种方法

855
00:48:53,160 --> 00:48:55,880
如果对于一个学习问题

856
00:48:56,080 --> 00:48:58,070
比如说你只有15个训练样本

857
00:48:58,370 --> 00:49:00,070
如果你的训练样本非常少

858
00:49:00,360 --> 00:49:06,400
那么可以采用留1交叉验证的方法

859
00:49:07,130 --> 00:49:08,370
什么问题?

860
00:49:08,700 --> 00:49:11,630
S:你知道  你证明了--

861
00:49:11,830 --> 00:49:15,160
训练样本数和VC维

862
00:49:15,330 --> 00:49:17,980
也许  将训练样本

863
00:49:18,420 --> 00:49:28,080
分成几组  我们可以用--

864
00:49:28,320 --> 00:49:30,200
I:是的  我的意思是--

865
00:49:30,360 --> 00:49:32,350
S:计算训练误差

866
00:49:32,520 --> 00:49:34,560
用它来计算一般误差

867
00:49:34,740 --> 00:49:36,210
I:是的  可以  但是--

868
00:49:36,710 --> 00:49:39,530
实际上  我个人不会想这样做

869
00:49:39,800 --> 00:49:43,220
实际上  VC维提供的界

870
00:49:43,410 --> 00:49:45,050
是比较宽松的界

871
00:49:45,260 --> 00:49:48,440
有人提出了结构风险最小化

872
00:49:48,640 --> 00:49:50,230
实际上就是你想做的

873
00:49:50,420 --> 00:49:52,910
但是我个人不建议这样做

874
00:50:09,690 --> 00:50:18,210
关于交叉验证还有问题吗?什么问题?

875
00:50:40,500 --> 00:50:43,300
I:是的

876
00:50:45,120 --> 00:50:47,250
I:是的

877
00:50:48,030 --> 00:50:50,490
I:不--好的

878
00:50:50,660 --> 00:50:51,820
实际上当你们证明

879
00:50:52,010 --> 00:50:52,940
学习理论的界时

880
00:50:53,160 --> 00:50:54,930
你们证明的界通常是非常宽松的

881
00:50:55,090 --> 00:50:57,290
因为实际上你们证明的是

882
00:50:57,450 --> 00:51:00,210
一种普适的针对最坏情形的界--

883
00:51:00,450 --> 00:51:06,920
应该怎么说呢--

884
00:51:07,130 --> 00:51:08,390
这是我之前证明的界  对吗?

885
00:51:08,600 --> 00:51:10,010
这个界对于任何分布的

886
00:51:10,200 --> 00:51:13,600
训练样本都是成立的  明白吗?

887
00:51:13,800 --> 00:51:15,080
假设我们的训练样本

888
00:51:15,310 --> 00:51:16,140
都是独立同分布的

889
00:51:16,350 --> 00:51:18,320
由某个概率分布D随机生成

890
00:51:18,860 --> 00:51:20,600
我们证明的界对于

891
00:51:20,810 --> 00:51:25,480
任意的D都成立

892
00:51:26,010 --> 00:51:29,400
有些时候你在日常生活中

893
00:51:29,700 --> 00:51:32,180
得到的分布

894
00:51:32,380 --> 00:51:34,060
例如像房屋的价格什么的

895
00:51:34,420 --> 00:51:36,830
可能并不想

896
00:51:37,010 --> 00:51:39,190
最坏情形那么差  明白吗?

897
00:51:39,350 --> 00:51:42,380
实际上如果你将

898
00:51:42,570 --> 00:51:44,650
界中的常量代入的话

899
00:51:44,900 --> 00:51:47,190
你经常会得到一个非常大的数

900
00:51:47,650 --> 00:51:50,060
以logistic回归为例--

901
00:51:50,310 --> 00:51:52,400
对于一个有10个参数

902
00:51:52,620 --> 00:51:55,580
错误界为0.01  而且

903
00:51:55,750 --> 00:51:56,710
概率为95%的logistic回归模型

904
00:51:56,910 --> 00:51:58,340
我应该需要多少训练样本?

905
00:51:58,530 --> 00:51:59,890
如果你将这些常量

906
00:52:00,110 --> 00:52:01,670
代入到学习理论的接种

907
00:52:01,930 --> 00:52:04,680
你会得到对于所需

908
00:52:04,870 --> 00:52:06,120
样本数的非常悲观的估计

909
00:52:06,380 --> 00:52:08,920
你可能会得到一个出奇大的数

910
00:52:09,140 --> 00:52:10,610
你可能发现需要10000个

911
00:52:10,790 --> 00:52:12,350
训练样本来拟合出10个参数

912
00:52:12,580 --> 00:52:19,270
一种好的对于学习理论界的

913
00:52:19,470 --> 00:52:21,060
理解方式是--这就是为什么

914
00:52:21,300 --> 00:52:24,410
当我在论文中写到学习理论的界时

915
00:52:24,620 --> 00:52:27,890
我通常会用大O符号表示

916
00:52:28,090 --> 00:52:30,150
而完全不考虑常量因子

917
00:52:30,340 --> 00:52:31,780
因为我们得到的界是非常宽松的

918
00:52:32,150 --> 00:52:34,900
有一些工作尝试用

919
00:52:35,110 --> 00:52:40,220
这些界指导模型的选择

920
00:52:40,510 --> 00:52:43,540
但是我个人倾向于用

921
00:52:43,810 --> 00:52:47,440
这些界来给出一些比较直观的理解

922
00:52:47,680 --> 00:52:52,470
例如  你需要的样本数

923
00:52:52,660 --> 00:52:55,170
和你的参数数量呈线性关系

924
00:52:55,380 --> 00:52:57,520
或者呈指数关系

925
00:52:57,740 --> 00:52:59,700
它是不是一个二次函数?

926
00:52:59,880 --> 00:53:01,390
所以通常情况下

927
00:53:01,630 --> 00:53:03,480
界的形状--

928
00:53:03,750 --> 00:53:04,830
你所需要的训练样本数

929
00:53:05,020 --> 00:53:06,390
和VC维呈线性关系

930
00:53:06,610 --> 00:53:08,100
这是你可以从这些理论中

931
00:53:08,330 --> 00:53:10,110
得到的有用的直观理解

932
00:53:10,350 --> 00:53:12,590
但是界的量级对于你的

933
00:53:12,840 --> 00:53:14,190
具体问题来说

934
00:53:14,490 --> 00:53:17,580
会非常宽松

935
00:53:18,150 --> 00:53:20,060
这回答你的问题了吗?

936
00:53:20,390 --> 00:53:22,120
S:是的

937
00:53:26,080 --> 00:53:27,670
I:好的 顺便提一句

938
00:53:27,890 --> 00:53:30,580
事实证明  我经常会使用一个经验法则

939
00:53:30,820 --> 00:53:32,640
例如  当你尝试拟合

940
00:53:32,840 --> 00:53:34,410
logistic回归模型时

941
00:53:34,900 --> 00:53:38,060
如果你有n个参数或n+1个参数

942
00:53:38,360 --> 00:53:40,550
那么如果训练样本数

943
00:53:40,750 --> 00:53:42,630
位10倍的参数数量

944
00:53:42,870 --> 00:53:44,420
那么你很可能会拟合出不错的形状

945
00:53:44,680 --> 00:53:46,110
如果训练样本数是

946
00:53:46,310 --> 00:53:47,870
几倍的参数数量

947
00:53:48,170 --> 00:53:51,570
那么也很可能拟合出不错的模型

948
00:53:52,630 --> 00:53:54,160
这些是你可以从界中

949
00:53:54,380 --> 00:53:56,470
得到的有用的直观理解

950
00:53:58,460 --> 00:54:00,080
S:在交叉验证中

951
00:54:00,260 --> 00:54:01,670
我们是否随机地划分这些样本?

952
00:54:01,840 --> 00:54:04,580
I:是的 习惯上我们会

953
00:54:04,780 --> 00:54:08,010
随机地对样本进行划分

954
00:54:23,110 --> 00:54:25,510
关于模型选择我还要讲一点

955
00:54:25,740 --> 00:54:27,830
有一种模型选择的特例

956
00:54:28,090 --> 00:54:29,640
称之为特征选择问题

957
00:54:39,150 --> 00:54:41,030
直观理解是这样的

958
00:54:41,270 --> 00:54:43,160
对于许多机器学习问题

959
00:54:43,400 --> 00:54:46,080
你可能会面对一个非常高维的特征空间

960
00:54:46,310 --> 00:54:50,560
输入特征向量x的维数可能非常高


961
00:54:51,710 --> 00:54:53,430
例如  对于文本分类--

962
00:54:53,930 --> 00:54:55,240
我之前提到过的一个

963
00:54:55,480 --> 00:54:57,060
文本分类的例子是区分垃圾邮件

964
00:54:57,460 --> 00:54:59,820
这个问题中特征空间的维数

965
00:55:00,010 --> 00:55:02,160
很容易达到30000或50000

966
00:55:02,460 --> 00:55:05,710
我记得之前的例子中我用的是50000

967
00:55:06,990 --> 00:55:08,640
所以如果你有那么多的特征

968
00:55:08,870 --> 00:55:10,140
50000个特征

969
00:55:10,850 --> 00:55:12,510
根据你用的学习算法

970
00:55:12,900 --> 00:55:14,780
可能存在过拟合的风险

971
00:55:15,300 --> 00:55:18,400
如果你能够

972
00:55:18,660 --> 00:55:20,730
减少特征的数量

973
00:55:20,940 --> 00:55:22,520
你也许就可以减少

974
00:55:22,710 --> 00:55:23,520
学习算法的方差

975
00:55:23,700 --> 00:55:25,240
从而降低过拟合的风险

976
00:55:25,470 --> 00:55:27,290
对于文本分类这个例子

977
00:55:28,200 --> 00:55:29,120
你可以想象也许

978
00:55:29,330 --> 00:55:31,460
只有少量的相关特征

979
00:55:31,810 --> 00:55:32,860
对于英文单词

980
00:55:33,200 --> 00:55:34,850
许多英文单词不会告诉你

981
00:55:35,090 --> 00:55:36,520
关于这封邮件是否

982
00:55:36,750 --> 00:55:38,580
为垃圾邮件的任何信息

983
00:55:39,200 --> 00:55:41,880
你知道  对于像"like"

984
00:55:42,070 --> 00:55:44,500
"the"  "of"  "a"  "and"这样的词

985
00:55:44,720 --> 00:55:45,970
它们不会告诉你任何关于

986
00:55:46,180 --> 00:55:47,550
邮件是否为垃圾邮件的信息

987
00:55:47,750 --> 00:55:49,640
所以这样会有少得多的特征

988
00:55:49,870 --> 00:55:53,360
是和学习问题真正相关的

989
00:55:53,840 --> 00:55:55,120
例如  如果你看到了诸如"buy"

990
00:55:55,360 --> 00:55:56,760
"Viagra"这样的词

991
00:55:57,000 --> 00:55:59,250
它们可能对于

992
00:55:59,420 --> 00:56:00,880
判别垃圾邮件非常有用

993
00:56:01,420 --> 00:56:02,740
如果你看到像"Stanford"

994
00:56:02,930 --> 00:56:04,770
"Machine Learning"  或者你的名字

995
00:56:05,000 --> 00:56:06,500
这样的词  它们可能是

996
00:56:06,700 --> 00:56:09,020
另外一类有助于判断的词

997
00:56:09,370 --> 00:56:11,440
所以在特征选择问题中

998
00:56:11,950 --> 00:56:14,290
我们会从原始的特征中

999
00:56:14,470 --> 00:56:17,430
选出一个子集

1000
00:56:17,600 --> 00:56:18,880
我们认为这个子集对于一个

1001
00:56:19,050 --> 00:56:20,480
特定的学习问题来说是最为相关的

1002
00:56:20,670 --> 00:56:23,040
这样我们有了一个更为简单的假设类

1003
00:56:23,210 --> 00:56:25,310
可以从中选取假设

1004
00:56:25,980 --> 00:56:28,000
因此  我们降低了过拟合的风险

1005
00:56:28,300 --> 00:56:33,130
即使我们一开始有50000个特征

1006
00:56:33,340 --> 00:56:37,520
你们应该怎样做呢?

1007
00:56:37,690 --> 00:56:42,170
如果你们有n个特征

1008
00:56:44,140 --> 00:56:52,610
那么一共可能有个可能的子集  对吗?

1009
00:56:54,750 --> 00:56:56,680
因为你知道  每个特征

1010
00:56:56,860 --> 00:56:58,570
都可能被选或不被选

1011
00:56:58,760 --> 00:57:00,160
所以一共有种可能

1012
00:57:00,440 --> 00:57:02,300
这是一个巨大的空间

1013
00:57:02,980 --> 00:57:04,990
在进行模型选取时

1014
00:57:05,230 --> 00:57:07,870
我们通常会使用不同的

1015
00:57:08,040 --> 00:57:10,340
启发式规则进行搜索--

1016
00:57:10,560 --> 00:57:12,290
我们用简单的搜索算法

1017
00:57:12,500 --> 00:57:14,630
尝试在这种可能的子集

1018
00:57:14,970 --> 00:57:17,220
构成的空间中进行搜索

1019
00:57:17,800 --> 00:57:20,240
这个空间太大了

1020
00:57:20,410 --> 00:57:21,900
我们不可能对每一种可能的子集进行枚举

1021
00:57:22,120 --> 00:57:24,620
来看一个具体的例子

1022
00:57:25,640 --> 00:57:28,490
这个算法被称为前向搜索算法

1023
00:57:28,760 --> 00:57:30,740
也被称为前向选择算法

1024
00:57:31,010 --> 00:57:33,810
这个算法非常简单

1025
00:57:34,130 --> 00:57:35,590
但是我会把它写出来

1026
00:57:35,800 --> 00:57:37,520
它写出来的形式实际上

1027
00:57:37,690 --> 00:57:38,930
比它实际的形式看起来更复杂

1028
00:57:40,050 --> 00:57:42,410
首先--初始化特征子集F

1029
00:57:45,730 --> 00:57:49,870
之后重复进行下列过程

1030
00:57:52,120 --> 00:57:59,720
对i等于1到n

1031
00:58:04,950 --> 00:58:13,030
尝试将特征i加到F中

1032
00:58:14,180 --> 00:58:20,840
对模型进行交叉验证

1033
00:58:25,750 --> 00:58:28,940
这里的交叉验证可以是任意一种形式

1034
00:58:29,200 --> 00:58:30,790
简单的保留交叉验证

1035
00:58:31,010 --> 00:58:32,200
或者k重交叉验证

1036
00:58:32,440 --> 00:58:34,160
或者留1保留交叉验证

1037
00:58:34,650 --> 00:58:42,840
之后  F应该等于

1038
00:58:43,290 --> 00:58:53,120
F并上(1)中的到的最好特征  明白吗?

1039
00:58:57,560 --> 00:59:01,260
最终  你将--明白吗?

1040
00:59:16,480 --> 00:59:18,420
所以前向选择算法的过程是:

1041
00:59:18,620 --> 00:59:20,230
首先从一个空集F开始

1042
00:59:20,520 --> 00:59:22,150
每次迭代

1043
00:59:22,450 --> 00:59:27,530
都考虑那些还没有被加到F中的特征

1044
00:59:27,720 --> 00:59:29,460
尝试将一个特征加入F

1045
00:59:29,770 --> 00:59:31,840
你对所有情况进行训练

1046
00:59:32,050 --> 00:59:34,060
之后利用交叉验证进行评估

1047
00:59:34,830 --> 00:59:36,860
之后  找到最好的一个特征

1048
00:59:37,090 --> 00:59:39,030
加入到F中

1049
00:59:39,580 --> 00:59:41,980
在第(2)步中

1050
00:59:42,220 --> 00:59:45,180
你将找到的特征加入到F中

1051
00:59:45,490 --> 00:59:47,320
这里我所说的最好的

1052
00:59:47,510 --> 00:59:49,710
特征或最好的模型

1053
00:59:49,900 --> 00:59:51,050
是指根据保留交叉验证

1054
00:59:51,250 --> 00:59:52,910
算法所选取的最好的特征

1055
00:59:53,430 --> 00:59:57,030
也就是说我希望找到

1056
00:59:57,260 --> 01:00:00,760
加入后具有最小的保留

1057
01:00:01,610 --> 01:00:03,360
交叉验证或交叉验证误差的特征

1058
01:00:03,560 --> 01:00:05,400
所以你一次只会加入一个特征

1059
01:00:05,660 --> 01:00:11,000
如果F中添加了所有的特征

1060
01:00:11,390 --> 01:00:13,370
那么算法就会结束

1061
01:00:13,580 --> 01:00:16,160
此时F包含所有的特征

1062
01:00:16,390 --> 01:00:17,530
这时你可以结束算法

1063
01:00:17,750 --> 01:00:20,220
或者根据一些经验法则

1064
01:00:20,470 --> 01:00:22,650
你知道你可能不

1065
01:00:22,830 --> 01:00:24,440
需要超过k个特征

1066
01:00:24,670 --> 01:00:25,840
你可能通过检查F的大小

1067
01:00:26,140 --> 01:00:28,460
是否超过了某个阈值k

1068
01:00:28,660 --> 01:00:30,400
来决定是否结束算法

1069
01:00:30,570 --> 01:00:32,100
可能你有100个训练样本

1070
01:00:32,320 --> 01:00:33,620
如果你在拟合logistic回归

1071
01:00:33,910 --> 01:00:35,320
你可能知道你不希望

1072
01:00:35,520 --> 01:00:36,980
有超过100个特征

1073
01:00:37,210 --> 01:00:40,110
所以当你发现F中

1074
01:00:40,300 --> 01:00:41,800
已经有100个特征时

1075
01:00:41,990 --> 01:00:43,670
算法就会结束 明白吗?

1076
01:00:45,820 --> 01:00:46,960
最后  进行了这些工作之后

1077
01:00:47,680 --> 01:00:49,350
找到最好的假设

1078
01:00:49,670 --> 01:00:51,070
这里所说的最好的假设  意思是说

1079
01:00:51,410 --> 01:00:52,750
当运行学习算法时

1080
01:00:53,030 --> 01:00:54,490
你可能会见到许多假设

1081
01:00:54,730 --> 01:00:55,960
你们需要训练大量的假设

1082
01:00:56,160 --> 01:00:57,630
并用交叉验证来测试它们

1083
01:00:57,960 --> 01:00:59,690
当我说输出最好的假设时

1084
01:00:59,920 --> 01:01:02,050
我的意思是  对于所有的

1085
01:01:02,260 --> 01:01:03,190
假设都首先用这个

1086
01:01:03,400 --> 01:01:04,790
过程进行特征选择

1087
01:01:05,040 --> 01:01:05,950
之后用交叉验证从所有

1088
01:01:06,150 --> 01:01:07,720
假设中选择具有

1089
01:01:07,960 --> 01:01:10,530
最小误差的那个假设  明白吗?

1090
01:01:10,710 --> 01:01:11,850
这就是前向选择

1091
01:01:35,290 --> 01:01:37,630
给这种算法七个名字

1092
01:01:37,840 --> 01:01:39,010
它是一个被称为

1093
01:01:39,210 --> 01:01:43,940
"封装特征选择"的例子

1094
01:01:47,860 --> 01:01:51,350
封装这个词指的是这个意思

1095
01:01:51,600 --> 01:01:53,830
刚才我介绍的算法是

1096
01:01:54,070 --> 01:01:55,770
前向选择或前向搜索算法

1097
01:01:56,020 --> 01:01:59,040
它像一个包装一样封装

1098
01:01:59,250 --> 01:02:00,780
在学习算法的外面

1099
01:02:01,030 --> 01:02:03,130
这意味着  当你进行前向选择的时候

1100
01:02:03,350 --> 01:02:05,660
你需要重复地使用

1101
01:02:05,960 --> 01:02:10,230
学习算法去训练模型

1102
01:02:10,540 --> 01:02:13,740
根据其结果选择特征子集 明白吗?

1103
01:02:14,270 --> 01:02:16,520
所以这样的算法被称为封装特征选择算法

1104
01:02:16,960 --> 01:02:19,820
这个算法的计算量会很大

1105
01:02:20,210 --> 01:02:21,850
因为当你进行搜索时

1106
01:02:22,100 --> 01:02:24,050
你一遍又一遍

1107
01:02:24,240 --> 01:02:26,180
地针对不同的

1108
01:02:26,380 --> 01:02:28,180
特征子集训练模型

1109
01:02:30,530 --> 01:02:31,790
让我们提一句

1110
01:02:32,070 --> 01:02:34,010
有一个算法的变形称之为

1111
01:02:34,210 --> 01:02:39,630
后向搜索或后向选择

1112
01:02:40,050 --> 01:02:43,930
你用所有的

1113
01:02:44,140 --> 01:02:48,080
特征集合初始化F

1114
01:02:49,460 --> 01:02:59,830
之后每次从F中删除一个特征  明白吗?

1115
01:03:00,210 --> 01:03:05,800
这就是后向搜索或后向选择算法

1116
01:03:06,340 --> 01:03:09,300
这是你可能用到的

1117
01:03:09,490 --> 01:03:10,870
另外一个特征选择算法

1118
01:03:12,620 --> 01:03:15,430
这样做是否有意义--

1119
01:03:15,730 --> 01:03:17,550
如果开始用

1120
01:03:17,760 --> 01:03:18,980
所有的特征初始化F

1121
01:03:19,180 --> 01:03:21,100
没有意义的话就会出现问题

1122
01:03:21,440 --> 01:03:24,950
如果你有100个训练样本和10000个特征

1123
01:03:25,680 --> 01:03:27,200
这经常会发生--

1124
01:03:27,550 --> 01:03:32,460
100封电子邮件和10000个特征

1125
01:03:32,680 --> 01:03:33,890
那么100个训练样本

1126
01:03:34,150 --> 01:03:35,540
取决于你使用的学习算法

1127
01:03:35,730 --> 01:03:36,760
将F初始化为

1128
01:03:36,940 --> 01:03:37,880
全部的特征

1129
01:03:38,060 --> 01:03:40,350
并且用全部特征训练模型

1130
01:03:40,610 --> 01:03:42,500
可能有意义  也可能没有意义

1131
01:03:42,780 --> 01:03:43,640
如果没有意义的话

1132
01:03:43,840 --> 01:03:45,250
你可以用全部的特征训练模型

1133
01:03:45,430 --> 01:03:47,300
那么前向选择算法会更常用一些

1134
01:03:49,600 --> 01:03:54,040
让我看看

1135
01:03:54,530 --> 01:03:55,870
封装特征选择算法

1136
01:03:56,070 --> 01:03:57,330
通常工作效果不错

1137
01:03:57,650 --> 01:04:00,430
实际上  它经常比我稍后

1138
01:04:00,640 --> 01:04:02,310
要讲的这一类

1139
01:04:02,510 --> 01:04:03,600
算法效果更好

1140
01:04:03,800 --> 01:04:04,600
但是它们的主要缺点是

1141
01:04:04,760 --> 01:04:06,210
它们需要大量的计算

1142
01:04:08,750 --> 01:04:10,680
在我继续开始讲之前你们还有问题吗?

1143
01:04:12,990 --> 01:04:16,400
什么问题?

1144
01:04:16,600 --> 01:04:18,710
I:是的  你是对的

1145
01:04:19,060 --> 01:04:21,060
前向搜索和后向搜索

1146
01:04:21,330 --> 01:04:22,660
都是一种启发式搜索

1147
01:04:22,830 --> 01:04:26,550
你不能保证它们会

1148
01:04:26,740 --> 01:04:28,470
找到最好的特征子集

1149
01:04:28,890 --> 01:04:32,530
实际上很多关于特征

1150
01:04:32,800 --> 01:04:33,990
选择问题的形式化定义都表明

1151
01:04:34,200 --> 01:04:35,680
选取最好的特征子集

1152
01:04:35,880 --> 01:04:37,440
实际上是一个NP难问题

1153
01:04:40,200 --> 01:04:41,530
但是实际应用中

1154
01:04:41,760 --> 01:04:42,920
前向选择和后向选择的

1155
01:04:43,090 --> 01:04:44,360
工作效果都还不错

1156
01:04:44,560 --> 01:04:45,690
你也可以用其他的方法

1157
01:04:45,870 --> 01:04:47,660
对这种可能的

1158
01:04:47,850 --> 01:04:49,150
特征子集构成

1159
01:04:49,320 --> 01:04:51,360
的空间进行搜索

1160
01:04:51,910 --> 01:04:58,710
让我看看

1161
01:05:06,220 --> 01:05:08,240
封装特征选择在计算能力

1162
01:05:08,470 --> 01:05:10,450
可以满足要求的情况下效果是不错的

1163
01:05:11,480 --> 01:05:15,140
但是对于文本分类这样的问题--

1164
01:05:15,480 --> 01:05:17,550
实际上对于文本分类

1165
01:05:17,960 --> 01:05:19,520
由于特征数目非常多

1166
01:05:19,770 --> 01:05:21,560
很容易达到50000个特征

1167
01:05:21,910 --> 01:05:24,510
前向选择的计算量将会非常非常大

1168
01:05:24,760 --> 01:05:27,570
有另外一类算法

1169
01:05:27,790 --> 01:05:32,150
可能一般误差不会太低

1170
01:05:32,380 --> 01:05:34,310
从而导致假设的工作效果不是很好

1171
01:05:34,630 --> 01:05:36,740
但是这类算法需要很少量的计算

1172
01:05:37,210 --> 01:05:39,830
我们将其称为过

1173
01:05:40,490 --> 01:05:44,540
滤特征选择算法

1174
01:05:47,270 --> 01:05:49,300
基本思想是

1175
01:05:49,930 --> 01:05:53,270
对于每个特征i

1176
01:05:53,800 --> 01:06:00,350
我们都要计算一些衡量标准

1177
01:06:04,320 --> 01:06:25,590
来衡量对于y的影响有多大 明白吗?

1178
01:06:25,900 --> 01:06:26,610
为了完成这个目的

1179
01:06:26,820 --> 01:06:28,120
我们需要用到一些简单的启发式规则

1180
01:06:28,540 --> 01:06:29,960
对于每个特征

1181
01:06:30,190 --> 01:06:33,670
我们尝试粗略地估计或

1182
01:06:33,750 --> 01:06:52,980
计算出对于y的影响度

1183
01:06:54,070 --> 01:06:56,180
有很多方法

1184
01:06:56,410 --> 01:06:58,380
一种方法是你可以

1185
01:06:58,580 --> 01:07:00,490
计算和y的相关度

1186
01:07:00,690 --> 01:07:02,430
对于每个特征都计算

1187
01:07:02,650 --> 01:07:05,910
它和y的相关度

1188
01:07:06,400 --> 01:07:10,190
之后选取和y相关度最高的k个特征

1189
01:07:12,790 --> 01:07:15,740
另外一种方式是--

1190
01:07:44,910 --> 01:07:53,310
对于文本分类来说

1191
01:07:53,540 --> 01:07:54,760
有另外一种方法

1192
01:07:54,970 --> 01:07:56,330
尤其是对于这k个特征--

1193
01:07:56,570 --> 01:08:02,010
有一种衡量影响度的

1194
01:08:02,230 --> 01:08:05,320
常用方法  称之为相互信息

1195
01:08:11,170 --> 01:08:15,050
我在problem set里会更多地介绍这个概念

1196
01:08:15,530 --> 01:08:17,130
但是我这里只是简要地说明

1197
01:08:17,520 --> 01:08:20,170
特征与y之间的相互信息

1198
01:08:22,460 --> 01:08:24,380
我会将定义写出来

1199
01:08:27,540 --> 01:08:29,530
比如说  这是一个文本分类问题

1200
01:08:29,760 --> 01:08:31,730
所以x可以取0或1

1201
01:08:32,240 --> 01:08:34,980
和y之间的相互信息应该等于

1202
01:08:35,180 --> 01:08:37,080
对于所有可能的x

1203
01:08:37,480 --> 01:08:39,330
以及所有可能的y

1204
01:08:40,120 --> 01:08:52,270
要对概率这个概率分布乘上这个式子求和

1205
01:08:53,110 --> 01:09:00,850
所有的这些概率分布--

1206
01:09:01,120 --> 01:09:03,610
这里的和y的joint概率

1207
01:09:04,120 --> 01:09:06,140
可以用训练数据进行估计

1208
01:09:12,750 --> 01:09:14,840
包括这些要用到的量也可以

1209
01:09:15,160 --> 01:09:17,230
你可以从训练数据中估计出:

1210
01:09:17,430 --> 01:09:18,690
x=0的概率

1211
01:09:18,910 --> 01:09:20,080
x=1的概率

1212
01:09:20,260 --> 01:09:22,030
x=0 y=0的概率

1213
01:09:22,230 --> 01:09:24,790
x=0 y=1的概率  等等

1214
01:09:25,950 --> 01:09:33,300
实际上  有一个标准的信息论中的概念

1215
01:09:33,580 --> 01:09:36,130
可以用来度量不同的概率分布之间的差异

1216
01:09:36,600 --> 01:09:38,590
这个结论我不会证明了

1217
01:09:38,810 --> 01:09:45,560
事实证明  相互信息就是--

1218
01:09:51,720 --> 01:09:56,260
用来度量不同的概率分布之间的差异的

1219
01:09:56,480 --> 01:09:57,830
概念被称之为KL距离

1220
01:09:58,110 --> 01:09:59,940
如果你们上过信息论的课

1221
01:10:00,220 --> 01:10:02,080
你们可能见过这个结论

1222
01:10:02,310 --> 01:10:03,360
将相互信息表示成KL距离

1223
01:10:03,590 --> 01:10:04,630
但是如果你没有上过  不用担心

1224
01:10:04,830 --> 01:10:06,340
你们只需要直观地了解

1225
01:10:06,540 --> 01:10:08,530
KL距离是一种关于

1226
01:10:08,770 --> 01:10:10,360
两个概率分布之间差异的

1227
01:10:10,570 --> 01:10:11,910
一种正式的度量标准

1228
01:10:12,260 --> 01:10:13,790
相互信息实际上度量的是

1229
01:10:14,030 --> 01:10:20,470
两个变量x  y之间的joint概率分布

1230
01:10:21,170 --> 01:10:23,810
与两个概率分布相乘

1231
01:10:24,080 --> 01:10:24,960
得到的概率分布之间的差异

1232
01:10:25,170 --> 01:10:27,380
如果你假设它们是独立的

1233
01:10:27,560 --> 01:10:28,870
如果x和y是独立的

1234
01:10:29,440 --> 01:10:32,950
那么P(x  y)应该等于P(x)P(y)

1235
01:10:33,660 --> 01:10:35,540
所以这两个概率

1236
01:10:35,800 --> 01:10:37,220
分布是相同的

1237
01:10:37,540 --> 01:10:38,950
所以KL距离应该是0

1238
01:10:39,270 --> 01:10:42,860
相反地  如果x与y之间的依赖性很高--

1239
01:10:43,150 --> 01:10:44,400
换句话说  如果x

1240
01:10:44,630 --> 01:10:46,020
对于y的值影响很大

1241
01:10:46,350 --> 01:10:48,680
那么KL距离将会很大

1242
01:10:49,090 --> 01:10:51,350
所以相互信息是一种关于

1243
01:10:51,600 --> 01:10:54,670
x和y之间关联性的一种正式的度量

1244
01:10:55,120 --> 01:10:56,300
如果x和y是高度关联的

1245
01:10:56,600 --> 01:10:57,650
那么这意味着通过x

1246
01:10:57,950 --> 01:11:00,240
很有可能会了解

1247
01:11:00,500 --> 01:11:01,570
一些关于y的信息

1248
01:11:01,840 --> 01:11:03,730
所以它们之间的相互信息将会很大

1249
01:11:04,030 --> 01:11:07,270
这种度量方式会告诉你x

1250
01:11:07,540 --> 01:11:08,870
可能是一个很好的特征

1251
01:11:09,620 --> 01:11:10,980
你可以在problem set

1252
01:11:11,170 --> 01:11:12,600
中慢慢研究

1253
01:11:12,790 --> 01:11:13,940
我这里不再多说了

1254
01:11:14,200 --> 01:11:17,710
你之后需要做的是--

1255
01:11:17,920 --> 01:11:19,800
选择了像相关度或者

1256
01:11:20,020 --> 01:11:22,180
相互信息这样的衡量标准之后

1257
01:11:22,480 --> 01:11:29,540
你需要选取前k个特征

1258
01:11:30,140 --> 01:11:33,140
这意味着你们对于所有特征

1259
01:11:33,350 --> 01:11:34,900
计算和y之间的相关度

1260
01:11:35,090 --> 01:11:36,450
或者相互信息

1261
01:11:36,730 --> 01:11:38,960
之后你再学习算法中

1262
01:11:39,230 --> 01:11:42,400
选取具有最大的相关度或

1263
01:11:42,670 --> 01:11:44,930
相互信息的前k个特征

1264
01:11:45,340 --> 01:11:47,330
我们还需要处理k的选择问题

1265
01:11:55,200 --> 01:11:58,090
你们可以使用交叉验证的方法  明白吗?

1266
01:11:58,300 --> 01:12:00,080
对于所有的特征

1267
01:12:00,390 --> 01:12:03,040
你可以将它们按照相互信息由大到小排序

1268
01:12:03,250 --> 01:12:05,880
之后依次选取前一个特征

1269
01:12:06,160 --> 01:12:08,710
前两个特征  前三个特征  等等

1270
01:12:08,960 --> 01:12:11,690
之后用交叉验证的方法

1271
01:12:11,970 --> 01:12:17,350
决定应该用几个特征  明白吗?

1272
01:12:17,550 --> 01:12:18,540
或者有时候你也

1273
01:12:18,720 --> 01:12:20,420
可以手动指定k的值

1274
01:12:22,440 --> 01:12:35,210
好的 有问题吗?很好

1275
01:12:35,430 --> 01:12:37,620
下节课我们继续

1276
01:12:37,900 --> 01:12:40,100
我会讲贝叶斯模型选择

1277
01:12:40,190 --> 01:12:41,130
模型选择部分已经快讲完了

