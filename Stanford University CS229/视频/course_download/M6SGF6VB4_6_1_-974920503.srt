1
00:00:23,290 --> 00:00:26,040
我今天要讲的内容

2
00:00:26,160 --> 00:00:29,620
包括对朴素贝叶斯的进一步讨论

3
00:00:29,730 --> 00:00:31,390
这是我上节课

4
00:00:31,550 --> 00:00:34,390
开始讲的学习算法

5
00:00:34,520 --> 00:00:37,410
之后我要讲几个

6
00:00:37,520 --> 00:00:39,140
朴素贝叶斯的事件模型

7
00:00:39,270 --> 00:00:41,620
之后我们额外地

8
00:00:41,730 --> 00:00:43,060
简单地介绍一下神经网络

9
00:00:43,170 --> 00:00:44,820
关于它

10
00:00:44,930 --> 00:00:46,380
我不会花太多时间

11
00:00:46,500 --> 00:00:48,880
之后我会开始讲

12
00:00:48,990 --> 00:00:50,010
支持向量机

13
00:00:50,130 --> 00:00:51,430
支持向量机

14
00:00:51,520 --> 00:00:53,490
是一种

15
00:00:53,590 --> 00:00:55,320
监督学习算法

16
00:00:55,440 --> 00:00:59,030
很多人认为

17
00:00:59,150 --> 00:01:01,990
它在众多的监督学习算法中是最高效的

18
00:01:02,110 --> 00:01:03,520
这种观点值得商榷

19
00:01:03,610 --> 00:01:04,980
但是确实有很多人

20
00:01:05,090 --> 00:01:06,000
持有这样的观点

21
00:01:06,150 --> 00:01:07,870
我们今天就讲一下这个算法

22
00:01:07,970 --> 00:01:09,560
大概会用

23
00:01:09,670 --> 00:01:11,180
几张讲义完成

24
00:01:11,270 --> 00:01:14,680
让我们来看一下朴素贝叶斯

25
00:01:14,790 --> 00:01:17,770
回顾一下上一讲

26
00:01:17,880 --> 00:01:25,750
我将垃圾邮件分类

27
00:01:25,850 --> 00:01:27,880
作为朴素贝叶斯的最经典的例子

28
00:01:27,990 --> 00:01:30,370
我们创造一个这样的特征限量

29
00:01:30,540 --> 00:01:34,930
每一项对应着词典中的一个词

30
00:01:35,030 --> 00:01:40,340
之后  根据

31
00:01:40,460 --> 00:01:42,010
每个词是否出现在邮件中

32
00:01:42,120 --> 00:01:44,300
我们得到了一个特征向量

33
00:01:44,480 --> 00:01:47,130
和每个词对应的取值为0或1

34
00:01:47,260 --> 00:01:52,060
朴素贝叶斯是一个生成学习算法

35
00:01:52,150 --> 00:02:01,330
这意味着这个算法

36
00:02:01,440 --> 00:02:03,220
是对p(x|y)进行建模

37
00:02:03,320 --> 00:02:07,400
对于朴素贝叶斯来说

38
00:02:07,520 --> 00:02:11,740
我们应该将其建模成∏_(i=1)^n?

39
00:02:11,840 --> 00:02:14,700
〖p(x_i |y)〗

40
00:02:14,700 --> 00:02:18,660
与此同时我们还需要对p(y)建模，

41
00:02:18,780 --> 00:02:20,900
之后根据贝叶斯公式

42
00:02:21,020 --> 00:02:22,010
将这两项合在一起

43
00:02:22,120 --> 00:02:24,570
所以我们需要预测

44
00:02:24,670 --> 00:02:26,430
你给定的电子邮件

45
00:02:26,540 --> 00:02:27,990
是否为垃圾邮件

46
00:02:28,790 --> 00:02:31,770
所以你会得到: argmax_y^  p(y|x)

47
00:02:31,900 --> 00:02:35,900
根据贝叶斯公式

48
00:02:36,020 --> 00:02:40,540
它应该等于argmax_y  p(x│y)p(y)   对吗?

49
00:02:40,660 --> 00:02:42,520
这就是朴素贝叶斯方法

50
00:02:42,650 --> 00:02:45,250
注意两件事

51
00:02:45,370 --> 00:02:47,070
第一  在这个模型中

52
00:02:47,190 --> 00:02:50,790
每个特征的取值都只能是0或1

53
00:02:50,900 --> 00:02:53,060
表示不同的词是否出现

54
00:02:53,150 --> 00:02:57,960
第二  特征向量的长度

55
00:02:58,020 --> 00:02:59,210
也就是这个特征向量的长度n

56
00:02:59,320 --> 00:03:01,940
应该等于词典中词的数目

57
00:03:02,060 --> 00:03:10,580
所以  长度的数量级

58
00:03:10,710 --> 00:03:13,710
比如说  可以是50000

59
00:03:13,820 --> 00:03:19,030
我现在要做的是

60
00:03:19,130 --> 00:03:21,050
是介绍算法的两个变化版本

61
00:03:21,160 --> 00:03:23,350
第一个比较简单

62
00:03:23,620 --> 00:03:25,890
它只是一种一般化

63
00:03:26,020 --> 00:03:34,490
可以让x_i取更多的值

64
00:03:34,610 --> 00:03:36,200
一件经常做的事情

65
00:03:36,310 --> 00:03:37,920
是使用朴素贝叶斯方法

66
00:03:38,040 --> 00:03:40,620
来解决那些某些x_i的取值

67
00:03:40,710 --> 00:03:42,780
可以是k个值  而不是仅仅有2个值

68
00:03:42,890 --> 00:03:49,120
这种情况下  你可以

69
00:03:49,200 --> 00:03:51,380
得到一个非常相似的p(x|y)的模型

70
00:03:51,480 --> 00:03:56,630
看起来是一样的

71
00:03:56,740 --> 00:03:58,400
区别是

72
00:03:58,510 --> 00:04:03,420
这里是一个多项式分布  而不是伯努利分布

73
00:04:03,540 --> 00:04:05,690
因为x_i

74
00:04:05,770 --> 00:04:07,000
最多可以取k个值

75
00:04:07,110 --> 00:04:09,420
实际上

76
00:04:09,520 --> 00:04:12,150
有一种情形是非常常见的

77
00:04:12,260 --> 00:04:14,360
如果你有一个特征

78
00:04:14,460 --> 00:04:16,080
需要取连续值

79
00:04:16,180 --> 00:04:18,500
你需要将其离散化

80
00:04:18,590 --> 00:04:19,360
你需要

81
00:04:19,470 --> 00:04:21,310
讲一个连续变量的取值  离散到

82
00:04:21,420 --> 00:04:24,160
只能取有限的k个值

83
00:04:24,270 --> 00:04:27,230
这是个很好的例子

84
00:04:27,320 --> 00:04:33,690
如果你记得我们的第一个监督学习的例子

85
00:04:33,780 --> 00:04:36,420
也就是房价预测的问题

86
00:04:36,550 --> 00:04:38,900
如果你需要对这些房屋的某些问题

87
00:04:39,020 --> 00:04:40,220
进行分类

88
00:04:40,340 --> 00:04:42,010
例如:根据房屋的特征

89
00:04:42,120 --> 00:04:43,290
你需要预测

90
00:04:43,400 --> 00:04:45,150
房屋是否会在未来六个月内卖掉

91
00:04:45,260 --> 00:04:46,260
这是一个分类问题

92
00:04:46,370 --> 00:04:47,960
一旦使用朴素贝叶斯方法

93
00:04:48,070 --> 00:04:50,500
之后会给定一个连续取值的特征

94
00:04:50,600 --> 00:04:55,000
例如:房屋面积

95
00:04:55,100 --> 00:04:56,670
一件通常要做的工作是

96
00:04:56,770 --> 00:04:59,040
将这些连续取值的房屋面积

97
00:04:59,140 --> 00:05:03,140
分散成几个

98
00:05:03,260 --> 00:05:14,400
离散的区段  之后

99
00:05:14,500 --> 00:05:15,780
根据房屋的面积

100
00:05:15,860 --> 00:05:17,150
是否小于500

101
00:05:17,260 --> 00:05:19,860
或者是否在1000和1500之间

102
00:05:19,950 --> 00:05:21,820
或者是否大于2000

103
00:05:21,940 --> 00:05:24,300
根据这些条件为特征取相应的值

104
00:05:24,430 --> 00:05:28,390
也就是1  2  3  4  明白吗?

105
00:05:28,480 --> 00:05:32,050
所以这是第一种朴素贝叶斯的变化形式

106
00:05:32,160 --> 00:05:34,100
它是一种一般化的形式

107
00:05:34,220 --> 00:05:36,940
我应该看看

108
00:05:37,030 --> 00:05:49,710
有问题吗?很好

109
00:05:49,820 --> 00:05:52,110
实际上  在现实应用中

110
00:05:52,220 --> 00:05:54,940
通常会用10个区段

111
00:05:55,060 --> 00:05:57,780
来划分连续取值的特征

112
00:05:57,890 --> 00:05:59,760
为了便于书写我这里只写了4个

113
00:05:59,860 --> 00:06:05,450
我想讲的第二种

114
00:06:05,550 --> 00:06:06,820
也是最后一种朴素贝叶斯的变化形式

115
00:06:06,930 --> 00:06:08,430
是一种变化形式

116
00:06:08,580 --> 00:06:12,600
用来专门处理文本文档

117
00:06:12,690 --> 00:06:14,600
或者更一般地说  用来对序列进行分类

118
00:06:14,720 --> 00:06:16,760
对于一个文本文档来说  例如一封电子邮件

119
00:06:16,850 --> 00:06:18,780
你可以将它看成一个词的序列

120
00:06:18,870 --> 00:06:21,420
你可以用

121
00:06:21,530 --> 00:06:22,780
我将要讲的这个模型

122
00:06:22,830 --> 00:06:24,420
去对其他的序列进行分类

123
00:06:24,550 --> 00:06:26,260
但是这里我们只考虑文本

124
00:06:26,380 --> 00:06:30,410
算法的思想是这样的

125
00:06:30,520 --> 00:06:34,530
所以对于我们现在为止

126
00:06:34,620 --> 00:06:36,350
讨论的朴素贝叶斯方法来说

127
00:06:36,470 --> 00:06:38,720
例如  给定一封电子邮件

128
00:06:38,820 --> 00:06:41,700
我们用0-1向量的值来表示它

129
00:06:41,820 --> 00:06:43,790
这种表示方法缺失了一些信息

130
00:06:43,900 --> 00:06:45,820
例如:不同的词语

131
00:06:45,900 --> 00:06:48,140
出现的次数  对吗?

132
00:06:48,220 --> 00:06:51,100
比如说  如果有些词出现了很多次

133
00:06:51,230 --> 00:06:52,420
比如说  你在邮件里看到

134
00:06:52,520 --> 00:06:53,530
"buy"这个词出现了很多次

135
00:06:53,620 --> 00:06:55,020
或者你看到了"Viagra"这个词

136
00:06:55,150 --> 00:06:56,780
这似乎在邮件中相当普遍

137
00:06:56,900 --> 00:06:59,050
如果你在邮件中看到"Viagra"出现了很多次

138
00:06:59,160 --> 00:07:01,710
那么和只出现一次的邮件相比

139
00:07:01,820 --> 00:07:02,860
它更可能像是一封垃圾邮件

140
00:07:02,860 --> 00:07:03,460
尽管我认为只出现

141
00:07:03,560 --> 00:07:04,980
一次就足以认为它是垃圾邮件了

142
00:07:05,080 --> 00:07:10,180
让我们来看一个不同的模型

143
00:07:10,250 --> 00:07:12,340
称之为朴素贝叶斯的事件模型

144
00:07:12,420 --> 00:07:14,280
会考虑到每个词

145
00:07:14,390 --> 00:07:15,960
出现在邮件中的次数

146
00:07:16,070 --> 00:07:18,720
给之前这个模型去个名字

147
00:07:18,840 --> 00:07:21,340
这个用于进行

148
00:07:21,430 --> 00:07:24,240
文本分类的模型

149
00:07:24,320 --> 00:07:33,050
被称之为:多元伯努利时间模型

150
00:07:33,160 --> 00:07:34,580
这个名字不是很重要

151
00:07:34,700 --> 00:07:36,310
所以不用在意这个名字的意义

152
00:07:36,390 --> 00:07:40,460
它实际上是指

153
00:07:40,570 --> 00:07:42,380
这里有许多伯努利随机变量

154
00:07:42,500 --> 00:07:43,350
但是真的不用在意

155
00:07:43,420 --> 00:07:44,850
这个名字的含义

156
00:07:44,940 --> 00:07:47,180
相对地  我现在要做的是

157
00:07:47,300 --> 00:07:49,020
描述一种不同的

158
00:07:49,140 --> 00:07:51,100
将电子邮件表示成特征向量的方法

159
00:07:51,220 --> 00:08:00,150
这称之为多项式事件模型

160
00:08:00,280 --> 00:08:02,210
这样的命名是有根据的

161
00:08:02,340 --> 00:08:03,370
但是有些费解

162
00:08:03,460 --> 00:08:04,570
所以不用担心为什么它被称之为

163
00:08:04,670 --> 00:08:06,000
多项式事件模型

164
00:08:06,090 --> 00:08:07,020
它只是个名字

165
00:08:07,180 --> 00:08:08,650
我们接下来要这样做

166
00:08:08,740 --> 00:08:10,340
给定一封邮件

167
00:08:10,450 --> 00:08:12,020
我要将它表示成

168
00:08:12,190 --> 00:08:13,300
一个特征向量

169
00:08:13,410 --> 00:08:17,620
所以第i个训练样本

170
00:08:17,740 --> 00:08:24,920
x_i 将会是一个特征向量

171
00:08:24,980 --> 00:08:37,140
(x_1^((i) )  x_2^((i) )  …  x_(n_i)^((i) ) ) n_i等于

172
00:08:37,260 --> 00:08:43,670
邮件中词的数量  明白吗?

173
00:08:43,780 --> 00:08:45,080
如果作为训练样本的

174
00:08:45,190 --> 00:08:46,800
邮件中有300个词

175
00:08:46,910 --> 00:08:50,850
那么我就将它表示成一个

176
00:08:50,930 --> 00:08:53,290
包含了300个元素的特征向量

177
00:08:53,400 --> 00:09:02,570
特征向量的每个元素

178
00:09:02,680 --> 00:09:05,040
让我看看  我把它写成x_j

179
00:09:05,160 --> 00:09:12,420
这个值会是到词典的一个索引  明白吗?

180
00:09:12,550 --> 00:09:15,410
如果我的词典有50000个词

181
00:09:15,540 --> 00:09:19,020
那么我的特征向量中的每个位置

182
00:09:19,140 --> 00:09:22,710
将会是一个可以取50000个可能值的变量

183
00:09:22,830 --> 00:09:27,510
对应着在邮件的

184
00:09:27,620 --> 00:09:31,060
第j个位置出现过的词  明白吗?

185
00:09:31,180 --> 00:09:33,390
换句话说

186
00:09:33,470 --> 00:09:34,420
对于所有在邮件中出现的词

187
00:09:34,540 --> 00:09:36,530
用一个特征向量

188
00:09:36,620 --> 00:09:39,260
来说明邮件中出现的词

189
00:09:39,370 --> 00:09:42,210
对应着词典中的哪个词  明白吗?

190
00:09:42,320 --> 00:09:45,470
对于n_i的定义有点不同

191
00:09:45,580 --> 00:09:47,430
n_i的值

192
00:09:47,530 --> 00:09:48,570
因训练样本而异

193
00:09:48,700 --> 00:09:51,920
x_j 是指向词典的索引

194
00:09:52,040 --> 00:09:54,530
所以  特征向量的每个分量

195
00:09:54,610 --> 00:09:56,630
都不再是一个二元随机变量了

196
00:09:56,740 --> 00:09:58,960
它们是指向词典的索引

197
00:09:59,060 --> 00:10:00,400
可以在一个大得多的集合内取值

198
00:10:00,500 --> 00:10:09,560
所以这种情况下我们的生成模型应该是

199
00:10:09,660 --> 00:10:23,700
p(xy)应该等于这个式子

200
00:10:23,790 --> 00:10:26,420
这里n是邮件的长度

201
00:10:26,530 --> 00:10:29,280
明白吗?你可以这样理解这个公式

202
00:10:29,390 --> 00:10:31,360
你可以想象

203
00:10:31,580 --> 00:10:34,950
邮件内容满足一些概率分布

204
00:10:35,060 --> 00:10:36,430
有一些随机分布

205
00:10:36,530 --> 00:10:37,550
在生成这些邮件

206
00:10:37,660 --> 00:10:39,680
这个过程是这样进行的:

207
00:10:39,730 --> 00:10:43,130
首先  y的值  也就是种类标签  被选中

208
00:10:43,260 --> 00:10:44,750
一个人发的邮件是否为垃圾邮件

209
00:10:44,870 --> 00:10:47,350
是由我们选择的

210
00:10:47,460 --> 00:10:50,570
所以首先  随机变量y

211
00:10:50,690 --> 00:10:51,580
也就是表示邮件是否

212
00:10:51,580 --> 00:10:52,780
垃圾的类别标签被生成出来

213
00:10:52,890 --> 00:10:56,450
决定了

214
00:10:56,540 --> 00:10:58,200
是否向你发送垃圾邮件之后

215
00:10:58,320 --> 00:11:02,380
有人遍历了邮件所有300个位置

216
00:11:02,490 --> 00:11:04,930
或者将要组成这封邮件的

217
00:11:05,020 --> 00:11:06,380
300个值

218
00:11:06,480 --> 00:11:09,500
之后会按照某种概率分布生成一些词

219
00:11:09,590 --> 00:11:11,190
基于它们是否选择

220
00:11:11,300 --> 00:11:12,520
向你发送垃圾邮件

221
00:11:12,630 --> 00:11:13,600
如果他们向你发送垃圾邮件

222
00:11:13,700 --> 00:11:14,790
那么他们会倾向于向你发送-

223
00:11:14,880 --> 00:11:15,900
或生成这样的词

224
00:11:16,010 --> 00:11:17,030
例如:"buy"  "Viagra"

225
00:11:17,140 --> 00:11:19,250
或者其他的词诸如:"discounts"  "sale"  或一些其他的词.

226
00:11:19,340 --> 00:11:21,530
如果有人选择向你发送非垃圾邮件

227
00:11:21,640 --> 00:11:22,900
那么他们会向你发送一些

228
00:11:22,990 --> 00:11:24,940
更为正常的词  明白吗?

229
00:11:25,030 --> 00:11:28,660
注意这里

230
00:11:28,740 --> 00:11:30,750
x_i和之前的事件模型中的

231
00:11:30,830 --> 00:11:32,240
定义不同

232
00:11:32,270 --> 00:11:34,020
n也和之前的事件模型中的

233
00:11:34,140 --> 00:11:35,180
定义不同

234
00:11:35,260 --> 00:11:41,190
这个模型的参数包括

235
00:11:41,220 --> 00:11:47,550
让我看看 φ_(k│y=1)

236
00:11:47,660 --> 00:11:55,700
它表示的是

237
00:11:55,810 --> 00:11:58,060
某人决定

238
00:11:58,120 --> 00:11:59,210
向你发送垃圾邮件时

239
00:11:59,320 --> 00:12:00,510
他们会选择

240
00:12:00,610 --> 00:12:01,730
在垃圾邮件中

241
00:12:01,860 --> 00:12:04,080
出现的下一个词

242
00:12:04,190 --> 00:12:10,340
是k的概率 所以类似的

243
00:12:10,470 --> 00:12:13,260
类似的结果

244
00:12:13,360 --> 00:12:15,740
我想我还是把它写出来吧-

245
00:12:15,860 --> 00:12:25,040
φ_y和之前一样  明白吗?

246
00:12:25,140 --> 00:12:27,100
这些是模型的参数

247
00:12:27,220 --> 00:12:32,320
给定一个训练集合

248
00:12:32,420 --> 00:12:33,490
你们可以求出

249
00:12:33,600 --> 00:12:35,260
这些参数的极大似然估计

250
00:12:35,380 --> 00:12:44,290
所以对这些参数的极大似然估计的

251
00:12:44,390 --> 00:12:48,820
结果应该等于-

252
00:12:48,930 --> 00:12:50,360
这里我会再一次用到

253
00:12:50,460 --> 00:12:52,410
之前提过的指示函数

254
00:12:52,520 --> 00:12:58,020
首先对训练样本的指示函数求和

255
00:12:58,140 --> 00:13:07,800
这里表示该样本是否为垃圾邮件

256
00:13:07,910 --> 00:13:09,260
之后乘以一个针对所有词的求和式

257
00:13:09,340 --> 00:13:11,560
这里n_i表示你的训练集合中

258
00:13:11,700 --> 00:13:14,050
的邮件的词的数目

259
00:13:14,170 --> 00:13:38,780
乘以:I  明白吗?

260
00:13:39,540 --> 00:13:42,420
分子的意思是  对所有

261
00:13:42,510 --> 00:13:44,550
标签为1的邮件

262
00:13:44,680 --> 00:13:45,990
求和

263
00:13:46,080 --> 00:13:47,430
也就是说  只考虑

264
00:13:47,540 --> 00:13:49,630
垃圾邮件  因为如果y=0

265
00:13:49,730 --> 00:13:51,320
这一项就是0  这一项就不存在了

266
00:13:51,400 --> 00:13:55,140
之后对垃圾邮件中的

267
00:13:55,230 --> 00:13:56,180
所有词求和

268
00:13:56,270 --> 00:13:57,860
它加起来应该是

269
00:13:57,970 --> 00:14:00,410
词k出现在垃圾邮件中的次数

270
00:14:00,520 --> 00:14:03,170
所以  换句话说  分子

271
00:14:03,280 --> 00:14:04,570
实际上就是对训练集合中的所有垃圾邮件

272
00:14:04,680 --> 00:14:06,190
中词k出现的次数

273
00:14:06,310 --> 00:14:07,870
进行求和

274
00:14:07,990 --> 00:14:14,260
分母的含义是

275
00:14:14,340 --> 00:14:18,000
对训练样本集合进行求和

276
00:14:18,110 --> 00:14:20,490
如果其中的一个样本是垃圾邮件

277
00:14:20,600 --> 00:14:22,670
那么就把它的长度加起来

278
00:14:22,750 --> 00:14:26,310
所以分母的含义是

279
00:14:26,440 --> 00:14:28,330
训练集合中所有垃圾邮件的总长

280
00:14:28,430 --> 00:14:35,050
所以这个比值的含义就是

281
00:14:35,150 --> 00:14:36,790
在所有垃圾邮件中

282
00:14:36,870 --> 00:14:38,810
词k所占的比例

283
00:14:38,910 --> 00:14:40,830
这是你估计得到的

284
00:14:40,930 --> 00:14:44,200
对于任何位置来说

285
00:14:44,310 --> 00:14:45,220
在垃圾邮件的下个位置

286
00:14:45,220 --> 00:14:46,420
生成词k的概率  明白吗?

287
00:14:46,530 --> 00:14:54,590
在上一讲的最后

288
00:14:54,670 --> 00:14:56,100
我讲到了拉普拉斯平滑

289
00:14:56,210 --> 00:14:57,850
所以对于这个式子

290
00:14:57,960 --> 00:15:00,790
可以在分子上加1

291
00:15:00,910 --> 00:15:02,840
在分母上加k  这就是应用了

292
00:15:02,840 --> 00:15:03,940
拉普拉斯平滑之后

293
00:15:04,020 --> 00:15:09,120
对于参数的估计结果  明白吗?

294
00:15:09,190 --> 00:15:10,810
类似的  你可以

295
00:15:10,920 --> 00:15:14,650
你可以用同样的方法

296
00:15:14,760 --> 00:15:16,200
得到其它参数的估计值  明白吗?

297
00:15:16,310 --> 00:15:19,050
结果非常类似 什么?

298
00:15:19,160 --> 00:15:22,220
对不起 右上角那里

299
00:15:22,290 --> 00:15:24,600
我想知道x_i是什么意思

300
00:15:24,700 --> 00:15:26,260
还有n-

301
00:15:26,350 --> 00:15:27,280
好的

302
00:15:27,390 --> 00:15:30,730
在第二个事件模型中  x_i

303
00:15:30,840 --> 00:15:33,180
和n的定义有些不同  对吗?

304
00:15:33,280 --> 00:15:37,340
这里  对于一个样本xy

305
00:15:37,450 --> 00:15:46,710
n表示给定的邮件的长度  对吗?

306
00:15:46,830 --> 00:15:48,750
如果这是第i封邮件

307
00:15:48,840 --> 00:15:50,320
那么n就应该是n_i

308
00:15:50,430 --> 00:15:52,460
所以对于不同的训练样本来说

309
00:15:52,560 --> 00:15:53,780
n是不一样的

310
00:15:53,890 --> 00:16:00,930
而x_i会从

311
00:16:01,060 --> 00:16:03,400
中取值

312
00:16:03,510 --> 00:16:08,620
x_i实际上就是邮件中

313
00:16:08,740 --> 00:16:15,220
第i个词的标识  明白吗?

314
00:16:15,320 --> 00:16:17,100
这就是为什么

315
00:16:17,220 --> 00:16:17,990
这里是

316
00:16:18,100 --> 00:16:19,370
对所有邮件中的词

317
00:16:19,460 --> 00:16:23,100
对应的p(x_i |y)

318
00:16:23,210 --> 00:16:26,000
的乘积 什么问题?

319
00:16:26,130 --> 00:16:28,900
【听不清】

320
00:16:28,990 --> 00:16:31,160
实际上

321
00:16:31,260 --> 00:16:32,390
这是我的错

322
00:16:32,490 --> 00:16:35,450
我刚刚才意识到这里的符号用重了

323
00:16:35,560 --> 00:16:37,820
我这里不应该用k

324
00:16:37,940 --> 00:16:40,410
让我换一个字母

325
00:16:40,510 --> 00:16:41,560
看看这样行不行

326
00:16:41,670 --> 00:16:44,430
这样行了吗?

327
00:16:44,520 --> 00:16:46,320
哦  对不起

328
00:16:46,410 --> 00:16:47,660
你是对的

329
00:16:47,750 --> 00:16:49,550
谢谢你

330
00:16:49,630 --> 00:16:52,120
如果使用拉普拉斯平滑  这里不应该是k

331
00:16:52,230 --> 00:16:56,640
这里应该是50000

332
00:16:56,740 --> 00:16:59,710
假设50000是你的词典中词的数量

333
00:16:59,820 --> 00:17:01,390
好的  谢谢 很好

334
00:17:01,490 --> 00:17:03,970
我从之前的讲义中照搬了符号

335
00:17:04,070 --> 00:17:06,260
没有进行合适的转换

336
00:17:06,370 --> 00:17:08,160
对于拉普拉斯平滑

337
00:17:08,270 --> 00:17:10,410
这里应该是随机变量x_i

338
00:17:10,490 --> 00:17:14,710
能够取的可能值的数量 很好

339
00:17:16,010 --> 00:17:18,350
明白的话请举手

340
00:17:18,450 --> 00:17:22,220
好的  一部分人明白了

341
00:17:22,280 --> 00:17:27,200
关于这个还有问题吗?

342
00:17:27,310 --> 00:17:33,280
在拉普拉斯平滑中

343
00:17:33,400 --> 00:17:36,560
分母加上的那个数

344
00:17:36,670 --> 00:17:39,300
是y可以取的值的数量吗?

345
00:17:39,430 --> 00:17:44,310
让我看看

346
00:17:44,430 --> 00:17:52,590
实际上拉普拉斯平滑是一种方法

347
00:17:52,690 --> 00:17:54,260
可以帮助你得到

348
00:17:54,360 --> 00:17:56,900
基于多项式分布的更好的概率分布的估计

349
00:17:57,020 --> 00:18:04,280
我之前用的是x还是y?

350
00:18:04,360 --> 00:18:06,040
为了估计概率

351
00:18:06,130 --> 00:18:07,250
基于多项式分布-

352
00:18:07,360 --> 00:18:08,830
我认为x和y是不同的

353
00:18:08,920 --> 00:18:11,860
是x还是y?

354
00:18:11,950 --> 00:18:13,520
我觉得是x 好的

355
00:18:13,640 --> 00:18:14,800
让我看看  好的

356
00:18:14,890 --> 00:18:16,790
我觉得我对随机变量y

357
00:18:16,870 --> 00:18:19,360
的定义是不同的  因为假设你

358
00:18:19,440 --> 00:18:20,680
有一个服从多项式分布的随机变量

359
00:18:20,780 --> 00:18:24,640
x可以取-

360
00:18:24,750 --> 00:18:29,230
让我们换个字母

361
00:18:29,290 --> 00:18:31,340
假设有一个服从多项式分布的

362
00:18:31,400 --> 00:18:33,530
随机变量x可以取l个不同的值

363
00:18:33,640 --> 00:18:37,330
那么对于p(x=k)

364
00:18:37,440 --> 00:18:42,000
的极大似然估计应该等于

365
00:18:42,120 --> 00:18:44,020
应该等于

366
00:18:44,110 --> 00:18:45,440
观察到的数量

367
00:18:45,520 --> 00:18:55,220
对于p(x=k)的

368
00:18:55,320 --> 00:18:57,050
极大似然估计的结果

369
00:18:57,160 --> 00:19:02,000
应该等于观察到的x=k的数量

370
00:19:02,110 --> 00:19:06,080
除以所有观测到的

371
00:19:06,180 --> 00:19:11,260
x的数量  明白吗?

372
00:19:11,370 --> 00:19:13,020
这是极大似然估计

373
00:19:13,120 --> 00:19:16,960
将拉普拉斯平滑引入进来

374
00:19:17,070 --> 00:19:19,140
你需要在分子上加1

375
00:19:19,250 --> 00:19:22,200
在分母上加l

376
00:19:22,310 --> 00:19:24,750
这里l是x可以取的

377
00:19:25,000 --> 00:19:27,840
值的数量 所以在这个例子中

378
00:19:27,950 --> 00:19:31,170
这是x=k的概率

379
00:19:31,300 --> 00:19:34,800
如果词典中有50000个词的话

380
00:19:34,920 --> 00:19:36,190
x有50000个可能的取值

381
00:19:36,300 --> 00:19:37,150
也可以是其他的

382
00:19:37,250 --> 00:19:38,960
这就是我为什么在分母加上50000的原因

383
00:19:39,070 --> 00:19:41,140
还有问题吗?什么?

384
00:19:41,250 --> 00:19:42,850
有没有对参数进行

385
00:19:42,940 --> 00:19:45,060
极大似然估计的具体定义?

386
00:19:45,150 --> 00:19:47,810
我们已经提过很多次了

387
00:19:47,890 --> 00:19:49,120
所有的例子似乎都讲得通

388
00:19:49,230 --> 00:19:50,370
但是我还是不知道

389
00:19:50,440 --> 00:19:51,730
进行极大似然估计的一般公式

390
00:19:51,830 --> 00:19:52,710
我知道了 好的

391
00:19:52,800 --> 00:19:54,020
极大似然估计的定义是-

392
00:19:54,110 --> 00:19:56,630
你的问题是极大似然估计

393
00:19:56,730 --> 00:19:57,800
的定义吗?

394
00:19:57,910 --> 00:20:03,630
实际上  在这一讲和之前

395
00:20:03,730 --> 00:20:05,630
我们讲到高斯判别分析的时候

396
00:20:05,740 --> 00:20:08,020
我都是直接把极大似然估计的结果

397
00:20:08,110 --> 00:20:10,340
直接写在黑板上  而并没有证明它们

398
00:20:10,420 --> 00:20:12,930
方法是这样的

399
00:20:13,040 --> 00:20:21,480
先把似然性公式写出来

400
00:20:21,590 --> 00:20:31,240
所以进行极大似然估计的

401
00:20:31,350 --> 00:20:33,030
方法是

402
00:20:33,150 --> 00:20:36,680
先写出参数的似然性公式

403
00:20:36,800 --> 00:20:45,690
l(φ_(k│y=1)   φ_(k│y=0)   φ_y)   对吗?

404
00:20:45,780 --> 00:20:48,250
所以给定一个训练集合  似然性

405
00:20:48,350 --> 00:20:49,780
我想应该写成对数似然性

406
00:20:49,860 --> 00:20:53,710
应该是这些式子的乘积

407
00:20:53,800 --> 00:20:57,870
明白吗?

408
00:20:57,980 --> 00:21:05,740
以这些为参数

409
00:21:05,830 --> 00:21:09,750
之后这一项

410
00:21:09,850 --> 00:21:25,390
应该等于这些式子相乘

411
00:21:25,500 --> 00:21:27,800
它们是以这些为参数的-

412
00:21:27,920 --> 00:21:29,510
我这里省略这些参数

413
00:21:29,600 --> 00:21:34,020
可以写的更加简便一些 好的

414
00:21:34,140 --> 00:21:48,600
乘以这一项  明白吗?

415
00:21:48,710 --> 00:21:50,700
这是我们的对数似然性

416
00:21:50,810 --> 00:21:53,510
而极大似然估计指的是

417
00:21:53,620 --> 00:21:55,300
对这些参数进行估计

418
00:21:55,390 --> 00:21:57,400
对于一个给定的训练集合

419
00:21:57,510 --> 00:21:59,050
给定一组固定的(x^((i) )  y^((i) ))

420
00:21:59,150 --> 00:22:03,010
你需要取参数使得这个式子的值最大化

421
00:22:03,110 --> 00:22:05,520
这样便得到了极大似然估计的值

422
00:22:05,630 --> 00:22:06,760
也就是我写出来的结果

423
00:22:06,850 --> 00:22:09,300
在之前的课上

424
00:22:09,400 --> 00:22:11,220
我写了一些极大似然估计的结果

425
00:22:11,330 --> 00:22:13,470
例如:在高斯判别分析模型中

426
00:22:13,560 --> 00:22:16,250
还有朴素贝叶斯中

427
00:22:16,380 --> 00:22:19,390
这些结果我都没有给出证明  但是你可以

428
00:22:19,490 --> 00:22:21,720
在作业题中自己证明一下

429
00:22:21,790 --> 00:22:25,130
我讲过的这些模型中的结论

430
00:22:25,220 --> 00:22:26,720
你们也以自己

431
00:22:26,800 --> 00:22:27,960
用极大似然估计证明一下

432
00:22:28,080 --> 00:22:29,930
希望你们使用极大似然估计

433
00:22:30,040 --> 00:22:31,740
可以得到和我黑板上的公式

434
00:22:31,840 --> 00:22:34,390
一样的结果

435
00:22:34,480 --> 00:22:36,100
但是找到结果的方法就是

436
00:22:36,190 --> 00:22:40,680
使这个式子最大化  明白吗?好的

437
00:22:40,780 --> 00:22:46,340
所以今天我讲的--

438
00:22:46,430 --> 00:22:52,360
差不多

439
00:22:52,470 --> 00:22:54,670
我讲的关于朴素贝叶斯的内容就这么多

440
00:22:54,740 --> 00:22:57,400
实际上对于文本分类

441
00:22:57,520 --> 00:23:00,400
使用第二种事件模型

442
00:23:00,510 --> 00:23:02,050
的朴素贝叶斯算法

443
00:23:02,150 --> 00:23:05,290
也就是我刚刚讲的

444
00:23:05,380 --> 00:23:06,580
多项式事件模型

445
00:23:06,670 --> 00:23:09,260
实际上总是比我讲的第一种事件模型

446
00:23:09,370 --> 00:23:13,320
效果更好

447
00:23:13,430 --> 00:23:15,890
当你们需要特别地

448
00:23:15,970 --> 00:23:17,840
处理文本分类问题的时候

449
00:23:17,950 --> 00:23:23,120
关于这个想象的一个猜测是

450
00:23:23,230 --> 00:23:25,020
第二个模型

451
00:23:25,110 --> 00:23:26,380
多项式事件模型

452
00:23:26,500 --> 00:23:28,320
考虑了一个文档中

453
00:23:28,470 --> 00:23:33,190
词出现的次数

454
00:23:33,270 --> 00:23:34,620
而之前的模型没有考虑

455
00:23:34,720 --> 00:23:38,580
我应该说明  实际上

456
00:23:38,690 --> 00:23:40,130
为什么第二个模型

457
00:23:40,230 --> 00:23:41,490
会比第一个模型效果好

458
00:23:41,590 --> 00:23:43,860
还没有被完全解释清楚

459
00:23:43,980 --> 00:23:47,530
研究者们关于这个问题仍然存在争论

460
00:23:47,600 --> 00:23:49,340
如果你需要处理一个文本分类问题

461
00:23:49,450 --> 00:23:51,630
尽管朴素贝叶斯方法可能不是

462
00:23:51,750 --> 00:23:53,930
迄今为止最好的方法

463
00:23:54,040 --> 00:23:57,110
但是它很容易实现

464
00:23:57,200 --> 00:23:59,160
因此它是一个不错的算法

465
00:23:59,270 --> 00:24:00,270
当你们需要处理文本

466
00:24:00,270 --> 00:24:02,040
分类问题时可以尝试一下  好吗?

467
00:24:02,160 --> 00:24:05,200
还有问题吗?什么?

468
00:24:05,300 --> 00:24:06,680
第二个模型

469
00:24:06,760 --> 00:24:07,790
并不关心词的位置是吗?

470
00:24:07,900 --> 00:24:09,310
它并不会去关心词在哪里?

471
00:24:09,410 --> 00:24:10,830
是的

472
00:24:10,950 --> 00:24:12,810
我的意思是  变量x

473
00:24:12,920 --> 00:24:14,720
如果我的模型包含了更多信息

474
00:24:14,810 --> 00:24:16,730
是否通常会表现地更好?

475
00:24:16,840 --> 00:24:18,630
好的 你的问题是

476
00:24:18,720 --> 00:24:19,870
关于第二个模型  对吗?

477
00:24:19,960 --> 00:24:22,390
第二个模型  多项式事件模型

478
00:24:22,490 --> 00:24:23,870
并不关心词的位置

479
00:24:23,960 --> 00:24:25,240
你可以重新排列邮件中词的顺序

480
00:24:25,350 --> 00:24:26,540
但得到的结果都是一样的

481
00:24:26,630 --> 00:24:28,890
所以在自然语言处理中

482
00:24:28,970 --> 00:24:30,430
它有另外一个名字

483
00:24:30,520 --> 00:24:31,670
称为:一元模型

484
00:24:31,730 --> 00:24:32,790
在自然语言处理中

485
00:24:32,890 --> 00:24:37,010
有一些其他的模型  例如

486
00:24:37,090 --> 00:24:38,390
马尔可夫模型

487
00:24:38,460 --> 00:24:40,150
会考虑词的顺序

488
00:24:40,240 --> 00:24:42,210
实际上对于文本分类这类问题

489
00:24:42,320 --> 00:24:48,150
像二元模型或三元模型这样的模型

490
00:24:48,240 --> 00:24:50,010
如果有改进的话

491
00:24:50,020 --> 00:24:52,380
我相信也只会是轻微的改进

492
00:24:52,470 --> 00:24:55,310
但是只是当

493
00:24:55,410 --> 00:24:56,380
你应用它们处理文本

494
00:24:56,380 --> 00:24:57,790
分类问题的时候  明白吗?

495
00:24:57,840 --> 00:25:04,590
好的  接下来我要开始

496
00:25:04,670 --> 00:25:08,360
再次讨论非线性分类器

497
00:25:08,480 --> 00:25:12,570
实际上

498
00:25:12,670 --> 00:25:28,440
我们讲的第一个分类算法

499
00:25:28,530 --> 00:25:30,390
是logistic回归

500
00:25:30,480 --> 00:25:33,960
假设的形式是这样的

501
00:25:34,050 --> 00:25:39,770
你可以认为当

502
00:25:40,330 --> 00:25:43,630
假设的值大于或等于0.5的时候

503
00:25:43,740 --> 00:25:46,390
预测的值是1

504
00:25:46,500 --> 00:25:49,290
当假设值小于0.5时

505
00:25:49,380 --> 00:26:02,480
预测的值是0 给定这样一个训练集合

506
00:26:02,590 --> 00:26:04,870
logistic回归会

507
00:26:04,980 --> 00:26:07,080
使用梯度下降算法或者牛顿方法

508
00:26:07,130 --> 00:26:09,980
找到这样一条直线

509
00:26:10,070 --> 00:26:11,530
能够将正负两类样本合理地分开

510
00:26:11,650 --> 00:26:13,970
但是有的时候一个数据集合

511
00:26:14,040 --> 00:26:15,380
不能被一条直线分开

512
00:26:15,480 --> 00:26:17,200
所以有没有一个算法

513
00:26:17,300 --> 00:26:19,430
可以学习

514
00:26:19,520 --> 00:26:20,780
这样的非线性的分界线呢?

515
00:26:20,880 --> 00:26:27,100
应该怎样得到一个

516
00:26:27,190 --> 00:26:28,770
非线性的分类器呢?

517
00:26:28,870 --> 00:26:32,260
记得我曾经讲过

518
00:26:32,340 --> 00:26:33,880
一个很酷的结论-

519
00:26:33,980 --> 00:26:35,580
当我们在讨论生成学习算法时

520
00:26:35,660 --> 00:26:39,840
我说过  如果假设y|x

521
00:26:39,930 --> 00:26:45,620
属于指数分布族  以η为参数

522
00:26:45,740 --> 00:26:49,620
如果你基于它构造一个生成学习算法

523
00:26:49,710 --> 00:26:53,070
用这个  η_1

524
00:26:53,160 --> 00:26:56,930
这个服从指数分布族的分布

525
00:26:57,020 --> 00:27:01,980
的自然参数是η_0  好的

526
00:27:02,090 --> 00:27:03,260
我记得当我们讲

527
00:27:03,380 --> 00:27:04,470
高斯判别分析的时候

528
00:27:04,580 --> 00:27:05,880
我说过  如果这样的条件满足的话

529
00:27:05,950 --> 00:27:08,360
之后你会得到一个logistic后验分布

530
00:27:08,450 --> 00:27:10,560
实际上朴素贝叶斯模型

531
00:27:10,670 --> 00:27:13,280
也属于这类模型

532
00:27:13,380 --> 00:27:15,070
所以朴素贝叶斯模型

533
00:27:15,170 --> 00:27:17,940
实际上也属于指数分布族  因此

534
00:27:18,050 --> 00:27:20,540
使用朴素贝叶斯模型时

535
00:27:20,650 --> 00:27:21,820
最后用到的

536
00:27:21,920 --> 00:27:24,830
还是线性分类器  明白吗?

537
00:27:24,940 --> 00:27:29,800
所以问题是  你应该如何

538
00:27:29,880 --> 00:27:31,070
得到非线性分类器?

539
00:27:31,150 --> 00:27:34,820
我今天要讲一个方法

540
00:27:34,920 --> 00:27:43,340
我们先简要地介绍一下它

541
00:27:43,430 --> 00:27:47,480
这只是一个简单的算法

542
00:27:47,570 --> 00:27:52,120
就像logistic回归一样  之后我们用它

543
00:27:52,220 --> 00:27:55,910
构造更加复杂的非线性分类器  明白吗?

544
00:27:56,030 --> 00:27:59,610
为了开始我们的讨论

545
00:27:59,700 --> 00:28:03,040
我们要用到一张图  让我看看

546
00:28:03,140 --> 00:28:06,900
假设你的特征是x_1  x_2  和x_3

547
00:28:07,010 --> 00:28:08,460
按照约定

548
00:28:08,540 --> 00:28:11,450
根据我们之前的一些约定俗成的条件

549
00:28:11,550 --> 00:28:15,180
x_0设成1  之后我要用这样的一张图

550
00:28:15,290 --> 00:28:21,300
来表示我们的logistic回归单元  明白吗?

551
00:28:21,420 --> 00:28:26,280
对于这张图

552
00:28:26,400 --> 00:28:29,100
你可以认为这个圆圈表示一个计算节点

553
00:28:29,180 --> 00:28:31,160
以这些特征为输入

554
00:28:31,260 --> 00:28:33,190
以一个数作为输出

555
00:28:33,280 --> 00:28:35,190
也就是h_? (x)

556
00:28:35,280 --> 00:28:37,470
这个函数是一个sigmoid函数

557
00:28:37,570 --> 00:28:39,550
所以这个小的计算单元

558
00:28:39,660 --> 00:28:43,740
以?为参数 现在

559
00:28:43,860 --> 00:28:46,260
为了获得非线性的分界线

560
00:28:46,360 --> 00:28:49,120
我们所需要做的全部--至少应该做的一件事

561
00:28:49,210 --> 00:28:55,320
就是找到一种方式

562
00:28:55,430 --> 00:28:57,390
表示出能够输出

563
00:28:57,500 --> 00:28:59,520
非线性分界线的假设

564
00:28:59,640 --> 00:29:03,200
好的  当你

565
00:29:03,310 --> 00:29:09,470
将若干我之前画的这些小单元

566
00:29:09,800 --> 00:29:12,560
拼在一起的时候

567
00:29:12,660 --> 00:29:14,810
你就会得到一个

568
00:29:14,920 --> 00:29:21,570
神经网络  你可以认为

569
00:29:21,660 --> 00:29:24,140
特征在这里

570
00:29:24,230 --> 00:29:32,120
我可以将它们反馈给

571
00:29:32,200 --> 00:29:35,710
这样一些小的sigmoid单元

572
00:29:35,790 --> 00:29:41,310
这些值会再反馈给另外一个

573
00:29:41,410 --> 00:29:42,950
sigmoid单元

574
00:29:43,060 --> 00:29:47,890
从而最终得到

575
00:29:47,960 --> 00:29:49,720
h_? (x)  明白吗?

576
00:29:49,830 --> 00:29:53,030
给这些东西起个名字

577
00:29:53,090 --> 00:29:55,890
让我把中间这些

578
00:29:55,980 --> 00:29:57,970
sigmoid单元的输出值

579
00:29:58,060 --> 00:30:00,070
称为:a_1  a_2  a_3

580
00:30:00,180 --> 00:30:03,590
让我们具体地将

581
00:30:03,670 --> 00:30:05,270
计算公式列出来

582
00:30:05,380 --> 00:30:09,370
每一个中间单元

583
00:30:09,440 --> 00:30:11,200
会有一系列属于自己的参数

584
00:30:11,260 --> 00:30:13,950
所以a_1应该等于

585
00:30:13,980 --> 00:30:17,080
g(x^T ?^((1) ))

586
00:30:17,130 --> 00:30:19,430
g(x^T ?^((1) ))

587
00:30:19,460 --> 00:30:22,120
?_1是参数  类似的

588
00:30:22,220 --> 00:30:28,130
a_2=g(x^T ?^((2) ))

589
00:30:28,210 --> 00:30:35,000
a_3=g(x^T ?^((3) ))

590
00:30:35,110 --> 00:30:38,580
其中g是sigmoid函数  对吗?

591
00:30:38,610 --> 00:30:46,310
最后  我们的假设最终的输出值是

592
00:30:46,330 --> 00:30:57,880
g(a ?^T ?^((4) ))  对吗?

593
00:30:57,970 --> 00:30:59,580
这里

594
00:30:59,680 --> 00:31:04,910
a ?是由a_1  a_2 和a_3 组成的向量

595
00:31:05,010 --> 00:31:08,860
如果你想的话  还可以

596
00:31:08,970 --> 00:31:10,570
再加一个  明白吗?

597
00:31:10,680 --> 00:31:15,970
让我画在上面这里--

598
00:31:16,080 --> 00:31:17,710
抱歉黑板很乱

599
00:31:17,830 --> 00:31:20,640
所以h_? (x)

600
00:31:20,750 --> 00:31:26,070
是一个从?_1

601
00:31:26,210 --> 00:31:30,170
到?_4的函数

602
00:31:30,280 --> 00:31:35,830
一种学习模型参数的方法

603
00:31:35,940 --> 00:31:40,190
是利用成本函数

604
00:31:40,290 --> 00:31:42,640
J(?)

605
00:31:42,720 --> 00:31:45,030
应该等于1/2 ∑_(i=1)^m?

606
00:31:45,150 --> 00:31:52,480
(y^((i) )-h_? (x^((i) ) ) )^2

607
00:31:54,780 --> 00:31:57,300
好的  这是我们熟悉的二次成本函数

608
00:31:57,410 --> 00:32:01,570
一种学习这样的算法中的

609
00:32:01,680 --> 00:32:03,600
参数的算法是

610
00:32:03,690 --> 00:32:06,450
使用梯度下降算法使J(?)

611
00:32:06,550 --> 00:32:08,950
作为?的函数最小  明白吗?

612
00:32:09,050 --> 00:32:12,860
这里使用梯度下降算法

613
00:32:12,920 --> 00:32:14,490
使得平方部分最小

614
00:32:14,590 --> 00:32:15,960
这意味着

615
00:32:16,070 --> 00:32:18,260
梯度下降算法使得你的

616
00:32:18,350 --> 00:32:20,390
神经网络的预测结果

617
00:32:20,510 --> 00:32:22,040
和你观察到的训练集合中的

618
00:32:22,130 --> 00:32:25,660
样本标签尽可能接近  明白吗?

619
00:32:25,750 --> 00:32:31,940
实际上梯度下降算法

620
00:32:31,990 --> 00:32:34,350
应用在神经网络中时有一个专门的名字

621
00:32:34,410 --> 00:32:36,720
实现了梯度下降的算法

622
00:32:36,820 --> 00:32:38,070
称之为反向传播

623
00:32:38,150 --> 00:32:39,560
所以如果你听到这个概念

624
00:32:39,670 --> 00:32:41,110
它实际上指的就是

625
00:32:41,200 --> 00:32:43,720
在这样的一个神经网络上

626
00:32:43,840 --> 00:32:46,100
对一个这样的成本函数执行梯度下降算法

627
00:32:46,190 --> 00:32:52,520
实际上  这个算法

628
00:32:52,610 --> 00:32:53,780
有一些优点  也有一些缺点

629
00:32:53,900 --> 00:32:57,730
让我展示给你们 让我看看

630
00:32:57,840 --> 00:32:59,610
有一个有趣的东西

631
00:32:59,700 --> 00:33:01,740
你一定想知道这些中间节点

632
00:33:01,850 --> 00:33:03,890
在计算什么东西  对吗?

633
00:33:03,970 --> 00:33:06,880
这样的一层中间单元

634
00:33:06,990 --> 00:33:10,750
被称为隐藏层  它在输出层之前

635
00:33:10,840 --> 00:33:13,440
更加一般地  你实际上

636
00:33:13,520 --> 00:33:16,790
可以将输入反馈给这些计算单元

637
00:33:16,880 --> 00:33:18,650
之后反馈给更多层的计算单元

638
00:33:18,720 --> 00:33:20,350
甚至可以再多层

639
00:33:20,450 --> 00:33:22,150
最终你到达了输出层

640
00:33:22,260 --> 00:33:25,370
你们可以仔细看看这些

641
00:33:25,480 --> 00:33:26,950
中间的计算单元

642
00:33:27,040 --> 00:33:29,280
看看这些被称之为

643
00:33:29,360 --> 00:33:31,310
隐藏层的计算单元  这真的很酷

644
00:33:31,400 --> 00:33:33,180
不要关心为什么它被称之为隐藏层

645
00:33:33,280 --> 00:33:34,680
看看这些隐藏单元的计算

646
00:33:34,760 --> 00:33:36,580
你可能会问这些隐藏单元

647
00:33:36,660 --> 00:33:40,210
在计算什么?所以

648
00:33:40,300 --> 00:33:41,840
为了更好地了解神经网络

649
00:33:41,930 --> 00:33:43,340
让我向你们展示一段视频--

650
00:33:43,430 --> 00:33:44,650
我要切换到笔记本--

651
00:33:44,750 --> 00:33:47,630
这是我的一个朋友做的

652
00:33:47,720 --> 00:33:50,930
他叫Yann LeCun  现在

653
00:33:51,030 --> 00:33:52,060
是纽约大学的教授

654
00:33:52,160 --> 00:33:54,400
我能播放笔记本上的视频吗?

655
00:33:54,480 --> 00:33:59,770
所以让我向你们展示一段来自于Yann LeCun

656
00:33:59,850 --> 00:34:03,280
的视频  展示的是他所开发的一个神经网络

657
00:34:03,390 --> 00:34:05,640
用来进行Hammerton数字的识别

658
00:34:05,760 --> 00:34:08,350
他在这个神经网络中还做了一项工作

659
00:34:08,460 --> 00:34:10,260
我不打算介绍了

660
00:34:10,370 --> 00:34:12,660
这项工作被称之为卷积神经网络--

661
00:34:12,760 --> 00:34:23,290
他的系统被称之为LeNet  让我们看看

662
00:34:23,370 --> 00:34:25,130
你能让笔记本显示出来吗?

663
00:34:25,220 --> 00:34:38,730
哦  实际上也许--

664
00:34:38,830 --> 00:34:40,730
你可以用旁边的那个屏幕吗?

665
00:34:40,840 --> 00:34:43,810
如果大屏幕不工作的话我们可以用那个

666
00:34:43,920 --> 00:35:14,810
让我看看 我在想  好的

667
00:35:14,910 --> 00:35:16,190
在我们等视频的这段时间里

668
00:35:16,290 --> 00:35:17,770
我该说些什么让大家娱乐一下呢?

669
00:35:17,860 --> 00:35:21,300
好的  我再讲点东西

670
00:35:21,390 --> 00:35:22,180
关于神经网络

671
00:35:22,280 --> 00:35:26,950
实际上  当你写出一个

672
00:35:27,060 --> 00:35:29,240
像我黑板上写的那样的

673
00:35:29,350 --> 00:35:30,680
二次成本函数

674
00:35:30,780 --> 00:35:33,720
实际上  不像logistic回归

675
00:35:33,830 --> 00:35:35,530
神经网络算法

676
00:35:35,630 --> 00:35:37,610
总是会对非凸优化问题做出相应

677
00:35:37,720 --> 00:35:42,160
而对于logistic回归

678
00:35:42,290 --> 00:35:43,760
如果你运行梯度下降算法或者牛顿方法

679
00:35:43,860 --> 00:35:45,620
你最终会收敛到一个全局最优值

680
00:35:45,740 --> 00:35:47,770
但是对于神经网络来说就不是这样

681
00:35:47,870 --> 00:35:50,770
通常情况下  有太多的局部最优值

682
00:35:50,880 --> 00:35:52,810
这使得全局优化变得更加困难

683
00:35:52,930 --> 00:35:57,810
所以  如果你对神经网络

684
00:35:57,920 --> 00:35:59,110
比较熟悉的话  并且比较擅长进行

685
00:35:59,220 --> 00:36:00,900
设计决策  例如:设定学习率

686
00:36:01,080 --> 00:36:03,370
指定隐藏单元的数量  等等

687
00:36:03,490 --> 00:36:06,440
那么你就可以让神经网络工作的比较搞笑

688
00:36:06,530 --> 00:36:10,910
实际上  经常会存在一些争论

689
00:36:11,050 --> 00:36:12,280
关于是这个算法更好

690
00:36:12,400 --> 00:36:13,440
或者那个算法更好?

691
00:36:13,550 --> 00:36:14,990
当今机器学习

692
00:36:15,080 --> 00:36:16,230
学者的主流观点

693
00:36:16,340 --> 00:36:18,760
还是倾向于支持向量机

694
00:36:18,870 --> 00:36:19,980
我之后会讲到

695
00:36:20,080 --> 00:36:21,870
它是一个更高效且无需定制的学习算法

696
00:36:21,980 --> 00:36:23,420
相对于神经网络算法

697
00:36:23,540 --> 00:36:25,730
这种观点存在着些许争议

698
00:36:25,840 --> 00:36:29,250
但是神经网络算法

699
00:36:29,360 --> 00:36:30,730
我个人用的并不多

700
00:36:30,850 --> 00:36:33,840
因为神经网络算法存在严重的优化问题

701
00:36:33,940 --> 00:36:35,370
你需要做很多工作

702
00:36:35,480 --> 00:36:37,970
它才可以工作

703
00:36:38,090 --> 00:36:39,180
实际上  多数情况下

704
00:36:39,180 --> 00:36:40,370
它的工作效率还是不错的

705
00:36:40,490 --> 00:36:43,580
仅仅由于它相对复杂

706
00:36:43,690 --> 00:36:45,080
所以这并不是一个

707
00:36:45,170 --> 00:36:48,490
我和我的朋友经常使用的算法

708
00:36:48,600 --> 00:36:52,120
好的 让我继续

709
00:36:52,230 --> 00:36:54,000
向你们展示一个神经网络的例子

710
00:36:54,120 --> 00:36:57,210
这是许多年中

711
00:36:57,320 --> 00:36:58,980
在支持向量机发明以前

712
00:36:59,090 --> 00:37:00,800
效率最高的学习算法

713
00:37:00,920 --> 00:37:04,840
这是Yan LeCun的视频

714
00:37:04,910 --> 00:37:08,800
实际上这个视频也有声音

715
00:37:08,880 --> 00:37:10,320
我会告诉你们视频讲的是什么

716
00:37:10,410 --> 00:37:14,030
你们看到的是一个训练好了的神经网络

717
00:37:14,130 --> 00:37:16,230
我鼠标

718
00:37:16,320 --> 00:37:17,270
指的这里

719
00:37:17,380 --> 00:37:22,680
这个很大的"3"就是这个神经网络的输入

720
00:37:22,780 --> 00:37:24,530
所以你们向神经网络展示这张图片

721
00:37:24,640 --> 00:37:26,490
它会认出这张图片中的数字

722
00:37:26,590 --> 00:37:29,190
神经网络的最终输出

723
00:37:29,290 --> 00:37:30,310
是这上边的数字

724
00:37:30,390 --> 00:37:32,260
就在"LeNet-5"字样下面

725
00:37:32,350 --> 00:37:36,020
神经网络正确地认出了这个数字

726
00:37:36,110 --> 00:37:37,150
是3

727
00:37:37,280 --> 00:37:39,660
如果你看到左边的这些图片

728
00:37:39,780 --> 00:37:42,950
非常有趣

729
00:37:43,050 --> 00:37:45,750
它们展示的是

730
00:37:45,870 --> 00:37:48,630
神经网络的中间计算结果

731
00:37:48,740 --> 00:37:50,360
换句话说  它展示的是

732
00:37:50,460 --> 00:37:53,270
神经网络隐藏层的计算结果

733
00:37:53,390 --> 00:37:55,540
例如  你可以看一下这张图片

734
00:37:55,630 --> 00:37:57,220
从上往下数第三张

735
00:37:57,340 --> 00:37:59,450
它似乎计算的是

736
00:37:59,560 --> 00:38:01,930
数字图形的边界  明白吗?

737
00:38:02,020 --> 00:38:04,000
我们计算了右手边

738
00:38:04,110 --> 00:38:07,080
就是下边右侧

739
00:38:07,190 --> 00:38:11,220
的输入图片的数字  明白吗?

740
00:38:11,330 --> 00:38:13,140
让我们播放这段视频

741
00:38:13,250 --> 00:38:18,120
你们可以看到一些

742
00:38:18,230 --> 00:38:19,410
神经网络的输入和输出

743
00:38:19,520 --> 00:38:22,710
它们是非常不同的字体

744
00:38:22,830 --> 00:39:16,480
算法对于噪声干扰的健壮性非常强

745
00:39:16,590 --> 00:39:45,840
多个数字的识别 非常酷

746
00:39:45,980 --> 00:39:55,170
放松一下

747
00:39:55,270 --> 00:39:59,210
我再给你们放一段视频

748
00:39:59,330 --> 00:40:05,000
这是另外一段视频

749
00:40:05,100 --> 00:40:06,400
来自"the machine that changed the world"

750
00:40:06,510 --> 00:40:09,920
由WGBH电视公司

751
00:40:10,040 --> 00:40:12,210
和British Foreclass 联合出品

752
00:40:12,320 --> 00:40:15,260
我记得这部片子几年前在PBS上播出过

753
00:40:15,370 --> 00:40:18,120
我要放的视频描述的是

754
00:40:18,230 --> 00:40:20,420
NETalk神经网络

755
00:40:20,510 --> 00:40:23,110
是由Terry Sejnowski发明的

756
00:40:23,200 --> 00:40:24,170
他是一个研究者

757
00:40:24,250 --> 00:40:27,900
NETtalk是神经网络发展史中

758
00:40:28,990 --> 00:40:30,970
重要的里程碑之一

759
00:40:31,100 --> 00:40:32,930
它的应用是

760
00:40:33,040 --> 00:40:36,120
可以用神经网络来阅读文本

761
00:40:36,230 --> 00:40:39,150
换句话说  你能将一段

762
00:40:39,260 --> 00:40:43,300
英文展示给一台计算机  让它阅读这段文本

763
00:40:43,430 --> 00:40:45,550
并能够让它根据这段文本的阅读方式

764
00:40:45,650 --> 00:40:48,970
将它完整的按照发音念出来

765
00:40:49,080 --> 00:40:52,530
实际上  在人工智能的历史上

766
00:40:52,640 --> 00:40:53,820
以及在机器学习的历史上

767
00:40:53,930 --> 00:40:58,000
这段视频着实让人们

768
00:40:58,120 --> 00:40:59,890
为神经网络和机器学习大为惊叹

769
00:40:59,990 --> 00:41:04,050
部分原因是因为  Terry Sejnowski

770
00:41:04,170 --> 00:41:07,710
在他的视频中  预见性地使用了

771
00:41:07,830 --> 00:41:11,650
一个孩子的声音来讲述

772
00:41:11,760 --> 00:41:12,910
拜访奶奶的故事

773
00:41:12,990 --> 00:41:14,450
你们一会儿会看到

774
00:41:14,510 --> 00:41:18,920
这段视频使人们对神经网络

775
00:41:19,300 --> 00:41:21,380
有了这样的印象

776
00:41:21,490 --> 00:41:23,250
它就像一个孩子一样  一边学习说话

777
00:41:23,360 --> 00:41:25,190
一遍讲着去看奶奶的故事

778
00:41:25,290 --> 00:41:28,050
这部视频确实产生了

779
00:41:28,140 --> 00:41:29,930
不小的轰动  包括学术界

780
00:41:30,030 --> 00:41:32,240
和非学术界

781
00:41:32,320 --> 00:41:33,720
在早期的神经网络的发展史中

782
00:41:33,820 --> 00:41:35,140
我接下来就要给你们放一下这段视频

783
00:41:35,250 --> 00:41:36,960
你们接下来首先会听到

784
00:41:37,050 --> 00:41:39,010
在开始训练的时候

785
00:41:39,090 --> 00:41:40,420
神经网络的发音

786
00:41:40,500 --> 00:41:41,930
它的发音听起来并不像是词

787
00:41:42,040 --> 00:41:44,670
但是它的发音会尝试着

788
00:41:44,770 --> 00:41:46,100
随着时间而越来越好

789
00:41:46,220 --> 00:41:55,270
神经网络读取字母

790
00:41:55,370 --> 00:41:58,390
尝试说出短语"grandmother’s house"

791
00:41:58,480 --> 00:42:01,850
进行一种随机化的发音尝试

792
00:42:07,360 --> 00:42:08,260
奶奶的房子

793
00:42:08,360 --> 00:42:10,870
猜测发音和标准发音之间的差异

794
00:42:10,980 --> 00:42:13,200
会被反馈回去

795
00:42:13,310 --> 00:42:14,730
通过神经网络

796
00:42:14,830 --> 00:42:19,710
奶奶的房子

797
00:42:19,820 --> 00:42:22,300
通过在每次尝试之后

798
00:42:22,410 --> 00:42:24,990
调整连接强度  网络会逐渐改进

799
00:42:25,100 --> 00:42:28,920
最终  通过一整晚的训练

800
00:42:29,010 --> 00:42:32,000
第二天早晨  它的发音变成了这样:

801
00:42:32,110 --> 00:42:34,140
奶奶的房子

802
00:42:34,280 --> 00:42:37,100
我愿意去奶奶的房子看她

803
00:42:37,200 --> 00:42:41,690
因为她会给我们糖  并且我们--

804
00:42:41,800 --> 00:42:44,250
NETtalk对于语言一无所知

805
00:42:44,360 --> 00:42:47,370
它只是简单地将字母和声音联系起来

806
00:42:47,470 --> 00:42:50,240
好的

807
00:42:50,360 --> 00:42:54,640
视频结束了  我只想说

808
00:42:54,730 --> 00:42:56,410
这真的是惊人之作

809
00:42:56,510 --> 00:43:00,190
尽管现在有很多

810
00:43:00,280 --> 00:43:01,860
文本发音系统要比刚才的系统

811
00:43:01,970 --> 00:43:03,040
性能更好

812
00:43:03,160 --> 00:43:06,390
而且你们也许会承认

813
00:43:06,500 --> 00:43:08,970
在今天  "从奶奶的房子里取糖果"并不比

814
00:43:09,080 --> 00:43:10,880
"道琼斯下跌15点"或者

815
00:43:10,970 --> 00:43:13,410
"如何抛售股票"更能吸引你

816
00:43:13,520 --> 00:43:17,280
但是我之所以要展示这段视频

817
00:43:17,370 --> 00:43:19,320
就是因为这项工作

818
00:43:19,410 --> 00:43:21,730
是神经网络历史上的一个标志

819
00:43:21,840 --> 00:43:30,010
好的  让我们回到黑板

820
00:43:30,110 --> 00:43:39,990
接下来我要给你们讲的是

821
00:43:40,100 --> 00:43:43,690
支持向量机

822
00:43:43,800 --> 00:43:45,620
对于神经网络的讨论今天就到这里

823
00:43:45,710 --> 00:44:10,440
记得我刚讲到神经网络的时候

824
00:44:10,520 --> 00:44:14,170
我举的那个例子

825
00:44:14,300 --> 00:44:16,320
得到一个非线性分类器  对吗?

826
00:44:16,440 --> 00:44:17,510
我并没有证明它

827
00:44:17,620 --> 00:44:19,600
实际上通过神经网络

828
00:44:19,700 --> 00:44:21,860
你是可以得到一个非线性的分界线的

829
00:44:21,960 --> 00:44:24,960
正如我之前黑板上画的那样

830
00:44:25,050 --> 00:44:28,540
支持向量机是

831
00:44:28,640 --> 00:44:29,890
另外一种学习算法

832
00:44:30,000 --> 00:44:30,960
可以给我们一种方式

833
00:44:31,040 --> 00:44:32,710
生成一个非线性分类器

834
00:44:32,820 --> 00:44:33,970
它是一个非常高效

835
00:44:34,070 --> 00:44:35,550
且无需定制的学习算法

836
00:44:35,660 --> 00:44:37,940
在我们的讲课过程中

837
00:44:38,040 --> 00:44:40,990
也就是在后续的

838
00:44:41,080 --> 00:44:42,130
讲课过程中

839
00:44:42,240 --> 00:44:44,550
我会开始先介绍另外一类

840
00:44:44,690 --> 00:44:46,360
线性分类器

841
00:44:46,530 --> 00:44:50,890
可以给出线性分界线  之后

842
00:44:51,000 --> 00:44:53,840
可能是下一讲或者下下讲

843
00:44:53,940 --> 00:44:55,940
我们会利用

844
00:44:56,040 --> 00:44:57,640
支持向量机的想法

845
00:44:57,750 --> 00:45:00,170
进行一些巧妙的改动和扩展  让它可以

846
00:45:00,270 --> 00:45:02,230
很好地生成非线性分界线

847
00:45:02,320 --> 00:45:03,200
明白吗?

848
00:45:03,310 --> 00:45:04,620
但是现在我们先要

849
00:45:04,730 --> 00:45:06,110
再讲一点线性分类器的知识

850
00:45:06,240 --> 00:45:13,000
为了引出这个问题  我需要介绍

851
00:45:13,120 --> 00:45:15,600
两种对于分类的直观理解

852
00:45:15,720 --> 00:45:21,530
第一种直观理解需要考虑logistic回归

853
00:45:21,670 --> 00:45:24,350
我们用一个logistic函数来表示

854
00:45:24,460 --> 00:45:26,350
y=1的概率

855
00:45:26,880 --> 00:45:32,110
它在x=0处和这条线相交

856
00:45:32,220 --> 00:45:34,400
所以当你进行logistic回归的时候

857
00:45:34,520 --> 00:45:39,020
你可以认为它是一个算法

858
00:45:39,130 --> 00:45:43,660
首先计算?^T x

859
00:45:43,720 --> 00:45:52,200
预测值为1  当且仅当

860
00:45:52,270 --> 00:45:54,190
?^T x>0  对吗?

861
00:45:54,290 --> 00:45:55,690
iff表示当且仅当

862
00:45:55,810 --> 00:45:58,420
它等同于双向蕴含

863
00:45:58,520 --> 00:46:05,350
它预测值为0  当且仅当

864
00:46:05,440 --> 00:46:07,860
?^T x小于0  对吗?

865
00:46:07,990 --> 00:46:16,490
那么对于?^T x

866
00:46:16,600 --> 00:46:17,880
远大于0的情况

867
00:46:18,450 --> 00:46:20,470
>>符号表示

868
00:46:20,560 --> 00:46:22,360
远大于

869
00:46:22,480 --> 00:46:24,540
所以如果?^T x?0

870
00:46:24,630 --> 00:46:27,780
那么你可以认为

871
00:46:27,850 --> 00:46:39,100
我们可以相当确定地预测

872
00:46:39,210 --> 00:46:40,290
y=1  对吗?

873
00:46:40,400 --> 00:46:43,080
如果?^T x?0

874
00:46:43,180 --> 00:46:44,460
我们会预测1

875
00:46:44,560 --> 00:46:46,380
并且我们对于这个预测结果相当确定

876
00:46:46,480 --> 00:46:47,600
在图上表示出来

877
00:46:47,670 --> 00:46:49,920
如果?^T x在这里

878
00:46:50,020 --> 00:46:53,020
那么我们根据sigmoid函数

879
00:46:53,110 --> 00:46:55,510
估计的y=1的概率

880
00:46:55,600 --> 00:46:57,050
将会非常接近于1

881
00:46:57,130 --> 00:47:01,240
同理  如果?^T x

882
00:47:01,330 --> 00:47:04,000
远小于0

883
00:47:04,080 --> 00:47:13,820
那我们会非常确定y=0

884
00:47:13,910 --> 00:47:20,610
所以此时我们会认为这样的模型是良好的--

885
00:47:20,710 --> 00:47:23,050
当我们通过训练集合

886
00:47:23,150 --> 00:47:24,350
根据logistic回归拟合出分类器时

887
00:47:24,450 --> 00:47:30,490
这样的分类器是良好的

888
00:47:30,590 --> 00:47:35,660
如果对于所有的i  如果y=1

889
00:47:35,740 --> 00:47:42,310
那么?^T x

890
00:47:42,360 --> 00:47:43,950
远大于0

891
00:47:44,070 --> 00:47:50,050
对于所有的i  如果y=0

892
00:47:50,170 --> 00:48:00,750
那么?^T x?0

893
00:48:00,860 --> 00:48:02,240
明白吗?所以如果这些条件成立的话

894
00:48:02,240 --> 00:48:03,230
这个模型的性质就会非常好.

895
00:48:03,330 --> 00:48:07,940
实际上  这句话的意思就是说

896
00:48:08,030 --> 00:48:10,310
如果我们根据训练集合找到了参数?

897
00:48:10,400 --> 00:48:13,020
那么我们的学习算法

898
00:48:13,110 --> 00:48:14,840
不仅需要保证分类结果正确

899
00:48:14,930 --> 00:48:17,550
更要进一步地

900
00:48:17,620 --> 00:48:18,470
保证对于分类结果的

901
00:48:18,560 --> 00:48:19,850
确定性

902
00:48:19,940 --> 00:48:24,240
这是我讲到的第一种直观理解

903
00:48:24,330 --> 00:48:27,560
我们之后会再回过头来看

904
00:48:27,630 --> 00:48:33,310
当我们讲到函数间隔这个概念的时候  好吗?

905
00:48:33,420 --> 00:48:34,660
我们稍后会定义这个概念

906
00:48:34,760 --> 00:48:45,050
我想讲的第二种直观理解

907
00:48:45,140 --> 00:48:59,330
实际上在今天剩下的内容中

908
00:48:59,390 --> 00:49:01,190
我都会假设我们的训练集合

909
00:49:01,310 --> 00:49:03,150
是线性可分隔的  明白吗?

910
00:49:03,270 --> 00:49:07,390
我的意思是在今天剩下的内容中

911
00:49:07,470 --> 00:49:08,510
我会假设

912
00:49:08,600 --> 00:49:10,470
一定有一条直线

913
00:49:10,590 --> 00:49:11,720
可以将训练集合分开

914
00:49:11,820 --> 00:49:14,420
我们稍后会去除这个假设

915
00:49:14,540 --> 00:49:16,210
为了进一步发展我们的算法

916
00:49:16,330 --> 00:49:18,790
我们将在稍后去除线性可分隔这个假设

917
00:49:18,890 --> 00:49:21,430
我们似乎可以感觉到

918
00:49:21,540 --> 00:49:22,790
在所有可以分隔

919
00:49:22,920 --> 00:49:24,200
这个训练集合的直线中

920
00:49:24,310 --> 00:49:25,230
也许这条线

921
00:49:25,320 --> 00:49:26,530
不是一种好的分隔

922
00:49:26,640 --> 00:49:30,990
这条线也不好

923
00:49:31,150 --> 00:49:36,070
但是中间的这条线

924
00:49:36,170 --> 00:49:38,040
比另外两条都要好  对吗?

925
00:49:38,150 --> 00:49:42,100
我之所以认为这条线最好

926
00:49:42,220 --> 00:49:46,740
是因为这条线

927
00:49:46,840 --> 00:49:48,110
里两边的数据都比较远  对吗?

928
00:49:48,190 --> 00:49:51,170
这条线分隔了两种数据

929
00:49:51,250 --> 00:49:52,450
使得正样本和负样本

930
00:49:52,520 --> 00:49:55,000
到它的举例更大  明白吗?

931
00:49:55,100 --> 00:49:57,260
我们稍后会来讨论

932
00:49:57,350 --> 00:49:58,710
这第二种直观理解

933
00:49:58,820 --> 00:50:02,450
也就是关于我最后话的这条线

934
00:50:02,560 --> 00:50:07,750
它可能是最好的线  因为它距两边样本

935
00:50:07,860 --> 00:50:09,340
都有一定距离

936
00:50:09,440 --> 00:50:10,740
这是我讲的第二种直观理解

937
00:50:10,800 --> 00:50:14,160
当我们讲到分类器的几何间隔时

938
00:50:14,250 --> 00:50:20,090
我们会再回来正式地讨论它  好吗?

939
00:50:20,200 --> 00:50:43,680
为了描述支持向量机

940
00:50:43,790 --> 00:50:46,080
很不幸  我需要

941
00:50:46,190 --> 00:50:50,250
改变一下之前使用的符号

942
00:50:50,330 --> 00:50:52,130
而且很不幸

943
00:50:52,240 --> 00:50:54,000
我们无法对logistic回归

944
00:50:54,130 --> 00:50:55,140
和支持向量机

945
00:50:55,250 --> 00:50:57,170
以及所有其他的算法

946
00:50:57,290 --> 00:50:59,490
使用一整套一致的符号表示

947
00:50:59,600 --> 00:51:03,190
我需要对线性分类器

948
00:51:03,300 --> 00:51:05,360
的符号表示进行少量的改动

949
00:51:05,470 --> 00:51:07,100
这些改动会使

950
00:51:07,190 --> 00:51:09,610
我们今天之后的课

951
00:51:09,720 --> 00:51:11,170
以及下周的课上

952
00:51:11,300 --> 00:51:13,140
对支持向量机的讲解更为便利

953
00:51:13,280 --> 00:51:15,990
这些符号改动我今天

954
00:51:16,750 --> 00:51:21,410
和下周的大部分时间都会使用

955
00:51:21,470 --> 00:51:24,000
我会让y可取的值

956
00:51:24,110 --> 00:51:27,220
从

957
00:51:27,330 --> 00:51:28,780
改为

958
00:51:28,880 --> 00:51:35,680
还需要为支持向量机做出一些修改

959
00:51:35,790 --> 00:51:44,720
我们会让h输出的假设值为

960
00:51:44,840 --> 00:51:50,870
-1或1

961
00:51:50,960 --> 00:51:59,370
我们会令g(z)=1

962
00:51:59,490 --> 00:52:01,380
当z>=0时

963
00:52:01,480 --> 00:52:03,710
令其等于-1  当z小于0时  明白吗?

964
00:52:03,810 --> 00:52:05,290
所以对于原有的0和1

965
00:52:05,400 --> 00:52:06,810
我们全部用-1和1代替

966
00:52:06,940 --> 00:52:12,290
最后  当我们之前写

967
00:52:12,410 --> 00:52:17,100
g(?^T x) 时

968
00:52:17,210 --> 00:52:19,960
我们约定

969
00:52:20,070 --> 00:52:22,590
x_0=1  对吗?

970
00:52:22,690 --> 00:52:27,090
所以x是一个n+1维的向量

971
00:52:27,200 --> 00:52:35,200
现在我要扔掉这个约定  即:

972
00:52:35,300 --> 00:52:37,810
x_0=1   且x是一个n+1维的向量

973
00:52:37,910 --> 00:52:40,030
我要使我们的线性分类器

974
00:52:40,160 --> 00:52:42,190
以w和b为参数

975
00:52:42,280 --> 00:52:50,290
它等于g(w^T x+b)  明白吗?

976
00:52:50,400 --> 00:52:55,490
所以现在b的作用和之前的?_0是一样的

977
00:52:55,600 --> 00:52:58,440
而w表示的是那些

978
00:52:58,530 --> 00:52:59,960
剩下的参数

979
00:53:00,040 --> 00:53:02,760
?_1 到?_n  明白吗?

980
00:53:02,860 --> 00:53:08,680
通过将截距b分离出来

981
00:53:08,800 --> 00:53:10,050
而不是像之前那样将它们合在一起

982
00:53:10,140 --> 00:53:11,230
我们可以更方便地

983
00:53:11,340 --> 00:53:24,890
引出支持向量机 什么?

984
00:53:25,010 --> 00:53:28,000
【听不清】

985
00:53:28,080 --> 00:53:29,360
是的

986
00:53:29,480 --> 00:53:35,820
w是一个n维实数向量

987
00:53:36,090 --> 00:53:39,500
x现在是一个n维向量而不是n+1维

988
00:53:39,600 --> 00:53:53,600
b是一个实数  好的

989
00:53:53,720 --> 00:53:57,830
现在让我们正式定义一下

990
00:53:57,920 --> 00:53:59,520
函数间隔和几何间隔

991
00:53:59,630 --> 00:54:02,990
让我们来定义它们

992
00:54:03,100 --> 00:54:06,190
我会这样定义:

993
00:54:06,310 --> 00:54:22,640
一个超平面(w  b)和某个

994
00:54:22,740 --> 00:54:27,630
特定的训练样本(x_i  y_i)相关的函数间隔

995
00:54:27,740 --> 00:54:30,560
(WRT表示和 相关)

996
00:54:30,680 --> 00:54:34,200
也就是一个超平面(w   b)和一个特定的

997
00:54:34,320 --> 00:54:37,000
训练样本对应的函数间隔

998
00:54:37,120 --> 00:54:40,910
被定义为:

999
00:54:41,020 --> 00:54:50,360
γ ?^((i) )=y^((i) ) (w^T x^((i) )+b)  明白吗?

1000
00:54:50,550 --> 00:54:52,870
参数(w   b)

1001
00:54:52,950 --> 00:54:53,780
定义了一个分类器

1002
00:54:53,870 --> 00:55:00,360
它可以是定义了一个线性的分界线

1003
00:55:00,460 --> 00:55:02,180
所以当我提到超平面的时候

1004
00:55:02,280 --> 00:55:05,020
我想表达的就是由(w   b)

1005
00:55:05,130 --> 00:55:09,170
确定的分界

1006
00:55:09,280 --> 00:55:13,360
如果你对超平面这个词感到困惑

1007
00:55:13,470 --> 00:55:15,250
就先忽略它吧

1008
00:55:15,360 --> 00:55:18,390
由(w  b)定义的超平面

1009
00:55:18,510 --> 00:55:21,400
和一个训练样本的函数间隔

1010
00:55:21,510 --> 00:55:23,390
由这个式子给出

1011
00:55:23,500 --> 00:55:29,300
对于这个式子的解释是  如果y^((i) )=1

1012
00:55:29,400 --> 00:55:31,640
为了获得较大的函数间隔

1013
00:55:31,760 --> 00:55:38,080
你需要令w^T x^((i) )+ b取较大的值  对吗?

1014
00:55:38,160 --> 00:55:42,120
如果y^((i) )=-1

1015
00:55:42,220 --> 00:55:45,980
为了获得较大的函数间隔

1016
00:55:46,090 --> 00:55:47,710
我们希望获得较大的函数间隔

1017
00:55:47,810 --> 00:55:49,650
为了使函数间隔取较大的值

1018
00:55:49,790 --> 00:55:51,490
如果y_i=-1

1019
00:55:51,600 --> 00:55:53,960
那么唯一使其取较大值的方式是

1020
00:55:54,070 --> 00:55:59,870
令:w^T x^((i) )+b?0  明白吗?

1021
00:55:59,980 --> 00:56:05,350
这个定义捕捉到了之前我们

1022
00:56:05,470 --> 00:56:07,390
对于函数间隔的直观理解的特点

1023
00:56:07,490 --> 00:56:11,780
在之前的直观理解中  如果y^((i) )=1

1024
00:56:11,880 --> 00:56:12,850
我们希望这一项取较大的值

1025
00:56:12,950 --> 00:56:13,950
如果y^((i) )=-1

1026
00:56:14,060 --> 00:56:17,670
我们希望这一项取较小的值

1027
00:56:17,780 --> 00:56:19,830
这个定义用一个命题捕捉到了

1028
00:56:19,940 --> 00:56:21,700
我们希望函数间隔取较大值的两种情况

1029
00:56:21,810 --> 00:56:27,450
同时注意到

1030
00:56:27,470 --> 00:56:31,860
只要y^((i) ) (w^T x^((i) )+b)

1031
00:56:31,960 --> 00:56:33,360
大于0

1032
00:56:33,450 --> 00:56:37,820
这就意味着我们的分类结果是正确的  对吗?

1033
00:56:37,930 --> 00:57:15,580
再来一个定义

1034
00:57:15,690 --> 00:57:18,960
我要将

1035
00:57:19,080 --> 00:57:21,530
一个超平面和

1036
00:57:21,610 --> 00:57:33,050
整个训练集合的函数间隔定义为:

1037
00:57:33,150 --> 00:57:36,220
γ ?  等于

1038
00:57:36,330 --> 00:57:39,080
所有的训练样本对应的γ ?^((i) )的最小值  明白吗?

1039
00:57:39,200 --> 00:57:41,930
如果你有一个训练集合

1040
00:57:42,070 --> 00:57:43,440
如果你有超过一个的训练样本

1041
00:57:43,560 --> 00:57:46,130
我会将相对于整个训练集合

1042
00:57:46,250 --> 00:57:48,500
的函数间隔定义为

1043
00:57:48,610 --> 00:57:51,170
所有相对于样本的

1044
00:57:51,290 --> 00:57:53,320
函数间隔的最坏情形

1045
00:57:53,420 --> 00:57:56,110
现在  我们应该认为

1046
00:57:56,210 --> 00:58:00,080
第一个函数像一个直观理解

1047
00:58:00,170 --> 00:58:02,850
使我们希望使函数间隔取较大的值

1048
00:58:02,980 --> 00:58:05,290
而对于我们现在的需求而言

1049
00:58:05,410 --> 00:58:08,000
我们希望最坏情况的

1050
00:58:08,120 --> 00:58:10,750
函数间隔取较大的值  明白吗?

1051
00:58:10,840 --> 00:58:12,750
我们之后会稍微改变需求

1052
00:58:12,860 --> 00:58:17,970
实际上  这种直观理解

1053
00:58:18,060 --> 00:58:22,200
存在一个小问题

1054
00:58:22,310 --> 00:58:24,610
实际上  要使函数间隔

1055
00:58:24,720 --> 00:58:26,570
取较大的值是非常容易的

1056
00:58:26,680 --> 00:58:27,800
比如说

1057
00:58:27,910 --> 00:58:30,820
如果我们的参数是w和b

1058
00:58:30,910 --> 00:58:34,480
那么我们可以将w变为原来的2倍

1059
00:58:34,590 --> 00:58:36,500
将b也变为原来的2倍

1060
00:58:36,610 --> 00:58:39,040
那么根据

1061
00:58:39,140 --> 00:58:41,210
函数间隔的定义

1062
00:58:41,330 --> 00:58:44,380
γ ?^((i) ) 等于

1063
00:58:44,490 --> 00:58:50,420
y^((i) ) (w^T x^((i) )+b)

1064
00:58:50,520 --> 00:58:53,710
如果我令w和b都加倍

1065
00:58:53,810 --> 00:58:57,620
那么我可以很容易地使函数间隔加倍

1066
00:58:57,710 --> 00:59:00,870
所以单纯地以最大化函数间隔为目标

1067
00:59:00,990 --> 00:59:02,950
是没有多大意义的

1068
00:59:03,050 --> 00:59:04,870
因为通过对参数翻倍

1069
00:59:04,950 --> 00:59:07,480
我们可以使函数间隔获得任意大的值

1070
00:59:07,590 --> 00:59:10,200
也许我们可以解决这个问题

1071
00:59:10,310 --> 00:59:14,480
通过添加一个正规化条件

1072
00:59:14,540 --> 00:59:17,640
例如  也许我们可以

1073
00:59:17,750 --> 00:59:19,780
加一个正规化条件

1074
00:59:19,900 --> 00:59:23,860
使得w的长度为1

1075
00:59:23,960 --> 00:59:28,210
我们会稍后再来看

1076
00:59:28,340 --> 00:59:35,280
接下来  我们要讲的是--

1077
00:59:35,390 --> 00:59:41,020
我们还剩15分钟

1078
00:59:41,110 --> 00:59:53,650
让我看看

1079
00:59:53,770 --> 01:00:04,630
接下来15分钟讲点什么

1080
01:00:04,740 --> 01:00:11,270
接下来我会讲几何间隔

1081
01:00:11,400 --> 01:00:15,750
训练样本的几何间隔

1082
01:00:15,860 --> 01:00:29,470
分类器的确定的边界

1083
01:00:29,580 --> 01:00:33,820
会由平面:

1084
01:00:33,940 --> 01:00:37,380
w^T x+b给出  对吗?

1085
01:00:37,880 --> 01:00:40,680
好的  这里是x_1和x_2轴

1086
01:00:40,790 --> 01:00:43,220
我们这里画上少量几个

1087
01:00:43,310 --> 01:00:44,540
训练样本

1088
01:00:44,660 --> 01:00:48,620
我是有意画这么少的

1089
01:00:48,720 --> 01:00:50,120
这样我就可以有空间

1090
01:00:50,230 --> 01:00:52,220
在这个图上再画一些东西了  明白吗?

1091
01:00:52,330 --> 01:00:56,670
假设我们正确地对样本进行了分类

1092
01:00:56,780 --> 01:00:58,960
我会将几何间隔定义为

1093
01:00:59,070 --> 01:01:04,550
一个训练样本

1094
01:01:04,660 --> 01:01:06,520
对应的点

1095
01:01:06,610 --> 01:01:07,630
是的

1096
01:01:07,720 --> 01:01:10,990
比如训练样本(x_i  y_i)

1097
01:01:11,100 --> 01:01:15,470
和由超平面确定的分隔线

1098
01:01:15,590 --> 01:01:18,430
之间的几何距离  明白吗?

1099
01:01:18,530 --> 01:01:19,630
这就是我对

1100
01:01:19,710 --> 01:01:21,650
几何间隔进行的定义

1101
01:01:21,770 --> 01:01:27,680
接下来我要快速地进行一些代数推导

1102
01:01:27,790 --> 01:01:29,800
如果你不是很明白

1103
01:01:30,020 --> 01:01:32,120
请课下仔细阅读讲义的相关内容

1104
01:01:32,230 --> 01:01:36,970
我们可以将法向量--

1105
01:01:37,060 --> 01:01:40,190
也就是说和超平面成

1106
01:01:40,300 --> 01:01:42,460
90度角的向量--表示为

1107
01:01:42,680 --> 01:01:46,240
w/||w||

1108
01:01:46,340 --> 01:01:48,680
这是高维平面的表示方式

1109
01:01:48,790 --> 01:01:52,100
我这里讲到的东西

1110
01:01:52,210 --> 01:01:54,230
都可以在讲义上看到

1111
01:01:54,330 --> 01:02:02,400
我们将这段距离表示为γ^((i) )

1112
01:02:02,480 --> 01:02:04,500
我们在符号表示上存在这样的约定

1113
01:02:04,590 --> 01:02:06,120
当我写γ ?时

1114
01:02:06,210 --> 01:02:07,750
我表示的是函数间隔

1115
01:02:07,850 --> 01:02:10,530
而当我写γ时  我表示的是几何间隔

1116
01:02:10,640 --> 01:02:12,720
由于在这个例子中我们表示的是

1117
01:02:12,800 --> 01:02:14,600
几何间隔  所以我们用γ^((i) )来表示

1118
01:02:14,720 --> 01:02:26,910
这意味着这个点

1119
01:02:27,000 --> 01:02:38,520
可以表示为x^((i) )-γ^((i) )*w/(||w||)  对吗?

1120
01:02:38,630 --> 01:02:43,430
因为w/||w||是单位向量

1121
01:02:43,550 --> 01:02:46,110
是一个长度为1且

1122
01:02:46,200 --> 01:02:47,800
与超平面垂直的向量

1123
01:02:47,900 --> 01:02:51,360
所以当我们用x^((i) )

1124
01:02:51,440 --> 01:02:53,430
减去γ^((i) )乘上这个单位向量

1125
01:02:53,530 --> 01:02:54,950
x^((i) )是这一点

1126
01:02:55,080 --> 01:02:57,400
用x^((i) )减去

1127
01:02:57,520 --> 01:03:00,530
这个小向量  将会得到

1128
01:03:00,630 --> 01:03:03,340
这个加粗的点  明白吗?

1129
01:03:03,450 --> 01:03:06,960
这个加粗的点等于x^((i) )减去这个向量

1130
01:03:07,040 --> 01:03:10,340
这个向量就是

1131
01:03:10,470 --> 01:03:12,810
γ^((i) )*w/(||w||)   明白吗?

1132
01:03:12,900 --> 01:03:18,580
因为这个加粗的点

1133
01:03:18,680 --> 01:03:21,370
在这个分隔平面上

1134
01:03:21,480 --> 01:03:29,040
所以  它应该还需要满足

1135
01:03:29,160 --> 01:03:36,010
这样的条件  对吗?

1136
01:03:36,120 --> 01:03:39,610
因为所有超平面上的点

1137
01:03:39,690 --> 01:03:41,560
都满足这个式子

1138
01:03:41,680 --> 01:03:43,940
所以这个

1139
01:03:44,050 --> 01:03:45,320
在超平面上的点

1140
01:03:45,430 --> 01:03:48,000
同样满足这个式子--

1141
01:03:48,110 --> 01:03:56,640
哦  对不起 还应该加上个b  明白了吗?

1142
01:03:56,730 --> 01:03:59,170
如果明白的话请举手

1143
01:03:59,260 --> 01:04:03,050
很好  大多数人 但是再强调一次

1144
01:04:03,130 --> 01:04:05,710
我在课上讲的这些关于几何的内容是非常快的

1145
01:04:05,820 --> 01:04:07,440
如果你们不是很确定

1146
01:04:07,520 --> 01:04:08,740
为什么这是一个法向量

1147
01:04:08,820 --> 01:04:09,430
或者为什么我要减去这些

1148
01:04:09,430 --> 01:04:10,040
或者一些其他的不明白的地方

1149
01:04:10,150 --> 01:04:12,100
回去请仔细看一下讲义

1150
01:04:12,200 --> 01:04:25,160
我接下来要做的

1151
01:04:25,240 --> 01:04:26,600
是通过这个公式

1152
01:04:26,680 --> 01:04:28,920
解出γ

1153
01:04:29,010 --> 01:04:30,160
这是我刚才写的公式

1154
01:04:30,270 --> 01:04:33,050
我要利用这个公式求解γ或者γ^((i) )

1155
01:04:33,150 --> 01:04:34,850
你会发现--

1156
01:04:34,980 --> 01:04:46,440
之前的那个公式

1157
01:04:46,530 --> 01:04:50,710
展开之后可以得到这个式子

1158
01:04:50,800 --> 01:04:54,800
可以得到:w^T x^((i) )+b等于

1159
01:04:54,890 --> 01:05:01,160
γ^((i) )  (w^T w)/(||w||)

1160
01:05:01,300 --> 01:05:06,470
这一项就等于||w||

1161
01:05:06,560 --> 01:05:11,000
因为w^T w等于||w||的平方

1162
01:05:11,080 --> 01:05:18,370
因此  γ就等于

1163
01:05:18,450 --> 01:05:32,880
这个式子  明白吗?

1164
01:05:33,010 --> 01:05:34,720
换句话说

1165
01:05:34,830 --> 01:05:36,970
这个计算告诉我们

1166
01:05:37,090 --> 01:05:41,540
对于一个训练样本x^((i) )

1167
01:05:41,670 --> 01:05:44,360
x^((i) )与

1168
01:05:44,450 --> 01:05:46,800
由参数w和b确定的

1169
01:05:46,910 --> 01:05:48,020
分隔平面之间的距离

1170
01:05:48,130 --> 01:05:53,100
可以由这个公式计算得到  明白吗?

1171
01:05:53,220 --> 01:06:04,190
我要做的最后一件事

1172
01:06:04,280 --> 01:06:07,950
是考虑正确对训练样本


1173
01:06:08,040 --> 01:06:11,080
进行分类的标识

1174
01:06:11,470 --> 01:06:12,830
因为一直以来我都在假设

1175
01:06:12,920 --> 01:06:15,500
我们已经对样本进行了正确的分类

1176
01:06:15,610 --> 01:06:22,510
所以  更为一般地

1177
01:06:22,630 --> 01:06:31,060
我们可以将几何间隔定义成

1178
01:06:31,160 --> 01:06:44,170
这样的形式  明白吗?

1179
01:06:44,300 --> 01:06:46,370
这样的定义形式和之前

1180
01:06:46,370 --> 01:06:49,070
函数间隔的定义形式非常相似

1181
01:06:49,200 --> 01:06:51,800
除了在这里我们队向量w进行了标准化

1182
01:06:51,890 --> 01:06:54,900
所以像往常一样

1183
01:06:55,010 --> 01:06:56,870
这个式子的意思是

1184
01:06:56,990 --> 01:06:59,680
我们希望几何间隔取较大的值

1185
01:06:59,760 --> 01:07:01,590
这也意味着

1186
01:07:01,710 --> 01:07:03,350
如果我们对训练样本进行了正确的分类

1187
01:07:03,470 --> 01:07:06,260
那么我们会理想地希望

1188
01:07:06,360 --> 01:07:08,700
这些样本距离分隔平面的距离越大越好

1189
01:07:08,800 --> 01:07:09,760
只要它在分隔平面

1190
01:07:09,870 --> 01:07:10,880
的正确的一侧

1191
01:07:10,980 --> 01:07:13,360
这也就是我们要乘上y^((i) )的原因

1192
01:07:21,310 --> 01:07:24,440
有几个非常容易得到的结论

1193
01:07:24,520 --> 01:07:30,130
一个是:如果||w||=1

1194
01:07:30,190 --> 01:07:34,980
那么函数间隔

1195
01:07:35,100 --> 01:07:37,130
等于几何间隔

1196
01:07:37,240 --> 01:07:39,630
这个结论非常简单

1197
01:07:39,750 --> 01:07:44,170
更为一般地  几何间隔

1198
01:07:44,280 --> 01:07:49,140
等于函数间隔除以

1199
01:07:49,280 --> 01:08:11,900
||w||  明白吗?

1200
01:08:12,010 --> 01:08:23,190
但现在为止我定义的最后一个概念

1201
01:08:23,260 --> 01:08:24,790
是相对于单个训练样本的

1202
01:08:24,900 --> 01:08:28,270
几何间隔  所以像往常一样

1203
01:08:28,390 --> 01:08:30,480
我要定义相对于

1204
01:08:30,580 --> 01:08:33,590
整个训练集合的几何间隔

1205
01:08:33,710 --> 01:08:36,820
定义为所有相对于单个样本的

1206
01:08:36,820 --> 01:08:40,170
几何间隔中最小的  明白吗?

1207
01:08:40,290 --> 01:08:49,300
所以最大间隔分类器

1208
01:08:49,380 --> 01:08:51,530
可以被看做是支持向量机的前身

1209
01:08:51,640 --> 01:09:01,360
它是一个学习算法

1210
01:09:01,470 --> 01:09:05,610
会选择特定的w和b

1211
01:09:05,690 --> 01:09:07,800
使得几何间隔最大化

1212
01:09:07,910 --> 01:09:10,040
我要将它写下来

1213
01:09:10,150 --> 01:09:13,250
最大间隔分类是指

1214
01:09:13,340 --> 01:09:15,080
这样的优化问题

1215
01:09:15,190 --> 01:09:20,550
我们需要选择γ  w和b

1216
01:09:20,660 --> 01:09:21,950
使几何间隔最大

1217
01:09:22,050 --> 01:09:26,700
同时满足这样的条件--

1218
01:09:26,800 --> 01:09:37,820
这仅仅可能的写法之一  同时满足

1219
01:09:37,930 --> 01:09:43,540
这样的条件 好的

1220
01:09:43,630 --> 01:09:46,470
除了这种写法之外还可以有其他的写法

1221
01:09:46,520 --> 01:09:47,930
我们接下来要做的是--

1222
01:09:48,000 --> 01:09:50,080
我想知道

1223
01:09:50,130 --> 01:09:51,350
我们是否能在五分钟内讲完

1224
01:09:51,400 --> 01:09:53,330
我估计这会很难

1225
01:09:53,410 --> 01:09:56,600
所以这个将分类最大化

1226
01:09:56,680 --> 01:09:58,340
这是个关于γ_W和γ_B

1227
01:09:58,450 --> 01:10:03,360
的最大值问题

1228
01:10:03,470 --> 01:10:06,260
事实证明几何间隔并不会随

1229
01:10:06,350 --> 01:10:08,420
||w||改变  对吗?

1230
01:10:08,470 --> 01:10:10,840
注意到在几何间隔的定义中

1231
01:10:10,920 --> 01:10:13,740
最后的结果中我们除以了||w||

1232
01:10:13,830 --> 01:10:15,820
所以你可以将

1233
01:10:15,920 --> 01:10:17,160
||w||设成任意你想要的值

1234
01:10:17,280 --> 01:10:19,200
你可以将w和b乘上任意的倍数

1235
01:10:19,290 --> 01:10:21,680
这些都不会改变几何间隔的值

1236
01:10:21,780 --> 01:10:23,860
这一点很重要

1237
01:10:23,960 --> 01:10:25,640
我们之后再回来探讨

1238
01:10:25,740 --> 01:10:28,540
注意到你们可以对参数

1239
01:10:28,660 --> 01:10:30,740
w和b的模

1240
01:10:30,840 --> 01:10:32,340
可以是任意常数

1241
01:10:32,460 --> 01:10:34,160
你也可以对w和b

1242
01:10:34,270 --> 01:10:36,730
乘上任意的倍数因子  例如  将它们替换成

1243
01:10:36,810 --> 01:10:38,290
10w或10b  或一些其他的值

1244
01:10:38,390 --> 01:10:42,140
这些都不会改变几何间隔  明白吗?

1245
01:10:42,250 --> 01:10:45,540
在第一个公式中

1246
01:10:45,660 --> 01:10:46,930
我加入了一个约束条件

1247
01:10:46,980 --> 01:10:48,230
使||w||=1

1248
01:10:48,370 --> 01:10:50,540
这种情况下函数间隔等于

1249
01:10:50,630 --> 01:10:51,540
几何间隔

1250
01:10:51,660 --> 01:10:53,220
之后我们会最大化

1251
01:10:53,300 --> 01:10:54,730
几何间隔  同时满足这样的条件--

1252
01:10:54,830 --> 01:10:56,950
对于所选取的最大的几何间隔γ

1253
01:10:57,060 --> 01:10:58,720
必须保证每个样本

1254
01:10:58,810 --> 01:11:01,190
的几何间隔都至少为γ

1255
01:11:01,300 --> 01:11:03,640
这个式子之所以为集合间隔

1256
01:11:03,740 --> 01:11:05,810
是因为在||w||=1的时候

1257
01:11:05,900 --> 01:11:06,890
函数间隔

1258
01:11:06,980 --> 01:11:09,460
等于几何间隔  明白吗?

1259
01:11:09,580 --> 01:11:12,800
这就是最大间隔分类器

1260
01:11:12,900 --> 01:11:15,200
实际上如果你使用了这个算法

1261
01:11:15,320 --> 01:11:17,240
它的结果会和

1262
01:11:17,350 --> 01:11:18,550
logistic回归的结果

1263
01:11:18,660 --> 01:11:20,560
差不多好

1264
01:11:20,660 --> 01:11:23,690
但是实际上

1265
01:11:23,800 --> 01:11:25,890
当我们进一步深入了解这个算法时

1266
01:11:25,980 --> 01:11:28,470
我们会用一种更为巧妙的方式

1267
01:11:28,580 --> 01:11:30,580
改变这个算法

1268
01:11:30,700 --> 01:11:32,260
并且可以让其支持无限维的特征空间

1269
01:11:32,370 --> 01:11:33,900
并且可以得到

1270
01:11:34,000 --> 01:11:35,570
非常有效的非线性分类器

1271
01:11:35,690 --> 01:11:39,700
在我们正式开始学习支持向量机之前

1272
01:11:39,830 --> 01:11:42,600
还有几步路要走  但是今天所讲的是第一步

1273
01:11:42,700 --> 01:11:47,510
有问题吗?什么?

1274
01:11:47,620 --> 01:12:02,140
【听不清】

1275
01:12:02,220 --> 01:12:04,180
到现在为止

1276
01:12:04,230 --> 01:12:05,550
我们都只处理一个固定的训练集合

1277
01:12:05,650 --> 01:12:06,920
现在你不必考虑

1278
01:12:07,020 --> 01:12:08,650
对于一个给定训练集合

1279
01:12:08,700 --> 01:12:11,310
的扩展问题

1280
01:12:11,380 --> 01:12:12,600
这些还不是我们现在考虑的问题  明白吗?

1281
01:12:12,740 --> 01:12:15,220
我讲到的知识全是基于一个固定的训练集合

1282
01:12:15,310 --> 01:12:18,680
你不能改变x

1283
01:12:18,780 --> 01:12:20,070
也不能改变y

1284
01:12:20,180 --> 01:12:22,020
还有其他问题吗?

1285
01:12:22,140 --> 01:12:32,580
好的  下周我们继续讲这个问题

1286
01:12:32,690 --> 01:12:34,070
我们会将一些更为正式的算法

1287
01:12:34,170 --> 01:12:37,150
利用今天讲的内容

1288
01:12:37,300 --> 01:12:38,670
引出一个非常高效

1289
01:12:38,780 --> 01:12:40,700
且无需定制的学习算法

1290
01:12:40,810 --> 01:12:43,380
提醒一下

1291
01:12:43,500 --> 01:12:45,370
下一节讨论课

1292
01:12:45,490 --> 01:12:47,620
是关于Matlab和Octave的

1293
01:12:47,730 --> 01:12:48,640
如果你们了解一下的话

1294
01:12:48,740 --> 01:12:50,270
欢迎参加

1295
01:12:50,380 --> 01:12:51,990
好的  下节课见

