1
00:00:21,440 --> 00:00:27,450
好的  欢迎回来

2
00:00:27,600 --> 00:00:31,230
我今天完成

3
00:00:31,340 --> 00:00:32,950
对支持向量机的讲解

4
00:00:33,090 --> 00:00:36,470
具体地  我首先要讲核的概念

5
00:00:36,610 --> 00:00:40,630
之后我们会讲L1 norm Soft Margin

6
00:00:40,740 --> 00:00:44,860
软间隔SVM  是SVM的一种变化形式

7
00:00:44,970 --> 00:00:47,260
可以用来处理非线性可分隔的数据

8
00:00:47,360 --> 00:00:48,960
最后我们会讲到SMO算法

9
00:00:49,060 --> 00:00:50,710
这个算法可以用来求解

10
00:00:50,820 --> 00:00:52,570
我们上一次提出的优化问题

11
00:00:54,580 --> 00:00:56,550
作为回顾

12
00:00:56,650 --> 00:00:59,420
我要将之前提出的

13
00:00:59,520 --> 00:01:03,730
凸优化问题写出来

14
00:01:03,840 --> 00:01:15,220
这里我们假设数据是线性可分隔的

15
00:01:15,370 --> 00:01:17,060
这个假设我稍后会修正 (文稿缺失)

16
00:01:17,190 --> 00:01:27,550
对于这个优化问题

17
00:01:27,690 --> 00:01:28,750
给定一个训练集合

18
00:01:28,860 --> 00:01:38,360
这个问题的算法会找到

19
00:01:38,470 --> 00:01:41,970
一个数据集合的最优间隔分类器

20
00:01:42,100 --> 00:01:43,890
可以使训练样本的几何间隔最大化

21
00:01:44,030 --> 00:01:48,080
在上一讲中

22
00:01:48,190 --> 00:01:50,440
我们推出了这个问题的对偶问题

23
00:01:50,590 --> 00:01:54,330
也就是要使这个式子最大化

24
00:02:22,920 --> 00:02:23,470
这是我们原始问题的对偶问题

25
00:02:23,610 --> 00:02:24,700
我这里用

26
00:02:24,810 --> 00:02:28,070
尖括号表示内积

27
00:02:28,200 --> 00:02:32,460
对于x^((i)) 和x^((j))来说

28
00:02:32,600 --> 00:02:35,260
这一项应该等于(x^((i) ) )^T x^((j) )

29
00:02:35,370 --> 00:02:39,750
我们同样求出

30
00:02:39,830 --> 00:02:45,290
参数w应该等于这个式子

31
00:02:45,380 --> 00:02:48,380
因此

32
00:02:48,480 --> 00:02:49,840
当你需要对分类做出预测时

33
00:02:49,920 --> 00:02:53,420
你需要对

34
00:02:53,530 --> 00:02:55,690
计算输入向量x计算假设的值

35
00:02:55,810 --> 00:02:57,230
它应该等于这个式子

36
00:02:57,340 --> 00:03:03,020
这里g是一个阈值函数

37
00:03:03,120 --> 00:03:05,170
输出为-1或1

38
00:03:05,290 --> 00:03:15,120
所以它应该等于这个式子

39
00:03:16,590 --> 00:03:21,400
所以里面这个式子可以写成

40
00:03:21,530 --> 00:03:26,780
样本数据与输入变量x的内积的形式

41
00:03:26,920 --> 00:03:34,150
接下来我要介绍核的概念

42
00:03:34,260 --> 00:03:35,910
这个概念具有这样的性质:

43
00:03:36,020 --> 00:03:38,980
算法对于x的依赖

44
00:03:39,140 --> 00:03:43,090
仅仅局限于这些内积

45
00:03:43,220 --> 00:03:44,630
实际上

46
00:03:44,750 --> 00:03:46,770
你甚至在整个算法的过程中

47
00:03:46,890 --> 00:03:49,360
都无需显示地直接使用向量x的值

48
00:03:49,470 --> 00:03:54,750
而只需要用到训练样本

49
00:03:56,540 --> 00:04:01,450
与输入特征向量的内积

50
00:04:01,560 --> 00:04:04,300
核的概念是这样的

51
00:04:04,400 --> 00:04:08,440
比如说你们有一个输入属性

52
00:04:08,550 --> 00:04:09,600
比如说是一个实数

53
00:04:09,750 --> 00:04:12,220
也许是房屋的面积

54
00:04:12,350 --> 00:04:15,170
你希望基于它预测出

55
00:04:15,280 --> 00:04:16,570
该房屋是否会

56
00:04:16,690 --> 00:04:17,660
在未来六个月之内被卖掉

57
00:04:17,780 --> 00:04:21,760
我们经常会将这个特征x映射到

58
00:04:21,890 --> 00:04:25,010
一组特征上

59
00:04:25,110 --> 00:04:25,940
例如:

60
00:04:26,060 --> 00:04:28,300
我们可以将x映射成

61
00:04:28,380 --> 00:04:31,890
这四个多项式特征

62
00:04:32,020 --> 00:04:36,090
我将这个映射表示为φ

63
00:04:36,260 --> 00:04:40,100
我们用φ(x)表示将原始特征

64
00:04:40,230 --> 00:04:41,220
转化成一些

65
00:04:41,360 --> 00:04:42,600
更高维特征的映射

66
00:04:42,720 --> 00:04:48,490
如果你想使用映射后的特征φ(x)

67
00:04:48,610 --> 00:04:51,900
你只需要回到原来的学习算法中

68
00:04:52,040 --> 00:04:56,050
将所有出现过的《x^((i) )

69
00:04:56,200 --> 00:05:02,730
x^((j) )>替换成《φ(x^((i) ) )

70
00:05:02,860 --> 00:05:05,510
φ(x^((j) ))>

71
00:05:05,650 --> 00:05:10,790
这意味着我们将使用

72
00:05:10,900 --> 00:05:13,130
φ(x)表示的向量来执行SVM算法

73
00:05:13,240 --> 00:05:15,770
而不是原始的一维输入特征

74
00:05:15,890 --> 00:05:20,890
有些情况下

75
00:05:21,000 --> 00:05:24,630
φ(x)的维度将会非常高

76
00:05:24,780 --> 00:05:32,530
有时

77
00:05:32,680 --> 00:05:34,100
例如  φ(x)可能包含

78
00:05:34,220 --> 00:05:35,850
非常高维的多项式特征

79
00:05:35,990 --> 00:05:39,540
有时φ(x)

80
00:05:39,650 --> 00:05:40,630
甚至有可能使无限维的

81
00:05:40,760 --> 00:05:42,080
我们遇到的问题是

82
00:05:42,190 --> 00:05:44,290
当φ(x)的维数非常高时

83
00:05:44,410 --> 00:05:46,160
你无法高效地

84
00:05:46,260 --> 00:05:49,160
计算出这些内积

85
00:05:49,290 --> 00:05:50,390
因为计算机需要

86
00:05:50,530 --> 00:05:53,460
将这些非常高维的向量表示出来

87
00:05:53,540 --> 00:05:54,910
之后求它们之间的内积

88
00:05:55,030 --> 00:05:56,080
这样的计算方式是非常低效的

89
00:05:56,190 --> 00:06:04,440
实际上在很多重要的特例下

90
00:06:04,560 --> 00:06:05,400
我们可以--

91
00:06:05,530 --> 00:06:08,980
我们将其称之为核函数  用K来表示

92
00:06:09,120 --> 00:06:12,520
它应该等于

93
00:06:12,640 --> 00:06:20,460
这些特征向量的内积

94
00:06:20,610 --> 00:06:23,430
实际上  在很多重要的特例下

95
00:06:23,650 --> 00:06:26,830
计算φ(x)的代价会很大

96
00:06:26,980 --> 00:06:29,030
甚至有些时候是不可能的

97
00:06:29,160 --> 00:06:30,390
当向量是无限维的时候

98
00:06:30,570 --> 00:06:32,470
你是无法计算无限维的向量的

99
00:06:32,620 --> 00:06:34,410
在一些重要的特例下

100
00:06:34,540 --> 00:06:36,620
φ(x)的表示代价会很大

101
00:06:36,800 --> 00:06:38,110
因为它的维度很高

102
00:06:38,250 --> 00:06:39,230
然而

103
00:06:39,410 --> 00:06:42,190
你可以计算出这两个向量之间的核

104
00:06:42,330 --> 00:06:43,710
你可以用很小的代价

105
00:06:43,820 --> 00:06:47,340
计算出这两个向量的内积

106
00:06:47,470 --> 00:06:51,690
支持向量机的思想是

107
00:06:51,910 --> 00:06:54,130
在算法中

108
00:06:54,260 --> 00:06:55,490
你看到的每个这样的内积

109
00:06:55,660 --> 00:06:58,980
我们都将其换成

110
00:06:59,140 --> 00:07:00,430
可以高效计算的核函数

111
00:07:00,560 --> 00:07:05,070
这样算法将在φ(x)空间运行

112
00:07:05,190 --> 00:07:08,850
尽管φ(x)空间是维度很高的向量空间

113
00:07:08,980 --> 00:07:14,000
让我们看看具体是怎么做的

114
00:07:14,130 --> 00:07:17,810
稍后  我会给你们展示

115
00:07:17,940 --> 00:07:20,480
一些具体的φ(x)

116
00:07:20,620 --> 00:07:21,460
和核的例子

117
00:07:21,590 --> 00:07:22,400
现在

118
00:07:22,530 --> 00:07:24,750
我们只是先显示地构造核函数

119
00:07:24,860 --> 00:07:27,460
这样可以更好地说明我们的例子

120
00:07:27,580 --> 00:07:35,510
比如说有两个输入:

121
00:07:35,660 --> 00:07:36,330
x和z

122
00:07:36,470 --> 00:07:39,640
通常情况下我会写x^((i) )和z^((i) )

123
00:07:39,790 --> 00:07:42,340
但是为了便于书写

124
00:07:42,430 --> 00:07:46,940
我这里就写成x和z

125
00:07:47,060 --> 00:07:48,430
比如说我们的核函数:

126
00:07:48,580 --> 00:07:51,770
K(x  z)=(x^T z)^2

127
00:07:51,900 --> 00:08:09,530
它应该等于这个式子  对吗?

128
00:08:09,650 --> 00:08:13,140
这里是x^T z

129
00:08:13,260 --> 00:08:14,160
这里也是x^T z

130
00:08:14,260 --> 00:08:16,090
所以这里是(x^T z)^2

131
00:08:16,200 --> 00:08:21,060
它应该等于这个

132
00:08:21,200 --> 00:08:37,590
所以这个核对应的特征映射φ(x)

133
00:08:37,690 --> 00:09:05,210
应该等于

134
00:09:05,320 --> 00:09:08,420
这里我假设n=3

135
00:09:08,530 --> 00:09:09,480
所以根据这个φ(x)的定义

136
00:09:09,560 --> 00:09:12,000
你们可以自己验证一下

137
00:09:12,420 --> 00:09:18,710
这个式子就变成了φ(x)和φ(z)的内积

138
00:09:18,850 --> 00:09:22,750
因为为了得到两个向量的内积

139
00:09:22,890 --> 00:09:25,140
你只需要将对应元素

140
00:09:25,270 --> 00:09:26,910
相乘之后再求和

141
00:09:27,050 --> 00:09:29,880
所以如果φ(x)是这样的形式

142
00:09:29,980 --> 00:09:31,670
那么φ(x)和φ(z)的内积

143
00:09:31,800 --> 00:09:35,700
将会是对应元素相乘之后

144
00:09:35,830 --> 00:09:38,510
再求和

145
00:09:38,680 --> 00:09:40,680
所以你会得到这个式子

146
00:09:40,840 --> 00:09:47,390
一件很酷的事情是

147
00:09:47,510 --> 00:09:50,210
为了计算出φ(x)

148
00:09:50,570 --> 00:09:57,930
你需要O(n^2)的时间复杂度

149
00:09:58,030 --> 00:10:07,610
如果n代表了x和z的维数

150
00:10:07,720 --> 00:10:10,140
那么φ(x)中的元素

151
00:10:10,250 --> 00:10:13,400
对应着所有不同的x^((i) )

152
00:10:13,520 --> 00:10:14,740
和x^((j) )的乘积

153
00:10:14,850 --> 00:10:17,760
所以φ(x)的长度是n^2

154
00:10:17,880 --> 00:10:19,990
所以你需要O(n^2)的时间复杂度

155
00:10:20,110 --> 00:10:21,740
来计算φ(x)

156
00:10:21,830 --> 00:10:24,720
但是为了计算K

157
00:10:24,840 --> 00:10:39,210
也就是核函数的值

158
00:10:39,340 --> 00:10:40,870
你只需要O(n)的时间复杂度

159
00:10:41,000 --> 00:10:46,300
因为核函数被定义为:

160
00:10:46,410 --> 00:10:49,720
(x^T z)^2

161
00:10:49,820 --> 00:10:57,550
所以你只需要先求x和z的内积

162
00:10:57,650 --> 00:10:58,830
这需要O(n)的时间复杂度

163
00:10:58,940 --> 00:11:00,050
之后用所得结果的平方

164
00:11:00,160 --> 00:11:01,490
作为核函数的值

165
00:11:01,630 --> 00:11:03,170
所以这样

166
00:11:03,320 --> 00:11:06,260
你仅仅用了O(n)的时间复杂度

167
00:11:06,400 --> 00:11:07,540
就计算出了

168
00:11:07,650 --> 00:11:08,760
两个包含n^2个元素的向量的内积

169
00:11:08,860 --> 00:11:12,590
S:对于每个你找到的x和z的核函数

170
00:11:12,710 --> 00:11:16,280
一定存在对应的φ吗?

171
00:11:16,380 --> 00:11:18,900
I:我们稍后会讲到这个问题

172
00:11:18,990 --> 00:11:21,730
我们稍后会讨论怎样的核是有效的

173
00:11:21,840 --> 00:11:25,570
明白的话请举手

174
00:11:25,690 --> 00:11:35,290
让我们进行一些

175
00:11:35,390 --> 00:11:39,590
简单的推广

176
00:11:39,730 --> 00:11:43,010
一种推广是

177
00:11:43,140 --> 00:11:49,380
我们将K(x  z)定义为:

178
00:11:49,520 --> 00:11:57,700
(x^T z+c)^2

179
00:11:57,850 --> 00:11:58,620
所以这个核函数

180
00:11:58,720 --> 00:12:00,560
你们可以在 O(n)的时间复杂度内计算出来

181
00:12:00,640 --> 00:12:03,420
实际上

182
00:12:03,500 --> 00:12:06,180
对于与其对应的特征向量

183
00:12:06,300 --> 00:12:09,940
我需要在底部

184
00:12:10,050 --> 00:12:24,260
加入一些新的元素

185
00:12:24,380 --> 00:12:25,230
它们

186
00:12:25,310 --> 00:12:27,130
是√2c x_1  √2c x_2  √2c x_3 和c

187
00:12:27,220 --> 00:12:33,540
我们创建的特征向量中

188
00:12:33,650 --> 00:12:34,950
包含了单项式

189
00:12:35,070 --> 00:12:36,730
也就是一次项

190
00:12:36,830 --> 00:12:40,240
以及x^((i) )与x^((j) )之间的

191
00:12:40,330 --> 00:12:42,710
二次项或内积项

192
00:12:42,800 --> 00:12:46,310
参数c可以让你

193
00:12:46,420 --> 00:12:49,060
控制单项式

194
00:12:49,170 --> 00:12:50,890
即一次项

195
00:12:51,000 --> 00:12:52,720
与二次项之间的相对权重

196
00:12:52,820 --> 00:12:57,430
这里我们再一次地用O(n)的时间

197
00:12:57,540 --> 00:12:58,950
复杂度计算出了

198
00:12:59,060 --> 00:13:01,630
两个包含了n^2个元素的

199
00:13:01,780 --> 00:13:06,980
特征向量之间的内积

200
00:13:07,130 --> 00:13:19,410
这里是一些其他的核的例子

201
00:13:19,520 --> 00:13:21,600
我刚刚写的

202
00:13:21,740 --> 00:13:27,950
核的一种一般化的形式是这样的

203
00:13:28,070 --> 00:13:35,190
这对应着

204
00:13:35,330 --> 00:13:40,740
((n+d)?d)个特征单项式

205
00:13:40,850 --> 00:13:42,430
单项式表示的是

206
00:13:42,560 --> 00:13:51,720
x_i  x_j  x_k的乘积

207
00:13:51,850 --> 00:13:55,510
它的元素个数取决于d

208
00:13:55,610 --> 00:13:56,910
((n+d)?d)的数量级大致

209
00:13:57,040 --> 00:13:58,190
和(n+d)^d相同

210
00:13:58,280 --> 00:14:00,600
所以向量的元素个数随d呈指数上升

211
00:14:00,730 --> 00:14:04,740
这是一个维数很高的特征向量

212
00:14:04,860 --> 00:14:06,210
但是你可以隐式地构造特征向量

213
00:14:06,350 --> 00:14:08,650
并求它们的内积

214
00:14:08,770 --> 00:14:10,990
这样的计算方式非常高效

215
00:14:11,120 --> 00:14:12,850
因为你只需要计算x和z之间的内积

216
00:14:12,980 --> 00:14:13,730
之后加上c

217
00:14:13,820 --> 00:14:14,840
最后计算这个结果的d次幂

218
00:14:14,970 --> 00:14:16,500
所以通过核的引入

219
00:14:16,610 --> 00:14:18,020
你已经在隐式地

220
00:14:18,120 --> 00:14:19,990
处理一个非常大的向量空间了

221
00:14:20,120 --> 00:14:30,320
我刚刚给出了一些创造核的例子

222
00:14:30,480 --> 00:14:34,650
我想为你们展示

223
00:14:34,750 --> 00:14:39,370
几个关于核的具体例子

224
00:14:39,490 --> 00:14:40,780
那么现在让我问你们

225
00:14:40,880 --> 00:14:41,850
一个更为一般化的问题

226
00:14:41,940 --> 00:14:43,240
当你们面对一个新的机器学习问题时

227
00:14:43,360 --> 00:14:44,730
你们怎样创造一个核?

228
00:14:44,840 --> 00:14:46,990
有很多思考这个问题的方法

229
00:14:47,110 --> 00:14:48,810
一种直观理解是比较有用的

230
00:14:48,980 --> 00:14:55,140
给定一组属性x

231
00:14:55,250 --> 00:14:57,010
将其转化成一个特征向量φ(x)

232
00:14:57,100 --> 00:15:00,920
给定一组属性z

233
00:15:01,060 --> 00:15:03,220
你们将会去使用输入特征向量φ(z)

234
00:15:03,320 --> 00:15:06,830
所以核就是计算

235
00:15:06,950 --> 00:15:08,280
两个特征向量

236
00:15:08,370 --> 00:15:09,840
φ(x)与φ(z)之间的内积

237
00:15:09,950 --> 00:15:14,670
所以这种直观理解是

238
00:15:14,840 --> 00:15:16,160
它是有偏差的

239
00:15:16,270 --> 00:15:20,390
它并不是一种非常严格的直观理解

240
00:15:20,500 --> 00:15:23,190
如果x和z非常相似

241
00:15:23,310 --> 00:15:25,860
那么φ(x)与φ(z)将会大概

242
00:15:25,980 --> 00:15:27,330
指向相同的方向

243
00:15:27,440 --> 00:15:31,160
因此内积将会比较大

244
00:15:31,290 --> 00:15:33,720
相反地

245
00:15:33,870 --> 00:15:35,760
如果x和z的相似度很低

246
00:15:35,890 --> 00:15:38,200
那么φ(x)和φ(z)

247
00:15:38,320 --> 00:15:39,720
将很可能指向不同的地方

248
00:15:39,860 --> 00:15:41,770
因此内积将会比较小

249
00:15:41,870 --> 00:15:44,100
这种直观理解的表述并不是非常严格

250
00:15:44,230 --> 00:15:45,930
但是非常有用

251
00:15:46,020 --> 00:15:51,130
如果你们面对一个新的学习问题

252
00:15:51,260 --> 00:15:54,270
如果我随便给你一些东西

253
00:15:54,350 --> 00:15:56,990
要你对它们进行分类

254
00:15:57,090 --> 00:15:59,630
你希望找到一个核

255
00:15:59,750 --> 00:16:04,730
一种方式是使K(x  z)取一个较大的值

256
00:16:04,890 --> 00:16:07,270
当你希望学习算法认为x和z是相似的

257
00:16:07,410 --> 00:16:12,320
反之则取一个较小的值

258
00:16:12,440 --> 00:16:21,240
这个结论并不总是正确的

259
00:16:21,390 --> 00:16:23,260
但是它是一种有用的直观理解之一

260
00:16:23,410 --> 00:16:26,720
如果你们要对一些全新的事物

261
00:16:26,890 --> 00:16:28,000
进行分类

262
00:16:28,110 --> 00:16:28,960
例如:DNA序列之类的事物

263
00:16:29,100 --> 00:16:32,090
你需要对这些陌生的事物进行分类

264
00:16:32,220 --> 00:16:33,890
一种生成核的方法是

265
00:16:34,000 --> 00:16:35,640
尝试让核生成一个较大的值

266
00:16:35,760 --> 00:16:37,870
当你希望算法

267
00:16:38,020 --> 00:16:39,340
认为两个东西是类似的时候

268
00:16:39,450 --> 00:16:42,460
反之则让核生成一个较小的值

269
00:16:42,640 --> 00:16:44,840
这给出了一个问题的答案

270
00:16:44,970 --> 00:16:48,090
比如说

271
00:16:48,230 --> 00:16:49,310
我希望对一些事物进行分类

272
00:16:49,450 --> 00:16:50,620
我写出了一个核函数

273
00:16:50,760 --> 00:16:55,550
并且认为它可以很好地表示

274
00:16:55,690 --> 00:16:56,790
我的特定问题中两个特征向量

275
00:16:56,930 --> 00:16:58,570
x和z的相似性

276
00:16:58,750 --> 00:17:03,380
我可以将K(x  z)定义成这样的形式

277
00:17:03,560 --> 00:17:09,080
我们写出了这个函数

278
00:17:09,240 --> 00:17:14,670
我认为它可以很好地度量

279
00:17:14,790 --> 00:17:17,090
x与z之间的相似度

280
00:17:17,210 --> 00:17:20,070
那么我们面对的问题是:

281
00:17:21,910 --> 00:17:22,470
这是否是一个合法的核?

282
00:17:22,600 --> 00:17:29,250
换句话说

283
00:17:29,370 --> 00:17:32,850
考虑一下我们对核的定义

284
00:17:32,940 --> 00:17:34,100
如果我们将核写成这种形式

285
00:17:34,220 --> 00:17:34,860
问题变为了:

286
00:17:34,990 --> 00:17:36,650
是否存在φ

287
00:17:36,760 --> 00:17:40,230
使得K(x  z)能够写成

288
00:17:40,370 --> 00:17:42,970
两个向量内积的形式?

289
00:17:43,120 --> 00:17:51,670
这个问题考虑的是一个核K的合法性

290
00:17:51,800 --> 00:18:01,990
实际上有一个结论

291
00:18:02,140 --> 00:18:04,410
给出了函数K

292
00:18:04,540 --> 00:18:06,270
是合法的核的充分必要条件

293
00:18:06,380 --> 00:18:08,560
我们接下来就要展示这个结论

294
00:18:08,710 --> 00:18:11,970
假设K是一个合法的核

295
00:18:12,110 --> 00:18:18,690
当我说K是一个核的时候

296
00:18:18,860 --> 00:18:24,930
我的意思是

297
00:18:25,070 --> 00:18:27,900
存在φ是

298
00:18:28,030 --> 00:18:29,110
这个条件为真

299
00:18:29,240 --> 00:18:40,700
给定任意点集合

300
00:18:40,830 --> 00:18:49,790
让我定义一个矩阵K

301
00:18:49,920 --> 00:18:56,820
很抱歉我这里使用了相同的符号

302
00:18:56,960 --> 00:18:59,480
K即表示

303
00:18:59,610 --> 00:19:00,770
一个核函数

304
00:19:00,890 --> 00:19:03,840
也表示一个矩阵

305
00:19:03,970 --> 00:19:08,920
很不幸  我们没有足够的字母

306
00:19:09,030 --> 00:19:10,960
这不是真的

307
00:19:11,110 --> 00:19:16,880
我们的核矩阵是一个m*m的矩阵

308
00:19:16,960 --> 00:19:19,080
K_ij等于

309
00:19:19,190 --> 00:19:25,350
核函数在这两个样本上的取值

310
00:19:25,450 --> 00:19:40,580
对于任何m维向量z

311
00:19:40,690 --> 00:19:42,310
我希望考虑

312
00:19:42,410 --> 00:19:47,350
z^T Kz的值

313
00:19:47,470 --> 00:20:09,170
根据矩阵乘法的定义

314
00:20:09,300 --> 00:20:10,730
它应该等于

315
00:20:10,860 --> 00:20:20,940
因为K_ij 是作用于

316
00:20:21,060 --> 00:20:22,210
x^((i) ) 和x^((j) ) 上的核函数的值

317
00:20:22,600 --> 00:20:24,720
所以它应该等于这个

318
00:20:24,830 --> 00:20:29,390
我假设K是一个合法的核函数

319
00:20:29,530 --> 00:20:31,970
所以满足要求的φ一定存在

320
00:20:32,110 --> 00:20:39,260
它等于两个特征向量的内积

321
00:20:39,380 --> 00:20:41,690
所以让我显式地将内积展开

322
00:20:41,790 --> 00:20:45,500
我会对这个向量的元素求和

323
00:20:45,620 --> 00:20:48,650
我会用(φ(x^((i) ) ) )_k来表示

324
00:20:48,760 --> 00:20:51,260
这个向量的第k个元素

325
00:20:51,390 --> 00:21:05,020
重新调整求和的顺序

326
00:21:05,150 --> 00:21:06,330
你会得到这个式子

327
00:21:06,450 --> 00:21:27,660
这个结果你们中肯定有人会比较熟悉

328
00:21:27,770 --> 00:21:46,300
它等于--对吗?

329
00:21:46,440 --> 00:21:49,920
因此  由于结果是一系列平方项之和

330
00:21:50,050 --> 00:21:53,730
那么结果一定大于等于0

331
00:21:53,860 --> 00:21:58,360
仔细看一会儿

332
00:21:58,510 --> 00:21:59,890
这些步骤确定你们都明白了

333
00:22:00,010 --> 00:22:01,600
学生: 只是为了确认你都买了?

334
00:22:01,720 --> 00:22:13,350
I:哦  这里表示的是

335
00:22:13,530 --> 00:22:16,600
φ(x)与φ(z)的内积

336
00:22:16,700 --> 00:22:22,070
所以它应该等于

337
00:22:22,190 --> 00:22:24,370
对应项乘积之和

338
00:22:24,480 --> 00:22:27,730
学生:

339
00:22:27,820 --> 00:22:28,930
I:哦  是的

340
00:22:29,060 --> 00:22:35,230
a^T b=∑_k?〖a_k b_k 〗

341
00:22:35,330 --> 00:22:36,920
所以这里是

342
00:22:37,040 --> 00:22:39,780
对向量的第k个元素的乘积进行求和

343
00:22:39,890 --> 00:22:41,520
在看一看整个过程确保你们明白了

344
00:22:41,640 --> 00:22:47,220
还有问题吗?

345
00:22:47,300 --> 00:22:51,230
总结一下

346
00:22:51,350 --> 00:23:00,360
我们证明了

347
00:23:00,480 --> 00:23:04,300
对于任何向量z  z^T Kz≥0

348
00:23:04,430 --> 00:23:08,720
关于这个性质的一种标准定义是

349
00:23:08,850 --> 00:23:12,680
称K为半正定矩阵

350
00:23:12,830 --> 00:23:18,340
当K为半正定矩阵时

351
00:23:18,460 --> 00:23:20,770
我们可以写为:K≥0

352
00:23:20,900 --> 00:23:25,260
总结一下

353
00:23:25,370 --> 00:23:26,730
这个结论表明

354
00:23:26,830 --> 00:23:32,720
如果K是一个合法的核

355
00:23:32,830 --> 00:23:33,460
换句话说

356
00:23:33,570 --> 00:23:34,760
如果K是一个函数

357
00:23:34,880 --> 00:23:37,770
存在φ

358
00:23:37,890 --> 00:23:41,610
使得K(x^((i) )  x^((j) ) )

359
00:23:41,740 --> 00:23:43,040
等于φ(x^((i) ))和φ(x^((j) ))的内积

360
00:23:43,160 --> 00:23:45,230
所以如果K是一个合法的核

361
00:23:45,350 --> 00:23:46,150
结论表明

362
00:23:46,250 --> 00:23:49,970
它所对应的核矩阵是半正定矩阵

363
00:23:50,120 --> 00:24:02,940
实际上逆命题也是成立的

364
00:24:03,110 --> 00:24:06,160
所以这给了你一种

365
00:24:06,320 --> 00:24:12,450
测试K是否合法的方式

366
00:24:12,610 --> 00:24:15,880
这是一个由Mercer提出的定理

367
00:24:16,010 --> 00:24:18,430
所以核有时被称为Mercer核

368
00:24:18,550 --> 00:24:19,590
它表示的是相同的东西

369
00:24:19,680 --> 00:24:20,860
它表示合法的核

370
00:24:20,960 --> 00:24:27,050
给定K(x  z)

371
00:24:27,160 --> 00:24:36,730
那么K是一个合法的核

372
00:24:36,810 --> 00:24:43,780
它是一个Mercer核

373
00:24:43,890 --> 00:24:50,760
即:存在φ

374
00:24:50,850 --> 00:24:59,530
使得K(x  z)=φ(x)^T φ(z)

375
00:24:59,630 --> 00:25:01,180
当且仅当

376
00:25:01,280 --> 00:25:07,150
对于任意一个包含m个样本的集合

377
00:25:07,250 --> 00:25:13,790
它的意思是对于任何一个

378
00:25:13,930 --> 00:25:15,090
包含m个点的集合

379
00:25:15,190 --> 00:25:16,160
并不一定是训练集合

380
00:25:16,240 --> 00:25:17,360
它可以是任何你选定的

381
00:25:17,450 --> 00:25:22,510
包含m个点的集合

382
00:25:22,690 --> 00:25:23,720
条件为真当且仅当核矩阵

383
00:25:23,830 --> 00:25:32,540
就是刚才定义的矩阵K

384
00:25:32,700 --> 00:25:39,210
是对称的半正定矩阵

385
00:25:39,330 --> 00:25:51,930
我刚刚证明了这个结论的一半

386
00:25:52,040 --> 00:25:54,010
我证明了如果核是合法的

387
00:25:54,110 --> 00:25:55,620
那么它对应的核矩阵K

388
00:25:55,690 --> 00:25:56,880
是对称半正定的

389
00:25:57,010 --> 00:25:59,280
但是逆命题我没有证明

390
00:25:59,410 --> 00:26:00,920
事实证明这个必要条件

391
00:26:01,060 --> 00:26:02,820
也是一个充分条件

392
00:26:02,940 --> 00:26:05,590
所以这个定理

393
00:26:05,710 --> 00:26:07,420
给了你们一个有用的测试方法

394
00:26:07,540 --> 00:26:08,520
用来测试你所选定的函数

395
00:26:08,640 --> 00:26:09,900
是否是一个核

396
00:26:10,020 --> 00:26:24,460
一个具体的用来判断

397
00:26:24,570 --> 00:26:28,770
核明显不合法的例子是

398
00:26:28,930 --> 00:26:34,210
如果你找到了一个输入x

399
00:26:34,320 --> 00:26:36,820
使得K(x  x)=-1

400
00:26:36,950 --> 00:26:38,830
那么这个函数

401
00:26:38,940 --> 00:26:43,100
一定不是一个合法的核

402
00:26:43,230 --> 00:26:43,970
因为φ(x)*φ(x)

403
00:26:44,060 --> 00:26:51,300
不可能等于-1

404
00:26:51,410 --> 00:26:54,350
这是根据定理判断出

405
00:26:54,430 --> 00:26:55,900
核不合法的例子之一

406
00:26:56,030 --> 00:26:58,710
因为一个向量

407
00:26:58,810 --> 00:27:30,350
和自己的内积一定大于等于0

408
00:27:30,460 --> 00:27:32,100
所以让我们将核的概念

409
00:27:32,230 --> 00:27:36,810
和SVM联系起来

410
00:27:36,940 --> 00:27:40,050
比如说我们需要在SVM中使用核

411
00:27:40,160 --> 00:27:43,330
你需要做的是选择K(x  z)函数

412
00:27:43,460 --> 00:27:44,760
所以你可以选择--

413
00:27:45,080 --> 00:27:46,620
实际上我之前写的这个函数

414
00:27:46,710 --> 00:27:47,380
是一个合法的核

415
00:27:47,460 --> 00:27:50,440
它被称之为高斯核

416
00:27:50,550 --> 00:27:52,750
因为它和高斯函数很像

417
00:27:52,860 --> 00:27:56,240
所以你可以选择它作为核函数

418
00:27:56,360 --> 00:28:02,620
或者你也可以选择(x^T z+c)^D

419
00:28:02,770 --> 00:28:05,620
将核的概念应用到SVM中

420
00:28:05,740 --> 00:28:07,010
你需要选择一个核函数

421
00:28:07,120 --> 00:28:08,320
函数的选择取决于你的问题

422
00:28:08,420 --> 00:28:09,440
它取决于对于你的问题来说

423
00:28:09,550 --> 00:28:13,760
用怎样的方式度量

424
00:28:13,890 --> 00:28:15,180
两个向量相似与否

425
00:28:15,290 --> 00:28:16,570
更为合适

426
00:28:16,700 --> 00:28:19,370
你需要回到

427
00:28:19,480 --> 00:28:21,200
我们对于SVM的形式化定义中

428
00:28:21,320 --> 00:28:23,740
你需要使用对偶形式

429
00:28:23,870 --> 00:28:29,140
之后将所有出现的这些量

430
00:28:29,280 --> 00:28:40,610
替换成K(x^((i) )  x^((j) ))

431
00:28:40,720 --> 00:28:42,820
你之后需要运行

432
00:28:43,070 --> 00:28:44,830
相同的SVM算法

433
00:28:44,940 --> 00:28:47,350
区别在于每一处出现内积的地方

434
00:28:47,460 --> 00:28:48,430
都要进行替换

435
00:28:48,500 --> 00:28:50,170
你所做的是

436
00:28:50,280 --> 00:28:52,680
在SVM算法中

437
00:28:52,780 --> 00:28:55,470
你隐式地将特征向量x

438
00:28:55,580 --> 00:28:59,580
替换成纬度很高的特征向量

439
00:28:59,700 --> 00:29:03,300
实际上

440
00:29:03,410 --> 00:29:06,160
高斯核对应着无限维的特征向量

441
00:29:06,260 --> 00:29:10,410
然而

442
00:29:10,510 --> 00:29:12,050
你可以在有限时间内

443
00:29:12,160 --> 00:29:13,860
运行SVM算法

444
00:29:13,950 --> 00:29:15,380
即使你们处理的是

445
00:29:15,490 --> 00:29:16,620
无限维的特征向量

446
00:29:16,740 --> 00:29:21,440
因为你需要做的仅仅是计算这些

447
00:29:21,570 --> 00:29:24,060
你并不需要将这些

448
00:29:24,140 --> 00:29:26,800
无限维的特征向量显式地表示出来

449
00:29:26,910 --> 00:29:31,380
那么这样做有什么好处呢?

450
00:29:31,460 --> 00:29:32,640
实际上--

451
00:29:32,740 --> 00:29:35,290
回忆一下我们是怎么引出的SVM

452
00:29:35,420 --> 00:29:39,970
我们引出SVM是为了解决

453
00:29:40,080 --> 00:29:41,260
非线性学习问题

454
00:29:41,360 --> 00:29:43,570
这张图很有用

455
00:29:43,680 --> 00:29:49,710
比如说你们的原始数据是这样的

456
00:29:49,810 --> 00:29:51,100
我不是有意画斜的

457
00:29:51,180 --> 00:29:52,760
比如说你们的输入数据是一维的

458
00:29:52,840 --> 00:29:54,880
你们有一个实数特征x

459
00:29:54,960 --> 00:29:57,970
核的作用是这样的

460
00:29:58,050 --> 00:30:00,840
它将你的原始输入数据

461
00:30:00,950 --> 00:30:02,660
映射到非常高维的特征空间

462
00:30:02,780 --> 00:30:04,100
例如在高斯核中

463
00:30:04,220 --> 00:30:05,640
它将输入数据映射到无限维的空间

464
00:30:05,780 --> 00:30:09,350
为了方便演示

465
00:30:09,480 --> 00:30:10,770
我这里画成二维空间

466
00:30:10,880 --> 00:30:15,690
比如说这是一个非常高维的特征空间

467
00:30:15,890 --> 00:30:24,440
所以高斯核将数据

468
00:30:24,570 --> 00:30:26,300
从一维空间映射到了无限维空间

469
00:30:26,440 --> 00:30:30,800
之后

470
00:30:30,940 --> 00:30:33,720
你在这个无限维空间

471
00:30:33,850 --> 00:30:35,200
或高维空间中运行SVM

472
00:30:35,300 --> 00:30:38,390
你会找到最优间隔分类器

473
00:30:38,500 --> 00:30:39,360
换句话说

474
00:30:39,460 --> 00:30:41,270
你找到的分类器

475
00:30:41,360 --> 00:30:42,530
将高维空间的数据分隔开来

476
00:30:42,640 --> 00:30:44,620
并且使几何间隔最大化

477
00:30:44,770 --> 00:30:49,690
在这个例子中

478
00:30:49,810 --> 00:30:52,240
在原始的一维空间中

479
00:30:52,370 --> 00:30:54,120
你的数据不是线性可分隔的

480
00:30:54,220 --> 00:30:55,280
但是当你将它映射到

481
00:30:55,380 --> 00:30:57,130
高维空间中后

482
00:30:57,270 --> 00:30:58,020
数据变成了线性可分隔的

483
00:30:58,150 --> 00:30:59,020
所以你可以使用

484
00:30:59,160 --> 00:31:02,680
线性分类器对原始空间中

485
00:31:02,780 --> 00:31:04,550
并非线性可分隔的数据进行分类

486
00:31:04,660 --> 00:31:08,330
这就是SVM

487
00:31:08,460 --> 00:31:11,890
输出非线性决策边界的整个过程

488
00:31:12,020 --> 00:31:13,700
你所需要做的仅仅是

489
00:31:13,820 --> 00:31:14,960
求解凸优化问题

490
00:31:15,080 --> 00:31:21,440
有问题吗?

491
00:31:21,560 --> 00:31:24,550
S:应该怎样决定σ的值?

492
00:31:24,670 --> 00:31:27,400
I:关于σ的值

493
00:31:27,510 --> 00:31:28,240
让我看看

494
00:31:28,320 --> 00:31:31,730
我们稍后在讨论

495
00:31:31,880 --> 00:31:33,730
关于这个参数的选择问题

496
00:31:33,820 --> 00:31:37,700
一种选择σ的方式是

497
00:31:37,790 --> 00:31:41,470
留出少量的训练样本

498
00:31:41,580 --> 00:31:42,520
比如说

499
00:31:42,620 --> 00:31:43,690
可以用剩下的三分之二的数据

500
00:31:43,770 --> 00:31:46,010
根据不同的σ值训练SVM

501
00:31:46,090 --> 00:31:47,480
对于不同的σ的值

502
00:31:47,570 --> 00:31:49,920
用留出的样本作为交叉验证集合

503
00:31:50,020 --> 00:31:50,860
测试一下看看哪个值最好

504
00:31:50,950 --> 00:31:56,020
我们讨论过的一些算法

505
00:31:56,160 --> 00:31:58,580
例如:

506
00:31:58,650 --> 00:31:59,650
局部加权线性回归算法中的波长参数

507
00:31:59,780 --> 00:32:00,960
对于这些参数的确定

508
00:32:01,100 --> 00:32:02,600
都可以使用类似的策略:

509
00:32:02,700 --> 00:32:03,570
留出一些样本数据

510
00:32:03,680 --> 00:32:05,330
用来对不同参数值进行测试

511
00:32:05,470 --> 00:32:08,930
我们之后会再来讨论模型选择的问题

512
00:32:09,060 --> 00:32:11,850
还有其他问题吗?

513
00:32:11,950 --> 00:32:13,520
S:你怎么知道将数据映射到

514
00:32:13,620 --> 00:32:16,220
更高维的空间就能得到

515
00:32:16,350 --> 00:32:18,520
线性分类器了呢?

516
00:32:18,650 --> 00:32:20,340
I:好问题

517
00:32:20,440 --> 00:32:22,640
通常情况下你不会准确地知道

518
00:32:22,790 --> 00:32:25,920
有时你可以

519
00:32:26,050 --> 00:32:27,210
但是大多数情况下

520
00:32:27,330 --> 00:32:28,340
你不会准确地知道数据

521
00:32:28,440 --> 00:32:29,300
是否在高维空间中是线性可分隔的

522
00:32:29,400 --> 00:32:31,780
所以下一个主题将会是

523
00:32:31,900 --> 00:32:32,940
l1 norm 软间隔SVM

524
00:32:33,050 --> 00:32:36,290
它使得SVM即使在非线性可分隔

525
00:32:36,430 --> 00:32:37,350
的情况下也能正常工作

526
00:32:37,470 --> 00:32:41,760
S:如果你通过映射到高维的方式

527
00:32:41,890 --> 00:32:43,900
尝试进行线性化粪

528
00:32:44,020 --> 00:32:47,500
那么你能不能直接使用(文稿缺失)?

529
00:32:47,630 --> 00:32:52,450
I:好的 这个问题问的就是

530
00:32:52,550 --> 00:32:53,940
如果不能在高维空间进行分隔时

531
00:32:54,060 --> 00:32:55,060
应该怎么办

532
00:32:55,190 --> 00:32:58,110
让我们在接下来对

533
00:32:58,210 --> 00:32:59,810
l1 norm 软间隔SVM的讨论中

534
00:32:59,940 --> 00:33:02,280
解决这个问题 好吗?

535
00:33:02,380 --> 00:33:09,270
S:你的SVM算法假设数据

536
00:33:09,380 --> 00:33:10,400
是线性可分隔的

537
00:33:10,510 --> 00:33:12,170
如果实际上它不是线性可分隔的

538
00:33:12,280 --> 00:33:14,830
应该怎么办?

539
00:33:14,970 --> 00:33:17,200
I:你们这些人

540
00:33:17,300 --> 00:33:18,630
关于数据是否可以线性分隔

541
00:33:18,730 --> 00:33:19,850
确实给我出了不少难题

542
00:33:19,950 --> 00:33:22,480
实际上

543
00:33:22,580 --> 00:33:23,920
如果数据不是线性可分隔的

544
00:33:24,030 --> 00:33:25,440
那么这个算法就是有问题的

545
00:33:25,540 --> 00:33:26,640
但是稍后我会对其进行修改

546
00:33:27,030 --> 00:33:31,890
使其能够应对这种情况

547
00:33:31,970 --> 00:33:37,670
在我继续讲下一个主题之前

548
00:33:37,790 --> 00:33:56,170
关于核我还有最后一点要说的

549
00:33:56,280 --> 00:33:58,500
之前我一直都在SVM的背景下

550
00:33:58,610 --> 00:34:03,850
介绍核的概念

551
00:34:03,950 --> 00:34:04,920
核的概念使SVM成为了一种

552
00:34:05,010 --> 00:34:06,260
非常强大的学习算法

553
00:34:06,360 --> 00:34:09,090
如果在今天课的最后有时间的话

554
00:34:09,200 --> 00:34:11,690
我会给你们介绍几个选择核的例子

555
00:34:11,800 --> 00:34:12,980
实际上  核的适用范围

556
00:34:13,120 --> 00:34:16,690
比SVM更广

557
00:34:16,810 --> 00:34:22,360
在我们的SVM算法中

558
00:34:22,490 --> 00:34:24,500
我们提出了

559
00:34:24,610 --> 00:34:27,340
对偶优化问题

560
00:34:27,460 --> 00:34:29,630
这使得我们能够将

561
00:34:29,760 --> 00:34:33,720
整个算法写成这些项的内积的形式

562
00:34:33,830 --> 00:34:36,060
实际上  在很多其他的算法中

563
00:34:36,170 --> 00:34:39,100
例如

564
00:34:39,230 --> 00:34:40,260
对于我们讲过的大多数先行算法

565
00:34:40,350 --> 00:34:40,960
比如:

566
00:34:41,040 --> 00:34:42,350
线性回归 logistic回归

567
00:34:42,440 --> 00:34:46,270
或者感知器算法

568
00:34:46,400 --> 00:34:47,550
实际上你们可以将这些算法

569
00:34:47,670 --> 00:34:49,950
重新写成内积的形式

570
00:34:50,060 --> 00:34:54,040
所以如果你将任何算法写成了

571
00:34:54,110 --> 00:34:55,890
内积的形式

572
00:34:55,990 --> 00:34:56,750
这意味着

573
00:34:56,840 --> 00:34:57,530
你可以将其中的内积

574
00:34:57,610 --> 00:35:02,410
替换成K(x^((i) )  x^((j) ))的形式

575
00:35:02,510 --> 00:35:05,710
这意味着你可以隐式地

576
00:35:05,830 --> 00:35:08,080
将原有的特征向量

577
00:35:08,180 --> 00:35:09,520
映射到更高维

578
00:35:09,620 --> 00:35:12,010
并且使得算法仍然能够工作

579
00:35:12,120 --> 00:35:16,490
核的概念在

580
00:35:16,580 --> 00:35:17,550
SVM中得到了广泛应用

581
00:35:17,640 --> 00:35:19,040
但是实际上它的用途要广得多

582
00:35:19,140 --> 00:35:21,260
对于你之前见过的

583
00:35:21,380 --> 00:35:23,330
或之后将要见到的许多算法

584
00:35:23,440 --> 00:35:26,840
你可以将它们写成内积的形式

585
00:35:26,920 --> 00:35:29,250
并将内积替换成核

586
00:35:29,370 --> 00:35:30,930
从而将特征空间映射到无限维

587
00:35:31,080 --> 00:35:34,660
你们在下一个problem set中

588
00:35:34,730 --> 00:35:36,480
将会遇到更多的这样的概念

589
00:35:36,780 --> 00:35:46,700
让我们来讨论非线性决策边界

590
00:35:46,820 --> 00:35:52,350
它被称之为

591
00:35:52,460 --> 00:35:57,020
l1 norm软边界SVM

592
00:35:57,250 --> 00:35:58,040
研究机器学习的人

593
00:35:58,120 --> 00:35:59,230
通常并不擅长于为概念起个好名字

594
00:35:59,310 --> 00:36:01,210
这个概念是这样的

595
00:36:01,310 --> 00:36:02,640
比如说我有一个数据集合

596
00:36:02,730 --> 00:36:13,970
它是线性可分隔的

597
00:36:14,170 --> 00:36:18,440
但是如果我有几个样本

598
00:36:18,590 --> 00:36:20,850
使得整个数据集合

599
00:36:20,960 --> 00:36:25,950
不再是线性可分隔的

600
00:36:26,100 --> 00:36:28,770
我应该怎么办?

601
00:36:28,880 --> 00:36:30,260
实际上  有些时候即使整个数据集合

602
00:36:30,370 --> 00:36:31,890
是线性可分隔的

603
00:36:31,990 --> 00:36:32,870
可能我们也不愿意这样做

604
00:36:32,950 --> 00:36:34,100
例如  这是一个非常漂亮的数据集合

605
00:36:34,180 --> 00:36:35,760
看起来这样的一个决策边界

606
00:36:35,840 --> 00:36:36,600
是完美的

607
00:36:36,680 --> 00:36:41,150
如果这里出现了一个异常数据

608
00:36:41,230 --> 00:36:44,210
我们应该怎么办?

609
00:36:44,390 --> 00:36:45,270
我仍然能够像这样分隔数据

610
00:36:45,400 --> 00:36:47,080
保证数据是线性可分隔的

611
00:36:47,190 --> 00:36:47,940
但是这样做的后果

612
00:36:48,020 --> 00:36:49,060
是我让一个看起来不是

613
00:36:49,170 --> 00:36:51,040
那么可靠的单一样本

614
00:36:51,140 --> 00:36:52,820
使我们的决策边界

615
00:36:52,930 --> 00:36:54,200
扭转了一个很大的角度

616
00:36:54,310 --> 00:36:59,040
我接下来要讲的是l1 norm软间隔SVM

617
00:36:59,150 --> 00:37:00,010
它是SVM优化问题的

618
00:37:00,110 --> 00:37:03,770
一种轻微修改后的

619
00:37:03,860 --> 00:37:06,390
形式化版本

620
00:37:06,480 --> 00:37:08,040
这样的改动将会帮助我们

621
00:37:08,170 --> 00:37:09,290
处理这两种情况

622
00:37:09,410 --> 00:37:11,210
一种情况下数据不是线性可分隔的

623
00:37:11,300 --> 00:37:12,010
另一种情况下

624
00:37:12,100 --> 00:37:13,130
也许对于某些训练样本

625
00:37:13,220 --> 00:37:15,620
我们宁可选择不考虑它

626
00:37:15,720 --> 00:37:17,650
也许对于这里的这个异常数据

627
00:37:17,750 --> 00:37:20,000
也许你会选择原始的决策边界

628
00:37:20,090 --> 00:37:23,270
而不会选择将这个训练样本考虑进来

629
00:37:23,380 --> 00:37:27,420
形式化表述是这样的

630
00:37:27,530 --> 00:37:48,920
我们的SVM原始问题是这样的

631
00:37:49,040 --> 00:38:04,270
这是我们的原始问题

632
00:38:04,400 --> 00:38:07,390
我们会对其进行修改

633
00:38:07,520 --> 00:38:09,900
加上这些

634
00:38:09,990 --> 00:38:23,980
换句话说

635
00:38:24,100 --> 00:38:26,470
我们会增加一些惩罚项  ξ_i

636
00:38:26,560 --> 00:38:28,470
我要求所有的训练样本都需要被分隔

637
00:38:28,550 --> 00:38:30,620
并且其函数间隔

638
00:38:30,720 --> 00:38:34,470
都要大于等于1-ξ_i

639
00:38:34,570 --> 00:38:36,670
你们应该记得

640
00:38:36,780 --> 00:38:47,900
如果这一项大于0

641
00:38:48,040 --> 00:38:51,050
记得上一讲或上上讲我们说过

642
00:38:51,130 --> 00:38:53,820
如果函数间隔大于0

643
00:38:53,920 --> 00:38:56,110
这意味着你的分类是正确的

644
00:38:56,250 --> 00:39:02,010
如果小于0

645
00:39:02,140 --> 00:39:03,110
那么分类就是错误的

646
00:39:03,220 --> 00:39:07,340
通过令一些ξ_i大于1

647
00:39:07,460 --> 00:39:09,920
我实际上允许其中的一些样本函数

648
00:39:10,030 --> 00:39:11,570
间隔小于0

649
00:39:11,700 --> 00:39:15,110
因此我允许我的算法

650
00:39:15,260 --> 00:39:16,990
对一些样本进行错误分类

651
00:39:17,120 --> 00:39:21,230
然而  我并不鼓励算法这样做

652
00:39:21,340 --> 00:39:23,520
通过在目标函数中增加这些惩罚项

653
00:39:23,640 --> 00:39:24,850
当将某些ξ_i设定为较大值时

654
00:39:24,950 --> 00:39:27,660
给予总体目标一些惩罚

655
00:39:27,790 --> 00:39:30,730
这个优化问题的参数是:

656
00:39:30,910 --> 00:39:34,860
w  b和所有的ξ_i

657
00:39:35,010 --> 00:39:39,670
这也是一个凸优化问题

658
00:39:39,780 --> 00:39:47,820
和之前我们推导出

659
00:39:47,910 --> 00:39:49,840
SVM的对偶问题的过程类似

660
00:39:49,920 --> 00:39:51,470
我们也可以推出

661
00:39:51,570 --> 00:39:52,950
这个优化问题的对偶问题

662
00:39:53,050 --> 00:39:54,870
我不打算具体地进行推导

663
00:39:54,980 --> 00:39:57,200
而只打算向你们展示一下过程

664
00:39:57,310 --> 00:39:58,610
你首先要构建拉格朗日算子

665
00:39:58,760 --> 00:40:05,520
这里的α和r表示拉格朗日乘数

666
00:40:05,600 --> 00:40:08,950
分别和我们之前的约束

667
00:40:09,090 --> 00:40:10,120
以及新加入的这组ξ_i≥0的约束对应

668
00:40:10,280 --> 00:40:11,880
这组约束对应了

669
00:40:12,000 --> 00:40:15,140
一组新的拉格朗日乘数

670
00:40:15,270 --> 00:40:17,150
拉格朗日算子应该等于

671
00:40:17,270 --> 00:40:25,620
优化目标函数

672
00:40:25,760 --> 00:40:48,080
减去这个求和式

673
00:40:48,350 --> 00:40:48,970
所以我们的拉格朗日算子应该等于

674
00:40:49,070 --> 00:40:50,260
要优化的目标函数

675
00:40:50,390 --> 00:40:51,160
减去拉格朗日乘数

676
00:40:51,240 --> 00:40:54,900
乘以这些≥0的约束

677
00:40:55,030 --> 00:40:56,210
我不打算重新推导一遍

678
00:40:56,320 --> 00:41:22,790
但是基本上和之前的过程是相同的

679
00:41:22,900 --> 00:41:30,610
当你推导出这个优化问题的对偶问题

680
00:41:30,750 --> 00:41:32,560
并且化简过之后

681
00:41:32,660 --> 00:41:36,000
你会发现你得到了这个式子

682
00:41:36,110 --> 00:41:37,810
你需要使W(α)最大化

683
00:41:37,950 --> 00:41:42,880
像往常一样

684
00:41:43,010 --> 00:42:21,560
事实证明

685
00:42:21,670 --> 00:42:24,190
当你推出对偶问题并且进行化简之后

686
00:42:24,260 --> 00:42:26,650
你会发现对偶问题

687
00:42:26,730 --> 00:42:29,580
和原问题唯一的一点区别是

688
00:42:29,680 --> 00:42:31,710
之前的α≥0

689
00:42:31,860 --> 00:42:34,090
而现在的α介于

690
00:42:34,200 --> 00:42:35,090
0和C之间

691
00:42:35,230 --> 00:42:38,100
推导过程并不是很难

692
00:42:38,210 --> 00:42:39,440
希望你们回去之后

693
00:42:39,550 --> 00:42:40,590
能试着自己推一下

694
00:42:40,670 --> 00:42:42,270
数学过程和之前是一样的

695
00:42:42,350 --> 00:42:43,320
当你化简之后

696
00:42:43,430 --> 00:42:48,110
你可以消除掉拉格朗日乘数r

697
00:42:48,210 --> 00:42:50,920
只剩下和α有关的约束

698
00:42:51,010 --> 00:43:05,780
还要补充一点

699
00:43:05,910 --> 00:43:10,100
我同样不会进行推导

700
00:43:10,250 --> 00:43:11,310
实际上

701
00:43:11,450 --> 00:43:12,740
记得我上节课

702
00:43:12,840 --> 00:43:15,570
讲到的KKT条件

703
00:43:15,660 --> 00:43:20,130
它们是约束优化问题

704
00:43:20,280 --> 00:43:21,910
找到最优解的必要条件

705
00:43:22,030 --> 00:43:22,900
所以如果你使用KKT条件

706
00:43:23,000 --> 00:43:27,790
实际上

707
00:43:27,880 --> 00:43:30,090
你可以推出收敛性条件

708
00:43:30,220 --> 00:43:32,360
如果我们希望求解一个优化问题

709
00:43:32,470 --> 00:43:34,350
我们怎样知道

710
00:43:34,460 --> 00:43:37,340
α已经收敛到全局最优解呢?

711
00:43:37,470 --> 00:43:40,160
实际上你们可以使用下面的结论

712
00:43:40,280 --> 00:44:19,170
关于这些结论我不打算讲太多

713
00:44:19,290 --> 00:44:24,550
实际上通过KKT条件

714
00:44:24,700 --> 00:44:26,930
你可以推出这些收敛性条件

715
00:44:27,040 --> 00:44:29,280
可以帮助一个算法

716
00:44:29,400 --> 00:44:31,440
找到作为最优解的α值

717
00:44:31,570 --> 00:44:38,140
这是l1 norm软间隔SVM

718
00:44:38,290 --> 00:44:40,760
它对原有算法进行了修改

719
00:44:40,910 --> 00:44:43,750
使我们可以处理非线性可分隔的情形

720
00:44:43,870 --> 00:44:44,840
同时对于那些包含异常数据的

721
00:44:44,990 --> 00:44:47,640
可线性分隔的数据集合

722
00:44:47,770 --> 00:44:48,550
通过这个算法也可以选择

723
00:44:48,700 --> 00:44:49,560
不进行完全正确的线性分隔

724
00:44:49,670 --> 00:44:56,440
有问题吗?

725
00:44:56,520 --> 00:45:18,300
如果明白的话请举手 好的

726
00:45:18,410 --> 00:46:00,600
最后一个要讲的东西

727
00:46:00,720 --> 00:46:06,920
是一个求解这个优化问题的算法

728
00:46:07,040 --> 00:46:11,690
我们写出了这个优化问题

729
00:46:11,790 --> 00:46:13,260
以及这些收敛性条件

730
00:46:13,330 --> 00:46:15,900
我们需要根据这些设计出一个

731
00:46:16,020 --> 00:46:17,940
算法能够高效地求解这个优化问题

732
00:46:18,050 --> 00:46:24,500
这里我首先要讲一个

733
00:46:24,620 --> 00:46:26,110
称之为坐标上升的算法

734
00:46:26,220 --> 00:46:27,440
将会非常有用

735
00:46:27,530 --> 00:46:30,970
我接下来要做的是

736
00:46:31,080 --> 00:46:34,170
介绍坐标上升法

737
00:46:34,290 --> 00:46:35,830
它是一个非常有用的算法

738
00:46:35,930 --> 00:46:37,950
我们不打算将这个算法的

739
00:46:38,060 --> 00:46:40,220
最简单的形式用于解决这个问题

740
00:46:40,310 --> 00:46:42,900
但是我们稍后会进行一些修改

741
00:46:42,990 --> 00:46:45,690
这将为我们提供一个非常高效的算法

742
00:46:45,970 --> 00:46:47,720
使我们可以解决这个优化问题

743
00:46:47,830 --> 00:46:49,950
这是我要推出

744
00:46:50,040 --> 00:46:52,060
对偶问题的另外一个原因

745
00:46:52,170 --> 00:46:54,250
不仅仅是我们需要用到核

746
00:46:54,360 --> 00:46:57,420
而且我们可以用到像SMO这样的算法

747
00:46:57,570 --> 00:47:02,840
首先  让我们来讨论坐标上升法

748
00:47:02,950 --> 00:47:05,780
它是另外一个优化问题

749
00:47:05,890 --> 00:47:19,550
为了描述坐标上升

750
00:47:19,650 --> 00:47:22,240
我希望你们考虑这样的问题

751
00:47:22,350 --> 00:47:29,680
我们希望最大化函数W

752
00:47:29,820 --> 00:47:32,380
它是一个α_1 到α_m的函数

753
00:47:32,480 --> 00:47:34,330
没有约束条件

754
00:47:34,410 --> 00:47:41,410
现在  忘掉之前提过的约束条件

755
00:47:41,560 --> 00:47:43,170
包括α_i 处于0到C之间

756
00:47:43,300 --> 00:47:46,100
以及对y^((i) ) α^((i) )求和

757
00:47:46,180 --> 00:47:47,000
必须等于0

758
00:47:47,110 --> 00:47:54,820
坐标上升算法是这样的

759
00:47:54,960 --> 00:47:57,490
算法会一直循环  直到收敛

760
00:47:57,600 --> 00:48:01,300
每次循环中

761
00:48:01,390 --> 00:48:02,360
对于i=1到m

762
00:48:02,470 --> 00:48:07,760
坐标上升算法保持

763
00:48:07,840 --> 00:48:10,280
除α_i之外的所有参数固定

764
00:48:10,390 --> 00:48:14,330
之后它会相对于这个参数

765
00:48:14,450 --> 00:48:16,850
使函数取最大值

766
00:48:16,960 --> 00:48:21,950
让我将它写成:

767
00:48:22,060 --> 00:48:33,610
表示的是

768
00:48:33,740 --> 00:48:43,830
保持除α_i之外的所有参数固定

769
00:48:43,940 --> 00:48:50,610
相对于α_i使函数W取最大值

770
00:48:50,740 --> 00:48:58,710
这只是一种巧妙的写法

771
00:48:58,820 --> 00:49:02,890
这是坐标上升法

772
00:49:02,970 --> 00:49:05,720
这幅图能够帮助你们

773
00:49:05,810 --> 00:49:15,890
更好地理解这个算法

774
00:49:15,990 --> 00:49:17,760
想象一下

775
00:49:17,880 --> 00:49:19,530
你尝试求一个二次函数的最优值

776
00:49:19,610 --> 00:49:25,740
它看起来是这样的

777
00:49:25,820 --> 00:49:30,740
这些是二次函数的轮廓

778
00:49:30,860 --> 00:49:31,860
这里是最小值

779
00:49:31,950 --> 00:49:34,540
坐标上升算法是这样的

780
00:49:34,630 --> 00:49:39,780
这两个坐标轴我分别表示为α_1和α_2

781
00:49:39,900 --> 00:49:42,000
在这两个坐标轴的空间中

782
00:49:42,160 --> 00:49:43,540
比如说我从这里开始

783
00:49:43,630 --> 00:49:47,610
之后我相对于α_1取最小值

784
00:49:47,720 --> 00:49:49,520
到了这里

785
00:49:49,830 --> 00:49:51,910
在新的位置

786
00:49:52,000 --> 00:49:53,640
我相对于α_2取最小值

787
00:49:53,730 --> 00:49:55,310
所以我可能会到达这个地方

788
00:49:55,410 --> 00:49:59,520
之后我会相对于α_1使函数取最小值

789
00:49:59,630 --> 00:50:00,430
之后是α_2  依此类推

790
00:50:00,540 --> 00:50:07,290
你总会沿着

791
00:50:07,370 --> 00:50:10,360
和坐标轴平行的方向取到最小值

792
00:50:10,470 --> 00:50:13,730
实际上可以对这个算法

793
00:50:13,820 --> 00:50:16,620
进行一些小的修改

794
00:50:16,720 --> 00:50:17,760
这个算法存在着一些变种

795
00:50:17,870 --> 00:50:19,860
在我对这个算法的描述中

796
00:50:19,960 --> 00:50:22,250
这些参数总是按顺序交替

797
00:50:22,350 --> 00:50:24,330
我们总是相对于α_1取最优值

798
00:50:24,410 --> 00:50:25,610
之后是α_2  之后是α_1  之后是α_2

799
00:50:25,710 --> 00:50:29,400
我要说的这种改进

800
00:50:29,520 --> 00:50:31,020
只能应用在更高的维度上

801
00:50:31,120 --> 00:50:33,460
实际上如果你有很多参数

802
00:50:33,570 --> 00:50:34,970
例如:从α_1 到α_m

803
00:50:35,090 --> 00:50:37,660
你可以不按照固定的顺序访问它们

804
00:50:37,760 --> 00:50:38,670
你可以根据是否能够

805
00:50:38,760 --> 00:50:40,430
使你前进更多作为判断标准

806
00:50:40,570 --> 00:50:42,910
来选取下一个α

807
00:50:43,050 --> 00:50:44,300
如果你有两个参数

808
00:50:44,410 --> 00:50:47,270
在两个参数之间进行轮换是有意义的

809
00:50:47,380 --> 00:50:48,380
如果你有更高维的参数

810
00:50:48,460 --> 00:50:51,680
有时你需要按照不同的顺序

811
00:50:51,800 --> 00:50:52,870
来进行更新

812
00:50:53,000 --> 00:51:01,330
如果你认为这样可以帮助你

813
00:51:01,440 --> 00:51:03,490
更快地逼近最大值的话

814
00:51:03,620 --> 00:51:05,650
实际上

815
00:51:05,770 --> 00:51:10,030
如果将坐标上升

816
00:51:10,140 --> 00:51:11,190
和之前我们学过的一些算法

817
00:51:11,310 --> 00:51:13,560
例如:牛顿方法  进行对比

818
00:51:13,650 --> 00:51:15,190
坐标上升通常会经过更多的步骤

819
00:51:15,300 --> 00:51:19,080
但是坐标上升的主要优点是

820
00:51:19,180 --> 00:51:20,380
有的时候相对于W的

821
00:51:20,460 --> 00:51:29,030
任何一个参数

822
00:51:29,110 --> 00:51:31,540
求其最优值的代价会非常小

823
00:51:31,660 --> 00:51:32,930
坐标上升法为了收敛

824
00:51:33,020 --> 00:51:36,770
所需要的迭代的次数

825
00:51:36,890 --> 00:51:40,020
要多于牛顿方法

826
00:51:40,140 --> 00:51:44,210
实际上有很多优化问题

827
00:51:44,320 --> 00:51:48,170
固定所有其他参数

828
00:51:48,290 --> 00:51:49,410
只对一个参数

829
00:51:49,500 --> 00:51:50,660
求最优值是非常容易的

830
00:51:50,770 --> 00:51:52,680
如果这一点成立

831
00:51:52,800 --> 00:51:55,870
那么坐标上升的相对于

832
00:51:55,970 --> 00:51:58,440
α_i的内层循环将会执行的非常快

833
00:51:58,560 --> 00:52:00,090
实际上当我们修改这个算法

834
00:52:00,210 --> 00:52:05,160
来解决SVM优化问题时

835
00:52:05,290 --> 00:52:06,540
这一点是成立的

836
00:52:06,690 --> 00:52:13,740
有问题吗?好的

837
00:52:13,830 --> 00:52:49,770
让我们继续应用这个算法

838
00:52:49,870 --> 00:52:52,450
来解决SVM对偶优化问题

839
00:52:52,550 --> 00:52:59,000
事实证明坐标上升的基本形式

840
00:52:59,090 --> 00:53:00,810
不能直接应用

841
00:53:01,110 --> 00:53:02,470
因为如下的原因

842
00:53:02,570 --> 00:53:04,150
原因是  我们对α_i存在约束

843
00:53:04,260 --> 00:53:09,030
记得我们之前的结果

844
00:53:09,120 --> 00:53:11,590
我们的约束是:

845
00:53:11,690 --> 00:53:17,490
∑_i?〖α_i y^((i) ) 〗=0

846
00:53:17,590 --> 00:53:18,770
所以如果你固定

847
00:53:18,860 --> 00:53:21,770
除了一个之外的所有的α

848
00:53:21,860 --> 00:53:23,350
那么你不可能仅改变一个α的同时

849
00:53:23,430 --> 00:53:26,300
不违反这个约束

850
00:53:26,410 --> 00:53:29,350
如果我只尝试改变α_1

851
00:53:29,450 --> 00:53:30,390
α_1实际上是其它α的函数

852
00:53:30,490 --> 00:53:32,110
因为这些项之和等于0

853
00:53:32,220 --> 00:53:36,040
顺便提一句

854
00:53:36,140 --> 00:53:38,170
SMO算法是由John Platt

855
00:53:38,240 --> 00:53:39,360
一个微软的同行提出的

856
00:53:39,430 --> 00:53:41,330
SMO算法

857
00:53:41,430 --> 00:53:44,390
相对于之前的只改变一个α

858
00:53:44,480 --> 00:53:52,880
选择一次改变两个α

859
00:53:52,980 --> 00:53:57,550
这个算法称之为SMO算法

860
00:53:57,680 --> 00:53:59,800
即:序列最小优化算法

861
00:53:59,890 --> 00:54:02,570
最小指的是我们希望

862
00:54:02,660 --> 00:54:05,990
一次改变最小数目的α_i

863
00:54:06,070 --> 00:54:07,130
这里我们只需要

864
00:54:07,210 --> 00:54:09,100
一次改变两个

865
00:54:09,210 --> 00:54:13,850
我们继续来列出这个算法

866
00:54:13,950 --> 00:54:25,040
我们选择两个要改变的α值

867
00:54:25,140 --> 00:54:26,380
根据一些启发式规则选取α_i 和α_j

868
00:54:26,470 --> 00:54:30,100
所谓的启发式规则就是指经验法则

869
00:54:30,220 --> 00:54:36,600
我们保持除α_i  α_j之外的

870
00:54:36,720 --> 00:54:45,680
所有参数固定

871
00:54:45,780 --> 00:54:57,590
同时相对于这两个参数使W取最优

872
00:54:57,680 --> 00:55:02,170
同时满足所有的约束条件

873
00:55:02,280 --> 00:55:19,540
实际上接下来我要继续讨论的

874
00:55:19,620 --> 00:55:22,650
关键步骤是这里

875
00:55:22,750 --> 00:55:28,940
怎样在满足所有约束条件的情况下

876
00:55:29,030 --> 00:55:31,520
相对于这两个参数使函数取最优值?

877
00:55:31,590 --> 00:55:36,720
稍后会讨论

878
00:55:36,790 --> 00:55:41,250
你需要一直运行这个算法

879
00:55:41,350 --> 00:55:46,070
直到满足这些收敛条件

880
00:55:46,160 --> 00:55:53,190
我接下来要讨论的是怎样

881
00:55:53,240 --> 00:55:55,100
完成这个关键步骤

882
00:55:55,170 --> 00:55:59,080
怎样在满足约束的情况下

883
00:55:59,190 --> 00:56:02,910
相对于α_i  α_j使W取最优

884
00:56:03,020 --> 00:56:04,400
事实证明你可以用SMO算法

885
00:56:04,520 --> 00:56:06,760
高效地完成这个工作

886
00:56:06,840 --> 00:56:09,380
SMO算法非常高效

887
00:56:09,460 --> 00:56:11,550
它可能需要更多次数的迭代

888
00:56:11,630 --> 00:56:12,460
以达到收敛

889
00:56:12,540 --> 00:56:14,100
但是每次迭代所需要的代价都非常小

890
00:56:14,200 --> 00:56:19,060
让我们来继续讨论

891
00:56:19,140 --> 00:56:43,820
为了推出这个步骤

892
00:56:43,900 --> 00:56:46,320
我们需要相对于α_i  α_j进行更新

893
00:56:46,410 --> 00:56:48,890
在我的例子中我会写α_1  α_2

894
00:56:48,970 --> 00:56:54,230
我要更新α_1  α_2 通常情况下

895
00:56:54,320 --> 00:56:55,940
这可以是任意的α_i  α_j

896
00:56:56,020 --> 00:56:58,700
但是为了使我黑板上的符号

897
00:56:58,780 --> 00:56:59,930
表达更加简明

898
00:57:00,030 --> 00:57:01,200
我仅仅对α_1  α_2进行推导

899
00:57:01,300 --> 00:57:05,320
而通常情况的推导过程很容易类比

900
00:57:05,440 --> 00:57:09,510
对于算法的每一步

901
00:57:09,590 --> 00:57:12,890
我们可以根据

902
00:57:12,990 --> 00:57:15,040
约束条件

903
00:57:15,150 --> 00:57:16,540
∑_i?〖α_i y^((i) ) 〗=0

904
00:57:16,640 --> 00:57:17,730
这是我们之前的

905
00:57:17,820 --> 00:57:21,450
对偶优化问题的约束之一

906
00:57:21,540 --> 00:57:23,080
这意味着:

907
00:57:23,170 --> 00:57:31,950
α_1+α_2应该等于这个式子

908
00:57:32,040 --> 00:57:43,600
我用ζ来表示

909
00:57:43,720 --> 00:57:53,700
我们还有一个约束:0≤α_i≤C

910
00:57:53,800 --> 00:57:56,120
我们的对偶问题有两个约束

911
00:57:56,230 --> 00:57:58,200
这是一个  这是另外一个

912
00:57:58,290 --> 00:58:00,970
我们用图来表示

913
00:58:01,050 --> 00:58:30,390
0≤α_i≤C这个约束经常被称为

914
00:58:30,460 --> 00:58:32,690
方形约束

915
00:58:32,870 --> 00:58:38,150
如果我将α_1  α_2画出来

916
00:58:38,240 --> 00:58:54,800
那么α_1  α_2表示的值

917
00:58:54,900 --> 00:58:57,280
一定位于这个从0到C的方形里

918
00:58:57,370 --> 00:59:00,460
算法的图是这样的

919
00:59:00,590 --> 00:59:03,680
我们的约束

920
00:59:03,760 --> 00:59:11,600
是α_1+α_2=ζ

921
00:59:11,680 --> 00:59:27,410
这意味着

922
00:59:27,540 --> 00:59:30,460
α_1应该等于这个式子

923
00:59:30,590 --> 00:59:38,080
所以我希望

924
00:59:38,150 --> 00:59:40,470
相对于它求最优值

925
00:59:40,540 --> 00:59:47,740
我能做的是

926
00:59:47,850 --> 00:59:49,140
将α_1作为α_2的函数代入

927
00:59:49,220 --> 00:59:53,050
所以α_1应该等于这个结果

928
00:59:53,120 --> 00:59:56,960
之后是α_2  α_3

929
00:59:57,040 --> 01:00:03,040
等等

930
01:00:03,120 --> 01:00:06,710
实际上由于W是一个二次函数

931
01:00:06,790 --> 01:00:08,290
如果你回去看看我们之前对W的定义

932
01:00:08,370 --> 01:00:09,730
你会发现它对于所有的α

933
01:00:09,810 --> 01:00:13,570
都是一个二次函数

934
01:00:13,940 --> 01:00:16,280
所以如果你将这个W视为α_2的函数

935
01:00:16,390 --> 01:00:19,260
你会发现这是一个

936
01:00:19,360 --> 01:00:21,070
标准的一元二次函数

937
01:00:21,160 --> 01:00:23,810
如果你保持其它参数固定的话

938
01:00:23,890 --> 01:00:29,400
所以这个式子可以被简化为:

939
01:00:29,480 --> 01:00:33,900
aα_2^2+bα_2+c

940
01:00:33,990 --> 01:00:36,400
这是一个标准的二次函数

941
01:00:36,480 --> 01:00:38,890
这个函数很容易求最优值

942
01:00:38,970 --> 01:00:42,290
我们都知道应该怎样求

943
01:00:42,410 --> 01:00:43,910
我们是什么时候学的?

944
01:00:44,000 --> 01:00:45,460
可能是在高中就学到了

945
01:00:45,540 --> 01:00:46,410
你们都知道应该怎样求

946
01:00:46,500 --> 01:00:47,470
这样的二次函数的最优值

947
01:00:47,550 --> 01:00:48,720
你只需要求出最优解

948
01:00:48,810 --> 01:00:51,180
从而得到α_2的最优值

949
01:00:51,260 --> 01:00:57,250
最后一步是处理方形约束

950
01:00:57,360 --> 01:00:57,990
在图中

951
01:00:58,070 --> 01:01:00,240
你知道你的解

952
01:01:00,330 --> 01:01:02,300
一定在这条线上

953
01:01:02,390 --> 01:01:06,540
在线上会有一些二次函数

954
01:01:06,610 --> 01:01:10,860
所以如果你使二次函数取最优值时

955
01:01:10,970 --> 01:01:12,100
你可能得到一个落在方形区域中的点

956
01:01:12,240 --> 01:01:13,010
如果是这样的话

957
01:01:13,110 --> 01:01:14,480
你就得到了想要的结果

958
01:01:14,580 --> 01:01:16,200
如果你的二次函数看起来是这样的

959
01:01:16,310 --> 01:01:18,200
也许当你最优化你的二次函数的时候

960
01:01:18,300 --> 01:01:20,090
你可能在方形外面得到一个点

961
01:01:20,200 --> 01:01:23,050
如果出现这种情况

962
01:01:23,150 --> 01:01:24,290
你需要对解进行裁剪

963
01:01:24,420 --> 01:01:27,680
以使得它落在方形区域中

964
01:01:27,800 --> 01:01:29,890
这样

965
01:01:29,980 --> 01:01:32,430
你会得到这个二次优化问题的最优解

966
01:01:32,570 --> 01:01:36,480
既满足方形约束

967
01:01:36,570 --> 01:01:39,240
同时也满足解在直线上的约束

968
01:01:39,380 --> 01:01:41,570
换句话说

969
01:01:41,660 --> 01:01:43,480
解一定在这个

970
01:01:43,570 --> 01:01:45,540
方形区域中的线段上

971
01:01:45,640 --> 01:01:55,390
按照这种方式解出α_2之后

972
01:01:55,500 --> 01:01:58,180
你可以根据方形约束对解进行裁剪

973
01:01:58,290 --> 01:01:59,250
之后根据α_1

974
01:01:59,360 --> 01:02:03,010
和α_2之间的函数关系求出α_1

975
01:02:03,120 --> 01:02:06,850
这样我们就求出了相对于α_1

976
01:02:06,970 --> 01:02:09,090
和α_2的W的最优值

977
01:02:09,190 --> 01:02:11,040
满足所有的约束

978
01:02:11,120 --> 01:02:12,670
该算法的关键步骤就是要

979
01:02:12,740 --> 01:02:15,980
对一个二次函数求最优解

980
01:02:16,080 --> 01:02:19,000
可以很快完成

981
01:02:19,110 --> 01:02:20,390
这使得SMO算法的内层循环非常高效

982
01:02:20,490 --> 01:02:33,320
S:之前你说过我们需要一次改变

983
01:02:33,420 --> 01:02:35,700
一个参数

984
01:02:35,800 --> 01:02:38,050
但是在SMO算法中

985
01:02:38,110 --> 01:02:42,170
我们一次可以改变两个

986
01:02:42,290 --> 01:02:43,340
应该怎样理解它?

987
01:02:43,420 --> 01:02:44,290
I:好的

988
01:02:44,370 --> 01:02:47,670
比如说我希望改变

989
01:02:47,760 --> 01:02:52,220
当我执行优化算法时

990
01:02:52,310 --> 01:02:54,230
我需要满足一个约束:

991
01:02:54,340 --> 01:03:01,330
∑_i?〖α_i y^((i) ) 〗=0

992
01:03:01,450 --> 01:03:06,500
在我们讨论坐标上升时

993
01:03:06,580 --> 01:03:10,960
是没有这样的约束的

994
01:03:11,040 --> 01:03:12,240
假设我尝试只改变α_1

995
01:03:12,350 --> 01:03:16,880
那么我知道α_1一定等于这个式子

996
01:03:17,000 --> 01:03:21,340
对吗?

997
01:03:21,470 --> 01:03:28,570
所以α_1

998
01:03:28,680 --> 01:03:30,750
实际上是α_2  α_3

999
01:03:30,850 --> 01:03:31,750
一直到α_m的函数

1000
01:03:31,850 --> 01:03:34,200
所以如果我保持α_2  α_3

1001
01:03:34,280 --> 01:03:36,480
一直到α_m固定

1002
01:03:36,580 --> 01:03:37,960
那么我就不能改变α_1

1003
01:03:38,050 --> 01:03:39,500
因为α_1是这些参数的函数

1004
01:03:39,590 --> 01:03:41,230
相反地

1005
01:03:41,300 --> 01:03:44,760
如果我希望同时改变α_1和α_2的值

1006
01:03:44,830 --> 01:03:48,580
此时同样存在约束

1007
01:03:48,680 --> 01:03:50,520
并且我知道

1008
01:03:50,610 --> 01:03:52,740
α_1  α_2一定满足这个线性约束

1009
01:03:52,870 --> 01:03:59,050
但是至少当我在改变α_1的时候

1010
01:03:59,190 --> 01:03:59,970
我可以相应地改变α_2

1011
01:04:00,060 --> 01:04:02,120
以使得约束条件被满足

1012
01:04:02,220 --> 01:04:04,820
S:ζ是什么意思?

1013
01:04:04,930 --> 01:04:12,050
I:ζ被定义在另外一块黑板上

1014
01:04:12,130 --> 01:04:16,670
对于每次迭代

1015
01:04:16,760 --> 01:04:19,570
我都有一组参数

1016
01:04:19,650 --> 01:04:20,860
从α_1到α_m

1017
01:04:20,930 --> 01:04:23,520
比如说我希望改变α_1  α_2

1018
01:04:23,620 --> 01:04:26,380
所以根据之前的迭代

1019
01:04:26,480 --> 01:04:30,400
我还没有改变参数的值

1020
01:04:30,510 --> 01:04:32,950
所以这个约束成立

1021
01:04:33,080 --> 01:04:34,310
所以我将ζ定义成这样的形式

1022
01:04:34,410 --> 01:04:40,480
α_1 y^((1) )+α_2 y^((2) )

1023
01:04:40,560 --> 01:04:41,910
等于这个式子

1024
01:04:41,990 --> 01:04:44,740
所以我将这个式子定义为ζ

1025
01:04:44,820 --> 01:05:00,090
S:【听不清】

1026
01:05:00,180 --> 01:05:02,550
I:每一次迭代

1027
01:05:02,650 --> 01:05:05,460
都需要选取一对α进行更新

1028
01:05:05,580 --> 01:05:08,660
具体的选取方式我不打算讨论

1029
01:05:09,260 --> 01:05:11,200
关于这个我会再讲几句

1030
01:05:11,300 --> 01:05:13,790
算法的基本流程是

1031
01:05:13,880 --> 01:05:15,650
每次迭代

1032
01:05:15,730 --> 01:05:17,910
你都需要选择一对

1033
01:05:20,490 --> 01:05:21,360
α_i  α_j来进行更新

1034
01:05:21,520 --> 01:05:23,150
所以根据一些启发式规则

1035
01:05:23,260 --> 01:05:24,910
这组α被选择进行更新

1036
01:05:25,000 --> 01:05:26,750
之后就用我刚刚描述的更新方法

1037
01:05:26,830 --> 01:05:29,120
更新这对参数

1038
01:05:29,210 --> 01:05:33,630
我刚刚讲的是相对于α_i  α_j

1039
01:05:33,720 --> 01:05:37,570
使函数W取最优值

1040
01:05:37,680 --> 01:05:40,240
我不打算讲具体的选择

1041
01:05:40,360 --> 01:05:43,780
α_i  α_j的启发式策略

1042
01:05:43,880 --> 01:05:48,190
S:函数W是什么?

1043
01:05:48,300 --> 01:05:53,270
I:W的定义在那上面

1044
01:05:53,380 --> 01:05:54,340
我再写一遍

1045
01:05:54,440 --> 01:05:55,980
我们之前定义过W(α)

1046
01:05:56,090 --> 01:05:57,860
它应该等于

1047
01:05:57,950 --> 01:06:01,700
它是关于解决

1048
01:06:01,790 --> 01:06:12,100
它等于这个式子

1049
01:06:12,190 --> 01:06:14,160
这是SVM要解决的优化问题

1050
01:06:14,270 --> 01:06:15,760
所以W(α)表示的是目标函数

1051
01:06:15,850 --> 01:06:25,800
S:交换一个α

1052
01:06:25,880 --> 01:06:28,300
使其取最优值

1053
01:06:28,410 --> 01:06:30,580
这样对另外一个

1054
01:06:30,670 --> 01:06:32,080
有影响吗?

1055
01:06:32,160 --> 01:06:33,450
I:你是什么意思?

1056
01:06:33,530 --> 01:06:37,460
S:它离最优值离得更远了

1057
01:06:37,560 --> 01:06:42,430
I:让我换种方式再说一遍

1058
01:06:42,510 --> 01:06:45,590
我们要做的是要使

1059
01:06:45,680 --> 01:06:47,380
目标函数W(α)取最优值

1060
01:06:47,490 --> 01:06:50,390
所以我们算法的整个执行过程中的

1061
01:06:50,470 --> 01:06:53,210
一个判别标准是:

1062
01:06:53,320 --> 01:06:58,180
每次迭代是否

1063
01:06:58,290 --> 01:07:01,380
使W(α)的值变得越来越好

1064
01:07:01,490 --> 01:07:03,420
这一点对于坐标上升

1065
01:07:03,500 --> 01:07:05,680
和SMO算法来说是成立的:

1066
01:07:05,760 --> 01:07:07,970
每一次迭代

1067
01:07:08,070 --> 01:07:10,440
W(α)的值只可能保持不变

1068
01:07:10,510 --> 01:07:13,650
或者变得更好

1069
01:07:13,750 --> 01:07:14,980
不可能变得更差

1070
01:07:15,080 --> 01:07:16,910
α将会逐渐地收敛到某些值

1071
01:07:17,020 --> 01:07:19,710
在中间的迭代过程中

1072
01:07:19,790 --> 01:07:20,400
其中的某个α的值

1073
01:07:20,470 --> 01:07:21,650
可能距离它的最终值的距离忽近忽远

1074
01:07:21,720 --> 01:07:28,860
但是我们真正关心的是W(α)

1075
01:07:28,970 --> 01:07:29,690
是否每次会变得更好这一点是成立的

1076
01:07:29,800 --> 01:07:31,570
在我们结束对SMO的讨论之前

1077
01:07:31,680 --> 01:07:32,910
再说几句

1078
01:07:33,020 --> 01:07:37,990
一个是John Platt的原始算法中

1079
01:07:38,070 --> 01:07:39,680
介绍了一个用来

1080
01:07:39,760 --> 01:07:43,470
选取参数对的启发式规则

1081
01:07:43,570 --> 01:07:45,350
这些规则在概念上并不复杂

1082
01:07:45,480 --> 01:07:49,360
但是解释起来

1083
01:07:49,490 --> 01:07:51,870
会比较复杂

1084
01:07:51,970 --> 01:07:53,080
所以我这里就不讲了

1085
01:07:53,160 --> 01:07:55,960
感兴趣的同学可以回去自己查阅

1086
01:07:56,070 --> 01:08:02,450
John Platt的关于SMO算法的论文

1087
01:08:02,550 --> 01:08:04,260
其中提到的启发式规则很容易读懂

1088
01:08:04,380 --> 01:08:11,370
稍后  我们会在课程主页上

1089
01:08:11,490 --> 01:08:15,740
放出一些简化版本的启发式规则

1090
01:08:15,900 --> 01:08:17,190
你们可以在Problem Set 中使用

1091
01:08:17,300 --> 01:08:19,500
你们可以更为深入地阅读一些材料

1092
01:08:19,610 --> 01:08:25,980
另一件我没有讲的事是

1093
01:08:26,050 --> 01:08:27,880
关于如何求得参数b

1094
01:08:27,960 --> 01:08:30,630
我们的算法一直在求解α

1095
01:08:30,720 --> 01:08:33,650
α可以让我们得到W

1096
01:08:33,730 --> 01:08:35,140
我并没有讲到

1097
01:08:35,220 --> 01:08:36,230
应该如何计算参数b

1098
01:08:36,350 --> 01:08:39,050
实际上计算过程并不是十分困难

1099
01:08:39,160 --> 01:08:42,930
我会让你们课后自行阅读

1100
01:08:43,050 --> 01:08:44,020
过程我会和下一个

1101
01:08:44,110 --> 01:08:46,580
Problem Set一起放出

1102
01:08:46,700 --> 01:08:54,280
作为今天的课的结尾

1103
01:08:54,370 --> 01:08:56,330
我想要给你们

1104
01:08:56,440 --> 01:08:58,690
简要地介绍几个SVM的应用

1105
01:08:58,790 --> 01:09:27,790
让我们考虑

1106
01:09:27,860 --> 01:09:29,350
Handler整数识别的问题

1107
01:09:29,470 --> 01:09:32,810
在Handler整数识别问题中

1108
01:09:32,890 --> 01:09:34,750
你会给定一个像素矩阵

1109
01:09:34,860 --> 01:09:39,730
可能来自于一张扫描图片

1110
01:09:39,850 --> 01:09:42,180
例如:英国某个地方的邮政编码

1111
01:09:42,300 --> 01:09:43,430
这是一个像素矩阵

1112
01:09:43,520 --> 01:09:45,720
有些像素是被填充的

1113
01:09:45,820 --> 01:09:47,670
有些像素没有被填充

1114
01:09:47,780 --> 01:09:50,980
这些像素组合在一起

1115
01:09:51,060 --> 01:09:53,510
可能表示的是数字1

1116
01:09:53,620 --> 01:09:56,870
问题是

1117
01:09:56,960 --> 01:09:58,980
给定一个像这样的特征向量

1118
01:09:59,070 --> 01:10:02,070
如果你有

1119
01:10:02,150 --> 01:10:05,730
10*10的像素矩阵

1120
01:10:05,830 --> 01:10:08,490
那么你会有一个100维的特征向量

1121
01:10:08,600 --> 01:10:16,660
如果你有10*10的像素矩阵

1122
01:10:16,770 --> 01:10:18,370
那么你会有100个特征

1123
01:10:18,480 --> 01:10:20,990
这些特征可能是0-1取值的

1124
01:10:21,090 --> 01:10:24,870
也可能取灰度值

1125
01:10:24,980 --> 01:10:27,470
表示每个像素有多暗

1126
01:10:27,540 --> 01:10:30,590
许多年来

1127
01:10:30,690 --> 01:10:34,870
神经网络都被视为用来解决

1128
01:10:34,960 --> 01:10:37,170
Handler数字识别问题的

1129
01:10:37,280 --> 01:10:38,860
最好的算法

1130
01:10:39,000 --> 01:10:41,150
实际上

1131
01:10:41,260 --> 01:10:44,930
你可以使用SVM算法  用这样的核

1132
01:10:45,250 --> 01:10:49,290
事实证明多项式核

1133
01:10:49,410 --> 01:11:04,360
和高斯核的效果都很好

1134
01:11:04,470 --> 01:11:08,290
将这些核代入SVM算法中

1135
01:11:08,370 --> 01:11:12,090
其性能和

1136
01:11:12,200 --> 01:11:14,730
最好的神经网络相当

1137
01:11:14,850 --> 01:11:16,420
这很令人惊讶

1138
01:11:16,520 --> 01:11:22,580
因为SVM并没有

1139
01:11:22,660 --> 01:11:25,100
任何关于像素的知识

1140
01:11:25,210 --> 01:11:30,310
它并不知道哪个像素挨着哪个像素

1141
01:11:30,410 --> 01:11:34,430
因为所有的像素

1142
01:11:34,550 --> 01:11:35,770
都被表示为一个向量

1143
01:11:35,850 --> 01:11:37,930
这意味着即使任意排列像素的顺序

1144
01:11:38,080 --> 01:11:39,480
SVM的性能

1145
01:11:39,560 --> 01:11:40,520
也不会受到任何影响

1146
01:11:40,600 --> 01:11:41,470
我们所谓的性能

1147
01:11:41,560 --> 01:11:49,850
和最好的神经网络相当

1148
01:11:49,970 --> 01:11:53,080
是相对于那些精心设计了

1149
01:11:53,180 --> 01:11:54,280
很多年的神经网络

1150
01:11:54,380 --> 01:12:01,950
我还要给你们讲一个很酷的例子

1151
01:12:02,050 --> 01:12:04,480
SVM还可以被用来

1152
01:12:04,570 --> 01:12:08,110
对一些比较神秘的事物进行分类

1153
01:12:08,210 --> 01:12:12,980
例如  比如说你希望将蛋白质序列

1154
01:12:13,120 --> 01:12:15,050
分成几个不同的类别

1155
01:12:15,130 --> 01:12:16,060
每次我这样做的时候

1156
01:12:16,140 --> 01:12:18,700
我都感觉

1157
01:12:18,780 --> 01:12:20,310
我在奉承那些教室里的生物学家

1158
01:12:20,410 --> 01:12:21,320
所以我表示歉意

1159
01:12:21,400 --> 01:12:25,090
总共有20中氨基酸

1160
01:12:25,150 --> 01:12:28,990
我们体内的蛋白质

1161
01:12:29,090 --> 01:12:30,210
是由氨基酸序列组成的

1162
01:12:30,320 --> 01:12:33,750
即使有20种氨基和26个字母

1163
01:12:34,220 --> 01:12:36,900
我仍然打算用

1164
01:12:37,010 --> 01:12:39,540
a-z来表示这些氨基酸

1165
01:12:39,650 --> 01:13:06,940
在这里向那些生物学家们表示歉意

1166
01:13:07,020 --> 01:13:09,080
一个氨基酸序列用一组字母表示

1167
01:13:09,150 --> 01:13:24,330
假设我需要根据蛋白质的类别

1168
01:13:24,450 --> 01:13:26,240
将其分成几类

1169
01:13:26,320 --> 01:13:28,310
问题是

1170
01:13:28,390 --> 01:13:30,850
我需要怎样构造我的特征向量?

1171
01:13:30,930 --> 01:13:34,780
这个问题因为几个原因而富有挑战性

1172
01:13:34,890 --> 01:13:35,870
其中的一个原因是

1173
01:13:35,980 --> 01:13:38,060
蛋白质序列的长度可能不同

1174
01:13:38,150 --> 01:13:40,090
有许多很长的蛋白质序列

1175
01:13:40,240 --> 01:13:41,210
也有许多很短的蛋白质序列

1176
01:13:41,290 --> 01:13:43,580
所以你不可以

1177
01:13:43,660 --> 01:13:46,990
取第100个位置的氨基酸

1178
01:13:47,080 --> 01:13:47,850
因为可能在蛋白质序列中

1179
01:13:47,920 --> 01:13:49,390
没有第100个序列

1180
01:13:49,500 --> 01:13:51,750
有些很长  有些很短

1181
01:13:51,850 --> 01:13:54,830
这是我的特征的表达方式

1182
01:13:54,930 --> 01:13:57,730
我会写出

1183
01:13:57,840 --> 01:14:02,870
所有的四个字母的组合

1184
01:14:02,960 --> 01:14:04,340
我会写AAAA

1185
01:14:04,430 --> 01:14:11,940
AAAB  AAAC一直到AAAZ

1186
01:14:12,020 --> 01:14:14,550
之后是AABA  依此类推

1187
01:14:14,640 --> 01:14:15,450
你们明白的

1188
01:14:15,530 --> 01:14:27,330
写出四个字母的所有可能组合

1189
01:14:27,430 --> 01:14:29,970
我的特征的表示方式是

1190
01:14:30,060 --> 01:14:32,810
我会检测氨基酸序列

1191
01:14:32,880 --> 01:14:34,390
并且统计出这些子序列

1192
01:14:34,490 --> 01:14:36,950
都出现过多少次

1193
01:14:37,040 --> 01:14:41,160
例如:BAJT出现过两次

1194
01:14:41,220 --> 01:14:42,820
所以这里我会写2

1195
01:14:42,910 --> 01:14:44,220
这些序列都没有出现过

1196
01:14:44,300 --> 01:14:45,590
所以我会写0

1197
01:14:45,690 --> 01:14:47,800
我想这里是1  这里是0

1198
01:14:47,910 --> 01:14:51,620
这个非常长的向量

1199
01:14:51,740 --> 01:14:55,910
将会是我对蛋白质序列的特征表示

1200
01:14:56,000 --> 01:14:59,840
这种表示方式与蛋白质的长度无关

1201
01:14:59,930 --> 01:15:01,370
向量有多长?

1202
01:15:01,450 --> 01:15:11,060
实际上向量的维数是〖20〗^4

1203
01:15:11,180 --> 01:15:12,340
也就是说

1204
01:15:12,420 --> 01:15:16,910
你有一个160000维的特征向量

1205
01:15:17,020 --> 01:15:19,480
即使用现代计算机的标准看来

1206
01:15:19,590 --> 01:15:21,350
也是相当大了

1207
01:15:21,450 --> 01:15:24,740
很明显  我们不想显示地

1208
01:15:24,820 --> 01:15:26,250
将这些高维向量表达出来

1209
01:15:26,330 --> 01:15:27,930
想象一下  你有1000个样本

1210
01:15:28,010 --> 01:15:29,890
你用double型变量进行存储

1211
01:15:29,970 --> 01:15:31,130
即使用现代计算机的标准看来

1212
01:15:31,230 --> 01:15:31,950
也是非常大了

1213
01:15:32,040 --> 01:15:34,990
实际上有一个

1214
01:15:35,070 --> 01:15:43,970
非常高效的动态规划算法

1215
01:15:44,060 --> 01:15:44,880
可以高效地计算这样的

1216
01:15:44,950 --> 01:15:45,870
两个特征向量的内积

1217
01:15:45,970 --> 01:15:46,770
所以我们可以使用

1218
01:15:46,840 --> 01:15:47,480
这样的特征表示方法

1219
01:15:47,560 --> 01:15:48,770
即使特征向量的维数出奇的大

1220
01:15:48,880 --> 01:15:53,060
我不会讲

1221
01:15:53,160 --> 01:15:54,310
这个动态规划算法

1222
01:15:54,390 --> 01:15:57,330
如果你们中的一些人

1223
01:15:57,410 --> 01:15:58,460
曾经见过这个算法

1224
01:15:58,550 --> 01:16:01,250
你们可以回忆一下

1225
01:16:01,320 --> 01:16:03,130
如果感兴趣的话

1226
01:16:03,220 --> 01:16:04,310
你们可以下课后自己查

1227
01:16:04,380 --> 01:16:07,270
这是另外一个很酷的关于核的例子

1228
01:16:07,360 --> 01:16:09,270
更为一般地

1229
01:16:09,360 --> 01:16:12,070
当你面对一些新的机器学习问题时

1230
01:16:12,170 --> 01:16:14,440
有时你会使用一些标准的核

1231
01:16:14,520 --> 01:16:15,170
例如:高斯核

1232
01:16:15,240 --> 01:16:19,070
有些时候会有一些论文去关注

1233
01:16:19,200 --> 01:16:21,470
怎样为一个新问题发明一个新的核

1234
01:16:21,580 --> 01:16:28,380
最后我想说两句话

1235
01:16:28,460 --> 01:16:30,770
我们现在到哪儿了?

1236
01:16:30,860 --> 01:16:32,280
我们刚刚讲完了SVM

1237
01:16:32,380 --> 01:16:34,170
很多人认为是

1238
01:16:34,250 --> 01:16:35,720
最为高效切

1239
01:16:35,790 --> 01:16:36,890
无需定制的学习算法

1240
01:16:36,990 --> 01:16:39,110
到今天

1241
01:16:39,190 --> 01:16:41,190
你们已经学过了很多学习算法

1242
01:16:41,680 --> 01:16:43,340
我想要祝贺你们

1243
01:16:43,450 --> 01:16:45,490
你们现在的基本知识已经合格了

1244
01:16:45,600 --> 01:16:47,070
已经可以用学习算法解决很多问题了

1245
01:16:47,150 --> 01:16:50,640
我们的课程刚刚进行了1/4

1246
01:16:50,730 --> 01:16:51,440
所以还有更多的内容

1247
01:16:51,530 --> 01:16:53,190
我们接下来要讲的是

1248
01:16:53,280 --> 01:16:55,570
如何真正地理解学习算法

1249
01:16:55,670 --> 01:16:57,080
什么时候它们工作的好

1250
01:16:57,160 --> 01:16:59,500
什么时候它们工作的差

1251
01:16:59,600 --> 01:17:01,050
回顾一下你们现有的工具

1252
01:17:01,140 --> 01:17:02,280
想想怎样才能把它们用好

1253
01:17:02,360 --> 01:17:04,200
我们会在下一讲中讲这些内容

1254
01:17:04,300 --> 01:17:05,640
谢谢

