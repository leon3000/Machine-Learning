1
00:00:24,130 --> 00:00:26,420
Okay. So good morning and welcome back.

2
00:00:27,420 --> 00:00:28,630
What I want to do today is

3
00:00:28,630 --> 00:00:31,280
actually begin a new chapter in 229

4
00:00:32,070 --> 00:00:35,880
in which I'm gonna start to talk about

5
00:00:35,880 --> 00:00:41,270
So today is I'm gonna just very briefly

6
00:00:41,270 --> 00:00:47,100
talk about clustering's, algorithm. [Inaudible]

7
00:00:47,100 --> 00:00:50,340
and a special case of the EM, Expectation

8
00:00:50,340 --> 00:00:52,030
Maximization, algorithm with a mixture of

9
00:00:52,030 --> 00:00:55,390
[inaudible] model to describe something called

10
00:00:55,390 --> 00:00:56,270
Jensen and Equality

11
00:00:56,270 --> 00:00:59,860
and then we'll use that derive a general form

12
00:00:59,860 --> 00:01:01,240
of something called the EM

13
00:01:01,240 --> 00:01:02,680
or the Expectation Maximization algorithm,

14
00:01:03,160 --> 00:01:04,560
which is a very useful algorithm.

15
00:01:04,790 --> 00:01:06,160
We sort of use it

16
00:01:06,160 --> 00:01:07,530
all over the place and different

17
00:01:07,530 --> 00:01:10,200
unsupervised machine or any application.

18
00:01:11,600 --> 00:01:19,930
So the cartoons that I used to draw for

19
00:01:19,930 --> 00:01:23,710
supervised learning was you'd be given the

20
00:01:23,710 --> 00:01:27,590
data set like this, right, and you'd use

21
00:01:27,590 --> 00:01:30,890
[inaudible] between the positive

22
00:01:30,890 --> 00:01:31,590
and negativecrosses

23
00:01:32,230 --> 00:01:33,730
and we'd call it the supervised learning

24
00:01:33,730 --> 00:01:35,770
because you're sort of told what the

25
00:01:35,770 --> 00:01:37,800
right cross label is for every training example,

26
00:01:39,150 --> 00:01:40,200
and that was the supervision.

27
00:01:41,870 --> 00:01:43,240
In unsupervised learning,

28
00:01:43,240 --> 00:01:44,390
we'll study a different problem.

29
00:01:47,390 --> 00:01:48,270
You're given a data set

30
00:01:54,880 --> 00:01:57,120
that maybe just comprises a set of points.

31
00:01:57,120 --> 00:01:59,730
You're just given a data set with no labels

32
00:01:59,730 --> 00:02:02,670
and no indication of what the "right answers"

33
00:02:02,670 --> 00:02:06,470
or where the supervision is and it's the job of

34
00:02:06,470 --> 00:02:09,040
the algorithm to discover structure in the data.

35
00:02:10,790 --> 00:02:13,140
So in this lecture and the next couple of weeks

36
00:02:13,140 --> 00:02:14,400
we'll talk about a variety of

37
00:02:14,400 --> 00:02:15,920
unsupervised learning algorithms

38
00:02:15,920 --> 00:02:18,560
that can look at data sets like these and discover

39
00:02:18,560 --> 00:02:20,710
there's different types of structure in it.

40
00:02:22,060 --> 00:02:23,840
In this particular cartoonthat I've drawn

41
00:02:23,840 --> 00:02:25,840
one has the structure that you and I can

42
00:02:25,840 --> 00:02:29,320
probably see as is that this data lives in two

43
00:02:29,320 --> 00:02:33,510
different crosses and so the first unsupervised

44
00:02:33,510 --> 00:02:34,980
learning algorithm that I'm just gonna talk

45
00:02:34,980 --> 00:02:36,240
about will be a clustering algorithm.

46
00:02:36,240 --> 00:02:38,820
It'll be an algorithm that looks for a data set like

47
00:02:38,820 --> 00:02:41,950
this and automatically breaks the data set

48
00:02:41,950 --> 00:02:44,660
into different smaller clusters.

49
00:02:47,330 --> 00:02:50,160
So let's see.

50
00:02:51,320 --> 00:02:52,450
When my laptop comes back up,

51
00:02:52,450 --> 00:02:53,860
I'll show you an example.

52
00:02:53,860 --> 00:02:56,110
So clustering algorithms like these

53
00:02:56,110 --> 00:02:59,450
have a variety of applications.

54
00:02:59,450 --> 00:03:03,340
Just to rattle off a few of the better-known ones

55
00:03:03,340 --> 00:03:04,980
I guess in biology application

56
00:03:04,980 --> 00:03:06,690
you often cross the different things here.

57
00:03:06,690 --> 00:03:08,420
You have [inaudible] genes

58
00:03:08,420 --> 00:03:09,750
and they cluster the different genes together

59
00:03:09,750 --> 00:03:10,870
in order to examine them

60
00:03:10,870 --> 00:03:12,800
and understand the biological function better.

61
00:03:13,540 --> 00:03:16,740
Another common application of clustering

62
00:03:16,740 --> 00:03:17,610
is market research.

63
00:03:17,610 --> 00:03:20,790
So imagine you have a customer database of

64
00:03:20,790 --> 00:03:22,230
how your different customers behave.

65
00:03:23,210 --> 00:03:25,850
It's a very common practice to apply clustering

66
00:03:25,850 --> 00:03:27,750
algorithms to break your database of

67
00:03:27,750 --> 00:03:29,680
customers into different market segments so

68
00:03:29,680 --> 00:03:32,830
that you can target your products towards

69
00:03:32,830 --> 00:03:33,850
different market segments

70
00:03:33,850 --> 00:03:35,030
and target your sales pitches

71
00:03:35,030 --> 00:03:36,840
specifically to different market segments.

72
00:03:38,180 --> 00:03:39,200
Something we'll do later today

73
00:03:39,200 --> 00:03:41,140
I don't want to do this now,

74
00:03:41,140 --> 00:03:42,970
but you actually go to a website,

75
00:03:42,970 --> 00:03:44,780
like, use.google.com

76
00:03:46,920 --> 00:03:49,550
and that's an example of a website that uses

77
00:03:49,550 --> 00:03:53,940
a clustering algorithm to everyday group

78
00:03:53,940 --> 00:03:56,540
related news articles together to display to you

79
00:03:56,540 --> 00:03:59,440
so that you can see one of the thousand news

80
00:03:59,440 --> 00:04:01,840
articles today on whatever the top story of

81
00:04:01,840 --> 00:04:03,760
today is and all the 500 news articles on all the

82
00:04:03,760 --> 00:04:06,320
different websites on different story of the day.

83
00:04:07,480 --> 00:04:09,360
And a very solid [inaudible] actually

84
00:04:09,360 --> 00:04:10,560
talks about image segmentation,

85
00:04:10,560 --> 00:04:14,930
which the application of when you might take a

86
00:04:14,930 --> 00:04:17,700
picture and group together different subsets of

87
00:04:17,700 --> 00:04:21,640
the picture into coherent pieces of pixels to try

88
00:04:21,640 --> 00:04:23,290
to understand what's contained in the picture.

89
00:04:23,290 --> 00:04:26,670
So that's yet another application of clustering.

90
00:04:27,710 --> 00:04:30,360
The next idea is given a data set like this,

91
00:04:30,360 --> 00:04:31,360
given a set of points,

92
00:04:32,120 --> 00:04:35,480
can you automatically group the data sets

93
00:04:35,480 --> 00:04:39,030
into coherent clusters.

94
00:04:39,950 --> 00:04:42,950
Let's see.

95
00:04:43,940 --> 00:04:45,510
I'm still waiting for the laptop to come back

96
00:04:45,510 --> 00:04:46,630
so I can show you an example.

97
00:04:47,860 --> 00:04:51,780
You know what, why don't I just start to

98
00:04:51,780 --> 00:04:53,580
write out the specific clustering algorithm

99
00:04:53,580 --> 00:04:55,010
and then I'll show you the animation later.

100
00:05:00,940 --> 00:05:03,140
So this is the called the k-means clustering

101
00:05:03,140 --> 00:05:05,450
algorithm for finding clustering's near the inset.

102
00:05:08,840 --> 00:05:11,150
The input to the algorithm will be

103
00:05:11,150 --> 00:05:12,630
an unlabeled data set

104
00:05:13,970 --> 00:05:20,590
which I write as X1, X2, [inaudible]

105
00:05:20,590 --> 00:05:22,320
and because we're now

106
00:05:22,320 --> 00:05:23,510
talking about unsupervised learning,

107
00:05:23,510 --> 00:05:25,500
you see a lot of this as [inaudible] with

108
00:05:25,500 --> 00:05:26,270
just the Xs

109
00:05:26,270 --> 00:05:27,700
and no cross labels Y.

110
00:05:28,990 --> 00:05:30,550
So what a k-means algorithm

111
00:05:30,550 --> 00:05:31,770
does is the following.

112
00:05:32,400 --> 00:05:33,740
This will all make a bit more sense

113
00:05:33,740 --> 00:05:38,020
when I show you the animation on my laptop.

114
00:05:39,990 --> 00:05:43,180
To initialize a set of points,

115
00:05:43,180 --> 00:05:44,450
called the cluster centroids,

116
00:05:52,310 --> 00:05:55,470
[inaudible] randomly

117
00:05:55,470 --> 00:05:59,230
and so if you're [inaudible] of training data are

118
00:05:59,230 --> 00:06:01,470
[inaudible] then your cluster centroids, these

119
00:06:01,470 --> 00:06:04,870
muse, will also be vectors and [inaudible]

120
00:06:06,280 --> 00:06:07,480
and then you repeat

121
00:06:11,100 --> 00:06:17,150
until convergence the following two steps.

122
00:06:37,960 --> 00:06:40,920
So the cluster centroids will be your guesses for

123
00:06:40,920 --> 00:06:42,870
where the centers of each of the clusters are

124
00:06:42,870 --> 00:06:46,820
and so in one of those steps you look at each

125
00:06:46,820 --> 00:06:50,480
point, XI and you look at which cluster centroid J

126
00:06:50,480 --> 00:06:53,390
is closest to it and then this step is called

127
00:06:53,390 --> 00:06:56,820
assigning your point XI to cluster J.

128
00:06:57,760 --> 00:06:59,160
So looking at each point

129
00:06:59,160 --> 00:07:01,000
and picking the cluster centroid

130
00:07:01,000 --> 00:07:02,160
that's closest to it

131
00:07:03,330 --> 00:07:10,320
and the other step is you update the cluster

132
00:07:28,190 --> 00:07:30,600
centroids to be the median of all the points

133
00:07:30,600 --> 00:07:31,550
that have been assigned to it.

134
00:07:46,580 --> 00:07:47,920
Okay. Let's see. Could you please

135
00:07:47,920 --> 00:07:49,610
bring down the display for the laptop?

136
00:08:56,680 --> 00:08:57,510
Okay. Okay. There we go.

137
00:09:01,720 --> 00:09:03,440
Okay. So here's an example of

138
00:09:03,440 --> 00:09:04,510
the k-means algorithm

139
00:09:04,510 --> 00:09:07,540
and hope the animation will make more sense.

140
00:09:07,540 --> 00:09:09,420
This is an inch chopped off.

141
00:09:09,420 --> 00:09:10,980
This is basically an example

142
00:09:10,980 --> 00:09:13,750
I got from Michael Jordan in Berkley.

143
00:09:14,470 --> 00:09:17,980
So these points in green are my data points

144
00:09:17,980 --> 00:09:20,640
and so I'm going to randomly initialize

145
00:09:20,640 --> 00:09:21,750
a pair of cluster centroids.

146
00:09:21,750 --> 00:09:25,650
So the [inaudible] blue crosses to

147
00:09:25,650 --> 00:09:27,830
note the positions of New1 and New2

148
00:09:27,830 --> 00:09:29,230
say if I'm going to guess

149
00:09:29,230 --> 00:09:30,650
that there's two clusters in this data.

150
00:09:33,830 --> 00:09:37,390
Sets of k-means algorithms as follow.

151
00:09:37,390 --> 00:09:40,170
I'm going to repeatedly go to all of the points in

152
00:09:40,170 --> 00:09:44,530
my data set and I'm going to associate each of

153
00:09:44,530 --> 00:09:47,090
the green dots with the closer of the two cluster

154
00:09:47,090 --> 00:09:50,940
centroids, so visually, I'm gonna denote that by

155
00:09:50,940 --> 00:09:54,820
painting each of the dots either blue or red

156
00:09:54,820 --> 00:09:57,190
depending on which is the closer cluster centroid.

157
00:09:57,880 --> 00:09:59,520
Okay. So all the points closer to the

158
00:09:59,520 --> 00:10:00,960
blue cross points are

159
00:10:00,960 --> 00:10:02,100
painted blue and so on.

160
00:10:03,030 --> 00:10:06,330
The other step is updating the cluster centroids

161
00:10:06,330 --> 00:10:09,800
and so I'm going to repeatedly look at all

162
00:10:09,800 --> 00:10:10,930
the points that I've painted blue

163
00:10:11,500 --> 00:10:14,810
and compute the average of all of the blue dots,

164
00:10:15,510 --> 00:10:18,120
and I'll look at all the red dots

165
00:10:18,120 --> 00:10:19,990
and compute the average of all the red dots

166
00:10:19,990 --> 00:10:22,810
and then I move the cluster centroids as follows

167
00:10:22,810 --> 00:10:25,710
to the average of the respective locations.

168
00:10:25,710 --> 00:10:29,000
So this is now [inaudible] of k-means on here,

169
00:10:29,950 --> 00:10:31,430
and now I'll repeat the same process.

170
00:10:31,430 --> 00:10:33,930
I look at all the points and assign all the points

171
00:10:33,930 --> 00:10:36,580
closer to the blue cross to the color blue

172
00:10:36,580 --> 00:10:37,960
and similarly red.

173
00:10:37,960 --> 00:10:41,070
And so now I have that assignments of points to

174
00:10:41,070 --> 00:10:45,820
the cluster centroids and finally, I'll again compute

175
00:10:45,820 --> 00:10:48,030
the average of all the blue points and compute

176
00:10:48,030 --> 00:10:50,280
the average of all the red points and update the

177
00:10:50,280 --> 00:10:52,300
cluster centroids again as follows

178
00:10:53,280 --> 00:10:56,450
and now k-means is actually [inaudible].

179
00:10:56,450 --> 00:10:58,920
If you keep running these two sets of k-means

180
00:10:58,920 --> 00:11:01,000
over and over, the cluster centroids and the

181
00:11:01,000 --> 00:11:02,940
assignment of the points closest to the cluster

182
00:11:02,940 --> 00:11:04,140
centroids will actually remain the same.

183
00:11:04,140 --> 00:11:05,010
Yeah.

184
00:11:05,010 --> 00:11:06,570
[Inaudible]

185
00:11:06,570 --> 00:11:07,430
Yeah, I'll assign that in a second.

186
00:11:07,430 --> 00:11:10,570
Yeah. Okay.

187
00:11:32,910 --> 00:11:35,540
So [inaudible]. Take a second to look at this again

188
00:11:35,540 --> 00:11:42,200
and make sure you understand how the algorithm

189
00:11:42,200 --> 00:11:43,520
I wrote out maps onto the animation

190
00:11:43,520 --> 00:11:44,290
that we just saw.

191
00:11:44,290 --> 00:11:45,070
Do you have a question?

192
00:11:45,070 --> 00:11:48,400
[Inaudible]

193
00:11:48,400 --> 00:11:50,560
I see. Okay. Let me answer on that in a second.

194
00:11:52,010 --> 00:11:53,880
Okay. So these are the two steps.

195
00:11:53,880 --> 00:11:56,900
This step 2.1 was assigning

196
00:11:56,900 --> 00:11:58,120
the points to the closest centroid

197
00:11:58,120 --> 00:12:01,660
and 2.2 was shifting the cluster centroid to be the

198
00:12:01,660 --> 00:12:03,750
mean of all the points assigned

199
00:12:03,750 --> 00:12:04,600
to that cluster centroid.

200
00:12:06,000 --> 00:12:09,900
Okay. Okay. [Inaudible] questions that we just had,

201
00:12:09,900 --> 00:12:11,500
one is, does the algorithm converge?

202
00:12:12,270 --> 00:12:13,480
The answer is yes,

203
00:12:13,480 --> 00:12:14,660
k-means is guaranteed to

204
00:12:14,660 --> 00:12:15,980
converge in a certain sense.

205
00:12:15,980 --> 00:12:31,060
In particular, if you define the distortion function

206
00:12:31,710 --> 00:12:44,660
to be J of C [inaudible] squared.

207
00:12:46,330 --> 00:12:48,710
You can define the distortion function

208
00:12:48,710 --> 00:12:50,830
to be a function of the cluster assignments,

209
00:12:50,830 --> 00:12:53,690
and the cluster centroids and [inaudible] square

210
00:12:53,690 --> 00:12:55,640
distances, which mean the points and the cluster

211
00:12:55,640 --> 00:12:56,850
centroids that they're assigned to,

212
00:12:57,690 --> 00:13:00,270
then you can show I won't really prove this here

213
00:13:00,790 --> 00:13:03,670
but you can show that k-means is called

214
00:13:03,670 --> 00:13:15,000
[inaudible] on the function J. In particular,

215
00:13:15,000 --> 00:13:17,650
who remembers, it's called in a sense as

216
00:13:17,650 --> 00:13:19,610
an authorization algorithm, I don't know,

217
00:13:19,610 --> 00:13:20,670
maybe about two weeks ago,

218
00:13:21,420 --> 00:13:23,960
so called in a sense is the algorithm

219
00:13:23,960 --> 00:13:30,190
that we'll repeatedly [inaudible] with respect to C.

220
00:13:30,190 --> 00:13:31,570
Okay. So that's called [inaudible].

221
00:13:32,090 --> 00:13:33,910
And so what you can prove is

222
00:13:33,910 --> 00:13:39,820
that k-means the two steps of k-means,

223
00:13:39,820 --> 00:13:43,430
are exactly optimizing this function with respect

224
00:13:43,430 --> 00:13:45,570
to C and will respect a new alternately.

225
00:13:46,360 --> 00:13:52,270
And therefore, this function, J of C, new,

226
00:13:52,270 --> 00:13:54,400
must be decreasing monotonically

227
00:13:54,400 --> 00:13:56,220
on every other variation

228
00:13:56,220 --> 00:13:59,120
and so the sense in which k-means converges is

229
00:13:59,120 --> 00:14:02,130
that this function, J of C, new, can only go down

230
00:14:02,130 --> 00:14:04,750
and therefore, this function

231
00:14:04,750 --> 00:14:06,710
will actually eventually converge in the sense

232
00:14:06,710 --> 00:14:08,090
that it will stop going down.

233
00:14:08,090 --> 00:14:12,060
Okay. It's actually possible that there may be

234
00:14:12,060 --> 00:14:15,990
several clustering's they give the same value of J

235
00:14:15,990 --> 00:14:19,150
of C, new and so k-means may actually switch back and

236
00:14:19,150 --> 00:14:21,760
forth between different clustering's that they [inaudible]

237
00:14:21,760 --> 00:14:24,700
in the extremely unlikely case, if there's multiple

238
00:14:24,700 --> 00:14:25,810
clustering's, they give exactly

239
00:14:25,810 --> 00:14:27,340
the same value for this objective function.

240
00:14:27,340 --> 00:14:29,410
K-means may also be [inaudible]

241
00:14:30,450 --> 00:14:31,070
it'll just never happen.

242
00:14:31,070 --> 00:14:34,570
That even if that happens,

243
00:14:34,570 --> 00:14:37,400
this function J of C, new will converge.

244
00:14:38,870 --> 00:14:40,240
Another question was

245
00:14:40,240 --> 00:14:41,590
how do you choose the number of clusters?

246
00:14:41,590 --> 00:14:45,840
So it turns out that in the vast majority of time

247
00:14:45,840 --> 00:14:47,070
when people apply k-means,

248
00:14:47,070 --> 00:14:49,660
you still just randomly pick a number of clusters

249
00:14:49,660 --> 00:14:51,450
or you randomly try a few

250
00:14:51,450 --> 00:14:52,630
different numbers of clusters

251
00:14:52,630 --> 00:14:57,160
and pick the one

252
00:14:57,160 --> 00:14:58,240
that seems to work best.

253
00:14:59,180 --> 00:15:00,730
The number of clusters in this algorithm

254
00:15:00,730 --> 00:15:01,750
instead of just one parameters,

255
00:15:01,750 --> 00:15:02,680
so usually I think

256
00:15:02,680 --> 00:15:04,500
it's not very hard to choose automatically.

257
00:15:05,360 --> 00:15:07,200
There are some automatic ways of

258
00:15:07,200 --> 00:15:08,750
choosing the number of clusters,

259
00:15:08,750 --> 00:15:10,970
but I'm not gonna talk about them.

260
00:15:10,970 --> 00:15:12,200
When I do this,

261
00:15:12,200 --> 00:15:13,200
I usually just pick of

262
00:15:13,200 --> 00:15:14,500
the number of clusters randomly.

263
00:15:14,500 --> 00:15:15,710
And the reason is

264
00:15:17,110 --> 00:15:20,630
I think for many clustering problems the "true"

265
00:15:20,630 --> 00:15:22,740
number of clusters is actually ambiguous

266
00:15:22,740 --> 00:15:23,640
so for example

267
00:15:23,640 --> 00:15:26,020
if you have a data set that looks like this,

268
00:15:30,330 --> 00:15:34,390
some of you may see four clusters, right,

269
00:15:34,390 --> 00:15:35,950
and some of you may see two clusters,

270
00:15:35,950 --> 00:15:38,610
and so the right answer for the

271
00:15:38,610 --> 00:15:40,280
actual number of clusters is sort of ambiguous.

272
00:15:41,620 --> 00:15:43,640
Yeah.

273
00:15:46,130 --> 00:15:52,070
[Inaudible]. [Inaudible] clusters [inaudible] far

274
00:15:52,070 --> 00:15:52,920
away from the data point [inaudible] points

275
00:15:52,920 --> 00:15:59,510
and the same cluster?

276
00:16:01,240 --> 00:16:01,760
I see.

277
00:16:01,760 --> 00:16:04,890
Right. So yes. K-means is susceptible

278
00:16:04,890 --> 00:16:08,780
to [inaudible] so this function,

279
00:16:08,780 --> 00:16:12,770
J of C, new is not a convex function

280
00:16:12,770 --> 00:16:16,530
and so k-means, sort of called in a sense

281
00:16:16,530 --> 00:16:18,830
on the non-convex function is not guaranteed to

282
00:16:18,830 --> 00:16:20,300
converge the [inaudible].

283
00:16:20,300 --> 00:16:21,910
So k-means is susceptible to

284
00:16:24,550 --> 00:16:31,650
local optimal and [inaudible].

285
00:16:31,650 --> 00:16:32,860
One thing you can do is

286
00:16:32,860 --> 00:16:35,020
try multiple random initializations

287
00:16:35,020 --> 00:16:37,670
and then run clustering a bunch of times

288
00:16:37,670 --> 00:16:39,560
and pick the solution

289
00:16:39,560 --> 00:16:40,390
that ended up with

290
00:16:40,390 --> 00:16:42,100
the lowest value for the distortion function.

291
00:16:42,100 --> 00:16:44,320
Yeah.

292
00:16:44,320 --> 00:16:52,170
[Inaudible]

293
00:16:52,170 --> 00:16:57,230
Yeah, let's see.

294
00:16:57,230 --> 00:16:59,230
Right. So what if one cluster centroid

295
00:16:59,230 --> 00:17:00,740
has no points assigned to it,

296
00:17:00,740 --> 00:17:02,050
again, one thing you could do

297
00:17:02,050 --> 00:17:03,930
is just eliminate it exactly the same.

298
00:17:03,930 --> 00:17:07,090
Another thing you can is you can just reinitialize

299
00:17:07,090 --> 00:17:09,760
randomly if you really [inaudible].

300
00:17:12,180 --> 00:17:13,020
More questions.

301
00:17:13,020 --> 00:17:18,000
[Inaudible]

302
00:17:19,290 --> 00:17:20,320
I see.

303
00:17:20,320 --> 00:17:22,540
Right. Is it usually two norms?

304
00:17:24,590 --> 00:17:28,060
Let's see. For the vast majority of applications

305
00:17:28,060 --> 00:17:30,330
I've seen for k-means, you do take two norms

306
00:17:30,330 --> 00:17:31,520
when you have data [inaudible].

307
00:17:32,340 --> 00:17:33,980
I'm sure there are others who

308
00:17:33,980 --> 00:17:35,890
have taken infinity norm and one norm as well.

309
00:17:35,890 --> 00:17:38,010
I personally haven't seen that very often,

310
00:17:38,010 --> 00:17:42,010
but there are other variations

311
00:17:42,010 --> 00:17:43,640
on this algorithm that use different norms,

312
00:17:43,640 --> 00:17:45,420
but the one I described is probably

313
00:17:45,420 --> 00:17:46,460
the most commonly used there is.

314
00:17:50,250 --> 00:17:59,300
Okay. So that was k-means clustering.

315
00:18:05,620 --> 00:18:08,440
What I want to do next and this will take longer to

316
00:18:08,440 --> 00:18:09,770
describe is actually talk about

317
00:18:09,770 --> 00:18:11,130
a closely related problem.

318
00:18:18,080 --> 00:18:20,290
In particular, what I wanted to do

319
00:18:20,290 --> 00:18:21,680
was talk about density estimation.

320
00:18:31,290 --> 00:18:33,750
As another k-means example, this is a problem

321
00:18:33,750 --> 00:18:35,150
that I know some guys that worked on.

322
00:18:36,190 --> 00:18:37,690
Let's say you have aircraft engine

323
00:18:37,690 --> 00:18:38,880
building off an assembly.

324
00:18:38,880 --> 00:18:41,400
Let's say you work for an aircraft company,

325
00:18:41,400 --> 00:18:42,850
you're building aircraft engines off

326
00:18:42,850 --> 00:18:47,060
the assembly line and as the aircraft engines

327
00:18:47,060 --> 00:18:49,150
roll off the assembly line, you test these aircraft

328
00:18:49,150 --> 00:18:50,990
engines and measure various different

329
00:18:50,990 --> 00:18:54,370
properties of it and to use [inaudible] example

330
00:18:54,370 --> 00:18:56,080
I'm gonna write these properties

331
00:18:56,080 --> 00:18:58,070
as heat and vibrations.

332
00:18:58,730 --> 00:19:00,360
Right. In reality, you'd measure different

333
00:19:00,360 --> 00:19:01,800
vibrations, different frequencies and so on.

334
00:19:01,800 --> 00:19:03,220
We'll just write the amount of

335
00:19:03,220 --> 00:19:04,910
heat produced and vibrations produced.

336
00:19:07,060 --> 00:19:27,610
Let's say that maybe it looks like

337
00:19:27,610 --> 00:19:30,810
that and what you would like to do is

338
00:19:30,810 --> 00:19:33,110
estimate the density of

339
00:19:33,110 --> 00:19:35,640
these [inaudible] of the joint distribution,

340
00:19:35,640 --> 00:19:36,760
the amount of heat produced

341
00:19:36,760 --> 00:19:37,830
and the amount of vibrations

342
00:19:38,780 --> 00:19:42,640
because you would like to detect [inaudible]

343
00:19:42,640 --> 00:19:44,930
so that as a new aircraft engine rolls off

344
00:19:44,930 --> 00:19:46,920
the assembly line, you can then measure

345
00:19:46,920 --> 00:19:49,110
the same heat and vibration properties.

346
00:19:50,540 --> 00:19:51,590
If you get a point there,

347
00:19:51,590 --> 00:19:53,270
you can then ask,

348
00:19:53,270 --> 00:19:55,180
"How likely is it that there was

349
00:19:55,180 --> 00:19:57,270
an undetected flaw in this aircraft engine

350
00:19:57,270 --> 00:20:00,330
that it needs to go undergo further inspections?"

351
00:20:00,860 --> 00:20:02,650
And so if we look at

352
00:20:02,650 --> 00:20:06,050
the typical distribution of features we get,

353
00:20:07,690 --> 00:20:09,500
and we build a model for P of X

354
00:20:11,160 --> 00:20:14,120
and then if P of X is very small

355
00:20:14,120 --> 00:20:15,700
for some new aircraft engine

356
00:20:15,700 --> 00:20:17,900
then that would raise a red flag

357
00:20:17,900 --> 00:20:20,010
and we'll say there's an anomaly aircraft engine

358
00:20:20,010 --> 00:20:22,870
and we should subject it to further inspections

359
00:20:22,870 --> 00:20:24,270
before we let someone fly with the engine.

360
00:20:25,680 --> 00:20:31,350
So this problem I just described is

361
00:20:31,350 --> 00:20:33,280
an instance of what is called anomaly detection

362
00:20:33,280 --> 00:20:35,250
and so a common way of

363
00:20:35,250 --> 00:20:36,960
doing anomaly detection is

364
00:20:36,960 --> 00:20:38,270
to take your training set

365
00:20:43,610 --> 00:20:48,060
and from this data set, build a model, P of X of the

366
00:20:48,060 --> 00:20:51,980
density of the typical data you're saying

367
00:20:51,980 --> 00:20:54,210
and if you ever then see an example

368
00:20:54,210 --> 00:20:57,090
with very low probability under P of X,

369
00:20:57,090 --> 00:20:59,980
then you may flag that as an anomaly example.

370
00:20:59,980 --> 00:21:03,960
Okay? So anomaly detection is also

371
00:21:03,960 --> 00:21:06,040
used in security applications.

372
00:21:16,590 --> 00:21:20,020
If many, very unusual transactions

373
00:21:20,020 --> 00:21:21,110
to start to appear on my credit card,

374
00:21:21,110 --> 00:21:22,410
that's a sign to me that someone

375
00:21:22,410 --> 00:21:23,060
has stolen my credit card.

376
00:21:24,510 --> 00:21:27,690
And what I want to do now is talk about specific

377
00:21:27,690 --> 00:21:29,530
algorithm for density estimation,

378
00:21:29,530 --> 00:21:31,620
and inparticular,

379
00:21:31,620 --> 00:21:33,560
one that works with data sets likethese,

380
00:21:33,560 --> 00:21:34,510
that, you know,

381
00:21:34,510 --> 00:21:40,790
this distribution like this doesn't really fall into

382
00:21:40,790 --> 00:21:42,890
any of the standard text book distributions.

383
00:21:42,890 --> 00:21:44,480
This is not really, like, a Gaussian

384
00:21:44,480 --> 00:21:46,300
or a [inaudible] explanation or anything.

385
00:21:46,300 --> 00:21:49,310
So can we come up with a model to

386
00:21:49,310 --> 00:21:51,070
estimate densities that may look like

387
00:21:51,070 --> 00:21:52,300
these somewhat unusual shapes?

388
00:21:53,410 --> 00:22:02,840
Okay. So to describe the algorithm a bit a more

389
00:22:02,840 --> 00:22:04,560
I'm also going to use a one dimensional example

390
00:22:04,560 --> 00:22:05,880
rather than a two D example,

391
00:22:06,820 --> 00:22:11,340
and in the example that I'm going to describe I'm

392
00:22:11,340 --> 00:22:21,750
going to say that let's imagine maybe a data set

393
00:22:21,750 --> 00:22:22,630
that looks like this

394
00:22:23,760 --> 00:22:26,740
where the horizontal access here is the X axis

395
00:22:26,740 --> 00:22:28,680
and these dots represent

396
00:22:28,680 --> 00:22:31,390
the positions of the data set that I have.

397
00:22:31,390 --> 00:22:32,240
Okay.

398
00:22:32,240 --> 00:22:33,720
So this data set looks like

399
00:22:33,720 --> 00:22:35,070
it's maybe coming from a density

400
00:22:38,280 --> 00:22:40,930
that looks like that as if this was

401
00:22:40,930 --> 00:22:43,220
the sum of two Gaussian distributions

402
00:22:43,220 --> 00:22:48,310
and so the specific model I'm gonna describe will

403
00:22:48,310 --> 00:22:49,620
be what's called a mixture of Gaussian's model.

404
00:22:50,660 --> 00:22:55,150
And just be clear that the picture I have is that

405
00:22:55,150 --> 00:22:59,840
when visioning that maybe there were two

406
00:22:59,840 --> 00:23:11,520
separate Gaussian's that generated this data set,

407
00:23:11,520 --> 00:23:13,530
and if only I knew what the two Gaussian's were,

408
00:23:13,530 --> 00:23:16,010
then I could put a Gaussian to my crosses,

409
00:23:16,010 --> 00:23:17,390
put a Gaussian to the Os

410
00:23:17,390 --> 00:23:21,420
and then sum those up to get

411
00:23:21,420 --> 00:23:22,610
the overall density for the two,

412
00:23:23,590 --> 00:23:26,150
but the problem is

413
00:23:26,150 --> 00:23:29,700
I don't actually have access to these labels.

414
00:23:29,700 --> 00:23:31,090
I don't actually know which of the

415
00:23:31,090 --> 00:23:33,010
two Gaussian's each of my data points came from

416
00:23:33,010 --> 00:23:36,910
and so what I'd like to do is come up with an

417
00:23:36,910 --> 00:23:40,380
algorithm to fit this mixture of Gaussian's model

418
00:23:40,380 --> 00:23:43,480
even when I don't know which of the

419
00:23:43,480 --> 00:23:45,990
two Gaussian's each of my data points came from.

420
00:23:54,450 --> 00:23:56,060
Okay. So here's the idea.

421
00:24:02,060 --> 00:24:03,260
In this model,

422
00:24:03,260 --> 00:24:04,370
I'm going to imagine

423
00:24:09,930 --> 00:24:12,100
there's a latent random variable,

424
00:24:19,970 --> 00:24:22,730
latent is just synonymous with

425
00:24:22,730 --> 00:24:24,800
hidden or unobserved,

426
00:24:35,040 --> 00:24:36,510
okay. So we're gonna imagine

427
00:24:36,510 --> 00:24:39,130
there's a latent random variable Z

428
00:24:39,130 --> 00:24:45,530
and XI, ZI have a joint distribution

429
00:24:53,710 --> 00:24:55,070
that is given as follows.

430
00:24:57,180 --> 00:25:03,240
We have that P of X, ZI by the chamber of

431
00:25:03,240 --> 00:25:05,910
probability, this is always like that.

432
00:25:05,910 --> 00:25:07,460
This is always true.

433
00:25:10,860 --> 00:25:14,430
And moreover, our [inaudible] is given by

434
00:25:14,430 --> 00:25:16,260
the following ZI is distributed

435
00:25:16,260 --> 00:25:21,820
multinomial with parameters I.

436
00:25:32,360 --> 00:25:34,140
And in the special case where I have just to

437
00:25:34,140 --> 00:25:36,160
make sure that two Gaussian's and ZI will be

438
00:25:46,690 --> 00:25:48,510
and so these parameter [inaudible] are

439
00:25:48,510 --> 00:25:50,580
the parameters of a multinomial distribution.

440
00:25:51,290 --> 00:25:56,800
And the distribution of XI conditioned on ZI being

441
00:25:56,800 --> 00:26:02,260
equal to J so it's P of XI given ZI is equal to J.

442
00:26:02,880 --> 00:26:04,890
That's going to be a Gaussian distribution

443
00:26:06,930 --> 00:26:11,970
with [inaudible] and covariant sigler.

444
00:26:13,100 --> 00:26:17,220
Okay. So this should actually

445
00:26:17,220 --> 00:26:18,640
look extremely familiar to you.

446
00:26:18,980 --> 00:26:23,870
What I've written down are pretty much the same

447
00:26:23,870 --> 00:26:25,480
equations that I wrote down for

448
00:26:25,480 --> 00:26:28,600
the Gaussian Discriminant Analysis algorithm

449
00:26:28,600 --> 00:26:29,910
that we saw way back, right,

450
00:26:29,910 --> 00:26:33,680
except that the differences instead of,

451
00:26:33,680 --> 00:26:36,220
I guess supervised learning

452
00:26:36,220 --> 00:26:39,070
where we were given the cross labels Y,

453
00:26:40,000 --> 00:26:42,960
I've now replaced Y in Gaussian Discriminant

454
00:26:42,960 --> 00:26:45,610
Analysis with these latent random variables

455
00:26:45,610 --> 00:26:47,320
or these unobserved random variables Z,

456
00:26:47,320 --> 00:26:48,350
and we don't actually know

457
00:26:48,350 --> 00:26:49,450
what the values of Z are.

458
00:26:50,680 --> 00:27:03,410
Okay. So just to make the link to the Gaussian

459
00:27:03,410 --> 00:27:05,710
Discriminant Analysis even a little more explicit

460
00:27:06,890 --> 00:27:08,560
if we knew what the Zs were,

461
00:27:13,400 --> 00:27:15,100
which was actually don't,

462
00:27:15,100 --> 00:27:16,860
but suppose for the sake of argument

463
00:27:16,860 --> 00:27:18,900
that we actually knew which of,

464
00:27:18,900 --> 00:27:20,460
say the two Gaussian's,

465
00:27:20,460 --> 00:27:22,190
each of our data points came from,

466
00:27:22,190 --> 00:27:23,190
then you can use [inaudible] estimation –

467
00:27:29,590 --> 00:27:31,890
you can write down the likelihood

468
00:27:31,890 --> 00:27:33,880
the parameters which would be that

469
00:27:43,330 --> 00:27:46,430
and you can then use [inaudible] estimation

470
00:27:48,210 --> 00:27:50,170
and you get exactly the same formula as

471
00:27:50,170 --> 00:27:52,060
in Gaussian Discriminant Analysis.

472
00:28:24,360 --> 00:28:26,650
Okay. So if you knew the value of the Z,

473
00:28:26,650 --> 00:28:28,900
you can write down the law of likelihood

474
00:28:30,630 --> 00:28:32,250
and do maximum likeliness this way,

475
00:28:33,020 --> 00:28:34,390
and you can then estimate

476
00:28:34,390 --> 00:28:35,560
all the parameters of your model.

477
00:28:38,230 --> 00:28:39,030
Does this make sense?

478
00:28:39,030 --> 00:28:40,450
Raise your hand if this makes sense.

479
00:28:42,780 --> 00:28:45,170
Cool. Some of you have questions?

480
00:28:45,170 --> 00:28:46,140
Some of you didn't raise your hands.

481
00:28:46,140 --> 00:28:46,730
Yeah.

482
00:28:46,730 --> 00:28:50,150
So this ZI is just a label, like, an X or an O?

483
00:28:50,150 --> 00:28:51,670
Yes. Basically.

484
00:28:52,320 --> 00:28:54,420
Any other questions?

485
00:29:05,080 --> 00:29:15,000
Okay. So if you knew the values of Z,

486
00:29:15,000 --> 00:29:17,230
the Z playing a similar role to the cross labels

487
00:29:17,230 --> 00:29:18,730
in Gaussian's Discriminant Analysis,

488
00:29:18,730 --> 00:29:21,430
then you could use maximum

489
00:29:21,430 --> 00:29:22,660
likeliness estimation parameters.

490
00:29:23,620 --> 00:29:25,170
But in reality,

491
00:29:25,170 --> 00:29:27,110
we don't actually know the values of the Zs.

492
00:29:27,110 --> 00:29:27,900
All we're given is

493
00:29:27,900 --> 00:29:28,930
this unlabeled data set

494
00:29:28,930 --> 00:29:32,320
and so let me write down

495
00:29:32,320 --> 00:29:34,160
the specific bootstrap procedure

496
00:29:35,480 --> 00:29:36,480
in which the idea is

497
00:29:36,480 --> 00:29:40,780
that we're going to use our model to

498
00:29:40,780 --> 00:29:43,040
try and guess what the values of Z is.

499
00:29:43,040 --> 00:29:44,320
We don't know our Z,

500
00:29:44,320 --> 00:29:46,550
but we'll just take a guess at the values of Z

501
00:29:46,550 --> 00:29:50,770
and we'll then use some of the values of Z that we

502
00:29:50,770 --> 00:29:52,950
guessed to fit the parameters of the rest of the

503
00:29:52,950 --> 00:29:55,490
model and then we'll actually iterate. And now

504
00:29:55,490 --> 00:29:57,270
that we have a better estimate for the parameters

505
00:29:57,270 --> 00:29:59,140
for the rest of the model, we'll then take another

506
00:29:59,140 --> 00:30:00,750
guess for what the values of Z are.

507
00:30:00,750 --> 00:30:02,630
And then we'll sort of use something like

508
00:30:02,630 --> 00:30:03,590
the maximum likeliness estimation

509
00:30:03,590 --> 00:30:07,580
to set even parameters of the model.

510
00:30:31,030 --> 00:30:33,000
So the algorithm I'm gonna write down is

511
00:30:33,000 --> 00:30:35,430
called the EM Algorithm

512
00:30:38,300 --> 00:30:40,260
and it proceeds as follows.

513
00:30:42,580 --> 00:30:43,700
Repeat until convergence

514
00:30:48,650 --> 00:30:49,520
and the E set,

515
00:30:49,520 --> 00:30:59,270
we're going to guess the values of the unknown ZIs

516
00:30:59,270 --> 00:31:20,760
and in particular, I'm going to set WIJ. Okay.

517
00:31:20,760 --> 00:31:22,230
So I'm going to compute the probability

518
00:31:22,230 --> 00:31:24,520
that ZI is equal to J.

519
00:31:24,520 --> 00:31:26,200
So I'm going to use the

520
00:31:26,200 --> 00:31:27,420
rest of the parameters in my model

521
00:31:27,420 --> 00:31:29,160
and then I'm gonna compute the probability

522
00:31:29,160 --> 00:31:35,430
that point XI came from Gaussian number J.

523
00:31:37,600 --> 00:31:39,600
And just to be sort of concrete about

524
00:31:39,600 --> 00:31:41,750
what I mean by this, this means that I'm going to

525
00:31:41,750 --> 00:32:03,070
compute P of XI. This step is sort of [inaudible], I

526
00:32:17,420 --> 00:32:19,070
guess. And again, just to be completely concrete

527
00:32:19,070 --> 00:32:21,270
about what I mean about this, the [inaudible] rate

528
00:32:21,270 --> 00:32:24,310
of P of XI given ZI equals J, you know,

529
00:32:24,310 --> 00:32:26,950
well that's the Gaussian density. Right?

530
00:33:00,160 --> 00:33:07,690
from O equals 1 to K ofof essentially the same

531
00:33:07,690 --> 00:33:08,690
That's one over E to the –and then divided by sum

532
00:33:07,690 --> 00:33:11,080
thing, but with J replaced by L. Okay. for

533
00:33:11,080 --> 00:33:14,230
the Gaussian and the numerator and the sum of

534
00:33:14,230 --> 00:33:16,030
the similar terms of the denominator.

535
00:33:17,680 --> 00:33:25,070
Excuse me. This is the sum from O equals 1

536
00:33:25,070 --> 00:33:26,600
through K in the denominator.

537
00:33:46,220 --> 00:33:59,680
Okay. Let's see. The maximization step

538
00:33:59,680 --> 00:34:01,530
where you would then update

539
00:34:01,530 --> 00:34:03,380
your estimates of the parameters.

540
00:34:03,380 --> 00:34:05,340
So I'll just lay down the formulas here.

541
00:34:06,580 --> 00:34:08,980
When you see these, you should compare them

542
00:34:08,980 --> 00:34:12,220
to the formulas we had for

543
00:34:12,220 --> 00:34:13,680
maximum likelihood estimation.

544
00:35:03,490 --> 00:35:11,210
And so these two formulas on top are very similar

545
00:35:11,210 --> 00:35:13,790
to what you saw for Gaussian Discriminant Analysis

546
00:35:13,790 --> 00:35:17,360
except that now, we have these [inaudible]

547
00:35:17,360 --> 00:35:21,480
so WIJ is you remember was the probability that

548
00:35:21,480 --> 00:35:25,570
we computed that point I came from Gaussian's.

549
00:35:27,330 --> 00:35:29,010
I don't want to call it cluster J,

550
00:35:29,010 --> 00:35:33,280
but that's what point I came from Gaussian J,

551
00:35:34,320 --> 00:35:35,940
rather than an indicator for

552
00:35:35,940 --> 00:35:37,370
where the point I came from Gaussian J.

553
00:35:39,000 --> 00:35:42,030
Okay. And the one slight difference

554
00:35:42,030 --> 00:35:46,550
between this and the formulas who

555
00:35:46,550 --> 00:35:47,890
have a Gaussian's Discriminant Analysis is

556
00:35:47,890 --> 00:35:49,390
that in the mixture of Gaussian's,

557
00:35:49,390 --> 00:35:52,800
we more commonly use different covariant

558
00:35:52,800 --> 00:35:54,310
[inaudible] for the different Gaussian's.

559
00:35:54,310 --> 00:35:56,710
So in Gaussian's Discriminant Analysis,

560
00:35:56,710 --> 00:35:57,980
sort of by convention,

561
00:35:57,980 --> 00:36:01,180
you usually model all of the crosses

562
00:36:01,180 --> 00:36:03,590
to the same covariant matrix sigma.

563
00:36:03,730 --> 00:36:05,710
when you dealing with Gaussian model

564
00:36:05,800 --> 00:36:09,230
this sort of, not always, but some time

565
00:36:09,320 --> 00:36:11,890
you are allowed the coherence matrix

566
00:36:11,990 --> 00:36:13,730
with different Gaussian areas

567
00:36:13,830 --> 00:36:16,350
for this formulation we have different coherence matrix sigma J

568
00:36:18,310 --> 00:36:22,280
and that is responding to the Gaussian analysis

569
00:36:23,600 --> 00:36:26,330
Why don't you just take a second

570
00:36:26,330 --> 00:36:28,240
to look at this and make sure it all makes sense?

571
00:36:49,970 --> 00:36:51,640
Do you have questions about this?

572
00:36:59,830 --> 00:37:02,230
Raise your hand if this makes sense to you?

573
00:37:02,230 --> 00:37:08,510
[Inaudible]. Okay. Only some of you.

574
00:37:10,830 --> 00:37:12,810
Let's see.

575
00:37:14,750 --> 00:37:16,580
So let me try to explain that a little bit more.

576
00:37:16,580 --> 00:37:18,420
Some of you recall

577
00:37:18,420 --> 00:37:20,440
that in Gaussian's Discriminant Analysis,

578
00:37:22,530 --> 00:37:32,070
right, if we knew the values for the ZIs so let's see.

579
00:37:33,470 --> 00:37:35,790
Suppose I was to give you labeled data sets,

580
00:37:35,790 --> 00:37:36,860
suppose I was to tell you

581
00:37:36,860 --> 00:37:38,960
the values of the ZIs for each example,

582
00:37:39,930 --> 00:37:43,330
then I'd be giving you a data set that looks like this.

583
00:37:44,390 --> 00:37:48,650
Okay. So here's my 1 D data set.

584
00:37:48,650 --> 00:37:50,200
That's sort of a typical 1 D

585
00:37:50,200 --> 00:37:51,820
Gaussian's Discriminant Analysis.

586
00:37:52,900 --> 00:37:55,950
So for Gaussian's Discriminant Analysis

587
00:37:55,950 --> 00:37:58,820
we figured out the maximum likeliness estimation

588
00:37:58,820 --> 00:38:00,350
and the maximum likeliness estimate

589
00:38:00,350 --> 00:38:01,980
for the parameters of GDA,

590
00:38:02,820 --> 00:38:05,410
and one of the estimates for

591
00:38:05,410 --> 00:38:10,010
the parameters for GDA was [inaudible]

592
00:38:10,010 --> 00:38:17,240
which is the probability that ZI equals J.

593
00:38:19,860 --> 00:38:21,110
You would estimate

594
00:38:21,110 --> 00:38:26,650
that as sum of I equals sum of I from 1 to M indicator

595
00:38:26,650 --> 00:38:32,570
ZI equals J and divide by N. Okay.

596
00:38:32,570 --> 00:38:36,890
When we're deriving GDA, [inaudible].

597
00:38:36,890 --> 00:38:39,260
If you knew the cross labels

598
00:38:39,260 --> 00:38:40,630
for every example you cross,

599
00:38:40,630 --> 00:38:43,630
then this was your maximum likeliness estimate for

600
00:38:43,630 --> 00:38:47,570
the chance that the labels came from the positive

601
00:38:47,570 --> 00:38:49,120
[inaudible] versus the negative [inaudible].

602
00:38:49,120 --> 00:38:50,510
It's just a fraction of examples.

603
00:38:50,510 --> 00:38:53,980
Your maximum likeliness estimate for probability of

604
00:38:53,980 --> 00:38:58,300
getting examples from cross J is just the fraction of

605
00:38:58,300 --> 00:38:59,710
examples in your training set

606
00:38:59,710 --> 00:39:01,150
that actually came from cross J.

607
00:39:01,150 --> 00:39:03,520
So this is the maximum likeliness estimation for

608
00:39:03,520 --> 00:39:04,690
Gaussian's Discriminant Analysis.

609
00:39:07,560 --> 00:39:10,560
Now, in the mixture of Gaussian's model

610
00:39:10,560 --> 00:39:11,720
and the EM problem

611
00:39:11,720 --> 00:39:14,090
we don't actually have these cross labels, right,

612
00:39:14,090 --> 00:39:15,690
we just have an unlabeled

613
00:39:15,690 --> 00:39:16,630
data set like this.

614
00:39:16,630 --> 00:39:17,730
We just have a set of dots.

615
00:39:20,880 --> 00:39:22,120
I'm trying to draw the same data set

616
00:39:22,120 --> 00:39:22,800
that I had above,

617
00:39:22,800 --> 00:39:24,030
but just with the cross labels.

618
00:39:25,690 --> 00:39:30,340
So now, it's as if you only get to observe the XIs,

619
00:39:31,830 --> 00:39:37,250
but the ZIs are unknown.

620
00:39:41,750 --> 00:39:43,280
Okay. So the cross label is unknown.

621
00:39:45,500 --> 00:39:47,230
So in the EM algorithm

622
00:39:49,230 --> 00:39:52,700
we're going to try to take a guess for

623
00:39:52,700 --> 00:39:54,250
the values of the ZIs,

624
00:39:55,930 --> 00:40:02,910
and specifically, in the E step we computed WIJ

625
00:40:02,910 --> 00:40:05,470
was our current best guess for the probability

626
00:40:05,470 --> 00:40:10,050
that ZI equals J given that data point.

627
00:40:10,050 --> 00:40:14,020
Okay. So this just means given my current hypothesis,

628
00:40:14,020 --> 00:40:16,280
the way the Gaussian's are, and given everything

629
00:40:19,170 --> 00:40:22,130
what was the [inaudible] probability that the point XI

630
00:40:22,130 --> 00:40:23,130
else, can I compute the [inaudible] probability –

631
00:40:22,130 --> 00:40:26,160
actually came from cross J? What is the probability

632
00:40:26,160 --> 00:40:28,740
that this point was a cross versus O?

633
00:40:28,740 --> 00:40:31,870
What's the probability that this point was [inaudible]?

634
00:40:34,200 --> 00:40:35,530
And now in the M step,

635
00:40:37,560 --> 00:40:42,370
my formula of estimating for the parameters

636
00:40:42,370 --> 00:40:45,830
[inaudible] will be given by 1 over M sum from I

637
00:40:45,830 --> 00:41:01,720
equals 1 through M, sum of WIJ. So WIJ is right. The

638
00:41:01,720 --> 00:41:03,960
probability is my best guess for the probability that

639
00:41:03,960 --> 00:41:08,070
point I belongs to Gaussian or belongs to cross J,

640
00:41:08,870 --> 00:41:15,440
and [inaudible] using this formula instead of this one.

641
00:41:17,760 --> 00:41:20,140
Okay. And similarly,

642
00:41:20,140 --> 00:41:22,450
this is my formula for the estimate for new J

643
00:41:22,450 --> 00:41:25,660
and it replaces the WIJs with

644
00:41:25,660 --> 00:41:28,230
these new indicator functions,

645
00:41:28,230 --> 00:41:30,610
you get back to the formula

646
00:41:30,610 --> 00:41:33,070
that you had in Gaussian's Discriminant Analysis.

647
00:41:35,310 --> 00:41:38,560
I'm trying to convey an intuitive sense of

648
00:41:38,560 --> 00:41:39,770
why these algorithm's make sense.

649
00:41:39,770 --> 00:41:41,830
Can you raise your hand if this makes sense now?

650
00:41:41,830 --> 00:41:44,120
Cool.

651
00:41:49,800 --> 00:41:52,440
So that's introduction of the EM algorithm,

652
00:41:52,530 --> 00:41:56,460
it turns out that the way I came out from this formular.

653
00:41:56,570 --> 00:42:02,320
this is actually same generation of EM algorithm,

654
00:42:02,410 --> 00:42:03,740
which we'll talk about next.

655
00:42:03,830 --> 00:42:10,090
So I hope this is efficient which can do the sense.

656
00:42:10,190 --> 00:42:11,420
but it exactly where it came from.

657
00:42:11,510 --> 00:42:14,420
It a question of answer of data.

658
00:42:38,910 --> 00:42:40,210
Okay. So what I want to do now is actually

659
00:42:40,210 --> 00:42:44,720
present a broader view of the EM algorithm.

660
00:42:44,720 --> 00:42:48,030
What you just saw was a special case of

661
00:42:48,030 --> 00:42:50,910
the EM algorithm for specially to

662
00:42:50,910 --> 00:42:52,140
make sure of Gaussian's model,

663
00:42:52,140 --> 00:42:55,230
and in the remaining half hour I have today

664
00:42:55,230 --> 00:43:00,910
I'm going to describe a general description of the EM

665
00:43:00,910 --> 00:43:03,460
algorithm and everything you just saw will be

666
00:43:03,460 --> 00:43:05,420
devised, sort of there's a special case of this more

667
00:43:05,420 --> 00:43:06,720
general view that I'll present now.

668
00:43:08,570 --> 00:43:17,750
And as a pre-cursor to actually deriving

669
00:43:17,750 --> 00:43:20,570
this more general view of the EM algorithm,

670
00:43:21,680 --> 00:43:23,810
I'm gonna have to describe

671
00:43:23,810 --> 00:43:25,200
something called Jensen's and Equality

672
00:43:25,200 --> 00:43:26,730
that we use in the derivation.

673
00:43:27,780 --> 00:43:30,060
So here's Jensen's and Equality.

674
00:43:32,170 --> 00:43:34,000
Just let F be a convex function.

675
00:43:45,360 --> 00:43:48,180
So a function is a convex of the second derivative,

676
00:43:48,180 --> 00:43:51,850
which I've written F prime prime to [inaudible].

677
00:43:51,850 --> 00:43:53,540
The functions don't have to

678
00:43:53,540 --> 00:43:54,870
be differentiatable to be convex,

679
00:43:54,870 --> 00:43:56,850
but if it has a second derivative,

680
00:43:56,850 --> 00:43:58,620
then F prime prime should be creating a 0.

681
00:44:00,590 --> 00:44:02,380
And let X be a random variable

682
00:44:12,760 --> 00:44:24,940
then the F applied to the expectation of X is less than

683
00:44:24,940 --> 00:44:26,940
the equal of 2D expectation of F of F.

684
00:44:27,710 --> 00:44:30,630
Okay. And hopefully you remember

685
00:44:30,630 --> 00:44:32,050
I often drop the square back,

686
00:44:32,050 --> 00:44:36,660
so E of X is the [inaudible],

687
00:44:37,810 --> 00:44:40,010
I'll often drop the square brackets.

688
00:44:43,640 --> 00:44:45,460
So let me draw a picture that would explain this

689
00:44:48,420 --> 00:44:52,270
and I think Many of my friends and I often don't

690
00:44:52,270 --> 00:44:55,100
remember is less than or great than or whatever,

691
00:44:55,100 --> 00:45:00,930
and the way that many of us remember the sign of

692
00:45:00,930 --> 00:45:04,890
that in equality is by drawing the following picture.

693
00:45:30,970 --> 00:45:31,810
For this example,

694
00:45:31,810 --> 00:45:36,920
let's say, X is equal to 1 with a probability of one-half

695
00:45:36,920 --> 00:45:40,290
and X is equal to 6 worth probability 1 whole.

696
00:45:40,290 --> 00:45:44,720
So I'll illustrate this inequality with an example.

697
00:45:47,420 --> 00:45:54,810
So let's see. So X is 1 with probability one-half

698
00:45:54,810 --> 00:45:56,520
and X is 6 with probably with half

699
00:45:56,520 --> 00:45:59,600
and so the expected value of X is 3.5.

700
00:45:59,600 --> 00:46:01,310
It would be in the middle here.

701
00:46:01,310 --> 00:46:04,290
So that's the expected value of X.

702
00:46:04,950 --> 00:46:07,170
The horizontal axis here is the X axis.

703
00:46:08,900 --> 00:46:13,740
And so F of the expected value of X,

704
00:46:13,740 --> 00:46:16,270
you can read of as this point here.

705
00:46:16,270 --> 00:46:20,470
So this is F of the expected value of X.

706
00:46:22,160 --> 00:46:24,650
Where as in contrast, let's see.

707
00:46:25,140 --> 00:46:32,830
If X is equal to 1 then here's F of 1

708
00:46:34,180 --> 00:46:39,370
and if X equaled a 6 then here's F of 6

709
00:46:41,200 --> 00:46:46,380
and the expected value of F of X,

710
00:46:48,000 --> 00:46:53,410
it turns out, is now averaging on the vertical axis.

711
00:47:08,040 --> 00:47:11,040
We're 50 percent chance you get F of 1

712
00:47:11,040 --> 00:47:12,550
with 50 percent chance you get F of 6

713
00:47:12,550 --> 00:47:15,240
and so these expected value of F of X

714
00:47:15,240 --> 00:47:17,220
is the average of F of 1 and F of 6,

715
00:47:17,220 --> 00:47:19,570
which is going to be the value in the middle here.

716
00:47:21,210 --> 00:47:24,930
And so in this example you see that the expected value

717
00:47:24,930 --> 00:47:26,700
of F of X is greater than

718
00:47:26,700 --> 00:47:30,270
or equal to F of the expected value of X.

719
00:47:30,270 --> 00:47:31,640
Okay.

720
00:47:33,860 --> 00:47:37,120
So the illustration why this cause in this case.

721
00:47:54,810 --> 00:47:58,690
And it turns out further

722
00:48:00,930 --> 00:48:05,820
that if F double prime of X makes [inaudible]

723
00:48:05,820 --> 00:48:06,940
than Z row,

724
00:48:06,940 --> 00:48:08,150
if this happens,

725
00:48:08,150 --> 00:48:09,860
we say F is strictly convex

726
00:48:18,240 --> 00:48:20,070
then the inequality holds

727
00:48:20,070 --> 00:48:21,810
an equality or in other words,

728
00:48:21,810 --> 00:48:26,240
E of F of X equals F of EX,

729
00:48:29,380 --> 00:48:38,130
if and only if, X is a constant with probability 1.

730
00:48:40,910 --> 00:48:44,650
Well, another way of writing this is X equals EX.

731
00:48:54,200 --> 00:48:55,400
Okay. So in other words,

732
00:48:55,400 --> 00:48:57,410
if F is a strictly convex function,

733
00:48:58,230 --> 00:49:01,590
then the only way for this inequality to hold its

734
00:49:01,590 --> 00:49:04,090
equality is if the random variable X

735
00:49:04,090 --> 00:49:06,380
always takes on the same value.

736
00:49:06,480 --> 00:49:10,910
if X=E[x],probably one of the this one estating.

737
00:49:13,550 --> 00:49:20,570
Em,further,so thus state for concave fuctions.

738
00:49:29,560 --> 00:49:36,130
then everything holds the direction of the any quality of verse.

739
00:49:36,130 --> 00:49:37,130
It turns out that if f''《=0.the effort of concave.

740
00:49:36,220 --> 00:49:46,250
So f''(Ex)>=E[f(x)],etc.

741
00:49:46,350 --> 00:49:47,990
So you just if you hava concave of function.

742
00:49:48,080 --> 00:49:51,240
then all the qualities hold the same way,

743
00:49:51,330 --> 00:49:53,410
because the direction in quality of verse.

744
00:49:53,500 --> 00:49:57,470
and it turn out later on we will derive the EM algorithm

745
00:49:57,560 --> 00:50:01,970
why use that concave version,would this quality verse.

746
00:50:05,360 --> 00:50:18,180
Okay. Any questions about this? Yeah.

747
00:50:18,270 --> 00:50:20,080
[Inaudible]

748
00:50:20,080 --> 00:50:21,610
Say that again?

749
00:50:21,610 --> 00:50:24,840
[inaudible]?

750
00:50:26,570 --> 00:50:29,310
What is the strictly convex [inaudible]?

751
00:50:29,310 --> 00:50:30,310
I still couldn't hear that. What is –

752
00:50:29,310 --> 00:50:30,750
Oh, I see.

753
00:50:30,750 --> 00:50:33,540
If double prime of X is strictly greater than 0

754
00:50:33,540 --> 00:50:35,500
that's my definition for strictly convex.

755
00:50:37,340 --> 00:50:40,790
If the second derivative of X is strictly greater than 0

756
00:50:40,790 --> 00:50:43,520
then that's what it means for F to be strictly convex.

757
00:50:43,520 --> 00:50:45,180
[Inaudible]

758
00:50:45,180 --> 00:50:46,100
I see. Sure.

759
00:50:46,100 --> 00:50:47,770
So for example,

760
00:50:50,250 --> 00:50:52,710
this is an example of a convex function

761
00:50:52,710 --> 00:50:53,980
that's not strictly convexed

762
00:50:53,980 --> 00:50:57,000
because there's part of this function is a straight line

763
00:50:57,000 --> 00:51:01,790
and so F double prime would be zero in this portion.

764
00:51:09,760 --> 00:51:10,480
Let's see. Yeah.

765
00:51:10,480 --> 00:51:13,180
It's just a less formal way of saying strictly convexed

766
00:51:13,180 --> 00:51:14,200
just means that you can't

767
00:51:14,200 --> 00:51:17,290
have a convex function within a straight line portion

768
00:51:17,290 --> 00:51:20,960
and then [inaudible]. Speaking very informally, think

769
00:51:20,960 --> 00:51:22,050
of this as meaning that there aren't

770
00:51:22,050 --> 00:51:26,070
any straight line portions. Okay.

771
00:51:46,150 --> 00:51:50,640
So here's the derivation

772
00:51:50,640 --> 00:51:53,930
for the general version of EM.

773
00:51:57,470 --> 00:52:00,720
The problem was face is as follows.

774
00:52:13,590 --> 00:52:14,830
We have some model for

775
00:52:14,830 --> 00:52:16,830
the joint distribution of X of Z,

776
00:52:18,520 --> 00:52:24,700
but we observe only X,

777
00:52:26,080 --> 00:52:31,310
and our goal is to maximize

778
00:52:41,770 --> 00:52:43,810
the law of likelihood of the parameters of model.

779
00:52:52,690 --> 00:52:54,890
Right. So we have some models

780
00:52:54,890 --> 00:52:56,710
for the joint distribution for X and Z

781
00:52:56,710 --> 00:52:59,210
and our goal is to find the

782
00:52:59,210 --> 00:53:01,230
maximum likeliness estimate of the parameters data

783
00:53:01,230 --> 00:53:06,020
where the likelihood is defined as something equals 1

784
00:53:06,020 --> 00:53:08,730
to M [inaudible] probably of our data as usual.

785
00:53:13,250 --> 00:53:17,230
And here X is parameterized by data is now given

786
00:53:17,230 --> 00:53:21,320
by a sum over all the values

787
00:53:21,320 --> 00:53:30,960
of ZI parameterized by data. Okay.

788
00:53:30,960 --> 00:53:33,860
So just by taking our model of

789
00:53:33,860 --> 00:53:35,120
the joint distribution of X and Z

790
00:53:35,120 --> 00:53:38,370
and marginalizing out ZI

791
00:53:38,370 --> 00:53:41,780
that we get P of XI parameterized by data.

792
00:53:44,860 --> 00:53:58,430
And so the EM algorithm will be a way of performing

793
00:53:58,430 --> 00:54:01,510
this maximum likeliness estimation problem,

794
00:54:01,510 --> 00:54:04,620
which is complicated by the fact that we have

795
00:54:04,620 --> 00:54:07,050
these ZIs in our model that are unobserved.

796
00:54:24,340 --> 00:54:25,570
Before I actually do the math,

797
00:54:25,570 --> 00:54:27,490
here's a useful picture to keep in mind.

798
00:54:31,910 --> 00:54:33,890
So the horizontal axis in this cartoon

799
00:54:33,890 --> 00:54:36,760
is the [inaudible] axis and there's some function,

800
00:54:36,760 --> 00:54:38,250
the law of likelihood of theta zero

801
00:54:38,250 --> 00:54:39,670
that we're trying to maximize,

802
00:54:46,990 --> 00:54:51,940
and usually maximizing our [inaudible] derivatives

803
00:54:51,940 --> 00:54:52,950
instead of the zero

804
00:54:52,950 --> 00:54:53,950
that would be very hard to do.

805
00:54:53,950 --> 00:54:57,600
What the EM algorithm will do is the following.

806
00:54:58,540 --> 00:55:00,770
Let's say it initializes some value of theta zero,

807
00:55:01,920 --> 00:55:04,210
what the EM algorithm will end up doing is

808
00:55:04,210 --> 00:55:07,330
it will construct a lower bound

809
00:55:07,330 --> 00:55:09,500
for this law of likelihood function

810
00:55:09,500 --> 00:55:14,370
and this lower bound will be tight [inaudible] of

811
00:55:14,370 --> 00:55:18,870
equality after current guessing the parameters

812
00:55:18,870 --> 00:55:21,890
and they maximize this lower boundary

813
00:55:21,890 --> 00:55:22,710
with respect to theta

814
00:55:22,710 --> 00:55:25,060
so we'll end up with say that value.

815
00:55:25,500 --> 00:55:27,370
So that will be data 1.

816
00:55:28,540 --> 00:55:30,740
Okay. And then EM algorithm look at theta 1

817
00:55:30,740 --> 00:55:36,720
and they'll construct a new lower bound of theta

818
00:55:36,720 --> 00:55:38,650
and then we'll maximize that.

819
00:55:39,380 --> 00:55:40,170
So you jump here.

820
00:55:40,170 --> 00:55:42,330
So that's the next theta 2

821
00:55:42,330 --> 00:55:44,150
and you do that again

822
00:55:45,390 --> 00:55:49,050
and you get the same 3, 4,

823
00:55:49,050 --> 00:55:51,010
and so on until you converge to

824
00:55:51,010 --> 00:55:53,930
local optimum on [inaudible] theta function.

825
00:55:54,680 --> 00:55:58,340
Okay. So this is a cartoon that displays

826
00:55:58,340 --> 00:55:59,600
what the EM algorithm will do.

827
00:56:00,320 --> 00:56:02,100
So let's actually make that formal now.

828
00:56:20,110 --> 00:56:27,420
So you want to maximize with respect to theta sum

829
00:56:27,420 --> 00:56:36,400
of [inaudible] there's my theta, so this is sum over

830
00:56:36,400 --> 00:56:41,500
1 [inaudible] sum over all values of Z.

831
00:56:50,470 --> 00:56:57,980
Okay. So what I'm going to do is multiply

832
00:56:57,980 --> 00:56:59,380
and divide by the same thing

833
00:56:59,380 --> 00:57:06,710
and I'm gonna write this as Q okay.

834
00:57:14,820 --> 00:57:17,940
So I'm going to construct the

835
00:57:17,940 --> 00:57:22,640
probability distribution QI, that would be over

836
00:57:22,640 --> 00:57:24,280
the latent random variables ZI

837
00:57:24,280 --> 00:57:27,460
and so these QI would get distribution

838
00:57:27,460 --> 00:57:30,600
so each of the QI would bring in a 0

839
00:57:30,600 --> 00:57:37,700
and sum over all the values of ZI of QI would be 1,

840
00:57:37,700 --> 00:57:40,250
so these Qs will be a probability distribution

841
00:57:40,250 --> 00:57:41,390
that I get to construct.

842
00:57:42,100 --> 00:57:46,130
Okay. And then I'll later go describe

843
00:57:46,130 --> 00:57:48,600
the specific choice of this distribution QI.

844
00:57:54,370 --> 00:57:58,490
So this QI is a probability distribution

845
00:57:58,490 --> 00:58:01,420
over the random variables of ZI

846
00:58:01,520 --> 00:58:04,660
so this is [inaudible].

847
00:58:04,750 --> 00:58:08,690
some of the probable Z(i) times some functional Z(i)

848
00:58:08,800 --> 00:58:12,700
So this innersermation is really an expectation

849
00:58:12,810 --> 00:58:25,360
is really the expected value of that. of this one.

850
00:58:34,660 --> 00:58:36,330
While the expectation is the

851
00:58:36,430 --> 00:58:40,110
distribution of the random variable z^((i) )

852
00:58:40,210 --> 00:58:42,970
draw the distribution Q_i

853
00:58:43,090 --> 00:58:48,190
It wasn't mean that we task the expected value of this formula

854
00:58:48,310 --> 00:58:57,240
just some [inaudible] of the probable z^((i) ) times that functional z^((i) )

855
00:58:59,070 --> 00:59:01,670
Right. I see some frowns.

856
00:59:01,670 --> 00:59:02,750
Do you have questions about this?

857
00:59:05,600 --> 00:59:06,390
No. Okay.

858
00:59:32,290 --> 00:59:43,390
So the log function looks like that

859
00:59:43,390 --> 00:59:45,770
and there's a concave function

860
00:59:46,960 --> 00:59:52,110
so that tells us that the log of E of X is greater than

861
00:59:52,110 --> 00:59:58,320
and equal to the expected value of log X by the

862
00:59:58,320 --> 01:00:01,570
other concave function form of Jensen's and Equality.

863
01:00:03,820 --> 01:00:06,560
And so continuing from the previous expression,

864
01:00:06,560 --> 01:00:15,350
this is a summary of a log and an expectation,

865
01:00:15,350 --> 01:00:19,040
that must therefore be greater than or equal to

866
01:00:19,040 --> 01:00:29,510
the expected value of the log of that.

867
01:00:45,490 --> 01:00:46,220
Okay.

868
01:00:46,980 --> 01:00:48,220
Using Jensen's and Equality.

869
01:00:51,380 --> 01:00:54,530
And lastly just to expand out this formula again.

870
01:01:15,620 --> 01:01:16,590
This is equal to that.

871
01:01:19,250 --> 01:01:22,160
Okay. Yeah.

872
01:01:22,160 --> 01:01:29,590
[Inaudible]

873
01:01:29,590 --> 01:01:31,370
[Inaudible]

874
01:01:31,630 --> 01:01:32,530
Yeah. Okay.

875
01:01:32,530 --> 01:01:47,340
So this has the [inaudible] so let's say Random

876
01:01:47,340 --> 01:01:51,510
variable Z, right, and Z has some distribution.

877
01:01:51,920 --> 01:01:54,380
Let's denote it P.

878
01:01:54,380 --> 01:01:57,070
And let's say I have some function G of Z.

879
01:01:58,090 --> 01:02:00,530
Okay. Then by definition,

880
01:02:00,530 --> 01:02:07,430
the expected value of G of Z, by definition, that's

881
01:02:07,430 --> 01:02:11,040
equal to sum over all the values of Z, the probability

882
01:02:11,040 --> 01:02:14,180
of that value of Z times Z of G. Right.

883
01:02:15,220 --> 01:02:18,360
That's sort of the definition of a random variable.

884
01:02:20,650 --> 01:02:23,810
And so the way I got from

885
01:02:23,810 --> 01:02:28,230
this step to this step is by using that.

886
01:02:28,880 --> 01:02:29,690
So in particular,

887
01:02:29,690 --> 01:02:35,100
now, I've been using distribution QI

888
01:02:35,100 --> 01:02:36,580
to denote the distribution of Z,

889
01:02:36,580 --> 01:02:41,720
so this is, like, sum over Z of P of Z times [inaudible].

890
01:02:56,870 --> 01:03:03,030
And so this is just the expected value with respect to

891
01:03:03,030 --> 01:03:04,460
a random variable Z joined from

892
01:03:04,460 --> 01:03:08,020
the distribution Q of G of Z.

893
01:03:11,420 --> 01:03:12,250
Are there questions?

894
01:03:15,460 --> 01:03:16,800
So in general when you're

895
01:03:16,800 --> 01:03:19,220
doing maximum likelihood estimations,

896
01:03:19,220 --> 01:03:23,210
the likelihood of the data,

897
01:03:23,210 --> 01:03:27,280
but in this case you only say probability of X

898
01:03:27,280 --> 01:03:30,230
because you only have observed X whereas

899
01:03:30,230 --> 01:03:32,510
previously we said probability of X given the labels?

900
01:03:32,510 --> 01:03:33,880
Yes. Exactly. Right. Right.

901
01:03:33,880 --> 01:03:38,170
[Inaudible] we want to choose

902
01:03:38,170 --> 01:03:39,610
the parameters that maximizes

903
01:03:39,610 --> 01:03:40,530
the probability of the data,

904
01:03:40,530 --> 01:03:45,050
and in this case, our data comprises only the Xs

905
01:03:45,520 --> 01:03:46,770
because we don't reserve the Zs,

906
01:03:46,770 --> 01:03:53,560
and therefore, the likelihood of parameters

907
01:03:53,560 --> 01:03:57,560
is given by the probability of the data, which is [inaudible].

908
01:04:05,800 --> 01:04:08,020
So this is all we've done, right,

909
01:04:08,020 --> 01:04:12,000
we wanted to maximize the law of likelihood of theta

910
01:04:12,000 --> 01:04:16,220
and what we've done, through these manipulations,

911
01:04:16,220 --> 01:04:17,820
we've know constructed a lower bound

912
01:04:17,820 --> 01:04:19,220
on the law of likelihood of data.

913
01:04:20,420 --> 01:04:22,850
Okay. And in particular, this formula that we came up,

914
01:04:22,850 --> 01:04:28,530
we should think of this as a function of theta then,

915
01:04:29,020 --> 01:04:31,020
theta are the parameters of your model, right,

916
01:04:31,020 --> 01:04:31,720
if you think about this

917
01:04:31,720 --> 01:04:34,040
as a function of your parameters theta,

918
01:04:34,040 --> 01:04:35,910
what we've just shown is

919
01:04:35,910 --> 01:04:40,150
that the law of likelihood of your parameters theta

920
01:04:40,150 --> 01:04:45,790
is lower bounded by this thing.

921
01:05:04,140 --> 01:05:07,510
Okay. Remember that cartoon of repeatedly

922
01:05:07,510 --> 01:05:08,770
constructing a lower bound

923
01:05:08,770 --> 01:05:10,120
and optimizing the lower bound.

924
01:05:10,120 --> 01:05:14,030
So what we've just done is construct a lower bound

925
01:05:14,030 --> 01:05:16,180
for the law of likelihood for theta.

926
01:05:18,200 --> 01:05:22,650
Now, the last piece we want for this lower bound is

927
01:05:22,650 --> 01:05:27,830
actually we want this inequality to hold with equality for

928
01:05:27,830 --> 01:05:29,020
the current value for theta.

929
01:05:29,020 --> 01:05:32,140
So just refrain back to the previous cartoon.

930
01:05:33,650 --> 01:05:36,570
If this was the law of likelihood for theta,

931
01:05:36,570 --> 01:05:38,980
we'd then construct some lower bound of it,

932
01:05:38,980 --> 01:05:40,040
some function of theta

933
01:05:40,040 --> 01:05:44,580
and if this is my current value for theta,

934
01:05:44,580 --> 01:05:46,920
then I want my lower bound to be tight.

935
01:05:46,920 --> 01:05:48,810
I want my lower bound to be equal to

936
01:05:48,810 --> 01:05:49,910
the law of likelihood of theta

937
01:05:49,910 --> 01:05:52,070
because that's what I need to guarantee

938
01:05:52,070 --> 01:05:54,060
that when I optimize my lower bound,

939
01:05:54,060 --> 01:05:56,530
then I'll actually do even better on

940
01:05:56,530 --> 01:05:58,560
the true objective function.

941
01:06:02,530 --> 01:06:05,890
and so to do that,to ensure that any equality

942
01:06:05,950 --> 01:06:07,960
actually hold the equal of equality.

943
01:06:08,050 --> 01:06:12,800
what I need is going back the [inaudible].

944
01:06:12,880 --> 01:06:14,910
What I need is in the steps that I use the [inaudible] of quality,

945
01:06:14,990 --> 01:06:18,970
I need these any quatity to hold of equality.

946
01:06:19,060 --> 01:06:24,770
and particular I am going to choose my distribute-

947
01:06:24,880 --> 01:06:26,660
so these hold ture for any probility

948
01:06:26,760 --> 01:06:28,470
distribution cute that am I choose.

949
01:06:28,550 --> 01:06:31,610
and so what I am going to do is I choose

950
01:06:31,710 --> 01:06:34,770
my probility ditribution Q to ensure

951
01:06:34,850 --> 01:06:39,400
that this any qulity holds ture if equality

952
01:06:39,470 --> 01:06:41,280
for my current value for data.

953
01:06:41,360 --> 01:06:42,320
Yeah.

954
01:06:42,390 --> 01:06:43,980
[inaudible]

955
01:06:45,060 --> 01:06:47,860
Excuse me. Yeah. Great question.

956
01:06:47,860 --> 01:06:49,510
How do I know that function is concave?

957
01:06:51,960 --> 01:06:53,330
Yeah. I don't think I've shown it.

958
01:06:53,330 --> 01:06:54,390
It actually turns out to be true for

959
01:06:54,390 --> 01:06:55,350
all the models we work with.

960
01:06:55,350 --> 01:06:59,730
Do I know that the law of bound

961
01:06:59,730 --> 01:07:00,940
is a concave function of theta?

962
01:07:00,940 --> 01:07:02,260
I think you're right.

963
01:07:02,260 --> 01:07:04,400
In general, this may not be a concave function of theta.

964
01:07:04,400 --> 01:07:05,870
For many of the models we work with,

965
01:07:05,870 --> 01:07:07,430
this will turn out to be a concave function,

966
01:07:07,430 --> 01:07:08,220
but that's not always true.

967
01:07:14,040 --> 01:07:16,140
Okay. So let me go ahead and choose a value for Q.

968
01:07:16,140 --> 01:07:19,140
And I'll refer back to Jensen's and Equality.

969
01:07:19,140 --> 01:07:22,770
We said that this inequality will become an equality

970
01:07:22,770 --> 01:07:27,310
if the random variable inside is a constant.

971
01:07:27,790 --> 01:07:29,270
Right. If you're taking an expectation

972
01:07:29,270 --> 01:07:30,360
with respect to

973
01:07:30,360 --> 01:07:32,780
constant valued variables.

974
01:07:58,800 --> 01:07:59,880
So what we want is to choose

975
01:07:59,960 --> 01:08:02,520
this distribution QI of ZIs

976
01:08:02,620 --> 01:08:16,630
So that is equal to constant.

977
01:08:16,730 --> 01:08:20,840
and by constant I mean these take on

978
01:08:20,910 --> 01:08:27,830
the same value for all values of Zi.

979
01:08:27,930 --> 01:08:30,160
no matter what Zi you plug in

980
01:08:30,240 --> 01:08:32,770
you end up the same value for this special.

981
01:08:32,870 --> 01:08:47,270
and so you'll set QI of Zi to be proportional to that,

982
01:08:47,360 --> 01:08:49,890
right, to make sure this constant.

983
01:08:53,300 --> 01:08:58,980
But also you know that this Zi must be-oh,excuse me,

984
01:08:59,090 --> 01:09:00,900
that Qi must be for this distribution,

985
01:09:00,980 --> 01:09:08,290
and so that sum of Zi is equal to 1.

986
01:09:08,390 --> 01:09:12,940
and as all know this except must escape here.

987
01:09:13,040 --> 01:09:19,350
that the choice is actually equal to-

988
01:10:18,540 --> 01:10:22,110
So the QI of ZIs must sum to 1

989
01:10:22,110 --> 01:10:24,890
and so to compute it you should just

990
01:10:24,890 --> 01:10:28,180
take P of XI, ZI, parameterized by theta

991
01:10:28,180 --> 01:10:29,790
and just normalize the sum to one.

992
01:10:29,790 --> 01:10:31,810
There is a step that I'm skipping here

993
01:10:31,810 --> 01:10:34,460
to show that this is really the right thing to do.

994
01:10:34,460 --> 01:10:35,980
Hopefully, you'll just be convinced it's true.

995
01:10:35,980 --> 01:10:39,090
For the actual steps that I skipped,

996
01:10:39,090 --> 01:10:40,900
it's actually written out in the lecture notes.

997
01:10:44,110 --> 01:10:53,240
So you then have the denominator, by definition, is that

998
01:10:56,380 --> 01:11:06,740
and so by the definition of conditional probability QI of ZI

999
01:11:06,740 --> 01:11:12,000
is just equal to P of ZI given XI and parameterized by theta.

1000
01:11:37,150 --> 01:11:39,110
Okay. And so to summarize the algorithm,

1001
01:11:45,920 --> 01:11:48,270
the EM algorithm has two steps.

1002
01:11:48,270 --> 01:11:49,990
And the E step,

1003
01:11:52,650 --> 01:11:56,540
we set, we choose the distributions QI,

1004
01:11:56,540 --> 01:12:01,800
so QI of ZI will set to be equal to

1005
01:12:09,550 --> 01:12:12,420
a P of ZI given [inaudible] by data.

1006
01:12:12,420 --> 01:12:13,750
That's the formula we just worked out.

1007
01:12:16,630 --> 01:12:20,050
And so by this step we've now created a lower bound

1008
01:12:20,050 --> 01:12:22,740
on the law of likelihood function that is now tight

1009
01:12:22,740 --> 01:12:24,200
at a current value of theta.

1010
01:12:25,810 --> 01:12:26,890
And in the M step,

1011
01:12:26,890 --> 01:12:30,800
we then optimize that lower bound with respect to

1012
01:12:30,800 --> 01:12:35,410
our parameters theta and specifically to

1013
01:12:35,410 --> 01:12:36,790
the [inaudible] of theta.

1014
01:12:57,890 --> 01:13:02,960
Okay. And so that's the EM algorithm.

1015
01:13:03,710 --> 01:13:05,270
I won't have time to do it today,

1016
01:13:05,270 --> 01:13:08,470
but I'll probably show this in the next lecture,

1017
01:13:08,470 --> 01:13:11,220
but the EM algorithm's that I wrote down for

1018
01:13:11,220 --> 01:13:13,040
the mixtures of Gaussian's algorithm

1019
01:13:13,040 --> 01:13:16,410
is actually a special case of this more general template

1020
01:13:16,410 --> 01:13:20,130
where the E step and the M step responded.

1021
01:13:20,490 --> 01:13:23,120
So pretty much exactly to this E step and this M step

1022
01:13:23,120 --> 01:13:23,670
that I wrote down.

1023
01:13:32,730 --> 01:13:35,620
The E step constructs this lower bound

1024
01:13:35,620 --> 01:13:38,000
and makes sure that it is tight to

1025
01:13:38,000 --> 01:13:39,000
the current value of theta.

1026
01:13:39,960 --> 01:13:41,070
That's in my choice of Q,

1027
01:13:41,070 --> 01:13:45,140
and then the M step optimizes

1028
01:13:45,140 --> 01:13:47,300
the lower bound with respect to [inaudible] data.

1029
01:13:48,260 --> 01:13:52,350
Okay. So lots more to say about this in the next lecture.

1030
01:13:52,350 --> 01:13:54,350
Let's check if there's any questions before we close.

1031
01:14:03,490 --> 01:14:04,430
No. Okay. Cool.

1032
01:14:04,430 --> 01:14:05,510
So let's wrap up for today

1033
01:14:05,510 --> 01:14:07,410
and we'll continue talking about this in the next session.

