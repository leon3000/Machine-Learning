1
00:00:16,050 --> 00:00:21,320
MachineLearning-Lecture17

2
00:00:21,420 --> 00:00:22,610
Okay, good morning. Welcome back.

3
00:00:22,710 --> 00:00:25,520
So I hope all of you had a good

4
00:00:25,600 --> 00:00:26,390
Thanksgiving break.

5
00:00:26,480 --> 00:00:28,420
After the problem sets,

6
00:00:28,490 --> 00:00:29,980
I suspect many of us needed one.

7
00:00:30,060 --> 00:00:34,390
Just one quick announcement so as I announced

8
00:00:34,460 --> 00:00:37,950
by email a few days ago, this afternoon we'll

9
00:00:38,040 --> 00:00:40,590
be doing another tape ahead of lecture,

10
00:00:40,670 --> 00:00:42,600
so I won't physically be here

11
00:00:42,680 --> 00:00:44,080
on Wednesday, and so we'll be taping this

12
00:00:44,160 --> 00:00:45,360
Wednesday's lecture ahead of time.

13
00:00:45,450 --> 00:00:47,820
If you're free this afternoon,

14
00:00:47,910 --> 00:00:51,660
please come to that; it'll be at 3:45 p.m.

15
00:00:51,740 --> 00:00:56,680
in the Skilling Auditorium in Skilling 193 at

16
00:00:56,770 --> 00:00:58,630
3:45. But of course, you can also

17
00:00:58,720 --> 00:01:01,730
just show up in class as usual at the usual time

18
00:01:01,830 --> 00:01:03,170
or just watch it online as usual also.

19
00:01:05,710 --> 00:01:08,680
Okay, welcome back.

20
00:01:08,780 --> 00:01:10,430
What I want to do today is

21
00:01:10,520 --> 00:01:12,370
continue our discussion on

22
00:01:12,470 --> 00:01:14,090
Reinforcement Learning in MDPs.

23
00:01:14,180 --> 00:01:16,920
Quite a long topic for me to go over today,

24
00:01:17,010 --> 00:01:19,590
so most of today's lecture will be on continuous

25
00:01:19,700 --> 00:01:24,400
state MDPs, and in particular, algorithms for

26
00:01:24,490 --> 00:01:27,380
solving continuous state MDPs, so I'll talk just

27
00:01:27,460 --> 00:01:28,730
very briefly about discretization.

28
00:01:28,830 --> 00:01:31,480
I'll spend a lot of time talking about models,

29
00:01:31,580 --> 00:01:35,090
assimilators of MDPs, and then talk about one

30
00:01:35,210 --> 00:01:38,320
algorithm called fitted value iteration

31
00:01:38,410 --> 00:01:42,210
and two functions which builds on that,

32
00:01:42,290 --> 00:01:46,140
and then hopefully, I'll have time to get

33
00:01:46,240 --> 00:01:47,530
to a second algorithm called,

34
00:01:47,620 --> 00:01:49,840
approximate policy iteration

35
00:01:49,950 --> 00:01:54,310
Just to recap, right, in the previous lecture,

36
00:01:54,420 --> 00:01:57,670
I defined the Reinforcement Learning problem

37
00:01:57,770 --> 00:02:01,080
and I defined MDPs, so let me just recap

38
00:02:01,170 --> 00:02:06,470
the notation. I said that an MDP or a Markov

39
00:02:06,580 --> 00:02:10,660
Decision Process, was a 5 tuple,

40
00:02:10,760 --> 00:02:16,220
comprising those

41
00:02:16,320 --> 00:02:20,790
things and the running example of

42
00:02:20,890 --> 00:02:25,860
those using last time was this one right, adapted

43
00:02:25,930 --> 00:02:28,340
from the Russell and Norvig AI

44
00:02:28,440 --> 00:02:34,270
textbook. So in this example MDP that I was

45
00:02:34,370 --> 00:02:38,040
using, it had 11 states, so that's where S

46
00:02:38,130 --> 00:02:41,920
was. The actions were compass directions:

47
00:02:42,020 --> 00:02:43,950
north, south, east and west.

48
00:02:44,040 --> 00:02:48,620
The state transition probability is to capture

49
00:02:48,750 --> 00:02:51,990
chance of your transitioning to every state

50
00:02:52,100 --> 00:02:54,120
when you take any action in any other given

51
00:02:54,230 --> 00:02:57,030
state and so in our example that captured the

52
00:02:57,150 --> 00:03:01,360
stochastic dynamics of our robot wondering

53
00:03:01,470 --> 00:03:03,520
around the spread, and we said if you take

54
00:03:03,620 --> 00:03:05,820
the action north and the south, you have

55
00:03:05,930 --> 00:03:07,350
a .8 chance of actually going north

56
00:03:07,440 --> 00:03:10,100
and .1 chance of veering off, so that .

57
00:03:10,220 --> 00:03:11,710
1 chance of veering off to the right

58
00:03:11,800 --> 00:03:14,540
so said model of the robot's

59
00:03:14,670 --> 00:03:18,010
noisy dynamic with a [inaudible]

60
00:03:18,120 --> 00:03:23,450
and the reward function was that +/-1

61
00:03:23,570 --> 00:03:37,920
at the absorbing states and -0.02 elsewhere.

62
00:03:38,030 --> 00:03:42,070
This is an example of an MDP, and that's what

63
00:03:42,170 --> 00:03:43,880
these five things were. Oh, and I used a

64
00:03:43,970 --> 00:03:49,890
discount factor G of usually a number slightly

65
00:03:50,000 --> 00:03:54,880
less than one, so that's the 0.99. And so our

66
00:03:54,980 --> 00:03:59,580
goal was to find the policy, the control policy

67
00:03:59,700 --> 00:04:03,740
and that's at ?, which is a function mapping

68
00:04:03,840 --> 00:04:07,370
from the states of the actions that tells us what

69
00:04:07,720 --> 00:04:10,660
action to take in every state, and our goal was

70
00:04:10,740 --> 00:04:14,020
to find a policy that maximizes the expected

71
00:04:14,110 --> 00:04:17,060
value of our total payoff. So we want to find a

72
00:04:17,140 --> 00:04:20,820
policy. Well, let's see. We define value

73
00:04:20,910 --> 00:04:36,300
functions Vp (s) to be equal to this. We said that

74
00:04:36,300 --> 00:04:37,300
the value of a policy π from State S was given

75
00:04:39,820 --> 00:04:43,620
by the expected value of the sum of discounted

76
00:04:43,700 --> 00:04:45,630
rewards, conditioned on your executing the

77
00:04:45,740 --> 00:04:51,570
policy ? and you're stating off your robot

78
00:04:51,650 --> 00:04:56,380
to say in the State S, and so our strategy

79
00:04:56,460 --> 00:05:00,360
for finding the policy was sort of comprised of

80
00:05:00,450 --> 00:05:12,490
two steps. So the goal is to find a good

81
00:05:12,570 --> 00:05:14,870
policy that maximizes the suspected value of

82
00:05:14,960 --> 00:05:18,090
the sum of discounted rewards, and so I said

83
00:05:18,180 --> 00:05:20,600
last time that one strategy for finding the

84
00:05:20,690 --> 00:05:23,150
[inaudible] of a policy is to first compute the

85
00:05:23,240 --> 00:05:25,530
optimal value function which I denoted V*(s)

86
00:05:25,610 --> 00:05:28,910
and is defined like that. It's the maximum

87
00:05:29,000 --> 00:05:31,410
value that any policy can obtain, and for

88
00:05:31,520 --> 00:05:48,680
example, the optimal value function for that

89
00:05:48,770 --> 00:06:02,030
MDP looks like this. So in other words,

90
00:06:02,120 --> 00:06:04,910
starting from any of these states, what's the

91
00:06:05,000 --> 00:06:07,880
expected value of the sum of discounted

92
00:06:07,970 --> 00:06:14,140
rewards you get, so this is V*. We also said that

93
00:06:14,240 --> 00:06:17,980
once you've found V*, you can compute the

94
00:06:18,070 --> 00:06:21,560
optimal policy using this.

95
00:06:36,280 --> 00:06:38,210
And so once you've found V*, we can use

96
00:06:42,060 --> 00:06:45,110
the last piece of this algorithm was Bellman's

97
00:06:45,110 --> 00:06:46,110
this equation to find the optimal policy π* and

98
00:06:46,190 --> 00:06:50,170
equations where we know that V*, the

99
00:06:50,240 --> 00:06:53,430
optimal sum of discounted rewards you can get

100
00:06:53,520 --> 00:06:56,140
for State S, is equal to the immediate

101
00:06:56,220 --> 00:06:57,070
reward you get just for

102
00:06:57,170 --> 00:07:00,270
starting off in that state +G

103
00:07:00,350 --> 00:07:04,740
(for the max over all the actions you

104
00:07:04,820 --> 00:07:14,400
could take)(your future sum of discounted

105
00:07:14,480 --> 00:07:18,170
rewards)(your future payoff starting from the

106
00:07:18,250 --> 00:07:20,530
State S(p) which is where you might transition

107
00:07:20,620 --> 00:07:24,970
to after 1(s). And so this gave us a value

108
00:07:25,040 --> 00:07:30,840
iteration algorithm, which was essentially V.I.

109
00:07:30,940 --> 00:07:33,140
I'm abbreviating value iteration as V.I.,

110
00:07:33,230 --> 00:07:35,660
so in the value iteration algorithm, in V.I.,

111
00:07:35,760 --> 00:07:38,030
you just take Bellman's equations

112
00:07:38,090 --> 00:07:39,930
and you repeatedly do this.

113
00:07:49,090 --> 00:07:52,760
So initialize some guess of the value functions.

114
00:07:52,860 --> 00:07:55,320
Initialize a zero as the sum rounding guess and

115
00:07:55,390 --> 00:07:57,520
then repeatedly perform this update for all

116
00:07:57,590 --> 00:07:59,470
states, and I said last time that if you do this

117
00:07:59,550 --> 00:08:03,000
repeatedly, then V(s) will converge to

118
00:08:03,080 --> 00:08:05,230
the optimal value function, V*(s)

119
00:08:05,320 --> 00:08:08,610
and then having found V*(s), you can

120
00:08:08,710 --> 00:08:10,710
compute the optimal policy ?*.

121
00:08:10,800 --> 00:08:16,900
Just one final thing I want to recap was

122
00:08:17,010 --> 00:08:32,910
the policy iteration algorithm in which

123
00:08:33,010 --> 00:08:35,060
we repeat the following two steps.

124
00:08:39,440 --> 00:08:41,770
So let's see, given a random initial policy,

125
00:08:41,860 --> 00:08:44,280
we'll solve for Vp. We'll solve for the value

126
00:08:44,370 --> 00:08:46,480
function for that specific policy.

127
00:08:46,560 --> 00:08:49,980
So this means for every state, compute

128
00:08:50,080 --> 00:08:51,690
the expected sum of discounted rewards

129
00:08:51,800 --> 00:08:54,890
for if you execute the policy ?

130
00:08:55,010 --> 00:08:59,130
from that state, and then the other step

131
00:08:59,220 --> 00:09:32,360
of policy iteration is having

132
00:09:32,440 --> 00:09:35,490
found the value function for your policy,

133
00:09:35,580 --> 00:09:37,670
you then update the policy pretending

134
00:09:37,760 --> 00:09:39,660
that you've already found the optimal

135
00:09:39,720 --> 00:09:42,680
value function, V*, and then you repeatedly

136
00:09:42,780 --> 00:09:44,480
perform these two steps where you solve for

137
00:09:44,560 --> 00:09:46,030
the value function for your current policy

138
00:09:46,130 --> 00:09:48,720
and then pretend that that's actually the optimal

139
00:09:48,800 --> 00:09:51,430
value function and solve for the policy given

140
00:09:51,520 --> 00:09:53,110
the value function, and you repeatedly

141
00:09:53,200 --> 00:09:57,060
update the value function or update the policy

142
00:09:57,150 --> 00:10:01,430
using that value function. And last time I said

143
00:10:01,490 --> 00:10:05,190
that this will also cause the estimated value

144
00:10:05,270 --> 00:10:08,270
function V to converge to V* and this will

145
00:10:08,340 --> 00:10:11,220
cause p to converge to ?*, the optimal policy.

146
00:10:11,340 --> 00:10:16,210
So those are based on our last lecture

147
00:10:16,290 --> 00:10:17,310
starts over MDPs and introduced a lot of new

148
00:10:17,390 --> 00:10:20,860
notation symbols and just summarize all that

149
00:10:20,940 --> 00:10:24,330
again. What I'm about to do now,

150
00:10:24,450 --> 00:10:26,210
what I'm about to do for the rest of today's

151
00:10:26,300 --> 00:10:29,560
lecture is actually build on these two algorithms

152
00:10:29,660 --> 00:10:34,450
so I guess if you have any questions about this

153
00:10:34,550 --> 00:10:37,130
piece, ask now since I've got to go on please.

154
00:10:37,210 --> 00:10:37,770
Yeah.

155
00:10:37,860 --> 00:10:38,750
Student:I don't see how those

156
00:10:38,810 --> 00:10:40,930
two algorithms are very different?

157
00:10:43,930 --> 00:10:47,090
I see, right, so yeah,

158
00:10:47,170 --> 00:10:49,280
do you see that they're different? Okay,

159
00:10:49,370 --> 00:10:52,840
how it's different. Let's see.

160
00:10:52,930 --> 00:10:56,170
So well here's one difference.

161
00:10:56,250 --> 00:10:58,810
I didn't say this 'cause no longer use it today.

162
00:10:58,900 --> 00:11:01,860
So value iteration and policy iteration

163
00:11:01,940 --> 00:11:06,290
are different algorithms. In policy iteration in this

164
00:11:06,370 --> 00:11:09,210
step, you're given a fixed policy,

165
00:11:09,290 --> 00:11:12,890
and you're going to solve for the value function

166
00:11:12,980 --> 00:11:18,450
for that policy and so you're given some fixed

167
00:11:18,540 --> 00:11:22,330
policy ?, meaning some function mapping from

168
00:11:22,400 --> 00:11:25,410
the state's actions. So give you some policy and

169
00:11:31,260 --> 00:11:36,320
whatever. That's just some policy; it's not

170
00:11:36,420 --> 00:11:41,950
a great policy. And in that step that I circled,

171
00:11:42,050 --> 00:12:02,010
we have to find the ? of S which means that

172
00:12:02,110 --> 00:12:04,260
for every state you need to compute

173
00:12:04,360 --> 00:12:06,860
your expected sum of discounted rewards

174
00:12:06,970 --> 00:12:09,530
or if you execute this specific policy

175
00:12:09,640 --> 00:12:13,790
and starting off the MDP in that state S.

176
00:12:13,890 --> 00:12:17,810
So I showed this last time. I won't go into

177
00:12:17,890 --> 00:12:20,640
details today, so I said last time that you can

178
00:12:20,730 --> 00:12:23,470
actually solve for Vp by solving a linear system

179
00:12:23,570 --> 00:12:26,300
of equations. There was a form of

180
00:12:26,380 --> 00:12:29,770
Bellman's equations for Vp, and it turned out to

181
00:12:29,870 --> 00:12:32,480
be, if you write this out, you end up with

182
00:12:32,550 --> 00:12:34,890
a linear system of 11 equations of 11 unknowns

183
00:12:34,980 --> 00:12:37,430
and so you can actually solve for the

184
00:12:37,520 --> 00:12:41,460
value function for a fixed policy by solving like

185
00:12:41,520 --> 00:12:43,360
a system of linear equations with 11

186
00:12:43,430 --> 00:12:49,010
variables and 11 constraints, and so that's

187
00:12:49,090 --> 00:12:52,820
policy iteration; whereas, in value iteration,

188
00:12:52,900 --> 00:12:57,470
going back on board, in value iteration

189
00:12:57,570 --> 00:13:00,610
you sort of repeatedly perform this update where

190
00:13:00,710 --> 00:13:02,090
you update the value of a state as the

191
00:13:02,180 --> 00:13:05,550
[inaudible]. So I hope that makes sense that the

192
00:13:05,640 --> 00:13:07,020
algorithm of these is different.

193
00:13:10,880 --> 00:13:12,320
Student: In this occasion, on the atomic kits

194
00:13:12,420 --> 00:13:13,670
so is the assumption

195
00:13:13,730 --> 00:13:15,560
that we can never get out of those states?

196
00:13:15,640 --> 00:13:20,000
Yes. There's always things that you where you

197
00:13:20,080 --> 00:13:21,560
solve for this [inaudible], for example,

198
00:13:21,640 --> 00:13:22,830
and make the numbers come up nicely,

199
00:13:22,900 --> 00:13:24,440
but I don't wanna spend too much time

200
00:13:24,520 --> 00:13:26,300
on them, but yeah, so the assumption is

201
00:13:26,390 --> 00:13:28,290
that once you enter the absorbing state,

202
00:13:28,400 --> 00:13:30,530
then the world ends or there're no more

203
00:13:30,610 --> 00:13:33,120
rewards after that and you can think of

204
00:13:33,200 --> 00:13:35,420
another way to think of the absorbing states

205
00:13:35,500 --> 00:13:37,700
which is sort of mathematically equivalent.

206
00:13:37,800 --> 00:13:39,690
You can think of the absorbing states as

207
00:13:39,780 --> 00:13:44,680
transitioning with probability 1 to sum 12 state,

208
00:13:44,770 --> 00:13:47,900
and then once you're in that 12th state, you

209
00:13:47,980 --> 00:13:50,630
always remain in that 12th state, and there're

210
00:13:50,710 --> 00:13:52,740
no further rewards from there. If you want, you

211
00:13:52,850 --> 00:13:55,120
can think of this as actually an MDP with

212
00:13:55,210 --> 00:13:58,590
12 states rather than 11 states, and the 12th state

213
00:13:58,680 --> 00:14:01,920
is this zero cost absorbing state that you

214
00:14:02,000 --> 00:14:03,220
get stuck in forever.

215
00:14:03,320 --> 00:14:05,680
Other questions?

216
00:14:05,760 --> 00:14:09,500
Yeah, please go.

217
00:14:09,580 --> 00:14:11,250
Student:Where did the Bellman's equations

218
00:14:11,350 --> 00:14:16,320
[inaudible] to optimal value [inaudible]?

219
00:14:16,430 --> 00:14:22,560
Boy, yeah. Okay, this Bellman's equations,

220
00:14:22,650 --> 00:14:27,640
this equation that I'm pointing to, I sort of tried

221
00:14:27,750 --> 00:14:32,440
to give it justification for this last time. I'll say

222
00:14:35,070 --> 00:14:39,720
it in one sentence so that's that the expected

223
00:14:39,820 --> 00:14:42,870
total payoff I get, I expect to get something

224
00:14:42,970 --> 00:14:44,800
from the state as is equal to my immediate

225
00:14:44,900 --> 00:14:48,330
reward which is the reward I get for starting a

226
00:14:48,430 --> 00:14:52,020
state. Let's see. If I sum the state, I'm gonna get

227
00:14:52,110 --> 00:14:54,430
some first reward and then I can transition to

228
00:14:54,510 --> 00:14:56,780
some other state, and then from that other state,

229
00:14:56,860 --> 00:14:58,210
I'll get some additional rewards from then.

230
00:14:58,320 --> 00:15:01,400
So Bellman's equations breaks that sum

231
00:15:01,500 --> 00:15:03,710
into two pieces. It says the value of a state

232
00:15:03,800 --> 00:15:06,360
is equal to the reward you get right away is

233
00:15:06,450 --> 00:15:17,600
really, well. V*(s) is really equal to +G,

234
00:15:31,190 --> 00:15:34,570
so this is V*(s) is, and so Bellman's equations

235
00:15:34,680 --> 00:15:38,030
sort of breaks V* into two terms and says that

236
00:15:38,110 --> 00:15:40,990
there's this first term which is the immediate

237
00:15:41,070 --> 00:15:45,120
reward, that, and then +G(the rewards you

238
00:15:45,200 --> 00:15:47,640
get in the future) which it turns out

239
00:15:47,750 --> 00:15:49,640
to be equal to that second row.

240
00:15:49,760 --> 00:15:53,510
I spent more time justifying this in the previous

241
00:15:53,600 --> 00:15:56,200
lecture, although yeah, hopefully,

242
00:15:56,290 --> 00:15:58,720
for the purposes of this lecture,

243
00:15:58,790 --> 00:16:00,160
if you're not sure where this is came,

244
00:16:00,240 --> 00:16:01,250
if you don't remember the justification of that,

245
00:16:01,340 --> 00:16:03,030
why don't you just maybe take my word for

246
00:16:03,110 --> 00:16:06,620
that this equation holds true since I use it

247
00:16:06,690 --> 00:16:09,870
a little bit later as well, and then the lecture notes

248
00:16:09,990 --> 00:16:13,250
sort of explain a little further the justification

249
00:16:13,340 --> 00:16:14,850
for why this equation might hold true.

250
00:16:14,930 --> 00:16:18,650
But for now, yeah, just for

251
00:16:18,740 --> 00:16:20,320
now take my word for it

252
00:16:20,410 --> 00:16:21,250
that this holds true

253
00:16:21,340 --> 00:16:23,430
cause we'll use it a little bit later today as well.

254
00:16:25,140 --> 00:16:35,170
Student:[Inaudible] and would it be in sort of

255
00:16:35,260 --> 00:16:40,440
turn back into [inaudible].

256
00:16:40,550 --> 00:16:42,600
Actually, [inaudible] right question

257
00:16:42,690 --> 00:16:44,340
is if in policy iteration if

258
00:16:44,420 --> 00:16:50,000
we represent ? implicitly, using V(s),

259
00:16:50,100 --> 00:16:52,250
would it become equivalent to valuation,

260
00:16:52,350 --> 00:16:55,690
and the answer is sort of no.

261
00:16:55,800 --> 00:17:00,030
Let's see. It's true that policy iteration

262
00:17:00,150 --> 00:17:01,980
and value iteration are closely

263
00:17:02,080 --> 00:17:03,770
related algorithms, and there's actually

264
00:17:03,870 --> 00:17:08,560
a continuum between them, but yeah, it actually

265
00:17:08,650 --> 00:17:25,430
turns out that, oh, no, the algorithms are not

266
00:17:26,000 --> 00:17:27,230
equivalent. It's just in policy iteration,

267
00:17:27,310 --> 00:17:30,160
there is a step where you're solving

268
00:17:30,260 --> 00:17:32,420
for the value function for the policy vehicle

269
00:17:32,510 --> 00:17:36,140
is V, solve for Vp. Usually, you can do this,

270
00:17:36,230 --> 00:17:40,530
for instance, by solving a linear system of

271
00:17:40,620 --> 00:17:42,800
equations. In value iteration, it is a different

272
00:17:42,880 --> 00:17:45,160
algorithm, yes. I hope it makes sense that at

273
00:17:45,230 --> 00:17:47,160
least cosmetically it's different.

274
00:17:47,270 --> 00:17:52,030
Student:[Inaudible] you have [inaudible]

275
00:17:52,110 --> 00:17:53,080
representing ? implicitly, then you won't have

276
00:17:53,150 --> 00:17:55,220
to solve that to [inaudible] equations.

277
00:17:57,850 --> 00:18:03,540
Yeah, the problem is - let's see. To solve for Vp,

278
00:18:03,640 --> 00:18:07,240
this works only if you have a fixed policy, so

279
00:18:07,330 --> 00:18:09,590
once you change a value function, if ? changes

280
00:18:09,670 --> 00:18:12,810
as well, then it's sort of hard to solve this.

281
00:18:12,900 --> 00:18:17,130
Yeah, so later on we'll actually talk about some

282
00:18:17,520 --> 00:18:19,650
examples of when ? is implicitly represented

283
00:18:19,770 --> 00:18:22,680
but at least for now it's I think there's

284
00:18:22,770 --> 00:18:27,900
yeah. Maybe there's a way to redefine

285
00:18:27,980 --> 00:18:29,880
something, see a mapping onto value iteration

286
00:18:29,960 --> 00:18:31,210
but that's not usually done.

287
00:18:31,320 --> 00:18:33,640
These are viewed as different algorithms.

288
00:18:37,090 --> 00:18:40,230
Okay, cool, so all good questions. Let me move

289
00:18:40,320 --> 00:18:44,690
on and talk about how to generalize these

290
00:18:44,790 --> 00:18:59,320
ideas to continuous states. Everything we've

291
00:18:59,410 --> 00:19:04,490
done so far has been for discrete states or

292
00:19:04,590 --> 00:19:06,930
finite-state MDPs. Where, for example, here we

293
00:19:07,020 --> 00:19:09,550
had an MDP with a finite set of 11 states

294
00:19:09,640 --> 00:19:15,810
and so the value function or V(s) or our

295
00:19:15,890 --> 00:19:17,820
estimate for the value function, V(s), could then

296
00:19:17,900 --> 00:19:20,480
be represented using an array of 11 numbers

297
00:19:20,560 --> 00:19:22,590
'cause if you have 11 states, the value

298
00:19:22,680 --> 00:19:26,680
function needs to assign a real number to each

299
00:19:26,770 --> 00:19:28,710
of the 11 states and so to represent V(s)

300
00:19:28,800 --> 00:19:33,440
using an array of 11 numbers. What I want to

301
00:19:33,500 --> 00:19:36,640
do for [inaudible] today is talk about

302
00:19:36,720 --> 00:19:43,290
continuous states, so for example, if you want

303
00:19:43,370 --> 00:19:47,290
to control any of the number of real

304
00:19:47,410 --> 00:19:50,410
[inaudible], so for example, if you want to

305
00:19:50,480 --> 00:19:56,240
control a car, a car is positioned given by

306
00:19:56,320 --> 00:20:01,070
XYT, as position and orientation and if you

307
00:20:01,150 --> 00:20:03,770
want to Markov the velocity as well, then

308
00:20:03,870 --> 00:20:06,970
Xdot, Ydot, Tdot, so these are so depending on

309
00:20:07,050 --> 00:20:08,130
whether you want to model the

310
00:20:08,220 --> 00:20:10,370
kinematics and so just position, or whether you

311
00:20:10,450 --> 00:20:12,120
want to model the dynamics, meaning the

312
00:20:12,190 --> 00:20:13,960
velocity as well.

313
00:20:14,060 --> 00:20:20,050
Earlier I showed you video of a helicopter that

314
00:20:20,140 --> 00:20:21,580
was flying, using a rain forest we're

315
00:20:21,670 --> 00:20:24,450
learning algorithms, so the helicopter which can

316
00:20:24,540 --> 00:20:26,450
fly in three-dimensional space rather

317
00:20:26,540 --> 00:20:29,550
than just drive on the 2-D plane, the state will

318
00:20:29,630 --> 00:20:35,380
be given by XYZ position, FT?, which is

319
00:20:35,480 --> 00:20:47,210
?[inaudible]. The FT? is sometimes used to note

320
00:20:47,300 --> 00:20:50,540
the P[inaudible] of the helicopter, just

321
00:20:50,630 --> 00:21:00,080
orientation, and if you want to control a

322
00:21:00,170 --> 00:21:02,350
helicopter, you pretty much have to model

323
00:21:02,440 --> 00:21:04,330
velocity as well which means both linear

324
00:21:04,470 --> 00:21:07,390
velocity as well as angular velocity, and so this

325
00:21:07,480 --> 00:21:09,200
would be a 12-dimensional state.

326
00:21:09,300 --> 00:21:15,560
If you want an example that is kind of fun but

327
00:21:15,670 --> 00:21:23,830
unusual is, and I'm just gonna use this as

328
00:21:23,930 --> 00:21:25,840
an example and actually use this little bit

329
00:21:25,930 --> 00:21:29,240
example in today's lecture is the inverted

330
00:21:29,330 --> 00:21:31,940
pendulum problem which is sort of a

331
00:21:32,030 --> 00:21:35,040
long-running classic in reinforcement learning

332
00:21:35,130 --> 00:21:42,260
in which imagine that you have a little cart

333
00:21:42,340 --> 00:21:44,840
that's on a rail. The rail ends at some point and

334
00:21:44,930 --> 00:21:48,570
if you imagine that you have a pole attached to

335
00:21:48,680 --> 00:21:51,060
the cart, and this is a free hinge and so the

336
00:21:51,140 --> 00:21:55,100
pole here can rotate freely, and your goal is to

337
00:21:55,270 --> 00:21:58,090
control the cart and to move it back and

338
00:21:58,200 --> 00:21:59,920
forth on this rail so as to keep the pole

339
00:22:00,040 --> 00:22:03,880
balanced. there's no long pole in this class

340
00:22:03,960 --> 00:22:06,700
but you know what I mean, so you can imagine.

341
00:22:06,770 --> 00:22:09,390
Oh, is there a long pole here?

342
00:22:09,490 --> 00:22:10,740
Student:Back in the corner.

343
00:22:11,030 --> 00:22:17,020
Oh, thanks. Cool. So I did not practice this but

344
00:22:17,220 --> 00:22:18,360
you can take a long pole

345
00:22:18,440 --> 00:22:20,150
and sort of hold it up,

346
00:22:20,230 --> 00:22:22,350
balance, so imagine that you can do it better

347
00:22:22,410 --> 00:22:24,650
than I can. Imagine these are [inaudible] just

348
00:22:24,700 --> 00:22:26,090
moving back and forth to try to keep the pole

349
00:22:26,170 --> 00:22:28,810
balanced, so you can actually us the

350
00:22:28,870 --> 00:22:30,090
reinforcement learning algorithm to do that.

351
00:22:30,160 --> 00:22:35,870
problems that people [inaudible] implement and

352
00:22:35,950 --> 00:22:37,170
play off using reinforcement learning

353
00:22:37,250 --> 00:22:41,250
algorithms, and so for this, the states would be

354
00:22:41,330 --> 00:22:45,570
X and T, so X would be the position of the cart,

355
00:22:45,650 --> 00:22:51,930
and T would be the orientation of the pole and

356
00:22:52,010 --> 00:22:59,420
also the linear velocity and the angular velocity

357
00:22:59,480 --> 00:23:02,190
of the pole, so I'll actually use this example a

358
00:23:02,390 --> 00:23:03,730
couple times.

359
00:23:15,570 --> 00:23:19,160
So to read continuous state space, how can you

360
00:23:19,250 --> 00:23:21,230
apply an algorithm like value iteration

361
00:23:21,320 --> 00:23:23,610
and policy iteration to solve the MDP to control

362
00:23:23,710 --> 00:23:26,080
like the car or a helicopter or something

363
00:23:26,160 --> 00:23:30,010
like the inverted pendulum? So one thing you

364
00:23:30,100 --> 00:23:32,250
can do and this is maybe the most

365
00:23:32,350 --> 00:23:35,980
straightforward thing is, if you have say a

366
00:23:36,060 --> 00:23:39,270
two-dimensional continuous state space, a S-1

367
00:23:39,330 --> 00:23:42,360
and S-2 are my state variables, and in all the

368
00:23:42,440 --> 00:23:44,260
examples there are I guess between

369
00:23:44,350 --> 00:23:45,910
4dimensional to 12-dimensional

370
00:23:46,010 --> 00:23:47,360
I'll just draw 2-D here.

371
00:23:47,450 --> 00:23:49,830
The most straightforward thing to

372
00:23:49,910 --> 00:23:51,260
do would be to take the continuous state space

373
00:23:51,360 --> 00:23:57,110
and discretize it into a number of discrete cells.

374
00:24:07,570 --> 00:24:10,620
And I use S-bar to denote

375
00:24:10,730 --> 00:24:12,130
they're discretized or

376
00:24:12,180 --> 00:24:13,790
they're discrete states, and so you can

377
00:24:13,870 --> 00:24:15,490
[inaudible] with this continuous state problem

378
00:24:15,570 --> 00:24:17,960
with a finite or discrete set of states and

379
00:24:18,050 --> 00:24:21,490
then you can use policy iteration or value

380
00:24:21,570 --> 00:24:32,730
iteration to solve for V*(s)-bar and ?*(s)-bar.

381
00:24:32,810 --> 00:24:40,440
And if you're robot is then in some state given

382
00:24:40,530 --> 00:24:43,360
by that dot, you would then figure out

383
00:24:43,440 --> 00:24:45,610
what discretized state it is in. In this case it's in,

384
00:24:45,730 --> 00:24:49,820
this discretized dygrid cell that's called

385
00:24:49,910 --> 00:24:53,870
S-bar, and then you execute. You choose the

386
00:24:53,950 --> 00:24:56,480
policy. You choose the action given by

387
00:24:56,570 --> 00:24:59,900
applied to that discrete state, so discretization is

388
00:24:59,970 --> 00:25:02,150
maybe the most straightforward way to

389
00:25:02,250 --> 00:25:04,130
turn a continuous state problem

390
00:25:04,240 --> 00:25:05,890
into a discrete state problem.

391
00:25:05,990 --> 00:25:08,200
Sometimes you can sorta make

392
00:25:08,290 --> 00:25:11,080
this work but a couple of reasons why this does

393
00:25:11,170 --> 00:25:14,440
not work very well. One reason is the

394
00:25:14,530 --> 00:25:19,420
following, and for this picture, let's even

395
00:25:19,520 --> 00:25:21,480
temporarily put aside reinforcement learning.

396
00:25:21,570 --> 00:25:25,140
Let's just think about doing regression for now

397
00:25:25,240 --> 00:25:29,810
and so suppose you have some invariable X and

398
00:25:29,900 --> 00:25:37,170
suppose I have some data, and I want to fill a

399
00:25:37,270 --> 00:25:39,090
function. Y is the function of X,

400
00:25:39,200 --> 00:25:43,110
so discretization is saying that I'm going to take

401
00:25:43,210 --> 00:25:47,550
my horizontal Xs and chop it up into a number

402
00:25:47,610 --> 00:25:50,630
of intervals. Sometimes I call these intervals

403
00:25:50,720 --> 00:25:54,170
buckets as well. We chop my horizontal Xs up

404
00:25:54,260 --> 00:25:56,010
into a number of buckets and then

405
00:25:56,090 --> 00:25:59,810
we're approximate this function using

406
00:25:59,890 --> 00:26:03,900
something that's piecewise constant in each of

407
00:26:03,970 --> 00:26:06,120
these buckets. And just look at this.

408
00:26:06,200 --> 00:26:07,970
This is clearly not a very good representation,

409
00:26:08,050 --> 00:26:09,810
right, and when we talk about regression,

410
00:26:09,910 --> 00:26:14,820
you just choose some features of X and run

411
00:26:14,900 --> 00:26:16,320
linear regression or something. You get a

412
00:26:16,410 --> 00:26:18,520
much better fit to the function. And so the sense

413
00:26:18,620 --> 00:26:21,420
that discretization just isn't a very good

414
00:26:21,520 --> 00:26:24,930
source of piecewise constant functions. This

415
00:26:25,020 --> 00:26:26,620
just isn't a very good function for

416
00:26:26,700 --> 00:26:29,300
representing many things, and there's also the

417
00:26:29,390 --> 00:26:32,760
sense that there's no smoothing or there's

418
00:26:32,840 --> 00:26:35,140
no generalization across the different buckets.

419
00:26:35,230 --> 00:26:37,550
And in fact, back in regression, I would

420
00:26:37,640 --> 00:26:40,910
never have chosen to do regression using this

421
00:26:40,990 --> 00:26:43,140
sort of visualization. It's just really doesn't

422
00:26:43,230 --> 00:26:44,640
make sense.

423
00:26:44,720 --> 00:26:51,640
And so in the same way, instead of X, V(s),

424
00:26:51,750 --> 00:26:55,140
instead of X and some hypothesis function of

425
00:26:55,240 --> 00:26:56,930
X, if you have the state here and you're trying

426
00:26:57,010 --> 00:26:58,380
to approximate the value function, then

427
00:26:58,470 --> 00:27:00,800
you can get discretization to work for many

428
00:27:00,900 --> 00:27:02,560
problems but maybe this isn't the best

429
00:27:02,670 --> 00:27:04,620
representation to represent a value function.

430
00:27:04,750 --> 00:27:11,070
The other problem with discretization and

431
00:27:11,160 --> 00:27:17,460
maybe the more serious problem is what's often

432
00:27:17,560 --> 00:27:19,620
somewhat fancifully called the curse of

433
00:27:19,720 --> 00:27:28,180
dimensionality. And just the observation that if

434
00:27:28,280 --> 00:27:34,850
the state space is in RN, and if you

435
00:27:36,030 --> 00:27:40,590
discretize each variable into K buckets, so if

436
00:27:40,680 --> 00:27:46,030
discretize each variable into K discrete

437
00:27:46,130 --> 00:27:51,580
values, then you get on the order of K to the

438
00:27:51,680 --> 00:27:56,200
power of N discrete states. In other words,

439
00:27:56,310 --> 00:27:58,550
the number of discrete states you end up with

440
00:27:58,630 --> 00:28:03,170
grows exponentially in the dimension of

441
00:28:03,250 --> 00:28:06,830
the problem, and so for a helicopter with

442
00:28:06,930 --> 00:28:09,610
12-dimensional state space, this would be

443
00:28:09,670 --> 00:28:12,720
maybe like 100 to the power of 12, just huge,

444
00:28:12,810 --> 00:28:15,630
and it's not feasible. And so discretization

445
00:28:15,720 --> 00:28:18,160
doesn't scale well at all with two problems in

446
00:28:18,230 --> 00:28:21,500
high-dimensional state spaces, and this

447
00:28:23,490 --> 00:28:27,750
observation actually applies more generally

448
00:28:27,830 --> 00:28:30,680
than to just robotics and continuous state

449
00:28:30,770 --> 00:28:34,060
problems. For example, another fairly

450
00:28:34,140 --> 00:28:38,210
well-known applications of reinforcement

451
00:28:38,280 --> 00:28:42,200
learning has been to factory automations.

452
00:28:42,310 --> 00:28:45,330
If you imagine that you have 20 machines

453
00:28:45,440 --> 00:28:48,040
sitting in the factory and the machines

454
00:28:48,130 --> 00:28:49,420
lie in a assembly line

455
00:28:49,510 --> 00:28:50,970
and they all do something to

456
00:28:51,040 --> 00:28:52,760
a part on the assembly line, then they route

457
00:28:52,850 --> 00:28:54,240
the part onto a different machine.

458
00:28:54,320 --> 00:28:55,640
You want to use

459
00:28:55,740 --> 00:28:56,980
reinforcement learning algorithms,

460
00:28:57,080 --> 00:28:58,840
[inaudible] the order in which the

461
00:28:58,940 --> 00:29:00,170
different machines operate

462
00:29:00,270 --> 00:29:01,840
on your different things that are flowing

463
00:29:01,920 --> 00:29:03,250
through your assembly line and maybe

464
00:29:03,340 --> 00:29:04,800
different machines can do different things.

465
00:29:04,900 --> 00:29:07,560
So if you have N machines and

466
00:29:07,650 --> 00:29:10,040
each machine can be in K states,

467
00:29:10,130 --> 00:29:12,200
then if you do this sort of discretization,

468
00:29:12,310 --> 00:29:13,700
the total number of

469
00:29:13,780 --> 00:29:15,010
states would be K to N as well.

470
00:29:15,130 --> 00:29:17,580
If you have N machines

471
00:29:17,660 --> 00:29:20,160
and if each machine can be in K states,

472
00:29:20,260 --> 00:29:22,620
then again, you can get this huge number

473
00:29:22,750 --> 00:29:26,450
of states. Other well-known examples would be

474
00:29:26,560 --> 00:29:30,020
if you have a board game is another example.

475
00:29:30,120 --> 00:29:32,400
You'd want to use reinforcement learning

476
00:29:32,480 --> 00:29:36,590
to play chess. Then if you have N pieces on

477
00:29:36,680 --> 00:29:39,620
your board game, you have N pieces on the

478
00:29:39,750 --> 00:29:41,870
chessboard and if each piece can be in K

479
00:29:41,950 --> 00:29:44,370
positions, then this is a game sort of

480
00:29:44,470 --> 00:29:46,750
the curse of dimensionality thing where the number of

481
00:29:46,820 --> 00:29:48,800
discrete states you end up with goes

482
00:29:48,900 --> 00:29:50,630
exponentially with the number of pieces in your

483
00:29:50,710 --> 00:29:57,850
board game. So the curse of

484
00:29:57,960 --> 00:30:03,490
dimensionality means that discretization scales

485
00:30:03,590 --> 00:30:05,810
poorly to high-dimensional state spaces or

486
00:30:05,880 --> 00:30:08,220
at least discrete representations scale poorly to

487
00:30:08,300 --> 00:30:10,990
high-dimensional state spaces. In practice,

488
00:30:11,050 --> 00:30:13,640
discretization will usually, if you have a

489
00:30:13,730 --> 00:30:15,120
2-dimensional problem, discretization will

490
00:30:15,190 --> 00:30:18,000
usually work great. If you have a 3-dimensional

491
00:30:18,110 --> 00:30:20,170
problem, you can often get discretization

492
00:30:20,270 --> 00:30:22,840
to work not too badly without too much trouble.

493
00:30:22,910 --> 00:30:25,990
With a 4-dimensional problem, you can

494
00:30:26,070 --> 00:30:28,870
still often get to where that they could be

495
00:30:28,960 --> 00:30:34,310
challenging and as you go to higher and higher

496
00:30:34,400 --> 00:30:39,390
dimensional state spaces, the odds and

497
00:30:39,480 --> 00:30:41,550
[inaudible] that you need to figure around to

498
00:30:41,640 --> 00:30:44,080
discretization and do things like non-uniform

499
00:30:44,190 --> 00:30:50,250
grids, so for example, what I've drawn for

500
00:30:50,340 --> 00:30:51,610
you is an example of a non-uniform

501
00:30:51,700 --> 00:30:54,010
discretization where I'm discretizing S-2

502
00:30:54,110 --> 00:30:55,960
much more finally than S-1.

503
00:30:56,040 --> 00:30:57,430
If I think the value function is

504
00:30:57,510 --> 00:31:01,020
much more sensitive to the value of state

505
00:31:01,100 --> 00:31:03,570
variable S-2 than to S-1, and so as you get into

506
00:31:03,660 --> 00:31:05,060
higher dimensional state spaces, you may

507
00:31:05,140 --> 00:31:07,630
need to manually fiddle with choices like these

508
00:31:07,720 --> 00:31:09,450
with no uniform discretizations and so on.

509
00:31:09,550 --> 00:31:11,860
But the folk wisdom seems to be that if you

510
00:31:11,950 --> 00:31:14,290
have 2- or 3-dimensional problems, it work

511
00:31:14,370 --> 00:31:16,130
fine. With 4-dimensional problems, you can

512
00:31:16,240 --> 00:31:17,960
probably get it to work but it'd be just

513
00:31:18,050 --> 00:31:21,700
slightly challenging and you can sometimes by

514
00:31:21,810 --> 00:31:23,930
fooling around and being clever, you can

515
00:31:24,020 --> 00:31:26,280
often push discretization up to let's say about

516
00:31:26,380 --> 00:31:27,790
6-dimensional problems but with some

517
00:31:27,880 --> 00:31:31,880
difficulty and problems higher than

518
00:31:31,950 --> 00:31:34,870
6-dimensional would be extremely difficult

519
00:31:34,960 --> 00:31:38,450
to solve with discretization. So that's just rough

520
00:31:38,560 --> 00:31:41,790
folk wisdom order of managing problems you

521
00:31:41,880 --> 00:31:43,560
think about using for discretization.

522
00:31:50,380 --> 00:31:53,630
But what I want to spend most of today talking

523
00:31:53,720 --> 00:31:57,060
about is [inaudible] methods that

524
00:31:57,150 --> 00:32:00,700
often work much better than discretization and which we

525
00:32:00,800 --> 00:32:05,240
will approximate V* directly without resulting

526
00:32:05,330 --> 00:32:09,420
to these sort of discretizations. Before I jump

527
00:32:09,510 --> 00:32:11,140
to the specific representation let me just spend

528
00:32:11,220 --> 00:32:13,270
a few minutes talking about the problem

529
00:32:13,370 --> 00:32:17,250
setup then. For today's lecture, I'm going to

530
00:32:17,330 --> 00:32:20,750
focus on the problem of continuous states

531
00:32:20,830 --> 00:32:29,220
and just to keep things sort of very simple in

532
00:32:29,300 --> 00:32:31,330
this lecture, I want view of continuous

533
00:32:31,430 --> 00:32:36,990
actions, so I'm gonna see discrete actions A.

534
00:32:37,070 --> 00:32:40,860
So it turns out actually that is a critical fact

535
00:32:40,950 --> 00:32:43,770
also for many problems, it turns out that

536
00:32:43,870 --> 00:32:48,710
the state space is much larger than the states of

537
00:32:48,810 --> 00:32:50,670
actions. That just seems to have worked out that

538
00:32:50,760 --> 00:32:51,790
way for many problems, so for example,

539
00:32:51,910 --> 00:32:55,030
for driving a car the state space is

540
00:32:55,130 --> 00:32:59,630
6-dimensional, so if XY T, Xdot, Ydot, Tdot.

541
00:32:59,710 --> 00:33:04,280
Whereas, your action has, you still have two

542
00:33:04,360 --> 00:33:05,630
actions. You have forward backward motion

543
00:33:05,710 --> 00:33:08,550
and steering the car, so you have 6-D states and

544
00:33:08,640 --> 00:33:10,720
2-D actions, and so you can discretize the

545
00:33:10,800 --> 00:33:12,690
action much more easily than discretize the

546
00:33:12,760 --> 00:33:15,410
states. The only examples down for a

547
00:33:15,510 --> 00:33:17,350
helicopter you've 12-D states in a

548
00:33:17,460 --> 00:33:19,260
4-dimensional action it turns out,

549
00:33:19,360 --> 00:33:21,810
and it's also often much easier

550
00:33:21,890 --> 00:33:24,360
to just discretize a continuous

551
00:33:24,460 --> 00:33:28,240
actions into a discrete sum of actions. And for

552
00:33:28,350 --> 00:33:30,180
the inverted pendulum, you have a 4-D state

553
00:33:30,260 --> 00:33:32,770
and a 1-D action. Whether you accelerate

554
00:33:32,870 --> 00:33:36,740
your cart to the left or the right is one D action

555
00:33:36,820 --> 00:33:40,590
and so for the rest of today, I'm gonna

556
00:33:40,700 --> 00:33:42,790
assume a continuous state but I'll assume that

557
00:33:42,870 --> 00:33:44,530
maybe you've already discretized your

558
00:33:44,620 --> 00:33:47,150
actions, just because in practice it turns out that

559
00:33:47,210 --> 00:33:49,040
not for all problems, with many problems

560
00:33:49,120 --> 00:33:54,070
large actions is just less of a difficulty than

561
00:33:54,140 --> 00:34:00,910
large state spaces. So I'm going to assume

562
00:34:01,000 --> 00:34:15,580
that we have a model or simulator of the MDP,

563
00:34:15,670 --> 00:34:18,900
and so this is really an assumption on

564
00:34:18,980 --> 00:34:21,390
how the state transition probabilities are

565
00:34:21,470 --> 00:34:25,080
represented. I'm gonna assume and I'm going

566
00:34:25,140 --> 00:34:27,370
to use the terms "model" and "simulator"

567
00:34:27,450 --> 00:34:28,950
pretty much synonymously,

568
00:34:29,040 --> 00:34:33,070
so specifically, what I'm going to assume

569
00:34:33,160 --> 00:34:34,770
is that we have a black

570
00:34:34,850 --> 00:34:42,820
box and a piece of code, so that I can input

571
00:34:42,900 --> 00:34:46,970
any state, input an action and it will output S

572
00:34:47,050 --> 00:34:52,580
prime, sample from the state transition

573
00:34:52,650 --> 00:34:55,370
distribution. Says that this is really my

574
00:34:55,430 --> 00:34:59,770
assumption on the representation I have for the

575
00:34:59,860 --> 00:35:02,960
state transition probabilities, so I'll assume I

576
00:35:03,010 --> 00:35:06,210
have a box that read take us in for the stated

577
00:35:06,290 --> 00:35:10,210
action and output in mixed state. And so since

578
00:35:17,590 --> 00:35:19,560
they're fairly common ways to get models

579
00:35:19,640 --> 00:35:21,830
of different MDPs you may work with, one is

580
00:35:21,940 --> 00:35:26,370
you might get a model from a physics

581
00:35:26,470 --> 00:35:33,650
simulator. So for example, if you're interested

582
00:35:33,720 --> 00:35:37,450
in controlling that inverted pendulum, so

583
00:35:37,530 --> 00:35:42,340
your action is A which is the magnitude of the

584
00:35:42,440 --> 00:35:45,120
force you exert on the cart to left or right,

585
00:35:45,230 --> 00:35:51,000
and your state is Xdot, T, Tdot. I'm just gonna

586
00:35:51,090 --> 00:36:01,640
write that in that order. And so I'm gonna

587
00:36:01,720 --> 00:36:03,070
write down a bunch of equations just for

588
00:36:03,150 --> 00:36:05,120
completeness but everything I'm gonna write

589
00:36:05,200 --> 00:36:06,990
below here is most of what I wanna write is a

590
00:36:07,060 --> 00:36:10,700
bit gratuitous, but so since I'll maybe flip

591
00:36:10,790 --> 00:36:12,490
open a textbook on physics, a textbook on

592
00:36:12,580 --> 00:36:14,020
mechanics, you can work out the equations of

593
00:36:14,110 --> 00:36:17,330
motion of a physical device like this, so you

594
00:36:17,440 --> 00:36:23,730
find that Sdot. The dot denotes derivative, so

595
00:36:23,800 --> 00:36:26,600
the derivative of the state with respect to time is

596
00:36:26,690 --> 00:36:34,970
given by Xdot, ?-L(B) cost B over M

597
00:36:35,060 --> 00:36:56,190
Tdot, B. And so on where A is the force is the

598
00:36:56,270 --> 00:36:58,440
action that you exert on the cart. L is the

599
00:36:58,540 --> 00:37:01,520
length of the pole. M is the total mass of the

600
00:37:01,590 --> 00:37:04,560
system and so on. So all these equations are

601
00:37:04,650 --> 00:37:06,010
good uses, just writing them down for

602
00:37:06,080 --> 00:37:10,500
completeness, but by flipping over, open like a

603
00:37:10,580 --> 00:37:12,250
physics textbook, you can work out these

604
00:37:12,320 --> 00:37:14,640
equations and notions yourself and this then

605
00:37:14,700 --> 00:37:19,200
gives you a model which can say that S-2+1.

606
00:37:19,270 --> 00:37:22,280
You're still one time step later will be equal

607
00:37:22,340 --> 00:37:29,160
to your previous state plus [inaudible], so in

608
00:37:29,230 --> 00:37:33,620
your simulator or in my model what happens

609
00:37:33,690 --> 00:37:36,790
to the cart every 10th of a second, so delta T would

610
00:37:36,840 --> 00:37:44,480
be within one second and then so plus delta T

611
00:37:44,550 --> 00:38:04,510
times that. And so that'd be one way to come

612
00:38:04,620 --> 00:38:08,530
up with a model of your MDP. And in this

613
00:38:08,610 --> 00:38:11,130
specific example, I've actually written down

614
00:38:11,210 --> 00:38:13,210
deterministic model because and by

615
00:38:13,280 --> 00:38:15,850
deterministic I mean that given an action in a

616
00:38:15,930 --> 00:38:20,400
state, the next state is not random, so would

617
00:38:20,480 --> 00:38:23,950
be an example of a deterministic model where I

618
00:38:24,020 --> 00:38:26,630
can compute the next state exactly as a

619
00:38:26,700 --> 00:38:28,660
function of the previous state and the previous

620
00:38:28,730 --> 00:38:31,920
action or it's a deterministic model

621
00:38:31,980 --> 00:38:34,230
because all the probability mass is on a single

622
00:38:34,320 --> 00:38:37,530
state given the previous stated action. You

623
00:38:37,610 --> 00:38:59,710
can also make this a stochastic model. A second

624
00:38:59,780 --> 00:39:01,960
way that is often used to attain a model

625
00:39:02,030 --> 00:39:10,650
is to learn one. And so again, just concretely

626
00:39:10,740 --> 00:39:14,350
what you do is you would imagine that you

627
00:39:14,440 --> 00:39:17,140
have a physical inverted pendulum system as

628
00:39:17,240 --> 00:39:19,730
you physically own an inverted pendulum

629
00:39:19,820 --> 00:39:24,140
robot. What you would do is you would then

630
00:39:24,230 --> 00:39:25,820
initialize your inverted pendulum robot to

631
00:39:25,910 --> 00:39:29,230
some state and then execute some policy, could

632
00:39:29,310 --> 00:39:31,110
be some random policy or some policy

633
00:39:31,190 --> 00:39:32,990
that you think is pretty good, or you could even

634
00:39:33,070 --> 00:39:35,120
try controlling yourself with a joystick or

635
00:39:35,200 --> 00:39:38,600
something. But so you set the system off in

636
00:39:38,650 --> 00:39:41,820
some state as zero. Then you take some

637
00:39:41,890 --> 00:39:45,260
action. Here's zero and the game could be

638
00:39:45,360 --> 00:39:47,260
chosen by some policy or chosen by you using

639
00:39:47,330 --> 00:39:49,230
a joystick tryina control your inverted

640
00:39:49,300 --> 00:39:52,190
pendulum or whatever. System would transition

641
00:39:52,290 --> 00:39:59,010
to some new state, S-1, and then you take some

642
00:39:59,100 --> 00:40:01,040
new action, A-1 and so on. Let's say you do

643
00:40:01,130 --> 00:40:06,220
this for two time steps and sometimes I call this

644
00:40:06,300 --> 00:40:10,440
one trajectory and you repeat this M

645
00:40:10,490 --> 00:40:15,660
times, so this is the first trial of the first

646
00:40:15,740 --> 00:40:19,690
trajectory, and then you do this again. Initialize

647
00:40:19,770 --> 00:40:37,960
it in some and so on. So you do this a bunch of

648
00:40:38,040 --> 00:40:41,280
times and then you would run the learning

649
00:40:41,370 --> 00:41:02,540
algorithm to estimate ST+1 as a function of ST

650
00:41:02,620 --> 00:41:06,250
and AT. And for sake of completeness,

651
00:41:06,340 --> 00:41:08,800
you should just think of this as inverted

652
00:41:08,880 --> 00:41:12,630
pendulum problem, so ST+1 is a 4-dimensional

653
00:41:12,700 --> 00:41:14,930
vector. ST, AT will be a 4-dimensional vector

654
00:41:15,030 --> 00:41:17,060
and that'll be a real number, and so you

655
00:41:17,140 --> 00:41:19,950
might run linear regression 4 times to predict

656
00:41:20,030 --> 00:41:21,920
each of these state variables as a function of

657
00:41:22,020 --> 00:41:27,240
each of these 5 real numbers and so on.

658
00:41:29,590 --> 00:41:34,140
Just for example, if you say that if you want to

659
00:41:34,230 --> 00:41:36,570
estimate your next state ST+1 as a linear

660
00:41:36,660 --> 00:41:42,530
function of your previous state in your action

661
00:41:42,630 --> 00:41:49,810
and so A here will be a 4 by 4 matrix, and

662
00:41:49,920 --> 00:41:54,510
B would be a 4-dimensional vector, then you

663
00:41:54,600 --> 00:42:02,330
would choose the values of A and B that

664
00:42:02,410 --> 00:42:34,710
minimize this. So if you want your model to be

665
00:42:34,810 --> 00:42:37,110
that ST+1 is some linear function of the

666
00:42:37,210 --> 00:42:40,360
previous stated action, then you might pose this

667
00:42:40,440 --> 00:42:42,820
optimization objective and choose A and

668
00:42:42,900 --> 00:42:45,400
B to minimize the sum of squares error in your

669
00:42:45,490 --> 00:42:48,100
predictive value for ST+1 as the linear

670
00:42:48,180 --> 00:42:58,580
function of ST and AT. I should say that this is

671
00:42:58,670 --> 00:43:01,580
one specific example where you're using

672
00:43:01,660 --> 00:43:04,750
a linear function of the previous stated action to

673
00:43:04,820 --> 00:43:06,140
predict the next state.Of course, you can also

674
00:43:06,200 --> 00:43:07,780
use other algorithms like low [inaudible]

675
00:43:07,860 --> 00:43:09,740
weight to linear regression or linear regression

676
00:43:09,850 --> 00:43:11,940
with nonlinear features or kernel linear

677
00:43:12,010 --> 00:43:14,970
regression or whatever to predict the next state

678
00:43:15,060 --> 00:43:17,310
as a nonlinear function of the current state as

679
00:43:17,380 --> 00:43:20,220
well, so this is just [inaudible] linear problems.

680
00:43:24,510 --> 00:43:27,270
And it turns out that low [inaudible] weight to

681
00:43:27,330 --> 00:43:30,070
linear regression is for many robots turns out to

682
00:43:30,140 --> 00:43:31,950
be an effective method for this learning

683
00:43:32,060 --> 00:43:48,760
problem as well. And so having learned to

684
00:43:48,830 --> 00:43:52,230
model, having learned the parameters A and B,

685
00:43:52,310 --> 00:43:55,060
you then have a model where you say that

686
00:43:55,150 --> 00:44:00,930
ST+1 is AST plus BAT, and so that would be an

687
00:44:01,000 --> 00:44:09,060
example of a deterministic model or having

688
00:44:09,170 --> 00:44:11,600
learned the parameters A and B, you might say

689
00:44:11,700 --> 00:44:28,630
that ST+1 is equal to AST + BAT + ?T. And so

690
00:44:28,740 --> 00:44:31,580
these would be very reasonable ways to come

691
00:44:31,660 --> 00:44:34,650
up with either a deterministic or a stochastic

692
00:44:34,740 --> 00:44:38,500
model for your inverted pendulum MDP. And

693
00:44:41,170 --> 00:44:45,030
so just to summarize, what we have now is a

694
00:44:45,110 --> 00:44:48,690
model, meaning a piece of code, where you

695
00:44:48,780 --> 00:44:54,340
can input a state and an action and get an ST+1.

696
00:44:54,420 --> 00:44:57,130
And so if you have a stochastic model,

697
00:44:57,220 --> 00:44:59,780
then to influence this model, you would

698
00:44:59,860 --> 00:45:02,220
actually sample ?T from this [inaudible]

699
00:45:02,310 --> 00:45:04,600
distribution in order to generate ST+1.

700
00:45:09,440 --> 00:45:18,700
So it actually turns out that in a preview,

701
00:45:18,810 --> 00:45:21,260
I guess, in the next lecture it actually turns out

702
00:45:21,360 --> 00:45:23,460
that in the specific case of linear dynamical

703
00:45:23,550 --> 00:45:25,480
systems, in the specific case where the next

704
00:45:25,550 --> 00:45:27,620
state is a linear function of the current stated

705
00:45:27,700 --> 00:45:29,810
action, it actually turns out that there're very

706
00:45:29,890 --> 00:45:32,110
powerful algorithms you can use. So I'm

707
00:45:32,180 --> 00:45:33,810
actually not gonna talk about that today. I'll talk

708
00:45:33,890 --> 00:45:35,980
about that in the next lecture rather than right

709
00:45:36,070 --> 00:45:40,120
now but turns out that for many problems

710
00:45:40,210 --> 00:45:42,320
of inverted pendulum go if you use low

711
00:45:42,410 --> 00:45:44,060
[inaudible] weights and linear regressionals and

712
00:45:44,140 --> 00:45:46,540
long linear algorithm 'cause many systems

713
00:45:46,620 --> 00:45:48,100
aren't really linear.

714
00:45:48,190 --> 00:45:50,170
You can build a nonlinear model.

715
00:45:53,680 --> 00:45:57,020
So what I wanna do now is talk about given

716
00:45:57,110 --> 00:45:59,810
a model, given a simulator for your MDP,

717
00:45:59,900 --> 00:46:03,650
how to come up with an algorithm to

718
00:46:03,730 --> 00:46:05,900
approximate the alpha value function piece.

719
00:46:05,980 --> 00:46:09,400
Before I move on, let me check if there're

720
00:46:09,490 --> 00:46:20,720
questions on this. Okay, cool.

721
00:46:20,820 --> 00:46:44,490
So here's the idea. Back when we talked about

722
00:46:44,600 --> 00:46:47,760
linear regression, we said that given some

723
00:46:47,860 --> 00:46:50,380
inputs X in supervised learning, given the input

724
00:46:50,470 --> 00:46:53,580
feature is X, we may choose some features of X

725
00:46:53,690 --> 00:46:58,370
and then approximate the type of variable

726
00:46:58,460 --> 00:47:01,060
as a linear function of various features of X,

727
00:47:01,160 --> 00:47:04,340
and so just do exactly the same thing to

728
00:47:04,510 --> 00:47:08,890
approximate the optimal value function, and in

729
00:47:08,970 --> 00:47:12,230
particular, we'll choose some features

730
00:47:12,340 --> 00:47:17,670
5-S of a state S.

731
00:47:17,770 --> 00:47:26,910
And so you could actually choose 5-S equals S.

732
00:47:27,010 --> 00:47:28,470
That would be one reasonable choice, if

733
00:47:28,580 --> 00:47:30,400
you want to approximate the value function as

734
00:47:30,490 --> 00:47:34,170
a linear function of the states, but you can

735
00:47:34,260 --> 00:47:37,420
also choose other things, so for example, for the

736
00:47:37,540 --> 00:47:39,180
inverted pendulum example, you may

737
00:47:39,300 --> 00:47:43,060
choose 5-S to be equal to a vector of features

738
00:47:43,140 --> 00:47:46,390
that may be [inaudible]1 or you may have

739
00:47:46,500 --> 00:47:54,580
Xdot2, Xdot maybe some cross terms, maybe

740
00:47:54,680 --> 00:48:00,150
times X, maybe dot2 and so on. So you

741
00:48:00,270 --> 00:48:05,120
choose some vector or features and then

742
00:48:07,020 --> 00:48:19,140
approximate the value function as the value of

743
00:48:19,230 --> 00:48:24,630
the state as is equal to data transfers times the

744
00:48:24,740 --> 00:48:26,610
features. And I should apologize in

745
00:48:26,700 --> 00:48:28,590
advance; I'm overloading notation here.

746
00:48:28,720 --> 00:48:32,030
It's unfortunate. I use data both to denote the

747
00:48:32,110 --> 00:48:35,340
angle of the cart of the pole inverted pendulum.

748
00:48:35,490 --> 00:48:39,610
So this is known as the angle T but also

749
00:48:39,700 --> 00:48:43,230
using T to denote the vector of parameters in

750
00:48:43,350 --> 00:48:45,080
my [inaudible] algorithm.

751
00:48:45,190 --> 00:48:46,750
So sorry about the overloading notation.

752
00:48:46,830 --> 00:48:54,490
Just like we did in linear regression,

753
00:48:54,580 --> 00:48:56,350
my goal is to come up with

754
00:48:56,440 --> 00:48:57,830
a linear combination of

755
00:48:57,920 --> 00:49:00,120
the features that gives me a good approximation

756
00:49:00,260 --> 00:49:02,610
to the value function and this is

757
00:49:02,720 --> 00:49:06,570
completely analogous to when we said that in

758
00:49:06,690 --> 00:49:14,670
linear regression our estimate, my response

759
00:49:15,030 --> 00:49:17,390
there but Y as a linear function a feature is at

760
00:49:17,480 --> 00:49:19,620
the input. That's what we have in linear

761
00:49:19,700 --> 00:49:51,490
regression. Let me just write down value

762
00:49:51,570 --> 00:49:54,060
iteration again and then I'll written down an

763
00:49:54,160 --> 00:49:57,930
approximation to value iteration, so for discrete

764
00:49:58,030 --> 00:50:02,050
states, this is the idea behind value

765
00:50:02,130 --> 00:50:05,200
iteration and we said that V(s) will be updated

766
00:50:05,280 --> 00:50:17,550
as R(s) + G [inaudible].

767
00:50:17,620 --> 00:50:20,330
That was value iteration and in the case of

768
00:50:20,440 --> 00:50:22,110
continuous states, this would be the replaced

769
00:50:22,160 --> 00:50:25,790
by an [inaudible], an [inaudible] over states

770
00:50:25,880 --> 00:50:33,200
rather via sum over states. Let me just write

771
00:50:33,320 --> 00:50:41,270
this as R(s) + G([inaudible]) and then that sum

772
00:50:41,360 --> 00:50:43,590
over T's prime. That's really an

773
00:50:43,700 --> 00:50:47,130
expectation with respect to random state as

774
00:50:47,220 --> 00:50:49,690
prime drawn from the state transition

775
00:50:49,770 --> 00:50:57,570
probabilities piece SA of V(s) prime. So this is

776
00:50:57,670 --> 00:50:59,440
a sum of all states S prime with the

777
00:50:59,480 --> 00:51:01,810
probability of going to S prime (value), so

778
00:51:01,910 --> 00:51:04,330
that's really an expectation over the random

779
00:51:04,410 --> 00:51:09,720
state S prime flowing from PSA of that. And so

780
00:51:09,820 --> 00:51:12,670
what I'll do now is write down an

781
00:51:12,760 --> 00:51:28,220
algorithm called fitted value iteration that's in

782
00:51:28,320 --> 00:51:31,530
approximation to this but specifically for

783
00:51:31,630 --> 00:51:34,880
continuous states. I just wrote down the first

784
00:51:34,960 --> 00:51:36,840
two steps, and then I'll continue on the next

785
00:51:36,920 --> 00:51:40,250
board, so the first step of the algorithm is we'll

786
00:51:40,350 --> 00:51:44,850
sample. Choose some set of states at

787
00:51:44,960 --> 00:51:59,080
random. So sample S-1, S-2 through S-M

788
00:51:59,210 --> 00:52:01,370
randomly so choose a set of states randomly

789
00:52:01,450 --> 00:52:10,190
and initialize my parameter vector to be equal

790
00:52:10,300 --> 00:52:13,840
to zero. This is analogous to in value

791
00:52:13,960 --> 00:52:17,530
iteration where I might initialize the value

792
00:52:17,640 --> 00:52:22,080
function to be the function of all zeros. Then

793
00:52:22,160 --> 00:52:44,710
here's the end view for the algorithm. Got quite

794
00:52:44,760 --> 00:54:17,460
a lot to write actually. Let's see. And so

795
00:56:12,270 --> 00:56:18,780
that's the algorithm. Let me just adjust the

796
00:56:18,830 --> 00:56:20,360
writing. Give me a second. Give me a minute

797
00:56:20,430 --> 00:56:21,780
to finish and then I'll step through this.

798
00:56:26,780 --> 00:56:29,160
Actually, if some of my handwriting is eligible,

799
00:56:29,240 --> 00:56:58,900
let me know. So let me step through this and so

800
00:56:59,010 --> 00:57:02,790
briefly explain the rationale. So the hear

801
00:57:02,870 --> 00:57:10,080
of the algorithm is - let's see. In the original

802
00:57:10,160 --> 00:57:11,950
value iteration algorithm, we would take the

803
00:57:12,020 --> 00:57:15,260
value for each state, V(s)I, and we will

804
00:57:15,340 --> 00:57:18,830
overwrite it with this expression here. In the

805
00:57:18,910 --> 00:57:21,750
original, this discrete value iteration algorithm

806
00:57:21,830 --> 00:57:26,420
was to V(s)I and we will set V(s)I to be

807
00:57:26,490 --> 00:57:31,030
equal to that, I think. Now we have in the

808
00:57:31,110 --> 00:57:32,590
continuous state case, we have an infinite

809
00:57:32,640 --> 00:57:33,980
continuous set of states and so you can't

810
00:57:34,040 --> 00:57:36,710
discretely set the value of each of these to that.

811
00:57:36,790 --> 00:57:39,630
So what we'll do instead is choose the

812
00:57:39,690 --> 00:57:44,900
parameters T so that V(s)I is as close as possible

813
00:57:45,000 --> 00:57:48,510
to this thing on the right hand side

814
00:57:48,590 --> 00:57:53,590
instead. And this is what YI turns out to be. So

815
00:57:53,680 --> 00:57:57,870
completely, what I'm going to do is I'm going

816
00:57:57,960 --> 00:58:01,560
to construct estimates of this term, and

817
00:58:01,640 --> 00:58:03,100
then I'm going to choose the parameters of my

818
00:58:03,180 --> 00:58:04,600
function approximator. I'm gonna choose

819
00:58:04,700 --> 00:58:08,700
my parameter as T, so that V(s)I is as close as

820
00:58:08,760 --> 00:58:10,980
possible to these. That's what YI is, and

821
00:58:11,070 --> 00:58:13,630
specifically, what I'm going to do is I'll choose

822
00:58:13,730 --> 00:58:16,550
parameters data to minimize the sum of

823
00:58:16,610 --> 00:58:19,230
square differences between T [inaudible] plus

824
00:58:19,300 --> 00:58:23,140
5SI. This thing here is just V(s)I because

825
00:58:23,310 --> 00:58:25,980
I'm approximating V(s)I is a linear function of

826
00:58:26,070 --> 00:58:29,100
5SI and so choose the parameters data to

827
00:58:29,180 --> 00:58:32,240
minimize the sum of square differences. So this

828
00:58:32,570 --> 00:58:36,170
is last step is basically the approximation

829
00:58:36,250 --> 00:58:40,130
version of value iteration. What everything else

830
00:58:40,210 --> 00:58:42,780
above was doing was just coming up

831
00:58:42,850 --> 00:58:46,900
with an approximation to this term, to this thing

832
00:58:46,990 --> 00:58:51,400
here and which I was calling YI. And so

833
00:58:51,480 --> 00:58:53,850
confluently, for every state SI we want to

834
00:58:53,930 --> 00:58:56,100
estimate what the thing on the right hand side is

835
00:58:56,180 --> 00:58:59,610
and but there's an expectation here. There's an

836
00:58:59,690 --> 00:59:01,710
expectation over a continuous set of

837
00:59:01,790 --> 00:59:03,850
states, may be a very high dimensional state so

838
00:59:03,960 --> 00:59:06,030
I can't compute this expectation exactly.

839
00:59:06,120 --> 00:59:09,290
What I'll do instead is I'll use my simulator to

840
00:59:09,370 --> 00:59:12,270
sample a set of states from this distribution

841
00:59:12,360 --> 00:59:15,490
from this P substrip, SIA, from the state

842
00:59:15,570 --> 00:59:18,530
transition distribution of where I get to if I take

843
00:59:18,600 --> 00:59:20,320
the action A in the state as I, and then I'll

844
00:59:20,410 --> 00:59:24,440
average over that sample of states to compute

845
00:59:24,520 --> 00:59:28,040
this expectation. And so stepping through the

846
00:59:28,130 --> 00:59:31,850
algorithm just says that for each state and

847
00:59:31,940 --> 00:59:34,170
for each action, I'm going to sample a set of

848
00:59:34,250 --> 00:59:37,510
states. This S prime 1 through S prime K

849
00:59:37,590 --> 00:59:39,430
from that state transition distribution, still using

850
00:59:39,510 --> 00:59:42,010
the model, and then I'll set Q(a) to be

851
00:59:42,080 --> 00:59:45,620
equal to that average and so this is my estimate

852
00:59:45,720 --> 00:59:50,790
for R(s)I + G(this expected value for that

853
00:59:50,870 --> 00:59:55,160
specific action A). Then I'll take the maximum

854
00:59:55,280 --> 00:59:58,310
of actions A and this gives me YI, and so

855
00:59:58,390 --> 01:00:02,060
YI is for S for that. And finally, I'll run really

856
01:00:02,150 --> 01:00:05,740
run linear regression which is that last of

857
01:00:05,840 --> 01:00:09,670
the set [inaudible] to get V(s)I to be close to the

858
01:00:09,780 --> 01:00:13,590
YIs. And so this algorithm is called fitted

859
01:00:13,690 --> 01:00:17,350
value iteration and it actually often works quite

860
01:00:17,450 --> 01:00:20,320
well for continuous, for problems with

861
01:00:20,450 --> 01:00:25,120
anywhere from 6- to 10- to 20-dimensional

862
01:00:25,220 --> 01:00:27,360
state spaces if you can choose appropriate

863
01:00:27,440 --> 01:00:35,150
features. Can you raise a hand please if this

864
01:00:35,250 --> 01:00:40,770
algorithm makes sense? Some of you didn't

865
01:00:40,850 --> 01:00:43,070
have your hands up. Are there questions for

866
01:00:43,150 --> 01:00:44,020
those, yeah?

867
01:00:44,100 --> 01:00:45,490
Student: Is there a recess [inaudible]

868
01:00:45,570 --> 01:00:48,010
function in this setup?

869
01:00:48,100 --> 01:00:56,480
Oh, yes. An MDP comprises SA,

870
01:00:56,580 --> 01:01:03,730
the state transition probabilities G and R

871
01:01:03,850 --> 01:01:06,290
and so for continuous state spaces,

872
01:01:06,390 --> 01:01:08,860
S would be like R4 for the inverted pendulum

873
01:01:08,990 --> 01:01:12,030
or something. Actions with discretized state

874
01:01:12,120 --> 01:01:13,200
transitions probabilities with specifying with

875
01:01:13,300 --> 01:01:18,160
the model or the simulator, G is just a real

876
01:01:18,250 --> 01:01:21,720
number like .99 and the real function is usually

877
01:01:21,820 --> 01:01:23,790
a function that's given to you. And so the

878
01:01:23,880 --> 01:01:28,420
reward function is some function of your

879
01:01:28,510 --> 01:01:32,420
4-dimensional state, and for example,

880
01:01:32,510 --> 01:01:35,240
you might choose a reward function to be

881
01:01:35,350 --> 01:01:41,280
minus I don't know. Just for an example

882
01:01:41,360 --> 01:01:44,800
of simple reward function, if we want a -1

883
01:01:44,870 --> 01:01:49,740
if the pole has fallen and there it depends

884
01:01:49,830 --> 01:01:51,560
on you choose your reward function to be -1

885
01:01:51,640 --> 01:01:53,560
if the inverted pendulum falls over

886
01:01:53,560 --> 01:01:54,560
and to find that its angle is greater than 30° or

887
01:01:57,940 --> 01:02:02,800
something and zero otherwise. So that

888
01:02:02,890 --> 01:02:05,930
would be an example of a reward function that

889
01:02:06,030 --> 01:02:07,140
you can choose for the inverted pendulum,

890
01:02:07,220 --> 01:02:08,780
but yes, assuming a reward function is given to

891
01:02:08,870 --> 01:02:11,230
you so that you can compute R(s)I for any

892
01:02:11,310 --> 01:02:16,680
state. Are there other questions?

893
01:03:23,020 --> 01:03:25,030
Actually, let me try asking a question,

894
01:03:25,150 --> 01:03:27,950
so everything I did here assume that we have a

895
01:03:28,060 --> 01:03:32,780
stochastic simulator. So it turns out I can simply

896
01:03:32,890 --> 01:03:35,310
this algorithm if I have a deterministic

897
01:03:35,400 --> 01:03:37,910
simulator, but deterministic simulator is given a

898
01:03:37,990 --> 01:03:40,390
stated action, my next state is always

899
01:03:40,460 --> 01:03:43,360
exactly determined. So let me ask you, if I have

900
01:03:43,450 --> 01:03:46,050
a deterministic simulator, how would I

901
01:03:46,140 --> 01:03:48,050
change this algorithm? How would I simplify

902
01:03:48,140 --> 01:03:50,020
this algorithm?

903
01:03:50,130 --> 01:03:54,690
Student:Lower your samples

904
01:03:54,790 --> 01:03:56,380
that you're drawing [inaudible].

905
01:03:56,450 --> 01:03:59,510
Right, so Justin's going right.

906
01:03:59,610 --> 01:04:00,830
If I have a deterministic simulator,

907
01:04:00,930 --> 01:04:02,940
all my samples from those would

908
01:04:03,020 --> 01:04:04,520
be exactly the same, and so if I have a

909
01:04:04,610 --> 01:04:07,090
deterministic simulator, I can set K

910
01:04:07,210 --> 01:04:09,930
to be equal to 1, so I don't need to draw

911
01:04:10,020 --> 01:04:12,770
K different samples. I really only need

912
01:04:12,870 --> 01:04:14,990
one sample if I have a deterministic simulator,

913
01:04:15,070 --> 01:04:21,020
so you can simplify this by setting K=1

914
01:04:21,120 --> 01:04:23,610
if you have a deterministic simulator. Yeah?

915
01:04:27,370 --> 01:04:28,280
Student:I guess I'm really confused about the,

916
01:04:28,380 --> 01:04:30,750
yeah, we sorta turned this [inaudible] into

917
01:04:30,860 --> 01:04:33,150
something that looks like linear state regression

918
01:04:33,250 --> 01:04:35,480
or some' you know the data transpose

919
01:04:35,570 --> 01:04:40,060
times something that we're used to but I guess

920
01:04:40,170 --> 01:04:41,520
I'm a little. I don't know really know what

921
01:04:41,610 --> 01:04:45,250
question to ask but like when we did this before

922
01:04:45,370 --> 01:04:46,860
we had like discrete states and

923
01:04:46,950 --> 01:04:50,340
everything. We were determined with finding

924
01:04:50,440 --> 01:04:55,620
this optimal policy and I guess it doesn't

925
01:04:55,740 --> 01:04:58,040
look like we haven't said the word policy in a

926
01:04:58,160 --> 01:04:59,920
while so kinda difficult.

927
01:05:00,040 --> 01:05:04,570
Okay, yeah, so [inaudible] matters back

928
01:05:04,660 --> 01:05:05,910
to policy but maybe I should just say

929
01:05:05,990 --> 01:05:08,350
a couple words so let me actually try

930
01:05:08,430 --> 01:05:09,900
to get at some of what maybe

931
01:05:09,990 --> 01:05:11,180
what you're saying.

932
01:05:11,270 --> 01:05:13,180
Our strategy for finding optimal policy has

933
01:05:13,270 --> 01:05:16,180
been to find some way to find

934
01:05:16,270 --> 01:05:18,570
V*, find some way to find the optimal value

935
01:05:18,650 --> 01:05:21,960
function and then use that to compute ?

936
01:05:22,050 --> 01:05:24,170
and some of approximations of ?*.

937
01:05:24,260 --> 01:05:26,970
So far everything I've been doing

938
01:05:27,020 --> 01:05:28,980
has been focused on how to find V*.

939
01:05:29,080 --> 01:05:34,640
I just want to say one more word.

940
01:05:34,730 --> 01:05:37,260
It actually turns out that for linear regression

941
01:05:37,360 --> 01:05:39,270
it's often very easy.

942
01:05:39,360 --> 01:05:40,690
It's often not terribly difficult

943
01:05:40,780 --> 01:05:42,040
to choose some resource of the features.

944
01:05:42,120 --> 01:05:45,150
Choosing features for approximating the value

945
01:05:45,230 --> 01:05:47,190
function is often somewhat harder,

946
01:05:47,280 --> 01:05:51,530
so because the value of a state is how good is

947
01:05:51,630 --> 01:05:54,130
starting off in this state. What is my expected

948
01:05:54,210 --> 01:05:58,710
sum of discounted rewards? What if I start in a

949
01:05:58,810 --> 01:06:01,300
certain state? And so what the feature of

950
01:06:01,400 --> 01:06:04,460
the state have to measure is really how good is

951
01:06:04,560 --> 01:06:08,800
it to start in a certain state? And so for

952
01:06:08,870 --> 01:06:12,450
inverted pendulum you actually have that states

953
01:06:12,530 --> 01:06:15,010
where the poles are vertical and when a

954
01:06:15,090 --> 01:06:16,690
cart that's centered on your track or something,

955
01:06:16,780 --> 01:06:18,580
maybe better and so you can come up

956
01:06:18,660 --> 01:06:20,370
with features that measure the orientation of the

957
01:06:20,450 --> 01:06:22,290
pole and how close you are to the center

958
01:06:22,370 --> 01:06:24,220
of the track and so on and those will be

959
01:06:24,320 --> 01:06:26,700
reasonable features to use to approximate V*.

960
01:06:26,800 --> 01:06:30,530
Although in general it is true that choosing

961
01:06:30,620 --> 01:06:32,250
features, the value function approximation, is

962
01:06:32,330 --> 01:06:35,210
often slightly trickier than choosing good

963
01:06:35,290 --> 01:06:36,710
features for linear regression.

964
01:06:36,820 --> 01:06:40,370
Okay and then Justin's questions of

965
01:06:40,480 --> 01:06:44,130
so given V*, how do you go back to actually

966
01:06:44,230 --> 01:06:50,530
find a policy? In the discrete case, so we have

967
01:06:50,630 --> 01:06:59,000
that ?*(s) is equal to all [inaudible] over A of

968
01:06:59,090 --> 01:07:07,010
that. So that's again, I used to write this as a

969
01:07:07,100 --> 01:07:09,850
sum over states [inaudible]. I'll just write

970
01:07:09,930 --> 01:07:13,540
this as an expectation and so then once you find

971
01:07:13,640 --> 01:07:16,960
the optimal value function V*, you can

972
01:07:17,060 --> 01:07:20,790
then find the optimal policy ?* by computing

973
01:07:20,890 --> 01:07:23,940
the [inaudible]. So if you're in a continuous

974
01:07:24,050 --> 01:07:27,810
state MDP, then you can't actually do this in

975
01:07:27,910 --> 01:07:30,190
advance for every single state because

976
01:07:30,270 --> 01:07:32,340
there's an infinite number of states and so you

977
01:07:32,440 --> 01:07:34,070
can't actually perform this computation in

978
01:07:34,180 --> 01:07:35,440
advance to every single state.

979
01:07:35,540 --> 01:07:40,060
What you do instead is whenever your robot is

980
01:07:40,170 --> 01:07:43,260
in some specific state S is only when your

981
01:07:43,370 --> 01:07:46,020
system is in some specific state S like your car

982
01:07:46,120 --> 01:07:48,000
is at some position orientation or your

983
01:07:48,100 --> 01:07:51,040
inverted pendulum is in some specific position,

984
01:07:51,140 --> 01:07:56,820
posed in some specific angle T. It's only

985
01:07:56,900 --> 01:08:01,610
when your system, be it a factor or a board

986
01:08:01,700 --> 01:08:04,470
game or a robot, is in some specific state S

987
01:08:04,570 --> 01:08:07,740
that you would then go ahead and compute this

988
01:08:07,850 --> 01:08:11,650
augmax, so it's only when you're in some

989
01:08:11,700 --> 01:08:14,410
state S that you then compute this augmax, and

990
01:08:22,040 --> 01:08:24,100
then you execute that action A and then

991
01:08:24,210 --> 01:08:26,930
as a result of your action, your robot would

992
01:08:27,010 --> 01:08:30,290
transition to some new state and then so it'll

993
01:08:30,350 --> 01:08:32,210
be given that specific new state that you

994
01:08:32,290 --> 01:08:34,590
compute as augmax using that specific state S

995
01:08:34,670 --> 01:08:37,150
that you're in.

996
01:08:43,200 --> 01:08:44,380
There're a few ways to do it. One way to do

997
01:08:44,470 --> 01:08:47,340
this is actually the same as in the inner loop

998
01:08:47,420 --> 01:08:49,400
of the fitted value iteration algorithm so

999
01:08:49,510 --> 01:08:52,770
because of an expectation of a large number of

1000
01:08:52,850 --> 01:08:56,090
states, you'd need to sample some set of states

1001
01:08:56,190 --> 01:08:58,550
from the simulator and then approximate

1002
01:08:58,640 --> 01:09:00,790
this expectation using an average over your

1003
01:09:00,880 --> 01:09:03,910
samples, so it's actually as inner loop of the

1004
01:09:04,030 --> 01:09:07,170
value iteration algorithm. So you could do that.

1005
01:09:07,270 --> 01:09:08,730
That's sometimes done. Sometimes it can

1006
01:09:08,830 --> 01:09:11,500
also be a pain to have to sample a set of states

1007
01:09:11,580 --> 01:09:14,400
to approximate those expectations every

1008
01:09:14,470 --> 01:09:16,340
time you want to take an action in your MDP.

1009
01:09:16,420 --> 01:09:20,230
Couple of special cases where this can be done,

1010
01:09:20,310 --> 01:09:21,820
one special case is

1011
01:09:21,920 --> 01:09:23,660
if you have a deterministic simulator.

1012
01:09:30,640 --> 01:09:34,050
If it's a deterministic simulator, so in

1013
01:09:34,150 --> 01:09:35,850
other words, if your similar is just

1014
01:09:35,940 --> 01:09:41,420
some function, could be a linear or a nonlinear

1015
01:09:41,500 --> 01:09:43,580
function. If it's a deterministic simulator

1016
01:09:43,680 --> 01:09:46,200
then the next state, ST+1, is just some function

1017
01:09:46,280 --> 01:09:50,450
of your previous stated action.

1018
01:09:50,540 --> 01:09:53,380
If that's the case then this expectation, well,

1019
01:09:53,460 --> 01:09:56,240
then this simplifies to

1020
01:09:56,330 --> 01:10:04,700
augmax of A of V* of F of I guess S,

1021
01:10:04,780 --> 01:10:09,260
A because this is really saying S

1022
01:10:09,350 --> 01:10:16,670
prime=F(s),A. I switched back and forth

1023
01:10:16,770 --> 01:10:18,350
between notation; I hope that's okay. S to

1024
01:10:18,460 --> 01:10:19,890
denote the current state,

1025
01:10:19,990 --> 01:10:21,880
and S prime to deterministic state

1026
01:10:21,970 --> 01:10:23,520
versus ST and ST+1 through

1027
01:10:23,620 --> 01:10:25,880
the current state. Both of these are sorta

1028
01:10:25,970 --> 01:10:28,930
standing notation and don't mind my switching

1029
01:10:29,050 --> 01:10:31,050
back and forth between them. But if it's a

1030
01:10:31,140 --> 01:10:33,040
deterministic simulator you can then just

1031
01:10:33,140 --> 01:10:35,150
compute what the next state S prime would be

1032
01:10:35,270 --> 01:10:37,150
for each action you might take from the current

1033
01:10:37,250 --> 01:10:40,030
state, and then take the augmax of actions

1034
01:10:40,110 --> 01:10:43,340
A, basically choose the action that gets you to

1035
01:10:43,430 --> 01:10:45,050
the highest value state.

1036
01:11:01,410 --> 01:11:06,680
And so that's one case where you can compute

1037
01:11:06,770 --> 01:11:08,500
the augmax and we can compute that

1038
01:11:08,600 --> 01:11:10,410
expectation without needing to sample an

1039
01:11:10,500 --> 01:11:13,730
average over some sample. Another very

1040
01:11:13,820 --> 01:11:16,070
common case actually it turns out is if you have

1041
01:11:16,150 --> 01:11:25,120
a stochastic simulator, but if your similar

1042
01:11:25,200 --> 01:11:28,510
happens to take on a very specific form of

1043
01:11:28,600 --> 01:11:35,140
ST+1=F(s)T,AT+?T

1044
01:11:35,260 --> 01:11:38,930
where this is galsie noise.

1045
01:11:39,020 --> 01:11:46,320
The [inaudible] is a very common way to build

1046
01:11:46,390 --> 01:11:48,430
simulators where you model the next state

1047
01:11:48,530 --> 01:11:50,540
as a function of the current state and action plus

1048
01:11:50,620 --> 01:11:53,100
some noise and so once specific example

1049
01:11:53,190 --> 01:11:55,520
would be that sort of mini dynamical system

1050
01:11:55,610 --> 01:12:00,080
that we talked about with linear function of

1051
01:12:00,170 --> 01:12:02,480
the current state and action plus galsie noise.

1052
01:12:02,550 --> 01:12:08,640
In this case, you can approximate augment

1053
01:12:08,700 --> 01:12:10,500
over A, well.

1054
01:12:19,850 --> 01:12:27,380
In that case you take that expectation that

1055
01:12:27,480 --> 01:12:33,870
you're trying to approximate. The expected

1056
01:12:33,950 --> 01:12:36,740
value of V* of S prime, we can approximate

1057
01:12:36,820 --> 01:12:45,490
that with V* of the expected value of S

1058
01:12:45,540 --> 01:12:47,030
prime, and this is approximation. Expected

1059
01:12:47,120 --> 01:12:51,460
value of a function is usually not equal to the

1060
01:12:51,540 --> 01:12:54,330
value of an expectation but it is often a

1061
01:12:54,410 --> 01:13:02,460
reasonable approximation and so that would be

1062
01:13:02,560 --> 01:13:05,610
another way to approximate that expectation

1063
01:13:05,710 --> 01:13:11,420
and so you choose the actions according to

1064
01:13:11,500 --> 01:13:15,850
watch we do the same formula as I wrote just

1065
01:13:15,920 --> 01:13:24,390
now. And so this would be a way of

1066
01:13:24,500 --> 01:13:31,180
approximating this augmax, ignoring the noise

1067
01:13:31,290 --> 01:13:35,110
in the simulator essentially. And this often

1068
01:13:35,170 --> 01:13:36,940
works pretty well as well just because many

1069
01:13:37,030 --> 01:13:39,380
simulators turn out to be the form of some

1070
01:13:39,470 --> 01:13:42,170
linear or some nonlinear function plus zero

1071
01:13:42,260 --> 01:13:45,150
mean galsie noise, so and just that ignore the

1072
01:13:45,230 --> 01:13:46,980
zero mean galsie noise, so that you can

1073
01:13:47,090 --> 01:13:49,320
compute this quickly.

1074
01:13:49,400 --> 01:13:54,860
And just to complete about this, what that is,

1075
01:13:54,950 --> 01:13:59,940
right, that V* F of SA, this you down rate as

1076
01:14:00,020 --> 01:14:07,160
data transfers Fi of S prime where S prime=F of

1077
01:14:07,250 --> 01:14:10,740
SA. Great, so this V* you would

1078
01:14:10,840 --> 01:14:13,770
compute using the parameters data that you just

1079
01:14:13,870 --> 01:14:16,140
learned using the fitted value iteration

1080
01:14:16,220 --> 01:14:23,920
algorithm. Questions about this?

1081
01:14:29,760 --> 01:14:34,610
Student:[Inaudible] case, for real-time

1082
01:14:34,740 --> 01:14:37,690
application is it possible to use that [inaudible],

1083
01:14:37,820 --> 01:14:41,870
for example for [inaudible].

1084
01:14:41,940 --> 01:14:43,660
Yes, in real-time applications is it possible to

1085
01:14:43,750 --> 01:14:45,790
sample case phases use [inaudible] expectation.

1086
01:14:51,440 --> 01:14:53,300
Computers today actually amazingly fast.

1087
01:14:53,380 --> 01:14:55,890
I'm actually often surprised by how much

1088
01:14:55,970 --> 01:14:59,420
you can do in real time so the helicopter

1089
01:14:59,520 --> 01:15:02,980
we actually flying a helicopter using

1090
01:15:03,060 --> 01:15:04,560
an algorithm different than this? I can't say.

1091
01:15:04,640 --> 01:15:08,090
But my intuition is that you could actually

1092
01:15:08,160 --> 01:15:11,300
do this with a helicopter. A helicopter would

1093
01:15:11,380 --> 01:15:13,340
control at somewhere between 10hz and 50hz.

1094
01:15:13,440 --> 01:15:15,130
You need to do this 10 times a second to 50

1095
01:15:15,220 --> 01:15:17,440
times a second, and that's actually plenty

1096
01:15:17,530 --> 01:15:20,260
of time to sample 1,000 states

1097
01:15:20,360 --> 01:15:21,700
and compute this expectation.

1098
01:15:21,800 --> 01:15:24,340
They're real difficult, helicopters because

1099
01:15:24,450 --> 01:15:27,320
helicopters are mission critical, and you do

1100
01:15:27,420 --> 01:15:30,230
something it's like fast. You can do serious

1101
01:15:30,310 --> 01:15:32,970
damage and so maybe not for good reasons.

1102
01:15:33,060 --> 01:15:36,010
We've actually tended to avoid tossing coins

1103
01:15:36,110 --> 01:15:38,630
when we're in the air, so the ideal of letting

1104
01:15:38,720 --> 01:15:41,180
our actions be some up with some random

1105
01:15:41,290 --> 01:15:43,900
process is slightly scary and just tend not to do

1106
01:15:44,000 --> 01:15:47,210
that. I should say that's probably not a great

1107
01:15:47,300 --> 01:15:49,160
reason because you average a large number of

1108
01:15:49,260 --> 01:15:52,500
things here very well fine but just as a maybe

1109
01:15:52,610 --> 01:15:54,420
overly conservative design choice, we

1110
01:15:54,490 --> 01:15:57,680
actually don't, tend not to find anything

1111
01:15:57,780 --> 01:16:00,810
randomized on which is prob'ly being over

1112
01:16:00,890 --> 01:16:02,700
conservative. It's the choice we made 'cause

1113
01:16:02,770 --> 01:16:05,210
other things are slightly safer. I think you

1114
01:16:05,310 --> 01:16:07,200
can actually often do this.

1115
01:16:07,300 --> 01:16:10,490
So long as I see a model can be evaluated fast

1116
01:16:10,580 --> 01:16:14,270
enough where you can sample 100 state

1117
01:16:14,380 --> 01:16:16,820
transitions or 1,000 state transitions, and then

1118
01:16:16,900 --> 01:16:18,940
do that at 10hz. They haven't said that.

1119
01:16:19,000 --> 01:16:21,090
This is often attained which is why we often

1120
01:16:21,170 --> 01:16:23,340
use the other approximations that don't

1121
01:16:23,420 --> 01:16:30,340
require your drawing a large sample. Anything

1122
01:16:30,430 --> 01:16:32,300
else? No, okay, cool. So now you know

1123
01:16:32,400 --> 01:16:34,100
one algorithm [inaudible] reinforcement

1124
01:16:34,210 --> 01:16:36,540
learning on continuous state spaces. Then we'll

1125
01:16:36,640 --> 01:16:39,140
pick up with some more ideas on some even

1126
01:16:39,220 --> 01:16:41,420
more powerful algorithms, the solving

1127
01:16:41,530 --> 01:16:42,890
MDPs of continuous state spaces.

1128
01:16:42,980 --> 01:16:44,850
Thanks. Let's close for today.

