1
00:00:23,040 --> 00:00:25,210
机器学习--第18课

2
00:00:25,360 --> 00:00:29,190
好 欢迎回来 我今天想要做的是

3
00:00:29,260 --> 00:00:32,710
谈论我最喜欢的算法之一

4
00:00:32,910 --> 00:00:36,270
控制MDPs  我认为这是一个

5
00:00:36,380 --> 00:00:38,330
更优雅  高效和强大的算法

6
00:00:38,450 --> 00:00:40,040
所以我将要做的是

7
00:00:40,120 --> 00:00:45,670
我将首先开始谈论MDPs的

8
00:00:45,760 --> 00:00:47,860
几个变种

9
00:00:47,940 --> 00:00:50,680
到目前为止

10
00:00:50,740 --> 00:00:52,500
与你看到的MDP的定义略有不同

11
00:00:52,590 --> 00:00:54,650
这些都是很常见的变种

12
00:00:54,730 --> 00:00:56,650
一个是状态行动的奖励

13
00:00:56,720 --> 00:01:01,700
和其他边界MDPs

14
00:01:01,780 --> 00:01:04,810
使用MDP的这一个半-改进的定义

15
00:01:04,890 --> 00:01:07,960
我将谈论非线性动力学系统

16
00:01:08,050 --> 00:01:09,860
我会花一点点时间  谈论在动力系统的模型

17
00:01:09,940 --> 00:01:11,830
然后谈LQR

18
00:01:11,900 --> 00:01:14,880
或线性二次型调节控制

19
00:01:14,980 --> 00:01:17,460
这将引导我们一些Riccati方程

20
00:01:17,540 --> 00:01:20,410
这是一些我们要解决东西

21
00:01:20,490 --> 00:01:23,480
以致让我们处理LQR控制

22
00:01:25,790 --> 00:01:30,310
因此  简要回顾一下

23
00:01:30,400 --> 00:01:33,810
我们已经多次看到这个定义了

24
00:01:33,890 --> 00:01:43,570
我们已经定义的MDP为 [听不清]状态行动

25
00:01:43,650 --> 00:01:45,720
状态转换可能性  折扣因子奖励系数

26
00:01:45,810 --> 00:01:54,750
伽马 这一折扣因子是

27
00:01:54,810 --> 00:01:57,710
一些0和1之间数 和R  奖励函数

28
00:01:57,780 --> 00:02:01,340
是从状态

29
00:02:01,430 --> 00:02:03,510
回报的函数映射

30
00:02:03,590 --> 00:02:05,670
是从状态

31
00:02:05,750 --> 00:02:07,310
实数的函数映射 ?

32
00:02:07,390 --> 00:02:11,580
因此  我们有值迭代  这将做到这一点

33
00:02:11,660 --> 00:02:23,550
因此  一段时间后  迭代的值

34
00:02:23,650 --> 00:02:29,030
将导致V至V*的转换

35
00:02:29,110 --> 00:02:31,240
然后找到最优值函数

36
00:02:31,330 --> 00:02:33,500
如果你本质上通过取上面方程的最大值  来

37
00:02:33,590 --> 00:02:35,710
计算最优政策

38
00:02:35,790 --> 00:02:45,580
其参数A的最大值

39
00:02:45,670 --> 00:02:53,670
因此  在值迭代  当迭代你这个

40
00:02:53,740 --> 00:02:55,610
你知道  执行此更新

41
00:02:55,670 --> 00:02:58,830
函数V 将[听不清]转换到V

42
00:02:58,910 --> 00:03:01,570
所以不会出现--因此不用定义的

43
00:03:01,630 --> 00:03:03,350
迭代次数  你会得到

44
00:03:03,420 --> 00:03:05,270
越来越接近V* 这实际上

45
00:03:05,350 --> 00:03:08,950
以指数方式迅速收敛到V

46
00:03:09,050 --> 00:03:11,470
我们决不会完全把转换到V

47
00:03:11,550 --> 00:03:13,310
和定义的迭代次数

48
00:03:13,400 --> 00:03:17,640
所以我现在想要做的是

49
00:03:17,750 --> 00:03:19,770
描述几个常见的变异MDPs

50
00:03:19,860 --> 00:03:23,970
这是略有不同的定义

51
00:03:24,060 --> 00:03:25,960
首先是奖励函数  然后第二

52
00:03:26,050 --> 00:03:29,540
我们会做一些与计算算略有不同的事情

53
00:03:29,630 --> 00:03:34,070
然后记得在上一讲  我说

54
00:03:34,170 --> 00:03:37,090
对于MDPs中的无限连续状态

55
00:03:37,170 --> 00:03:39,950
我们可能不应用值迭代的最简单的版本

56
00:03:40,030 --> 00:03:42,330
因为如果你有连续状态的MDP

57
00:03:42,410 --> 00:03:44,530
我们需要使用一些

58
00:03:44,640 --> 00:03:47,750
近似的 最优值函数

59
00:03:47,840 --> 00:03:51,970
结果  稍后在演讲中

60
00:03:52,050 --> 00:03:54,650
我将谈论一个MDPs的特殊情况

61
00:03:54,750 --> 00:03:56,850
在那里你可以精确地表示价值函数

62
00:03:56,930 --> 00:03:59,790
即使你有无限的状态空间

63
00:03:59,880 --> 00:04:02,590
或者即使你有一个连续状态空间

64
00:04:02,690 --> 00:04:04,800
实际上  我将做到这一点

65
00:04:04,900 --> 00:04:07,450
谈无限的状态MDPs中这些特殊的常量

66
00:04:07,540 --> 00:04:10,050
使用这一新的奖励函数

67
00:04:10,170 --> 00:04:12,170
并且选择进行计算

68
00:04:12,260 --> 00:04:14,050
因此开始使构想更容易一些

69
00:04:14,150 --> 00:04:18,600
所以我想谈的第一个变化是选择奖励

70
00:04:18,710 --> 00:04:26,450
所以我要改变的奖励函数的定义

71
00:04:26,550 --> 00:04:28,890
如果这事实证明

72
00:04:28,990 --> 00:04:31,700
这不会是一个没有太大影响

73
00:04:31,810 --> 00:04:34,100
特别是  我把奖励函数

74
00:04:34,210 --> 00:04:41,740
改变为从状态行动 到映射到实数的函数

75
00:04:41,820 --> 00:04:43,340
我的意思如下

76
00:04:43,400 --> 00:04:49,410
在把某些状态设置为0

77
00:04:49,490 --> 00:04:51,960
你采取行动A_0作为你的行动的

78
00:04:52,040 --> 00:04:53,690
选择状态的结果 你过渡到了

79
00:04:53,770 --> 00:04:56,700
一些新的状态  S_1 你采取一些行动  A_1

80
00:04:56,770 --> 00:04:58,350
你过渡到一些新的状态  S_2

81
00:04:58,410 --> 00:05:00,690
可以采取一些行动  A_2  等等

82
00:05:00,770 --> 00:05:02,910
因此  你看到这是该状态行动的顺序

83
00:05:02,990 --> 00:05:06,680
因此  在一个MPP中

84
00:05:06,770 --> 00:05:10,300
你有一个状态行动的奖励

85
00:05:10,380 --> 00:05:15,940
你的总收益现在定义为这个

86
00:05:16,030 --> 00:05:18,780
在那里你的奖励函数

87
00:05:18,880 --> 00:05:21,010
现在是一个当前状态  和当前状态下的

88
00:05:21,090 --> 00:05:27,470
动作的函数 所以这是我的总回报 ?

89
00:05:27,540 --> 00:05:31,340
然后像往常一样  我的目标

90
00:05:31,420 --> 00:05:33,680
是要找到一个政策 寻找从该状态

91
00:05:33,730 --> 00:05:36,580
行动的函数映射  当我执行这一政策时

92
00:05:36,660 --> 00:05:39,580
我可以最大化我的总收益的预期值

93
00:05:39,680 --> 00:05:44,900
所以这个定义  它实际上变成了

94
00:05:44,960 --> 00:05:46,940
MDP的状态行动回报

95
00:05:47,010 --> 00:05:48,620
其实你可以--由状态的定义

96
00:05:48,700 --> 00:05:50,720
你可以实际减少到一个

97
00:05:50,810 --> 00:05:52,340
拥有该状态下的

98
00:05:52,420 --> 00:05:53,520
回报函数的MDP? ?

99
00:05:53,610 --> 00:05:55,850
可能是显而易见的

100
00:05:55,950 --> 00:05:57,510
也可能不是

101
00:05:57,590 --> 00:06:01,290
不要担心  如果它不是 但是  使用状态

102
00:06:01,380 --> 00:06:03,400
行动回报使你能够更直接的模式化问题

103
00:06:03,480 --> 00:06:06,660
在不同的动作中

104
00:06:06,730 --> 00:06:08,150
我们有不同的成本

105
00:06:08,250 --> 00:06:10,390
因此  正在运行的例子是机器人 ?

106
00:06:10,450 --> 00:06:14,120
可能对于机器人来说  让机器人动起来

107
00:06:14,200 --> 00:06:16,250
比让它停留在原地更昂贵

108
00:06:16,330 --> 00:06:19,760
停留的

109
00:06:19,850 --> 00:06:22,260
可能有更低的成本

110
00:06:22,340 --> 00:06:24,060
因为你不使用电池电源

111
00:06:24,140 --> 00:06:25,500
充电使它可以行动

112
00:06:25,590 --> 00:06:30,250
另一个例子是--实际上

113
00:06:30,330 --> 00:06:34,010
另一种导航的例子是

114
00:06:34,100 --> 00:06:37,720
如果你有一个户外车辆

115
00:06:37,810 --> 00:06:41,870
你需要一些户外地形驾驶

116
00:06:41,970 --> 00:06:44,380
像非常粗糙的石子路或从草地上驶过

117
00:06:44,480 --> 00:06:46,580
它可能是更昂贵的  更困难的

118
00:06:46,660 --> 00:06:49,780
比驾驶在  比如  一个柏油路上

119
00:06:49,880 --> 00:06:54,600
所以  你可以指定一个动作

120
00:06:54,680 --> 00:06:56,620
要求驾驶在草地或

121
00:06:56,710 --> 00:06:58,340
驾驶在石子路上比在柏油路上更昂贵

122
00:07:05,060 --> 00:07:06,800
因此  这其实不是MDP的定义的

123
00:07:06,860 --> 00:07:11,660
一个大的变化 我真的不会刻意

124
00:07:11,740 --> 00:07:16,110
去证明这个  但[无声]方程

125
00:07:16,200 --> 00:07:21,000
可能是你期望的一般化的方式

126
00:07:21,090 --> 00:07:27,010
V*(S)现在是等于它的

127
00:07:36,900 --> 00:07:39,100
所以之前  当奖励函数

128
00:07:39,170 --> 00:07:41,090
只是一个函数状态  S

129
00:07:41,140 --> 00:07:46,080
我们可以采取的最大值  把它放在这里

130
00:07:46,170 --> 00:07:47,800
但是  现在  那个回报是你

131
00:07:47,880 --> 00:07:49,930
采取行动的函数  那个最大值来自于外部

132
00:07:50,030 --> 00:07:52,210
所以说  你的预计总回报  从状态出发

133
00:07:52,300 --> 00:07:54,360
当执行汽车保险单时  等于立即回报

134
00:07:54,450 --> 00:07:58,280
R(s  a)  用于执行一些动作  A

135
00:07:58,380 --> 00:08:01,410
在状态S下

136
00:08:01,500 --> 00:08:04,980
然后加伽玛乘以  预计未来总回报

137
00:08:05,070 --> 00:08:10,040
因此  这是你预期的总收益

138
00:08:10,140 --> 00:08:12,500
如果你采取行动A  从当前的状态

139
00:08:12,580 --> 00:08:16,250
因此  尽管这些【无声】的最优值函数

140
00:08:16,340 --> 00:08:18,180
所以  你实际上是最佳的预期

141
00:08:18,260 --> 00:08:20,520
总收益是右边的

142
00:08:20,600 --> 00:08:22,560
这个事情行动的最大值

143
00:08:22,660 --> 00:08:28,650
让我们来看看 值迭代

144
00:08:28,740 --> 00:08:31,660
我缩写为VI  其实是相同的算法 ?

145
00:08:34,660 --> 00:08:36,180
V(s)是更新为最大的max a

146
00:08:36,260 --> 00:08:42,290
R(SA)  同样的事情

147
00:08:42,380 --> 00:08:44,350
只要列在方程的

148
00:08:44,440 --> 00:08:48,570
右边更新V(s)

149
00:08:48,650 --> 00:08:50,330
使用[无声]方程 同样  你得到了值迭代

150
00:08:50,420 --> 00:08:52,260
以完全相同的方式

151
00:08:52,380 --> 00:08:58,720
最后  找到最优值函数  V

152
00:08:58,820 --> 00:09:02,210
用值迭代算法  你便可以计算出

153
00:09:02,300 --> 00:09:04,900
最佳的政策

154
00:09:04,980 --> 00:09:08,830
像以前一样π*(S) 在状态S中

155
00:09:08,940 --> 00:09:12,470
采取最好的行动A

156
00:09:12,570 --> 00:09:14,640
右边的东西最大化

157
00:09:14,730 --> 00:09:30,660
因此  使

158
00:09:30,740 --> 00:09:34,600
用值迭代计算最优值函数

159
00:09:34,700 --> 00:09:36,880
然后你可以使用它找到π

160
00:09:36,980 --> 00:09:46,000
因此  目前为止  这是两个更简单的

161
00:09:46,110 --> 00:09:49,560
两个MDP的变式 有问题吗?

162
00:09:49,650 --> 00:09:51,290
其实  对这个有什么问题吗?

163
00:10:00,970 --> 00:10:05,230
所以另一个变式  另一种定义

164
00:10:05,320 --> 00:10:12,060
将有限的边界MDPs

165
00:10:15,710 --> 00:10:22,290
因此  有限的边界的MDP

166
00:10:22,380 --> 00:10:25,580
包括[听不清] SA状态转换可能性

167
00:10:25,670 --> 00:10:31,060
可能有这些  参数T和奖励函数

168
00:10:31,170 --> 00:10:38,710
在这里  T是一个参数

169
00:10:38,810 --> 00:10:43,470
称为边界时间 具体地说

170
00:10:43,560 --> 00:10:47,220
它的真正含义是  我们将在

171
00:10:47,330 --> 00:10:51,100
MDP的采取行动  只为一个总的

172
00:10:51,190 --> 00:10:53,590
大写T的倍数  所以我们不会再使用此计数

173
00:10:53,680 --> 00:10:59,780
[声音中断]

174
00:10:59,860 --> 00:11:04,060
在某些情况下为零  采取行动A_0

175
00:11:04,150 --> 00:11:07,620
得到一些其他状态S_1  采取行动A_1等

176
00:11:07,710 --> 00:11:11,420
最终  你会得到一些状态的S_TA_T

177
00:11:11,500 --> 00:11:16,950
经过T次[听不清] 所以我总的回报

178
00:11:17,050 --> 00:11:23,270
现在  将将给出这个总和  从0时间

179
00:11:23,360 --> 00:11:25,490
到T时间  回报的总和 OK?像往常一样

180
00:11:25,590 --> 00:11:33,350
我的目标--所以这是我的总收益 ?

181
00:11:33,460 --> 00:11:36,160
像往常一样  我的目标是最大化

182
00:11:36,280 --> 00:11:38,000
我的总收益的预期值

183
00:11:38,100 --> 00:11:40,090
我们要拿出一个政策

184
00:11:40,200 --> 00:11:42,830
以最大化这个总回报的预期值

185
00:11:42,940 --> 00:11:46,060
关键的区别是

186
00:11:46,170 --> 00:11:48,540
世界上唯一会存在[无声]

187
00:11:48,640 --> 00:11:50,590
并在这之后  没有要纠正的奖励

188
00:11:50,720 --> 00:11:53,990
因此  这原来是[无声]的差别

189
00:11:54,090 --> 00:11:55,960
因为事实证明  最优的政策

190
00:11:56,070 --> 00:12:07,860
可能是非平稳的 这一项

191
00:12:07,950 --> 00:12:10,820
固定的  是指它不依赖于时间

192
00:12:10,930 --> 00:12:13,870
非平稳的意思是  它可能取决于时间

193
00:12:13,960 --> 00:12:18,350
因此  非平稳大致的意思

194
00:12:18,450 --> 00:12:21,360
我采取的最佳行动

195
00:12:21,440 --> 00:12:23,240
将是随着不同的时间步而不同

196
00:12:23,340 --> 00:12:24,640
这就是非平稳的意思

197
00:12:24,740 --> 00:12:29,310
正如一个例子  假设我们

198
00:12:29,400 --> 00:12:32,280
有一些机器人 比方说  机器人是在这里

199
00:12:32,380 --> 00:12:38,620
比方说  有一个很好的

200
00:12:38,710 --> 00:12:43,470
那边+1奖励

201
00:12:43,590 --> 00:12:47,820
在那一边  有一个+10的奖励

202
00:12:47,920 --> 00:12:49,650
因此  取决于你剩余多少时间

203
00:12:49,750 --> 00:12:52,370
它可以更好地去获得

204
00:12:52,470 --> 00:12:54,120
+1或+10的奖励

205
00:12:54,220 --> 00:12:57,360
如果它仍然是处于游戏的初期

206
00:12:57,440 --> 00:12:59,230
你仍然有大量的时间

207
00:12:59,320 --> 00:13:01,190
它可能是更好的走向+10的奖励

208
00:13:01,270 --> 00:13:04,220
以获得更大的奖励

209
00:13:04,320 --> 00:13:06,650
如果你只剩下几个时间步长

210
00:13:06,740 --> 00:13:09,060
如果时间已接近末尾了  大写T

211
00:13:09,160 --> 00:13:11,040
那么你可能没有足够的时间

212
00:13:11,120 --> 00:13:13,340
去获得+10的奖励

213
00:13:13,430 --> 00:13:14,830
你最好获得+1的奖励  它近得多

214
00:13:14,930 --> 00:13:17,910
因此  这个例子说明的是

215
00:13:18,030 --> 00:13:21,300
当你在该状态的时候

216
00:13:21,390 --> 00:13:23,610
可以采取最好的行动是  向左或向右走

217
00:13:23,720 --> 00:13:25,540
取决于它是什么时间

218
00:13:25,640 --> 00:13:28,470
所以只是作为一个例子

219
00:13:28,580 --> 00:13:29,900
说明政策是如何成为非平稳的

220
00:13:30,020 --> 00:13:39,160
事实上  因为我们的序列中

221
00:13:39,280 --> 00:13:44,680
我要做下一步  我将允许

222
00:13:44,780 --> 00:13:48,530
非平稳过渡的概率

223
00:13:48,640 --> 00:13:52,640
所以我就写在那里 我的意思是

224
00:13:52,770 --> 00:13:55,600
到目前为止  假设状态S_T+1

225
00:13:55,750 --> 00:13:57,760
从状态转移概率

226
00:13:57,870 --> 00:14:01,150
加入[听不清]  通过以前的状态

227
00:14:01,250 --> 00:14:03,120
和以往的动作

228
00:14:03,230 --> 00:14:04,950
我一直在假设  这些状态的

229
00:14:05,060 --> 00:14:06,710
转移概率在所有时间上是相同的

230
00:14:06,840 --> 00:14:08,390
所以 我想说在某些状态下

231
00:14:08,520 --> 00:14:11,520
并采取一些行动  一个固有的分布

232
00:14:11,630 --> 00:14:16,940
它不依赖于时间 所以我打算

233
00:14:17,020 --> 00:14:19,550
研究更一般的定义

234
00:14:19,690 --> 00:14:23,090
我们有非平稳状态转移概率

235
00:14:23,200 --> 00:14:25,930
使你最终[听不清]的机会

236
00:14:26,060 --> 00:14:28,600
也可能取决于

237
00:14:28,720 --> 00:14:32,310
它是什么时间 ?

238
00:14:32,430 --> 00:14:35,510
因此  这种非平稳状态转移概率的例子

239
00:14:35,630 --> 00:14:39,790
一个例子是  如果你的建立模型的飞机

240
00:14:39,900 --> 00:14:43,830
要飞很长的距离 然后  飞机飞行时

241
00:14:43,940 --> 00:14:46,250
你燃烧燃料  变得更轻

242
00:14:46,370 --> 00:14:48,970
所以飞机的动态随着时间的推移而变化

243
00:14:49,100 --> 00:14:50,910
飞机的质量随时间变化显著

244
00:14:51,020 --> 00:14:53,240
因为你燃烧燃料 因此

245
00:14:53,360 --> 00:14:55,920
取决于它是什么时间

246
00:14:56,060 --> 00:14:58,180
你的混合状态实际上

247
00:14:58,300 --> 00:15:02,690
可能取决于不仅是你的当前状态

248
00:15:02,820 --> 00:15:05,030
和你的行动  但也取决于你烧多少燃料

249
00:15:05,150 --> 00:15:06,820
即  它处于什么时间 ?

250
00:15:06,940 --> 00:15:09,230
其他的例子  另一个航天之一

251
00:15:09,370 --> 00:15:12,230
如果你在接下来的24小时内的天气预测

252
00:15:12,380 --> 00:15:16,210
说  你知道在未来24小时内的

253
00:15:16,350 --> 00:15:17,970
大风和降水情况 话又说回来

254
00:15:18,100 --> 00:15:20,020
如果你飞的飞机  比如说

255
00:15:20,180 --> 00:15:22,720
从这里到纽约  它可能成本不同

256
00:15:22,840 --> 00:15:25,660
按不同的路线飞行 在不同的时间中

257
00:15:25,820 --> 00:15:28,130
也许在落基山脉的

258
00:15:28,250 --> 00:15:31,120
飞行成本总额可能不同

259
00:15:31,250 --> 00:15:33,520
取决于现在你是否处理它

260
00:15:33,640 --> 00:15:35,190
当有一个好天气

261
00:15:35,320 --> 00:15:36,940
或者如果你从现在几个小时开始

262
00:15:37,090 --> 00:15:38,860
天气预测可能真的很糟

263
00:15:38,990 --> 00:15:42,920
举一个你每天看到的例子

264
00:15:43,050 --> 00:15:45,290
同样的事情  对于交通  对不?

265
00:15:45,420 --> 00:15:48,820
有至少--取决于你位于哪里

266
00:15:48,950 --> 00:15:50,590
当然这里在加利福尼亚州

267
00:15:50,720 --> 00:15:52,230
一天中的时间  在许多地方

268
00:15:52,350 --> 00:15:53,550
这里交通实在是太差

269
00:15:53,690 --> 00:15:55,770
因此  一些道路驾驶的成本

270
00:15:55,910 --> 00:15:57,570
可能会有所不同  这取决于

271
00:15:57,700 --> 00:15:59,200
它是一天中什么时间

272
00:15:59,340 --> 00:16:01,700
很多其他的例子 工业自动化

273
00:16:01,830 --> 00:16:03,870
工厂里有不同的机器

274
00:16:04,010 --> 00:16:06,200
可能会在一天的不同时间  不同文化程度

275
00:16:06,320 --> 00:16:08,060
聘请不同的工人  他们的成本数额不同

276
00:16:08,190 --> 00:16:09,990
这取决于你是否随着时间的推移

277
00:16:10,110 --> 00:16:13,200
付晚间加班费等等 所以  在工厂

278
00:16:13,330 --> 00:16:14,770
做不同的事情的成本

279
00:16:14,890 --> 00:16:16,820
也可以是时间的函数

280
00:16:16,950 --> 00:16:20,450
状态转移概率

281
00:16:20,540 --> 00:16:28,150
也可以是时间的函数

282
00:16:28,280 --> 00:16:31,010
最后  当我们这样做的时候

283
00:16:31,150 --> 00:16:33,510
这是完全通用的  我们也可能会

284
00:16:33,650 --> 00:16:39,130
有非固定奖励  你也可能会指出这些时间

285
00:16:39,260 --> 00:16:41,890
和命令奖励函数

286
00:16:42,040 --> 00:16:44,740
处理不同的事情的成本

287
00:16:44,900 --> 00:16:46,490
可能取决于时间

288
00:16:46,640 --> 00:16:51,380
其实  还有更多的

289
00:16:51,510 --> 00:16:53,600
非平稳MDP的例子  所以让我们的

290
00:16:53,730 --> 00:16:56,880
所以现在我们有一个非平稳的政策

291
00:16:57,020 --> 00:16:58,410
让我们来谈谈一个算法来尝试

292
00:16:58,550 --> 00:17:00,120
找到最佳的政策

293
00:17:00,250 --> 00:17:05,600
因此  让我做如下定义

294
00:17:05,730 --> 00:17:15,440
这是目前的最优值函数的

295
00:17:15,550 --> 00:17:17,130
定义的略作修改

296
00:17:17,290 --> 00:17:20,570
我就写下来  我猜

297
00:17:34,510 --> 00:17:40,510
所以我要来定义最优值函数

298
00:17:40,630 --> 00:17:43,050
这将会是索引T  下标为T

299
00:17:43,220 --> 00:17:45,570
T时间的一种状态的最优值

300
00:17:45,710 --> 00:17:49,860
我们要定义为奖励最佳的总和

301
00:17:49,980 --> 00:17:55,070
如果你在该状态开始MDP

302
00:17:55,200 --> 00:17:58,360
如果在时间的时钟

303
00:17:58,500 --> 00:18:03,620
小写t开始

304
00:18:03,730 --> 00:18:05,560
所以一个状态的最优值

305
00:18:05,680 --> 00:18:07,010
将取决于它是什么时候

306
00:18:07,120 --> 00:18:10,830
以及你有多少时间来运行此MDP

307
00:18:10,950 --> 00:18:14,370
因此  右边的总和  总共为时间T的

308
00:18:14,490 --> 00:18:18,020
时间T+1  时间T+2  到时间大写T

309
00:18:18,170 --> 00:18:23,990
我在用英语陈述一遍

310
00:18:24,120 --> 00:18:27,370
这是你预期的最佳总回报

311
00:18:27,500 --> 00:18:32,780
如果你启动系统在一个状态  S

312
00:18:32,900 --> 00:18:35,550
如果时钟已经在时间  小写t

313
00:18:35,700 --> 00:18:40,050
因此  原来有一个[无声]

314
00:18:40,160 --> 00:18:43,110
你可以估价[听不清]

315
00:18:43,250 --> 00:18:45,320
让我为它写出价值[听不清]算法

316
00:18:45,450 --> 00:18:50,130
事实证明  你可以--

317
00:18:50,260 --> 00:18:57,340
好  让我只写了这一点

318
00:18:57,490 --> 00:18:59,350
我在下面写下来

319
00:18:59,490 --> 00:19:01,550
原来  你可以使用以下递归

320
00:19:01,690 --> 00:19:04,570
来计算MDP的最优值函数

321
00:19:04,710 --> 00:19:08,040
这和我们的值迭代是非常相似的

322
00:19:08,190 --> 00:19:12,930
我们要设置的V(s)等于max 下面A

323
00:19:13,070 --> 00:19:16,500
和之前一样  对吗??

324
00:19:38,770 --> 00:19:43,980
OK?所以  如果我在时钟时间T

325
00:19:44,110 --> 00:19:48,140
并从状态S开始  我的预期总收益

326
00:19:48,390 --> 00:19:50,600
等于最大[听不清]的动作

327
00:19:50,720 --> 00:19:53,190
我可能需要立即回报 采取

328
00:19:53,330 --> 00:19:55,590
这一行动A  在那个状态S中

329
00:19:55,740 --> 00:19:58,530
他然后加上我的预期未来收益

330
00:19:58,700 --> 00:20:02,660
所以  如果我采取行动  我将过渡到

331
00:20:02,780 --> 00:20:04,910
[听不清]P下标sa  S`到一些新的状态S`

332
00:20:05,030 --> 00:20:07,850
如果我得到达状态S`

333
00:20:07,990 --> 00:20:10,940
我从状态S`得到的

334
00:20:11,090 --> 00:20:12,850
预计总回报

335
00:20:12,970 --> 00:20:16,180
将是这些V*_t+1

336
00:20:16,330 --> 00:20:19,690
S` 下标t+1反映

337
00:20:19,820 --> 00:20:21,720
我已经采取了一个动作后

338
00:20:21,850 --> 00:20:24,380
我的时钟将时间t

339
00:20:24,510 --> 00:20:27,980
前进到时间t+1

340
00:20:28,140 --> 00:20:32,770
因此  这个表达V*_t

341
00:20:32,910 --> 00:20:37,450
依据V*_t+1 最后  启动这个递归

342
00:20:37,590 --> 00:20:42,970
你将得到V*_T(s)等于--

343
00:20:43,080 --> 00:20:52,330
它只是等于它 如果你已经在时间

344
00:20:52,450 --> 00:20:56,120
大写T  那么你只得到一个动作

345
00:20:56,190 --> 00:20:57,850
然后时钟耗尽 ?因此  这是V*_T

346
00:20:57,970 --> 00:21:00,820
开始在一些状态S的值

347
00:21:00,970 --> 00:21:03,640
没有时间--只是一个时间步长

348
00:21:03,760 --> 00:21:06,090
就没有时间了

349
00:21:06,220 --> 00:21:10,640
因此  在有限边界的MDP的情况下

350
00:21:10,780 --> 00:21:14,030
这实际上给了一个非常漂亮的

351
00:21:14,180 --> 00:21:16,870
动态规划算法  你可以

352
00:21:17,010 --> 00:21:18,930
在其中开始计算V*_T

353
00:21:19,060 --> 00:21:25,060
则使用这种反向的变换

354
00:21:25,180 --> 00:21:27,970
来计算V*_T-1  V*_T-2等等

355
00:21:28,080 --> 00:21:29,980
我们计算V*_T  V*_T-1等

356
00:21:30,110 --> 00:21:31,390
它再次反向计算

357
00:21:31,490 --> 00:21:33,330
为所有时间

358
00:21:33,460 --> 00:21:36,450
步长计算V

359
00:21:36,580 --> 00:21:44,320
你能看到这块黑板吗?酷

360
00:21:44,430 --> 00:21:48,170
然后  最后一步是--以前

361
00:21:48,300 --> 00:21:50,180
我们说  Π*(s)

362
00:21:50,300 --> 00:21:51,850
我回来改动一点点时

363
00:21:51,970 --> 00:21:54,090
[无声的]R(s  a)加[听不清] P(s  a)

364
00:21:54,200 --> 00:22:05,100
这就是我们得到的东西

365
00:22:05,210 --> 00:22:10,400
在有限边界的情况下

366
00:22:10,530 --> 00:22:12,500
最终的行动可能取决于它是什么时候

367
00:22:12,630 --> 00:22:14,030
所以最终要采取的行动  时间T

368
00:22:14,170 --> 00:22:15,490
是[无声的]行动  A

369
00:22:15,630 --> 00:22:24,370
这基本上是完全相同

370
00:22:24,500 --> 00:22:25,790
和右边是同样的事情

371
00:22:25,910 --> 00:22:28,890
在我们的动态规划算法中

372
00:22:29,000 --> 00:22:32,170
所以你为每一个时间步这样处理

373
00:22:32,300 --> 00:22:35,360
现在你计算它

374
00:22:35,520 --> 00:22:38,920
不同的时间步长的最佳政策

375
00:22:39,050 --> 00:22:43,440
再次  这是一个非平稳的政策

376
00:22:43,570 --> 00:22:45,280
因为Π*(S)是依靠于什么时间

377
00:22:45,410 --> 00:22:48,770
所以[听不清]这个和值迭代的

378
00:22:48,910 --> 00:22:52,500
更早期版本之间的区别是

379
00:22:52,610 --> 00:22:55,000
所以你要做的就是计算V*_T

380
00:22:55,130 --> 00:22:57,420
然后使用反向递归的

381
00:22:57,550 --> 00:22:59,410
那个【无声】算法

382
00:22:59,520 --> 00:23:01,380
计算V*_T-1  V*_T-2  等等

383
00:23:01,500 --> 00:23:04,500
最后到V*_0

384
00:23:04,620 --> 00:23:07,610
你用这种方法计算Π

385
00:23:07,730 --> 00:23:09,590
等等

386
00:23:09,710 --> 00:23:15,890
所以1--不存在巨大的差异

387
00:23:16,020 --> 00:23:18,870
但1减的区别[无声]无限的边界上

388
00:23:18,980 --> 00:23:23,710
打折的情况下

389
00:23:23,850 --> 00:23:26,090
通过运行此递归一次

390
00:23:26,210 --> 00:23:28,800
你现在有完全正确的价值函数

391
00:23:29,140 --> 00:23:31,020
因此  这只是计算价值函数

392
00:23:31,130 --> 00:23:33,670
而非只是收敛[听不清]

393
00:23:33,790 --> 00:23:36,660
这只是一次给你正确的价值函数

394
00:23:36,790 --> 00:23:42,550
酷 有任何问题吗?是啊 受访者:

395
00:23:42,660 --> 00:23:44,570
[听不清]

396
00:23:44,690 --> 00:23:53,170
这种计算比估值更简短得多

397
00:23:53,300 --> 00:23:55,220
所以yes还是no

398
00:23:55,330 --> 00:23:57,270
这取决于大写T有多大

399
00:23:57,400 --> 00:24:01,060
受访者:?[听不清]正常的MDP

400
00:24:01,170 --> 00:24:03,590
可以[无声]  然后这种情况下

401
00:24:03,700 --> 00:24:05,630
使用这个例子呢?

402
00:24:05,740 --> 00:24:07,180
我明白了 因此正常的MDP

403
00:24:07,290 --> 00:24:10,640
你可以假设大写T  然后假设这个?

404
00:24:10,750 --> 00:24:16,590
实际上  事实证明--这是一个很好的问题

405
00:24:16,700 --> 00:24:18,030
让我以这个手-波浪方式来回答

406
00:24:18,150 --> 00:24:19,730
因此  它实际上对于

407
00:24:19,860 --> 00:24:22,340
贴现无限的边界MDP

408
00:24:22,470 --> 00:24:25,950
一些折扣因子γ 所以你可以做的是

409
00:24:26,100 --> 00:24:30,210
你可以说  在γT次方后

410
00:24:30,350 --> 00:24:33,830
γ^T将是非常小

411
00:24:33,960 --> 00:24:36,130
它会像是0.00001之类的东西

412
00:24:36,270 --> 00:24:38,640
很多次之后  我不在乎发生了什么

413
00:24:38,780 --> 00:24:40,510
因为奖励是伽马乘以T

414
00:24:40,630 --> 00:24:42,440
之后  我不在乎

415
00:24:42,570 --> 00:24:44,510
所以  你可以问  我可以采用

416
00:24:44,640 --> 00:24:47,330
我的无限的边界贴现的MDP

417
00:24:47,450 --> 00:24:50,910
并且使用无限的边界MDP来估计

418
00:24:51,050 --> 00:24:53,570
次数  步骤T  是选择的

419
00:24:53,690 --> 00:24:55,280
所以[听不清]是真的 因此

420
00:24:55,400 --> 00:24:56,520
原来你可以这样做 然后

421
00:24:56,660 --> 00:24:59,870
你最终一些T的价值

422
00:24:59,990 --> 00:25:03,030
你可以使它为真  解出T

423
00:25:03,140 --> 00:25:05,270
结果你可以证明

424
00:25:05,420 --> 00:25:07,380
如果你使用原来的值

425
00:25:07,520 --> 00:25:10,490
迭代算法

426
00:25:10,610 --> 00:25:13,260
如果你运行[无声]算法的原始价值

427
00:25:13,400 --> 00:25:14,710
贴现MDPs版本

428
00:25:14,850 --> 00:25:17,770
如果你运行此相同的时间步数

429
00:25:17,880 --> 00:25:20,340
你会最终得到

430
00:25:20,500 --> 00:25:22,080
价值函数的近似值

431
00:25:22,220 --> 00:25:24,810
是这个的逼近

432
00:25:24,960 --> 00:25:26,420
一些很小的常量因子

433
00:25:26,590 --> 00:25:29,340
所以要这样做  你最终得到

434
00:25:29,470 --> 00:25:32,440
大致相同估算 然后你实际上

435
00:25:32,590 --> 00:25:34,220
是一个非平稳的政策

436
00:25:34,330 --> 00:25:35,910
这是更昂贵的来保持

437
00:25:36,050 --> 00:25:38,790
你需要保持每一个时间步长

438
00:25:38,930 --> 00:25:41,550
有不同的政策  这不是很好的

439
00:25:41,670 --> 00:25:43,930
如果你有固定的政策

440
00:25:44,040 --> 00:25:45,920
任何时候都相同的政策

441
00:25:46,010 --> 00:25:49,860
因此  还有其他原因

442
00:25:49,980 --> 00:25:52,100
但有时你可能需要一个无限的

443
00:25:52,200 --> 00:25:54,140
边界贴现问题  并且估算

444
00:25:54,350 --> 00:25:58,650
有限边界问题 但是  这个特别的原因不是

445
00:25:58,750 --> 00:26:04,560
这个 明白么  有其它的问题吗?

446
00:26:04,690 --> 00:26:14,810
受访者:[无声]??

447
00:26:14,930 --> 00:26:16,260
这里面有γ吗?所以如果你想

448
00:26:16,410 --> 00:26:19,070
你其实可以改变MDP的定义

449
00:26:19,170 --> 00:26:21,790
并用有限的边界贴现MDP

450
00:26:21,910 --> 00:26:26,470
如果你愿意  你可以做到这一点

451
00:26:26,580 --> 00:26:29,690
实际上  你可以进来

452
00:26:29,820 --> 00:26:34,010
把γ放进去  并使用这个

453
00:26:34,130 --> 00:26:36,910
来计数有限的边界

454
00:26:37,010 --> 00:26:39,180
原来  通常情况下

455
00:26:39,300 --> 00:26:40,270
对于人们处理的大多数问题

456
00:26:40,420 --> 00:26:41,830
你可以使用贴现  或使用有限的边界

457
00:26:41,930 --> 00:26:45,150
做到两者已经不太常见

458
00:26:45,250 --> 00:26:47,630
但你肯定可以做

459
00:26:47,750 --> 00:26:50,370
关于贴现的好处之一是

460
00:26:50,490 --> 00:26:53,740
它使你的价值函数是有限的

461
00:26:53,860 --> 00:26:56,000
算法上和数学上

462
00:26:56,120 --> 00:26:59,230
使用贴现的原因之一

463
00:26:59,350 --> 00:27:01,450
是因为你乘以指数形式的回报

464
00:27:01,570 --> 00:27:03,730
这是一个几何[听不清]系列

465
00:27:03,880 --> 00:27:05,460
它表明的价值函数始终是有限的

466
00:27:05,570 --> 00:27:06,790
这是一个非常好的

467
00:27:06,920 --> 00:27:08,710
当你做贴现的时候

468
00:27:08,830 --> 00:27:11,180
所以  当你有一个有限的边界

469
00:27:11,320 --> 00:27:13,550
那么值函数  也保证是有限的

470
00:27:13,660 --> 00:27:14,820
所以  你不必使用折扣

471
00:27:14,930 --> 00:27:16,860
但是  如果你愿意

472
00:27:16,970 --> 00:27:18,540
你也可以折扣

473
00:27:18,690 --> 00:27:23,790
受访者:[听不清] ?

474
00:27:23,880 --> 00:27:25,700
是的  没错  你说得对 如果你愿意

475
00:27:25,810 --> 00:27:28,110
你可以重新定义奖励函数

476
00:27:28,250 --> 00:27:30,310
进入向下的奖励函数

477
00:27:30,450 --> 00:27:32,230
因为我们有非平稳奖励 ?

478
00:27:36,970 --> 00:27:45,940
所以这是有限的边界的MDPs

479
00:27:46,060 --> 00:27:49,800
我现在想要做的是  使用的这两个想法

480
00:27:49,920 --> 00:27:51,690
你的状态行动回报和有限边界

481
00:27:51,820 --> 00:27:54,920
MDPs来描述MDP的情况

482
00:27:55,050 --> 00:27:58,560
来做一个非常强的假设  关于与该问题的

483
00:27:58,680 --> 00:28:01,690
但是  这些假设对于许多系统是合理的

484
00:28:01,830 --> 00:28:04,550
有了这些假设

485
00:28:04,650 --> 00:28:06,390
我们可得到  我觉得

486
00:28:06,530 --> 00:28:08,590
都很好  很优雅的算法

487
00:28:08,730 --> 00:28:10,380
来解决即使是非常大的MDPs

488
00:28:10,540 --> 00:28:33,930
因此  让我们来谈谈

489
00:28:45,320 --> 00:28:53,410
线性二次型法则

490
00:28:53,530 --> 00:28:55,980
我们刚才谈到有关有限边界MDPs的

491
00:28:56,100 --> 00:28:57,850
动态规划  所以要记住那个算法

492
00:28:58,010 --> 00:29:00,520
当我回过头来谈谈解决的LQR问题的算法

493
00:29:00,610 --> 00:29:02,550
实际上  我使用完全相同的

494
00:29:02,700 --> 00:29:04,240
动态规划算法  你刚才看到

495
00:29:04,350 --> 00:29:07,420
有限的边界MDPs

496
00:29:07,550 --> 00:29:10,750
我会再次使用该算法

497
00:29:10,860 --> 00:29:12,180
因此  现在要记住它

498
00:29:12,330 --> 00:29:17,330
因此  让我们谈谈LQR

499
00:29:17,460 --> 00:29:19,930
所以我想采取这些想法

500
00:29:20,070 --> 00:29:22,970
并把它们应用到连续状态空间

501
00:29:23,100 --> 00:29:26,990
甚至连续动作空间的MDPs中

502
00:29:27,120 --> 00:29:34,000
因此  指定MDPs  我需要给你

503
00:29:34,120 --> 00:29:35,930
这个5元组的状态行动

504
00:29:36,050 --> 00:29:38,840
转换可能性在奖励中 我要利用有限的边界

505
00:29:38,950 --> 00:29:41,370
大写T  而不是折现

506
00:29:41,500 --> 00:29:45,770
所以LQR问题  我会假设以下

507
00:29:45,930 --> 00:29:48,670
我要假设状态

508
00:29:48,810 --> 00:29:52,800
空间是R^n

509
00:29:52,920 --> 00:29:57,310
我要去假设  同时  连续的动作设置

510
00:29:57,460 --> 00:30:00,970
位于R^d(原文RT)中

511
00:30:01,100 --> 00:30:07,680
要指定状态转移概率  P_sa

512
00:30:07,800 --> 00:30:10,670
我要告诉你什么是混合状态分布

513
00:30:10,790 --> 00:30:13,240
鉴于目前的状态和当前的行动

514
00:30:13,360 --> 00:30:15,870
因此  我们在上一讲看到过这一点点

515
00:30:16,010 --> 00:30:18,290
我要假设未来的状态  S_t+1

516
00:30:18,430 --> 00:30:21,250
将是之前状态的一个

517
00:30:21,430 --> 00:30:31,550
哦  对不起 我的意思是下标

518
00:30:53,680 --> 00:30:57,610
其中Wt是高斯[无声]意味零

519
00:30:57,730 --> 00:31:00,270
并给出了一些由Sigma W

520
00:31:00,390 --> 00:31:07,010
A和B下标T以显示

521
00:31:07,140 --> 00:31:10,290
这些矩阵随着时间的推移  可以改变

522
00:31:10,410 --> 00:31:12,710
因此  这就是非平稳动态

523
00:31:12,830 --> 00:31:17,550
作为一个表示法的要点  不幸的是

524
00:31:17,660 --> 00:31:21,110
编译多个文献的思想

525
00:31:21,210 --> 00:31:23,870
所以不幸的是  大写A

526
00:31:24,000 --> 00:31:28,240
表示的是行动  以及一套矩阵 ?

527
00:31:28,370 --> 00:31:31,840
当你以后看到A时

528
00:31:31,930 --> 00:31:34,120
将通常被用来表示一个矩阵

529
00:31:34,230 --> 00:31:36,120
而不是一组动作 [听不清]重用这个符号

530
00:31:36,240 --> 00:31:39,670
但不幸的是  符号约定

531
00:31:39,790 --> 00:31:42,680
当你在多个研究团体中的研究思路

532
00:31:42,820 --> 00:31:44,610
往往他们共享相同的符号

533
00:31:44,730 --> 00:31:48,600
所以具体点  A_t是一个

534
00:31:48,710 --> 00:31:51,720
N*N的矩阵

535
00:31:51,840 --> 00:32:03,180
B_t是N*d的矩阵

536
00:32:03,310 --> 00:32:05,860
对  矩阵A和B

537
00:32:06,000 --> 00:32:07,780
我要假设  是固定的  提前已知的

538
00:32:07,900 --> 00:32:10,020
所以我打算给你的矩阵

539
00:32:10,130 --> 00:32:13,010
A和B  我要去给你ΣW

540
00:32:13,130 --> 00:32:15,300
你的工作是要为这个

541
00:32:15,450 --> 00:32:17,440
MDP找到一个好的政策 ?

542
00:32:17,530 --> 00:32:20,620
所以  换句话说  这是我的

543
00:32:20,730 --> 00:32:25,540
状态转移概率的规范 往前看

544
00:32:25,620 --> 00:32:30,160
我们等下看这个  事实证明这个噪音项

545
00:32:30,290 --> 00:32:39,870
不是很重要 因此  事实证明

546
00:32:39,970 --> 00:32:43,110
这个噪音项的处理不是很重要

547
00:32:43,230 --> 00:32:45,170
我们会看到这一点

548
00:32:45,280 --> 00:32:49,490
我们几乎可以忽略的噪音项

549
00:32:49,610 --> 00:32:52,560
我们还是能做好 这仅仅是一个警告

550
00:32:52,680 --> 00:32:56,020
在后果中  我后来做的是  我可能会在处理

551
00:32:56,150 --> 00:32:58,930
噪音项的时候稍有马虎

552
00:32:59,070 --> 00:33:01,320
在这个非常特殊的情况下

553
00:33:01,490 --> 00:33:03,800
这将是不重要的

554
00:33:03,940 --> 00:33:08,120
最后一个我要指定的东西  是一些边界时间

555
00:33:08,250 --> 00:33:14,050
然后我也有一些的奖励函数

556
00:33:14,170 --> 00:33:18,290
对于LQR控制  我要假定的奖励函数

557
00:33:18,390 --> 00:33:20,840
可以写成这样  其中

558
00:33:20,980 --> 00:33:36,730
U_T是一个N*N的矩阵

559
00:33:36,870 --> 00:33:42,870
V_T 是一个D*D的矩阵  我假设  UT

560
00:33:43,010 --> 00:33:49,390
和VT都是正的半正定的

561
00:33:49,510 --> 00:33:54,320
都是Psd的 因此  事实上  U_T和V_T

562
00:33:54,430 --> 00:33:57,730
两个半正定矩阵  这就意味着

563
00:33:57,840 --> 00:34:03,470
S^T_t U_T  S_T 大于等于零

564
00:34:03,600 --> 00:34:07,950
同样  a^T_t是V_t  A_T  大于等于零

565
00:34:08,070 --> 00:34:16,690
因此  这意味着

566
00:34:16,820 --> 00:34:19,060
你的回报总是负的

567
00:34:19,170 --> 00:34:22,090
这是一个有点令人沮丧的MDP

568
00:34:22,240 --> 00:34:25,090
其中只有成本并没有正的奖励

569
00:34:25,190 --> 00:34:29,240
好的  因为那里的减号

570
00:34:43,630 --> 00:34:48,280
因此  作为一个完整的例子

571
00:34:48,410 --> 00:34:50,860
你怎么样去应用它  你看过了直升机录像

572
00:34:50,980 --> 00:34:52,400
对不对?所以一件事情是

573
00:34:52,500 --> 00:34:58,040
例如  假设你有一个直升机

574
00:34:58,170 --> 00:35:02,710
你想要的状态S_T要尽可能接近零

575
00:35:02,800 --> 00:35:10,450
这时  你可能选择UT等于单位矩阵

576
00:35:10,580 --> 00:35:13,360
这将使得R(S_T  A_T)

577
00:35:13,490 --> 00:35:19,300
等于S^T_tS_T 但是

578
00:35:19,430 --> 00:35:25,210
这只是--我就写下来的

579
00:35:25,310 --> 00:35:34,370
[无声]哦  对不起--减号

580
00:35:34,470 --> 00:35:36,940
负的矩阵向量[听不清]的平方

581
00:35:37,040 --> 00:35:39,850
所以这将是惩罚系统二次方

582
00:35:39,940 --> 00:35:42,310
对于远离0的状态

583
00:35:42,430 --> 00:35:45,200
假设为那是零的原状态

584
00:35:45,290 --> 00:35:48,010
所以

585
00:35:48,110 --> 00:35:49,620
如果使直升机徘徊在0状态附近

586
00:35:49,730 --> 00:35:51,630
那么你可能会选择这种奖励函数 ?

587
00:35:51,780 --> 00:35:56,690
事实证明  选择行动的成本也非常普遍

588
00:35:56,810 --> 00:35:59,890
因此  假设我选择的V_T

589
00:36:00,010 --> 00:36:02,380
等于一个单位矩阵

590
00:36:02,490 --> 00:36:04,070
我得到-a^T_ta_t

591
00:36:04,190 --> 00:36:11,100
然后减去[听不清]行动

592
00:36:11,240 --> 00:36:15,350
包括行动的二次方成本

593
00:36:15,450 --> 00:36:17,530
它也是一个相当普遍的事情

594
00:36:17,640 --> 00:36:19,230
在实践中

595
00:36:19,340 --> 00:36:22,840
这往往是有效你的系统

596
00:36:22,980 --> 00:36:24,590
被颠簸控制

597
00:36:24,710 --> 00:36:26,320
这阻止形成非常巨大的控制命令

598
00:36:26,410 --> 00:36:29,300
有类似这样的奖励函数

599
00:36:29,400 --> 00:36:31,060
往往使许多系统表现得更好

600
00:36:31,170 --> 00:36:33,890
当然  [听不清]选择不同的值

601
00:36:34,030 --> 00:36:35,600
我们有不同的对角线上的值

602
00:36:35,730 --> 00:36:38,320
给予不同的状态变量  不同的权重等

603
00:36:38,450 --> 00:36:41,150
因此  许多为U和B可能的选择

604
00:36:41,300 --> 00:36:43,110
这是一个例子 ?

605
00:36:43,240 --> 00:37:00,200
因此  接下来的几个步骤

606
00:37:00,340 --> 00:37:02,860
我会写出来的东西  我要得出的东西

607
00:37:03,000 --> 00:37:06,330
一般情况下  是非稳态动力学

608
00:37:06,480 --> 00:37:09,740
我要--我写出更多的数学和

609
00:37:09,860 --> 00:37:12,630
更多LQR的方程  我要尝试写出

610
00:37:12,770 --> 00:37:15,710
相当一般情况下时间变化的动态

611
00:37:15,850 --> 00:37:18,580
和时间不同的奖励函数

612
00:37:18,720 --> 00:37:23,140
所以我[无声]函数 但为了

613
00:37:23,230 --> 00:37:26,660
达到了解这种材料的目的

614
00:37:26,780 --> 00:37:28,940
你可能要考虑的

615
00:37:29,060 --> 00:37:30,570
只要忽略许多下标  含T的项

616
00:37:30,710 --> 00:37:35,140
[无声]材料的缘故  所以你可能

617
00:37:35,280 --> 00:37:37,050
在精神上假设

618
00:37:37,180 --> 00:37:39,250
有一些固定的矩阵  A

619
00:37:39,390 --> 00:37:44,140
使A等于A_1等于A_2  等于A_3和等等

620
00:37:44,300 --> 00:37:47,300
同样  还有一些矩阵B  OK?

621
00:37:47,430 --> 00:37:53,310
所以写出来  是完全通用的

622
00:37:53,450 --> 00:37:56,530
非平稳的情况

623
00:37:56,670 --> 00:37:59,110
但你可能只是想忽略的时间下标

624
00:37:59,240 --> 00:38:00,830
和想象现在的固定的情况下

625
00:38:00,990 --> 00:38:10,890
讲了这么多之后  我们会谈论

626
00:38:11,050 --> 00:38:13,780
这个所谓的微分动态规划的扩展

627
00:38:13,880 --> 00:38:15,760
将实际使用一个非稳定

628
00:38:15,880 --> 00:38:18,680
[听不清]非常强大的效果的

629
00:38:18,820 --> 00:38:20,450
一个特别的算法

630
00:38:20,610 --> 00:38:22,470
但是我们大致要做的

631
00:38:22,590 --> 00:38:24,440
只是假装MDP的是静止的

632
00:38:49,070 --> 00:38:52,180
好了 因此  在我们谈论模型之前

633
00:38:52,310 --> 00:38:54,680
让我说几句话  关于你将如何

634
00:38:54,820 --> 00:38:56,010
去建立线性模型的

635
00:38:56,140 --> 00:38:57,850
在这个模型中的关键假设是

636
00:38:57,970 --> 00:39:00,270
动力学是线性的

637
00:39:00,390 --> 00:39:02,290
还有假设奖励函数是二次的

638
00:39:02,410 --> 00:39:04,610
但让我们来谈谈

639
00:39:04,750 --> 00:39:06,130
动力学是线性的情况

640
00:39:06,260 --> 00:39:12,740
S_T+1等于A_ST + B_at

641
00:39:12,910 --> 00:39:15,080
也许不同的时间  也许固定

642
00:39:15,220 --> 00:39:16,510
我现在只是写成固定的

643
00:39:16,640 --> 00:39:18,900
那么  你如何得到这样的模型呢?

644
00:39:19,040 --> 00:39:21,850
实际上  我们已经在前面的讲座中

645
00:39:21,970 --> 00:39:23,560
看到了这样的一个例子

646
00:39:23,690 --> 00:39:25,160
如果你有一个倒立摆系统

647
00:39:25,280 --> 00:39:27,330
和你想使用这样一个线性模型

648
00:39:27,500 --> 00:39:30,600
对倒立摆建模  也许[听不清]

649
00:39:30,730 --> 00:39:32,680
我不会把它写下来

650
00:39:32,820 --> 00:39:35,620
你可以做的事情之一

651
00:39:35,760 --> 00:39:37,590
是运行你的倒立摆  在状态0开始

652
00:39:37,720 --> 00:39:41,940
采取一些行动  A_0  得到一些状态

653
00:39:42,070 --> 00:39:44,670
S_1 采取行动A_1等  得到一些状态的S_T

654
00:39:44,800 --> 00:39:49,270
我们的索引是1

655
00:39:49,390 --> 00:39:50,740
这是我第一次试验 ?

656
00:39:50,880 --> 00:39:52,390
然后  你可以重复很多次

657
00:39:52,520 --> 00:39:55,530
你可以重复这个N次

658
00:39:55,680 --> 00:40:00,310
我只是执行你的物理机器人的行动

659
00:40:00,450 --> 00:40:02,890
这可能是一个机器人  它可能是一个化工厂

660
00:40:03,040 --> 00:40:04,830
它可能是任何东西 在你的系统尝试

661
00:40:05,010 --> 00:40:13,820
不同的行动  并且看看它得到什么状态

662
00:40:13,960 --> 00:40:18,840
因此  对于你的数据的线性模型

663
00:40:19,000 --> 00:40:26,230
并选择参数A和B

664
00:40:26,380 --> 00:40:48,970
最小化二次误差项

665
00:40:49,090 --> 00:40:52,780
所以说  A_st + B_at S_t+1有多好

666
00:40:52,870 --> 00:40:54,800
所以你尽量减少二次误差项

667
00:40:54,920 --> 00:40:56,570
这将是一个合理的方法

668
00:40:56,690 --> 00:40:59,330
来为一个物理的机器人

669
00:40:59,470 --> 00:41:02,470
还是一个什么系统的物理化学的一部分

670
00:41:02,590 --> 00:41:04,200
来估计一个线性动态系统的参数

671
00:41:04,360 --> 00:41:11,580
另外一个线性模型的一贯方法是

672
00:41:11,710 --> 00:41:21,770
如果我想控制  是采取非线性

673
00:41:21,880 --> 00:41:25,140
模型并使其线性化

674
00:41:25,260 --> 00:41:26,950
让我告诉你我的意思

675
00:41:27,110 --> 00:41:39,300
所以  你可以有非线性模型

676
00:41:39,410 --> 00:41:42,760
所以让我们说你有一些非线性模型  表达式

677
00:41:42,890 --> 00:41:49,540
为:S_t+1  = f(S_t a_t)

678
00:41:49,610 --> 00:41:51,530
例如在前面的演讲中

679
00:41:51,650 --> 00:41:56,630
我说的倒立摆[听不清]

680
00:41:56,750 --> 00:41:59,430
通过参考物理规律

681
00:41:59,570 --> 00:42:02,380
实际上是通过下载现成的软件

682
00:42:02,500 --> 00:42:04,510
来进行物理模拟

683
00:42:04,660 --> 00:42:06,970
所以  如果你之前还没有看到过它

684
00:42:07,120 --> 00:42:09,970
你可以去网上

685
00:42:10,120 --> 00:42:12,130
你可以轻松地找到

686
00:42:12,250 --> 00:42:14,090
这些简单的设备的物理模拟的许多开源包

687
00:42:14,200 --> 00:42:15,640
下载软件

688
00:42:15,740 --> 00:42:17,750
输入你的机器人规格类型

689
00:42:17,880 --> 00:42:19,790
它会模拟你使用的物理模型

690
00:42:19,910 --> 00:42:21,180
有很多这样的开放源码软件的补丁

691
00:42:21,300 --> 00:42:22,750
你可以下载它们

692
00:42:22,860 --> 00:42:25,490
但这样的事情  你现在可以建立

693
00:42:25,630 --> 00:42:28,660
一个物理模拟  预计状态为

694
00:42:28,760 --> 00:42:30,160
一个以前的状态

695
00:42:30,300 --> 00:42:31,680
和以前的动作的函数

696
00:42:31,840 --> 00:42:35,050
所以  你实际上说  提出一些函数

697
00:42:35,200 --> 00:42:43,680
状态[无声]下一次 [无声]向量

698
00:42:43,810 --> 00:42:46,180
将是一些当前状态

699
00:42:46,320 --> 00:42:54,060
和当前的行动的函数

700
00:42:54,190 --> 00:42:57,010
在这种情况下的行动

701
00:42:57,180 --> 00:42:58,190
仅仅是一个实数

702
00:42:58,280 --> 00:42:59,950
说你如何努力加速向左或向右

703
00:43:00,100 --> 00:43:04,480
然后你就可以采取这种非线性模型

704
00:43:04,620 --> 00:43:06,370
其实我在上一讲的写了一个模型例子

705
00:43:06,460 --> 00:43:07,840
但在一般情况下

706
00:43:07,960 --> 00:43:09,460
F将一些非线性函数

707
00:43:09,590 --> 00:43:11,420
[听不清]一个线性函数

708
00:43:11,530 --> 00:43:15,010
所以  我的意思是  通过如下进行线性化

709
00:43:15,120 --> 00:43:17,180
因此  这里的只是一个卡通

710
00:43:17,330 --> 00:43:18,730
我马上写出该数学公式

711
00:43:18,880 --> 00:43:22,600
比方说  横轴是输入状态

712
00:43:22,730 --> 00:43:26,760
S_T  和输出的状态  S_T+1

713
00:43:26,890 --> 00:43:35,460
正如我所说的 这里是F的函数

714
00:43:35,600 --> 00:43:37,180
所以在下一个状态  S_T+1

715
00:43:37,310 --> 00:43:39,770
将一些关于以前的状态S_T和行动A_T的

716
00:43:39,900 --> 00:43:43,920
函数 因此  使这个模型线性化

717
00:43:44,040 --> 00:43:47,200
你要做的是  选择一个点

718
00:43:47,300 --> 00:43:51,540
我们把这个叫做S-t

719
00:43:51,670 --> 00:43:56,590
你会采取这个函数的导数

720
00:43:56,710 --> 00:43:58,550
对于该函数的[听不清]直线

721
00:43:58,690 --> 00:44:02,430
因此  这可以让你表达下一个状态

722
00:44:02,570 --> 00:44:05,600
S_T+1 可以估计下一个状态

723
00:44:05,750 --> 00:44:09,670
S_T+1  因为这是以前状态ST的线性函数

724
00:44:09,800 --> 00:44:14,590
因此  要使这个卡通变正确

725
00:44:14,770 --> 00:44:16,480
水平访问这里

726
00:44:16,620 --> 00:44:20,030
真的是一个状态行动配对

727
00:44:20,160 --> 00:44:22,660
在附近做线性化

728
00:44:22,760 --> 00:44:25,240
因此  这仅仅是一个卡通

729
00:44:25,370 --> 00:44:26,850
横向访问表示输入状态和输入动作

730
00:44:29,460 --> 00:44:46,290
所以刚才用数学知识表示了它

731
00:44:46,410 --> 00:44:48,250
我马上写一个简单的情况

732
00:44:48,400 --> 00:44:52,030
和一个一般的情况

733
00:44:52,180 --> 00:44:54,060
假设水平访问只有这种状态

734
00:44:54,210 --> 00:44:55,960
因此  让我们假设相互作用[听不清]

735
00:44:56,090 --> 00:44:58,430
S_T+1只是一些S_T的函数

736
00:44:58,560 --> 00:45:01,300
然后那个我写的一个线性函数  是S_T+1

737
00:45:01,440 --> 00:45:06,310
我们估计为F的导数

738
00:45:06,430 --> 00:45:10,680
f`(S-_t)*(S_t- S-_t)+ f (S-_t)

739
00:45:10,810 --> 00:45:19,510
用这个表达式 你把S_T+1表示

740
00:45:19,630 --> 00:45:24,390
为S_T的线性函数

741
00:45:24,520 --> 00:45:27,250
请注意  S-t是一个常数

742
00:45:27,360 --> 00:45:29,370
这不是一个变量

743
00:45:29,470 --> 00:45:34,000
明白么?S-t是一个常数

744
00:45:34,130 --> 00:45:35,810
f`(S-t)是在S点T时的函数

745
00:45:35,940 --> 00:45:39,490
这是真的只是线性函数方程

746
00:45:39,610 --> 00:45:42,280
所以  你然后可以转换

747
00:45:42,420 --> 00:45:44,110
这个为A和B矩阵 ?

748
00:45:51,170 --> 00:45:55,890
回到上一个黑板  我要指出的

749
00:45:56,020 --> 00:45:58,480
另一件事情 比方说  我看这条直线

750
00:45:58,610 --> 00:46:01,910
并且我问  这条直线能多

751
00:46:02,030 --> 00:46:04,530
好近似我的函数F呢

752
00:46:04,620 --> 00:46:07,020
我原来的模拟器  我原来的函数F

753
00:46:07,110 --> 00:46:09,700
然后你通知  在这附近

754
00:46:09,850 --> 00:46:14,110
在S-附近  那里的一个很好的近似

755
00:46:14,280 --> 00:46:16,460
它的相当接近 但是

756
00:46:16,580 --> 00:46:18,950
然后  当你移动更远

757
00:46:19,120 --> 00:46:21,250
移动到很远的左边这里

758
00:46:21,400 --> 00:46:23,250
它变成一个非常糟糕的近似 ?

759
00:46:23,420 --> 00:46:30,610
所以  当你线性化一个非线性模型

760
00:46:30,720 --> 00:46:34,500
来应用LQR的时候

761
00:46:34,620 --> 00:46:36,510
你要选择的参数之一  是在该点的周围

762
00:46:36,620 --> 00:46:39,470
来线性化你的非线性模型

763
00:46:39,660 --> 00:46:43,220
所以  如果你期待你的倒立摆系统

764
00:46:43,350 --> 00:46:47,370
其大部分时间在这种状态下附近

765
00:46:47,470 --> 00:46:51,490
那么它会是合理的线性化这个状态

766
00:46:51,610 --> 00:46:54,480
因为这意味着线性近似

767
00:46:54,650 --> 00:46:56,390
将是一个很好的近似

768
00:46:56,530 --> 00:46:59,460
通常的状态

769
00:46:59,580 --> 00:47:01,590
你期望[听不清]花大部分时间

770
00:47:01,740 --> 00:47:04,820
相反  如果你期望的系统花费

771
00:47:04,950 --> 00:47:07,230
其大部分时间在左边很远的状态

772
00:47:07,370 --> 00:47:09,270
那么这将是一个可怕的位置

773
00:47:09,420 --> 00:47:12,660
来进行线性化 因此  一个经验法则

774
00:47:12,810 --> 00:47:14,650
是按照你期望的系统花费大部分的时间

775
00:47:14,810 --> 00:47:16,450
来选择线性化  使线性逼近

776
00:47:16,570 --> 00:47:19,570
往往会在附近的状态[听不清]

777
00:47:19,680 --> 00:47:24,140
是一个准确逼近的位置

778
00:47:24,260 --> 00:47:28,040
公平的说  它是关于选择点

779
00:47:28,160 --> 00:47:32,830
S-  A-  我们将使用一个线性函数

780
00:47:32,930 --> 00:47:35,890
我们就假设这是一个

781
00:47:36,510 --> 00:47:38,900
关于我原来的非线性函数

782
00:47:39,010 --> 00:47:41,540
F  的一个很好的近似

783
00:47:55,630 --> 00:47:58,730
因此  对于像倒立摆问题的一个例子

784
00:47:58,870 --> 00:48:01,350
这个问题  如果你希望

785
00:48:01,490 --> 00:48:04,470
在这个问题处理的非常好

786
00:48:04,610 --> 00:48:11,330
那么你所期望的状态往往是接近零的状态

787
00:48:11,510 --> 00:48:14,860
如果S等于零  相当于X是

788
00:48:15,030 --> 00:48:18,390
倒立摆的铁路轨道的中心

789
00:48:18,540 --> 00:48:20,850
你希望做的相当不错

790
00:48:21,020 --> 00:48:23,830
你期望的杆子在垂直[无声]垂直

791
00:48:24,010 --> 00:48:26,290
在零度或90度  我猜

792
00:48:26,420 --> 00:48:29,050
所以你通常选择的任何状态使杆子垂直

793
00:48:29,240 --> 00:48:31,470
0速度[听不清]

794
00:48:31,640 --> 00:48:33,930
接近零的速度

795
00:48:34,030 --> 00:48:37,130
在轨道中间

796
00:48:37,270 --> 00:48:39,680
所以通常选择作为一个状态

797
00:48:39,820 --> 00:48:43,600
来线性化倒立摆的动态

798
00:48:43,720 --> 00:48:47,000
这是一个

799
00:48:47,090 --> 00:48:49,350
你可能希望近似地很好的一个区域

800
00:48:55,320 --> 00:48:57,970
所以我把它写了下来

801
00:48:58,110 --> 00:49:00,310
回到这个公式  我写了一维

802
00:49:00,450 --> 00:49:02,570
状态变量的特殊情况

803
00:49:02,690 --> 00:49:06,560
如果没有任何行动

804
00:49:06,670 --> 00:49:09,940
一般的线性近似公式是

805
00:49:10,060 --> 00:49:15,460
S_T+1 约等于f(S-t  A-t ) +

806
00:49:54,250 --> 00:49:56,710
OK?这些倒三角形是一个特殊的符号

807
00:49:56,840 --> 00:50:00,810
用于获取F关于[听不清]

808
00:50:00,940 --> 00:50:03,390
矢量值的导数  第二个参数

809
00:50:03,520 --> 00:50:07,730
因此  通过选择一个适当的状态

810
00:50:07,850 --> 00:50:10,460
S-_t  A-_t  在周围进行线性化

811
00:50:10,550 --> 00:50:15,820
你现在表达为:S_T+1表示为

812
00:50:15,940 --> 00:50:20,310
当前状态和当前的行动A_T的线性函数

813
00:50:20,450 --> 00:50:25,260
同样  这些东西  S-_t

814
00:50:25,390 --> 00:50:28,680
是你之前选择的一个常数

815
00:50:28,800 --> 00:50:30,480
A-_t也是一样

816
00:50:30,620 --> 00:50:37,710
最后  线性化了这个东西

817
00:50:37,830 --> 00:50:42,680
然后你可以转换成这样的矩阵

818
00:50:42,810 --> 00:50:50,780
因此  现在的S_T+1是S_T和A_T的

819
00:50:50,900 --> 00:50:57,610
线性函数 关于这个有问题吗??

820
00:51:09,560 --> 00:51:14,290
所以一个微小的细节

821
00:51:14,410 --> 00:51:16,630
它真的不是一个很大的问题

822
00:51:16,750 --> 00:51:18,320
是下面这件事情:技术上一个[听不清]函数

823
00:51:18,440 --> 00:51:20,060
有可能实际上是一个额外的常数

824
00:51:20,200 --> 00:51:24,730
但是这不是一个非线性动力学系统的定义

825
00:51:24,860 --> 00:51:28,360
的一个很难的概括

826
00:51:28,470 --> 00:51:30,440
一种来处理该常量的方式是

827
00:51:30,560 --> 00:51:32,250
实际上是做一些这样事情

828
00:51:32,380 --> 00:51:33,480
采取这种状态下你的定义

829
00:51:33,610 --> 00:51:35,080
比方说  X X 点θθ点

830
00:51:35,210 --> 00:51:37,870
有一个额外的元素  1

831
00:51:38,000 --> 00:51:40,060
然后

832
00:51:40,210 --> 00:51:42,800
你可以扩大你的状态向量

833
00:51:42,880 --> 00:51:45,500
用元素1  解出A矩阵

834
00:51:45,610 --> 00:51:47,960
你也可以采取额外的常数  C

835
00:51:48,090 --> 00:51:50,660
所以  你可以在技术上处理这件事情--

836
00:51:50,790 --> 00:51:52,590
因为这个额外的位移

837
00:51:52,700 --> 00:51:54,640
它是一个仿射函数  而不是一个线性函数

838
00:51:54,750 --> 00:51:56,420
但是  这仅仅是一个

839
00:51:56,550 --> 00:51:57,940
为你自己[听不清]记号

840
00:51:58,070 --> 00:51:59,390
不是一个很大的问题

841
00:52:18,170 --> 00:52:19,030
因此  总结一下  你看我有这个

842
00:52:19,170 --> 00:52:20,270
你可以学到一种模式

843
00:52:20,380 --> 00:52:22,200
你可以采取一个非线性模型

844
00:52:22,330 --> 00:52:24,000
你的非线性模型可以是一个物理模型

845
00:52:24,140 --> 00:52:25,510
或你学到的一个非线性模型

846
00:52:25,640 --> 00:52:26,830
并且进行线性化

847
00:52:26,920 --> 00:52:31,590
现在  我将提出一个LQR的问题

848
00:52:31,710 --> 00:52:35,840
即我们在状态R^N的MDP

849
00:52:35,950 --> 00:52:37,950
在R^D的行动的规范

850
00:52:38,070 --> 00:52:42,910
并且状态通过[无声]

851
00:52:43,040 --> 00:52:45,300
线性方程的零概率

852
00:52:45,400 --> 00:52:50,070
S_T+1等于A_TS_T+B_TA_T

853
00:52:50,180 --> 00:52:54,650
我们的回报

854
00:52:54,770 --> 00:52:56,050
将是这些二次函数

855
00:52:56,170 --> 00:52:59,810
MDP的规范意味着

856
00:52:59,920 --> 00:53:02,070
我们知道A矩阵  B矩阵

857
00:53:02,150 --> 00:53:04,330
U矩阵和V矩阵

858
00:53:04,450 --> 00:53:07,450
我们的目标是要找出政策

859
00:53:07,570 --> 00:53:11,600
最大化我们有限的奖励边界的总和

860
00:53:11,720 --> 00:53:20,620
因此  我们的目标是拿出一个政策

861
00:53:20,710 --> 00:53:24,350
首先  要最大化

862
00:53:24,450 --> 00:53:27,940
这个有限的奖励边界的

863
00:53:28,060 --> 00:53:30,330
总和的预期值

864
00:53:47,160 --> 00:53:51,040
好了 所以我们来解决这个问题的方法

865
00:53:51,160 --> 00:53:57,250
是我们早些时候在此演讲中制定出的

866
00:53:57,370 --> 00:53:59,140
有限边界的动态规划算法

867
00:53:59,300 --> 00:54:02,490
特别是

868
00:54:02,630 --> 00:54:06,750
我找到最佳的政策的策略

869
00:54:06,850 --> 00:54:14,270
将先找到V*_T  大写T

870
00:54:14,380 --> 00:54:17,330
然后我将通过使用递归

871
00:54:17,470 --> 00:54:19,700
找到V*_T-1  V*_T-2等等 ?

872
00:54:19,840 --> 00:54:25,280
在我们制定出来的动态规划算法中

873
00:54:25,390 --> 00:54:28,360
V*_T(S_T)  这是最大的[听不清]

874
00:54:28,500 --> 00:54:32,800
你可能需要在时间

875
00:54:32,920 --> 00:54:39,340
R(S_T  A_T) 同样

876
00:54:39,450 --> 00:54:41,870
只是为了了解这些材料

877
00:54:41,980 --> 00:54:45,550
也许你可以假设奖励和动力

878
00:54:45,650 --> 00:54:46,930
实际上是静止的

879
00:54:47,050 --> 00:54:48,590
我会总是写出

880
00:54:48,710 --> 00:54:50,670
所有这些上标[无声]

881
00:54:50,790 --> 00:54:52,780
如果你第一次读它 ?

882
00:54:52,920 --> 00:54:58,120
那个奖励等于 max at 减去

883
00:55:12,480 --> 00:55:13,600
对不?我希望这不会引起混淆

884
00:55:13,700 --> 00:55:14,920
上标T表示转置 小写t

885
00:55:15,040 --> 00:55:16,840
表示时间索引大写T

886
00:55:16,960 --> 00:55:20,810
所以这只是一个我的下一个

887
00:55:20,920 --> 00:55:22,680
二次奖励的定义

888
00:55:22,810 --> 00:55:26,320
因此  这是很清楚的

889
00:55:26,440 --> 00:55:38,830
最大化为:-S^T_TU_tS_t

890
00:55:38,900 --> 00:55:43,060
因为这最后一项是--

891
00:55:43,180 --> 00:55:45,220
这是大于或等于0

892
00:55:45,320 --> 00:55:46,770
这给了我一个假设  V_T是[无声]半正定

893
00:55:46,890 --> 00:55:48,200
因此  在最后一个时间步长

894
00:55:48,310 --> 00:55:51,520
采取最好的行动是行动0

895
00:55:51,620 --> 00:55:59,300
因此  π*T(ST)等于[听不清]

896
00:55:59,420 --> 00:56:04,920
同样的事情的行动

897
00:56:05,050 --> 00:56:13,420
这只是零 选择零行动

898
00:56:13,560 --> 00:56:17,060
A^T_TV_TA_T变为零

899
00:56:17,200 --> 00:56:20,230
这就是奖励是如何最大化的

900
00:56:44,770 --> 00:56:52,190
有问题么?或者有模糊的地方吗??

901
00:56:52,350 --> 00:57:01,580
好了 所以  现在让我们做的动态规划步骤

902
00:57:01,690 --> 00:57:09,910
我的目标是V_T+1

903
00:57:10,020 --> 00:57:13,060
我想计算的V_T

904
00:57:13,180 --> 00:57:15,170
鉴于V*_T+1

905
00:57:15,320 --> 00:57:16,970
我要计算V*_T

906
00:57:17,110 --> 00:57:18,760
所以这是一个动态的编程步骤

907
00:57:18,880 --> 00:57:26,880
所以我以前写下来的DP步骤是这样的

908
00:57:26,980 --> 00:57:29,620
因此  有限状态的情况下

909
00:57:29,730 --> 00:57:31,120
我写下如下

910
00:58:32,350 --> 00:58:35,410
因此  这正是我以前写的

911
00:58:35,500 --> 00:58:38,890
有限的状态方程  在那里你

912
00:58:39,020 --> 00:58:40,750
有这些状态转移概率的

913
00:58:40,900 --> 00:58:42,640
我们可以总结这些状态系列

914
00:58:42,760 --> 00:58:46,760
现在  我们再继续作一个无限状态

915
00:58:46,910 --> 00:58:48,330
所以这种状态的总和

916
00:58:48,450 --> 00:58:50,460
将变成一种积分

917
00:58:50,610 --> 00:58:52,710
我要跳过这个积分步骤

918
00:58:52,850 --> 00:58:54,680
我们只需要继续前进

919
00:58:54,820 --> 00:58:57,200
这里写这最后一项  作为期望值 因此

920
00:58:57,370 --> 00:59:05,210
这是max 在at之上 加上

921
00:59:05,350 --> 00:59:07,250
然后这将成为对随机混合状态的期望

922
00:59:07,350 --> 00:59:09,090
ST+1  [听不清]从状态过渡概率

923
00:59:09,240 --> 00:59:12,150
S_t+1 P_St A_t

924
00:59:12,260 --> 00:59:21,400
[ V*_t+1(S_t+1) ]

925
00:59:21,510 --> 00:59:24,180
因此  这是同样的公式

926
00:59:24,280 --> 00:59:26,370
作为一个期望写下来 ?

927
00:59:26,480 --> 00:59:30,070
因此  我需要做的是

928
00:59:30,210 --> 00:59:33,380
给定的V*t+1的表达式

929
00:59:33,450 --> 00:59:38,620
我需要找到V*t  所以事实证明  LQR

930
00:59:38,740 --> 00:59:40,980
具有以下有用的属性

931
00:59:41,110 --> 00:59:44,020
原来  这些价值函数

932
00:59:44,110 --> 00:59:48,840
每一个都可以表达为二次函数

933
00:59:48,980 --> 00:59:58,820
所以具体的  让我们的假设  V*_t+1

934
00:59:58,920 --> 01:00:00,510
假设  这可以是一个像这样的二次函数

935
01:00:00,640 --> 01:00:12,980
其中的矩阵Φ_T+1

936
01:00:13,110 --> 01:00:18,010
是一个N*N的矩阵

937
01:00:18,120 --> 01:00:23,850
并且Ψ_T+1表示是只是一个实数

938
01:00:23,960 --> 01:00:25,480
所以换句话说  假设V*_t+1

939
01:00:25,590 --> 01:00:33,160
仅仅是一个状态S_T+1的二次函数

940
01:00:33,240 --> 01:00:40,940
然后  我们可以表明

941
01:00:41,070 --> 01:00:43,450
当你做一个动态的编程步骤

942
01:00:43,560 --> 01:00:47,880
当你插入V*_t+1的定义到

943
01:00:47,990 --> 01:00:50,240
我刚才定义的动态规划的步骤的方程中

944
01:00:50,360 --> 01:00:52,360
可以显示

945
01:00:52,480 --> 01:00:58,020
你会得到V*_t

946
01:00:58,160 --> 01:01:01,800
也将是一个具有相同的形式的二次函数

947
01:01:01,940 --> 01:01:13,270
[听不清]在这里  对不?

948
01:01:13,380 --> 01:01:16,160
对于一些适当的矩阵

949
01:01:16,260 --> 01:01:20,400
Φ_T  和一些适当的实数  Ψ_T

950
01:01:20,520 --> 01:01:24,930
所以你可以做的是  开始递归于

951
01:01:25,040 --> 01:01:32,490
好  明白不?所以你可以做的是

952
01:01:32,620 --> 01:01:37,260
开始于递归如下

953
01:01:37,410 --> 01:01:39,630
以前  我们摸索出V*_T(S_t)

954
01:01:39,770 --> 01:01:41,740
我们说

955
01:01:41,860 --> 01:01:45,300
这是-S^T_TU_TS_T

956
01:01:45,420 --> 01:01:51,700
因此  我们有大Φ_T

957
01:01:51,820 --> 01:01:57,390
等于U_T  Ψ_T

958
01:01:57,510 --> 01:02:02,760
等于0  V*T(ST)

959
01:02:02,910 --> 01:02:08,850
等于S^T_TΦ_T S_T+Ψ_T

960
01:02:08,990 --> 01:02:10,630
因此  你可以开始递归方式:

961
01:02:10,740 --> 01:02:12,580
是Φ_T = -U_T

962
01:02:12,690 --> 01:02:14,470
并且Ψ_T  = 0 ?

963
01:02:14,590 --> 01:02:18,360
然后制定出递归是什么

964
01:02:18,500 --> 01:02:24,540
我不会演示全部的推导演示

965
01:02:24,680 --> 01:02:27,740
这可能是代数

966
01:02:27,860 --> 01:02:32,960
其实你做过很多这样的功课

967
01:02:33,120 --> 01:02:34,740
像高斯期望数学

968
01:02:34,850 --> 01:02:41,440
所以我不会做充分的推导

969
01:02:41,560 --> 01:02:46,620
我就勾勒出每一步

970
01:02:46,740 --> 01:02:49,010
因此  在动态规划步骤中

971
01:02:49,130 --> 01:02:54,440
V*S_T= max/at  的

972
01:02:54,560 --> 01:02:56,750
立即奖励 ?

973
01:02:56,870 --> 01:03:07,740
所以这是我在动态规划

974
01:03:07,870 --> 01:03:10,030
步骤方程的R(SA ) 然后再加上

975
01:03:10,150 --> 01:03:14,470
一个随机的混合状态的预期值

976
01:03:14,590 --> 01:03:18,380
S_T+1  来自高斯分布意味着

977
01:03:18,510 --> 01:03:25,030
A_TS_T加B_TA_T和协变量Σ_W

978
01:03:25,090 --> 01:03:30,160
所以这是什么  这实在

979
01:03:30,290 --> 01:03:33,740
是我的S_TA_T P的规范

980
01:03:33,890 --> 01:03:36,290
这是我的状态分布在LQR设置过渡

981
01:03:36,430 --> 01:03:39,230
这是我的状态过渡分布[听不清]采取行动

982
01:03:39,360 --> 01:03:42,240
在状态S_T 然后  我的下一个状态

983
01:03:42,360 --> 01:03:47,230
分布式高斯意味着A_TS_T

984
01:03:47,340 --> 01:03:49,640
加B_TA_T和协变西格玛W

985
01:03:49,750 --> 01:03:55,810
然后在此状态下 ?

986
01:04:10,620 --> 01:04:17,650
当然  这只是V*_T+1(S_t+1)

987
01:04:17,770 --> 01:04:24,540
我希望你们能明白

988
01:04:24,650 --> 01:04:27,200
这是我曾在以前的动态规划步骤

989
01:04:27,330 --> 01:04:29,640
采用的方程 所以V*_T(S_T)

990
01:04:29,780 --> 01:04:32,640
等于max/at 即时的回报

991
01:04:32,760 --> 01:04:37,090
加上预期值的混合状态V*_T

992
01:04:37,240 --> 01:04:40,540
时钟前进一步

993
01:04:40,670 --> 01:04:43,880
所以  我刚刚插在

994
01:04:44,280 --> 01:04:45,800
所有的定义中

995
01:04:45,920 --> 01:04:47,990
作为状态的回报[无声]分配和

996
01:04:48,090 --> 01:04:50,080
价值函数

997
01:04:50,210 --> 01:04:56,900
其实  如果你听懂了

998
01:04:56,990 --> 01:05:03,020
请举手?酷

999
01:05:03,190 --> 01:05:05,810
所以  如果你写了这一点

1000
01:05:05,920 --> 01:05:07,680
并且你扩大期望 我知道你已经做了很多次

1001
01:05:07,760 --> 01:05:08,810
所以我不会做

1002
01:05:08,940 --> 01:05:11,530
右侧的整个东西

1003
01:05:11,640 --> 01:05:12,660
可以简化为一个

1004
01:05:12,770 --> 01:05:14,390
大的行动的二次函数

1005
01:05:14,500 --> 01:05:22,990
A_T 所以这整个事情

1006
01:05:23,100 --> 01:05:38,320
简化为一个大的二次函数的行动

1007
01:05:38,450 --> 01:05:40,960
我们要最大化这个行动AT

1008
01:05:41,080 --> 01:05:43,810
因此  要最大化一个

1009
01:05:43,910 --> 01:05:46,320
大的二次函数

1010
01:05:46,460 --> 01:05:48,360
你只是采取函数关于A_T的导数

1011
01:05:48,470 --> 01:05:50,430
设置导数等于零

1012
01:05:50,530 --> 01:05:52,250
然后你最大化的右侧

1013
01:05:52,390 --> 01:05:54,650
关于行动  A_T

1014
01:05:54,760 --> 01:05:58,080
原来--为了完整性

1015
01:05:58,200 --> 01:06:00,070
我只是写下这个表达式

1016
01:06:00,210 --> 01:06:02,670
你自己可以在任何时候导出它

1017
01:06:02,770 --> 01:06:05,310
事实证明  如果你真的最大化

1018
01:06:05,410 --> 01:06:07,500
右边关于行动A_T的函数

1019
01:06:07,600 --> 01:06:10,190
你会发现  [听不清]A_T将*S_T

1020
01:06:24,280 --> 01:06:31,170
不要担心这个表达式

1021
01:06:31,290 --> 01:06:33,290
你可以从[听不清]

1022
01:06:33,420 --> 01:06:34,740
并从中自己导出

1023
01:06:34,860 --> 01:06:36,470
但关键要注意的是  最佳的行动

1024
01:06:36,620 --> 01:06:38,980
A_T将是一个大的矩阵

1025
01:06:39,090 --> 01:06:46,670
我们要调用这个东西的L_T * S_T

1026
01:06:46,770 --> 01:06:50,980
换句话说  在这个给定的

1027
01:06:51,120 --> 01:06:53,860
状态下的最佳的行动将是

1028
01:06:53,970 --> 01:06:59,620
一些状态S_T的线性函数

1029
01:06:59,750 --> 01:07:01,350
因此  做完了动态规划

1030
01:07:01,470 --> 01:07:03,610
你还记得  当我们制定了

1031
01:07:03,710 --> 01:07:05,360
有限的边界MDP的动态规划算法

1032
01:07:05,480 --> 01:07:09,070
我们说的方式来计算的最优政策

1033
01:07:09,220 --> 01:07:13,520
π*(S_T) 这始终是

1034
01:07:13,610 --> 01:07:16,070
对同一事物的[听不清]

1035
01:07:16,160 --> 01:07:20,430
[无声]在相同的事情采取行动

1036
01:07:20,530 --> 01:07:26,590
S_TA_T加上你的预期值[听不清]

1037
01:07:26,730 --> 01:07:33,890
P_STA_T  V*_T+1 右侧的

1038
01:07:34,000 --> 01:07:37,330
这个东西是始终作为

1039
01:07:37,450 --> 01:07:39,710
我们最大[听不清]

1040
01:07:39,840 --> 01:07:42,230
同样的事情

1041
01:07:42,350 --> 01:07:44,530
因此  这是什么意思呢

1042
01:07:44,640 --> 01:07:46,740
当我说这是一个价值的最大化

1043
01:07:46,860 --> 01:07:48,610
那么这意味着在S_T的状态

1044
01:07:48,730 --> 01:07:50,610
采取最优的行动

1045
01:07:50,710 --> 01:07:56,420
实际上等于L_T * S_T

1046
01:07:56,530 --> 01:08:06,220
结果表明  当你在一些状态

1047
01:08:06,360 --> 01:08:07,770
S_T  该状态的最佳行动

1048
01:08:07,900 --> 01:08:10,070
将是一些矩阵

1049
01:08:10,180 --> 01:08:13,690
L_T  它可以计算  乘以状态

1050
01:08:13,810 --> 01:08:19,690
S_T 换句话说  最佳的行动实际上

1051
01:08:19,820 --> 01:08:21,830
是一个状态的线性函数

1052
01:08:21,970 --> 01:08:24,690
我只是要指出  这里不是

1053
01:08:24,840 --> 01:08:27,820
一个近似的函数  对的 我们并没有这样做

1054
01:08:27,910 --> 01:08:32,990
我们不说  让我们找到最佳的线性政策

1055
01:08:33,100 --> 01:08:35,080
我们不说  让我们看看

1056
01:08:35,200 --> 01:08:36,940
在最佳的政策

1057
01:08:37,040 --> 01:08:38,200
然后我们将适应

1058
01:08:38,320 --> 01:08:39,820
这条直线的最优政策

1059
01:08:39,950 --> 01:08:42,270
这不是用直线近似的最优政策

1060
01:08:42,370 --> 01:08:44,560
这个推导是说

1061
01:08:44,720 --> 01:08:47,410
最佳的政策是一条直线

1062
01:08:47,510 --> 01:08:57,570
最佳行动是当前状态的一个线性函数

1063
01:08:57,680 --> 01:09:00,570
此外  当你已经计算出

1064
01:09:00,700 --> 01:09:04,430
这是一个A_T  最大化右侧

1065
01:09:04,580 --> 01:09:06,740
这个东西的值 所以你用这个

1066
01:09:06,860 --> 01:09:09,060
往回做动态规划递归 你会发现

1067
01:09:09,170 --> 01:09:20,490
所以你使用AT回来做最大化处理

1068
01:09:20,620 --> 01:09:23,200
实际上你得到这个

1069
01:09:23,300 --> 01:09:29,450
公式  V*_t(S_t) ?

1070
01:09:29,640 --> 01:09:36,750
所以你发现  它确实是

1071
01:09:36,880 --> 01:09:39,460
一个如下形式的二次函数

1072
01:09:39,580 --> 01:09:43,290
我只是为了完整性

1073
01:09:43,430 --> 01:09:46,060
把方程写出来

1074
01:09:46,170 --> 01:09:47,850
不要过多担心他们的形式

1075
01:09:48,010 --> 01:09:57,760
你自己可以导出它

1076
01:10:47,060 --> 01:10:51,570
所以  概括一下

1077
01:10:51,670 --> 01:10:53,000
不用过多担心这些方程的形式

1078
01:10:53,100 --> 01:10:56,140
我们所做的是  写下递归

1079
01:10:56,280 --> 01:11:00,280
表达式:Φ_T 和Ψ_T

1080
01:11:00,370 --> 01:11:03,070
是Φ_T+1和Ψ_T+1的函数

1081
01:11:03,190 --> 01:11:06,810
因此  这可以让你计算

1082
01:11:06,920 --> 01:11:08,620
时钟时间小写t的

1083
01:11:08,730 --> 01:11:12,270
最优值函数

1084
01:11:12,400 --> 01:11:13,990
作为时钟在时间t+1时的

1085
01:11:14,120 --> 01:11:16,190
最优值函数

1086
01:11:16,340 --> 01:11:29,780
因此  总结一下  GSELQG

1087
01:11:29,890 --> 01:11:32,080
这里有一个有限的边界

1088
01:11:32,190 --> 01:11:35,590
实际上  只是为了给这个等式一个名称

1089
01:11:35,690 --> 01:11:37,000
这个Φ_T  这个递归

1090
01:11:37,120 --> 01:11:40,670
这就是所谓的Bacardi离散方程

1091
01:11:40,750 --> 01:11:54,810
[听不清]递归

1092
01:11:54,930 --> 01:12:00,160
让Ψ_T和Ψ_T+1 ?

1093
01:12:00,270 --> 01:12:14,030
因此  要总结下  我们找到

1094
01:12:14,140 --> 01:12:16,210
确切的解决方案  有限边界的LQR问题的

1095
01:12:16,320 --> 01:12:24,730
算法如下 我们初始化

1096
01:12:24,830 --> 01:12:34,410
Φ_T = - U_T和 Ψ_T = 0

1097
01:12:34,540 --> 01:12:47,340
然后递归  作为Φ_T的函数和Ψ_T

1098
01:12:47,480 --> 01:12:53,860
作为一个Φ_T +1

1099
01:12:53,960 --> 01:13:02,010
和Ψ_T+1的函数  对不起

1100
01:13:02,140 --> 01:13:06,860
因此  递归计算Φ_T的函数

1101
01:13:06,950 --> 01:13:10,370
和Ψ_T作为一个Φ_T +1和Ψ_T+1的函数

1102
01:13:10,510 --> 01:13:11,710
正如我表明

1103
01:13:11,810 --> 01:13:16,210
使用离散时间的Bacardi方程

1104
01:13:16,330 --> 01:13:20,160
所以  你做到这一点  T = T-1  T-2  等等

1105
01:13:20,290 --> 01:13:24,140
知道时间为0

1106
01:13:24,260 --> 01:13:33,700
然后你计算作为一个函数的L_T

1107
01:13:33,860 --> 01:13:40,220
实际上  这是Φ_T还是Φ_T+1?

1108
01:13:40,290 --> 01:13:43,720
Φ_T+1  我认为 作为Φ_T+1

1109
01:13:43,860 --> 01:13:46,710
和Ψ_T+1的函数 这实际上

1110
01:13:46,820 --> 01:13:48,430
只是Φ_T+1函数 你并不真的需要

1111
01:13:48,540 --> 01:13:54,150
Ψ_T+1 现在  你有了最优政策

1112
01:13:54,260 --> 01:14:03,650
因此  在计算的L_TS_T

1113
01:14:03,780 --> 01:14:07,230
你现在有最佳的行动

1114
01:14:07,360 --> 01:14:10,660
在状态S_T下  通过这个线性方程

1115
01:14:10,780 --> 01:14:29,700
我还剩多少时间?好了

1116
01:14:35,330 --> 01:14:37,150
在我关闭之前  我只想说对此

1117
01:14:37,270 --> 01:14:46,060
说最后一件事情

1118
01:14:46,160 --> 01:14:49,090
也许我会在下周做

1119
01:14:49,220 --> 01:14:51,400
我想我会在下一章节中来处理

1120
01:14:51,520 --> 01:14:53,170
因此它实际上原来有一个很酷的属性

1121
01:14:53,240 --> 01:14:55,560
是一种微妙的  你会在下一讲发现

1122
01:14:55,660 --> 01:14:58,140
我们今天结束之前  关于这个是否有问题?

1123
01:15:04,210 --> 01:15:08,980
所以关于离散时间的LQR问题的解决方案

1124
01:15:09,060 --> 01:15:12,140
是非常酷的事情--有限边界的

1125
01:15:12,250 --> 01:15:15,320
LQR问题是  这是一个连续状态

1126
01:15:15,420 --> 01:15:17,260
在一个无限状态问题

1127
01:15:17,380 --> 01:15:20,330
但尽管如此  在我们所做的假设下

1128
01:15:20,440 --> 01:15:22,330
你可以证明价值函数

1129
01:15:22,450 --> 01:15:25,480
是一个状态的二次函数

1130
01:15:25,580 --> 01:15:27,790
因此  仅仅通过计算这些矩阵Φ_T

1131
01:15:27,900 --> 01:15:30,540
和实数Ψ_T  你实际上

1132
01:15:30,700 --> 01:15:33,130
可以完全表示价值函数

1133
01:15:33,250 --> 01:15:35,720
即使是这些无限大的状态空间

1134
01:15:35,830 --> 01:15:37,460
即使是连续状态空间

1135
01:15:37,580 --> 01:15:39,600
所以这些算法的计算估

1136
01:15:39,670 --> 01:15:43,540
算像立方

1137
01:15:43,630 --> 01:15:46,080
仅作为状态变量

1138
01:15:46,210 --> 01:15:49,080
而多项式多状态变量[听不清]维问题

1139
01:15:49,190 --> 01:15:50,860
在[听不清]中

1140
01:15:50,990 --> 01:15:52,860
我们指数形式的算法

1141
01:15:52,980 --> 01:15:57,370
而LQR尺度

1142
01:15:57,510 --> 01:15:59,010
就像立方维数的问题

1143
01:15:59,140 --> 01:16:01,790
因此  这很容易适用于

1144
01:16:01,880 --> 01:16:03,380
甚至是非常大的状态空间中

1145
01:16:03,540 --> 01:16:05,450
所以  我们实际上往往

1146
01:16:05,560 --> 01:16:07,700
运用这种算法的变种

1147
01:16:07,810 --> 01:16:09,770
一些子集  一些特别的事情

1148
01:16:09,910 --> 01:16:11,650
我们直升机的处理

1149
01:16:11,760 --> 01:16:14,460
它具有高维状态空间

1150
01:16:14,560 --> 01:16:18,120
它有12维或更高的维度

1151
01:16:18,250 --> 01:16:19,780
这能够很好地处理

1152
01:16:19,890 --> 01:16:22,240
因此  原来还有更多的事情可以做

