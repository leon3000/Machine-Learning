1
00:00:18,820 --> 00:00:29,250
欢迎回来

2
00:00:29,340 --> 00:00:31,910
我今天想要做的是

3
00:00:32,560 --> 00:00:35,720
继续讨论主成分分析

4
00:00:35,820 --> 00:00:36,870
即PCA

5
00:00:36,950 --> 00:00:40,480
特别是 有一个应用程序

6
00:00:40,940 --> 00:00:41,820
我没有在上一讲

7
00:00:42,710 --> 00:00:43,510
关于latent Semitic索引 LSI

8
00:00:44,430 --> 00:00:46,960
然后 我想花一点点的时间

9
00:00:47,500 --> 00:00:49,030
来谈论如何实现PCA

10
00:00:49,950 --> 00:00:51,610
尤其是为解决大问题的

11
00:00:52,280 --> 00:00:53,880
特别是 我会花一点点的时间

12
00:00:54,490 --> 00:00:55,530
谈论奇异值分解

13
00:00:56,290 --> 00:00:57,150
(singular value decomposition)

14
00:00:57,820 --> 00:00:59,950
或主成分分析的SVD实施

15
00:01:00,380 --> 00:01:03,700
所以今天的讲座第二部分

16
00:01:03,900 --> 00:01:06,120
我想谈谈不同的算法

17
00:01:06,680 --> 00:01:08,390
称为独立成分分析

18
00:01:08,930 --> 00:01:09,580
(independent component analysis)这是

19
00:01:09,990 --> 00:01:10,820
在某些方面 和PCA是有联系的

20
00:01:11,300 --> 00:01:14,870
但在其他许多方面 它和和PCA相比

21
00:01:15,330 --> 00:01:16,950
却能完成非常不同的东西

22
00:01:17,560 --> 00:01:20,610
因此 这次讲座

23
00:01:21,190 --> 00:01:23,120
将全神贯注来讨论我们的无监督学习

24
00:01:23,560 --> 00:01:25,170
下一讲 我们

25
00:01:25,750 --> 00:01:26,790
将开始谈论强化学习算法

26
00:01:27,520 --> 00:01:34,070
回顾一下

27
00:01:34,770 --> 00:01:45,050
我们使用PCA 主成分分析 我在PCA中说

28
00:01:45,460 --> 00:01:47,110
我们想象

29
00:01:47,550 --> 00:01:49,350
我们在一些小的空间中

30
00:01:50,080 --> 00:01:53,240
可能要存储一些很大尺寸的数据

31
00:01:53,720 --> 00:01:54,980
所以 如果你有这样的数据集

32
00:01:55,760 --> 00:01:56,770
你可能会发现

33
00:01:57,240 --> 00:01:59,380
数据的第一个主成分

34
00:01:59,840 --> 00:02:04,490
而且这是2-D数据的第二个成分

35
00:02:04,900 --> 00:02:11,630
来总结这个算法 我们有三个步骤

36
00:02:12,000 --> 00:02:15,030
第一步是把数据

37
00:02:15,410 --> 00:02:21,300
标准化为0 means 和uint variance

38
00:02:21,940 --> 00:02:28,810
因此 跟踪你训练的例子的意思

39
00:02:29,150 --> 00:02:31,260
因此 现在已经为0 means 然后正常化

40
00:02:31,670 --> 00:02:32,320
您的每一个特性

41
00:02:32,890 --> 00:02:35,290
使每个功能的差异是现在

42
00:02:37,990 --> 00:02:40,100
下一步是【无声】

43
00:02:40,670 --> 00:02:43,710
0 means数据的computical方差矩阵

44
00:02:44,250 --> 00:02:46,660
所以 你做如下计算

45
00:02:47,280 --> 00:02:50,030
所有产品的总和

46
00:02:51,170 --> 00:03:02,190
然后你找到的K的Σ特征向量

47
00:03:02,860 --> 00:03:11,070
因此 我们上一次看到这个应用程序

48
00:03:11,460 --> 00:03:15,130
例如 一个应用程序是特征面

49
00:03:15,650 --> 00:03:22,380
那里每一个训练例子 x^((i) ) 是一个图像

50
00:03:22,790 --> 00:03:28,750
所以 如果你有一张100*100图像

51
00:03:29,240 --> 00:03:32,770
如果你的面孔的照片是100像素*100个像素

52
00:03:33,180 --> 00:03:37,810
那么你的每一个训练例子 x^((i) ) 将是一个

53
00:03:38,180 --> 00:03:40,190
10  000空间向量 对应

54
00:03:40,820 --> 00:03:44,740
10  000灰度像素值

55
00:03:45,150 --> 00:03:47,260
每张100*100图像中

56
00:03:47,670 --> 00:03:49,390
有10  000像素值

57
00:03:49,910 --> 00:03:52,880
因此 特征面孔应用程序的训练例子

58
00:03:53,420 --> 00:03:55,230
包括

59
00:03:55,620 --> 00:03:57,440
人的面孔的照片

60
00:03:58,270 --> 00:04:02,310
然后我们运行PCA 然后测量之间

61
00:04:02,970 --> 00:04:05,640
这个脸和这个脸的距离

62
00:04:06,000 --> 00:04:06,900
我们可以

63
00:04:07,320 --> 00:04:09,250
到子空间中设计两个人脸图像

64
00:04:09,860 --> 00:04:13,180
然后测量沿着子空间的距离

65
00:04:13,600 --> 00:04:14,700
因此 在特征人脸中

66
00:04:15,380 --> 00:04:17,060
你可以使用50个主成分一样的东西

67
00:04:17,560 --> 00:04:21,430
所以这些问题的

68
00:04:21,850 --> 00:04:22,850
难点在于

69
00:04:23,690 --> 00:04:27,850
该算法的第二个步骤 我们构造

70
00:04:28,140 --> 00:04:29,520
协方差矩阵Σ

71
00:04:29,970 --> 00:04:32,450
协方差矩阵现在

72
00:04:32,950 --> 00:04:40,170
变成了10  000*10  000空间矩阵

73
00:04:40,580 --> 00:04:41,170
这是巨大的

74
00:04:42,130 --> 00:04:46,560
这1亿个记录 那是巨大的

75
00:04:47,250 --> 00:04:52,290
因此 让我们应用PCA以非常大的空间数据

76
00:04:52,760 --> 00:04:54,630
作为减少尺寸的要点

77
00:04:55,050 --> 00:04:57,550
但这种算法的第二步的这一步

78
00:04:57,820 --> 00:04:59,580
你在那里构造【无声】

79
00:04:59,770 --> 00:05:02,060
所以这个非常大的矩阵 你不能这样做

80
00:05:02,150 --> 00:05:04,600
回过头来 看看这个

81
00:05:04,690 --> 00:05:05,850
PCA的一个

82
00:05:05,910 --> 00:05:11,300
经常使用的应用

83
00:05:11,430 --> 00:05:13,070
是关于文本数据

84
00:05:13,170 --> 00:05:15,430
所以这里是我的意思

85
00:05:15,540 --> 00:05:20,270
还记得我们的电子邮件矢量表示法吗?

86
00:05:20,910 --> 00:05:23,670
因此 这是以前的方式 当我们谈论

87
00:05:24,040 --> 00:05:25,190
一个独立分类的

88
00:05:25,540 --> 00:05:27,540
监督学习算法

89
00:05:27,960 --> 00:05:30,520
你还记得我说过一段email

90
00:05:30,900 --> 00:05:32,000
或一段文本文档

91
00:05:32,640 --> 00:05:33,950
你可以使用

92
00:05:34,360 --> 00:05:35,060
高维的向量来表示它 --

93
00:05:35,710 --> 00:05:36,880
通过在

94
00:05:37,510 --> 00:05:41,770
你的字典写下所有的单词清单

95
00:05:42,130 --> 00:05:46,110
在某处你有单词learn

96
00:05:46,580 --> 00:05:50,140
在某处你有单词study等

97
00:05:50,590 --> 00:05:55,920
根据每个单词

98
00:05:56,350 --> 00:05:57,880
是否出现在你的文本文件中

99
00:05:57,980 --> 00:06:00,370
你设置为1或0

100
00:06:00,970 --> 00:06:02,990
这是一个我们使用电极5

101
00:06:03,820 --> 00:06:05,670
或电极6的一个表示方法

102
00:06:06,110 --> 00:06:09,390
用来表示我们基于分类的【听不清】

103
00:06:09,790 --> 00:06:10,890
而建设的【听不清】的文本文档

104
00:06:11,560 --> 00:06:14,760
因此 事实证明

105
00:06:15,390 --> 00:06:17,930
PCA的常见的应用之一

106
00:06:18,520 --> 00:06:21,910
实际上是这个文本数据表示法

107
00:06:22,510 --> 00:06:24,720
当你对这种数据应用PCA时

108
00:06:25,150 --> 00:06:26,930
由此产生的算法

109
00:06:27,500 --> 00:06:29,600
它往往只是生成一个不同的名称

110
00:06:29,970 --> 00:06:31,170
只是潜在语义索引

111
00:06:32,180 --> 00:06:45,000
为了完整起见 我应该说 在LSI中

112
00:06:45,600 --> 00:06:47,440
你通常会跳过预处理步骤

113
00:07:05,540 --> 00:07:08,110
由于种种原因 在LSI中

114
00:07:08,690 --> 00:07:09,990
你通常不会正常化

115
00:07:10,540 --> 00:07:12,070
那个数据为1 并且

116
00:07:12,670 --> 00:07:13,380
你通常不会标准化

117
00:07:13,980 --> 00:07:15,180
特征方差为1

118
00:07:15,970 --> 00:07:18,880
这些差异相对较小

119
00:07:19,410 --> 00:07:22,150
结果是 它做的东西跟PCA非常类似

120
00:07:22,670 --> 00:07:28,280
规范化文本数据的方差为1

121
00:07:28,900 --> 00:07:31,280
实际上是一个坏主意 因为所有的单词都-

122
00:07:32,120 --> 00:07:34,590
因为这将有

123
00:07:34,690 --> 00:07:38,200
大幅增加

124
00:07:38,300 --> 00:07:41,240
不经常出现的单词的重量

125
00:07:41,700 --> 00:07:43,400
因此 举例来说  这个词aardvark

126
00:07:43,770 --> 00:07:45,060
几乎没有出现在任何文件

127
00:07:45,770 --> 00:07:50,200
所以标准化 第二特征方差为1

128
00:07:50,570 --> 00:07:52,220
你最终-- 你大幅增加了

129
00:07:52,650 --> 00:07:53,800
aardvark单词的重量

130
00:07:54,320 --> 00:07:56,560
如果你们不明白我所讲的 不要担心

131
00:07:57,270 --> 00:08:03,280
让我们看看 【无声】这门语言

132
00:08:03,760 --> 00:08:07,260
我们想要做的往往是

133
00:08:07,790 --> 00:08:12,270
给它两个文件 x^((i) )和x^((j) )

134
00:08:13,470 --> 00:08:15,690
来比较它们的相似性

135
00:08:20,360 --> 00:08:23,630
因此 举例来说 我可能给你一份文件

136
00:08:24,090 --> 00:08:27,480
并希望你能找出与其类似的其他文档

137
00:08:27,880 --> 00:08:28,450
我们读一些关于

138
00:08:28,940 --> 00:08:30,180
当今一些用户事件的文章

139
00:08:31,140 --> 00:08:33,060
并希望能找出 其他的新闻文章

140
00:08:33,190 --> 00:08:35,430
所以 我给你一份文档

141
00:08:35,440 --> 00:08:37,680
并要你浏览所有其他的文档

142
00:08:38,190 --> 00:08:39,810
你有很多很多的文档

143
00:08:40,100 --> 00:08:41,390
并找到类似这样的文档

144
00:08:41,840 --> 00:08:46,980
因此 这是一个典型的文本应用

145
00:08:47,370 --> 00:08:54,480
所以要衡量两个文件 x^((i) )和x^((j) )之间的相似性

146
00:08:54,860 --> 00:08:59,450
每个文件都是作为

147
00:08:59,830 --> 00:09:01,770
高维向量来表示

148
00:09:02,310 --> 00:09:04,570
一种常用的处理方法是

149
00:09:04,990 --> 00:09:06,710
作为某种非常高维的向量

150
00:09:07,240 --> 00:09:10,730
来查看每个文件

151
00:09:11,090 --> 00:09:17,190
因此 这些都是在非常高维空间向量

152
00:09:17,570 --> 00:09:19,440
向量的维数

153
00:09:19,780 --> 00:09:21,880
等于你的字典中的单词数量

154
00:09:22,260 --> 00:09:28,770
因此 也许每一个文件

155
00:09:29,120 --> 00:09:31,260
都在约50  000维空间中

156
00:09:31,650 --> 00:09:33,150
如果在你的字典中有50  000个单词

157
00:09:33,690 --> 00:09:36,130
所以这两个文件间经常使用的一个类似性质

158
00:09:36,480 --> 00:09:39,510
是这两个文件

159
00:09:41,230 --> 00:09:43,820
之间的角度

160
00:09:44,400 --> 00:09:55,220
特别是 如果这两个向量之间的角度很小

161
00:09:55,600 --> 00:09:58,930
那么这两个文件 我们会认为它们是相似的

162
00:09:59,410 --> 00:10:01,670
如果这两个向量之间的角度是大的

163
00:10:02,170 --> 00:10:03,950
那么我们认为这两个文件是不类似的

164
00:10:04,530 --> 00:10:08,500
所以更正式地说 一个常用的探索式

165
00:10:08,840 --> 00:10:10,590
民族语言的处理 是说

166
00:10:11,100 --> 00:10:13,350
两个文件之间的相似性是

167
00:10:13,690 --> 00:10:17,420
它们之间的角度theta的余弦

168
00:10:17,910 --> 00:10:24,480
无论如何 余弦是关于theta的递减函数

169
00:10:24,860 --> 00:10:27,390
因此 它们之间的角度较小 相似性越大

170
00:10:27,840 --> 00:10:32,740
两个向量之间的余弦 当然

171
00:10:32,830 --> 00:10:44,090
只是x(i)T x(j)除以--OK?

172
00:10:44,180 --> 00:10:53,180
这只是线性代数或几何定义

173
00:10:53,630 --> 00:10:55,860
两个向量之间的余弦标准

174
00:11:03,530 --> 00:11:07,080
这里的LSI所做的背后的直觉

175
00:11:07,180 --> 00:11:21,540
希望 像往常一样 可能会出现一些

176
00:11:21,650 --> 00:11:24,030
有趣的数据变更的坐标轴

177
00:11:24,620 --> 00:11:28,640
有可能其他一些坐标轴 只是噪音

178
00:11:29,230 --> 00:11:30,840
因此所有通过在低维

179
00:11:30,840 --> 00:11:32,640
子空间中设计你的数据

180
00:11:33,020 --> 00:11:33,770
希望是

181
00:11:34,660 --> 00:11:36,480
通过这种方式在你的文本数据运行PCA

182
00:11:36,800 --> 00:11:38,360
你可以删除一些数据中的噪声

183
00:11:38,710 --> 00:11:39,590
并更好的

184
00:11:40,020 --> 00:11:41,670
比较文件之间的相似性

185
00:11:42,040 --> 00:11:43,960
让我们一点点深入这些例子

186
00:11:44,420 --> 00:11:46,240
更直观地感觉LSI做了什么

187
00:11:46,670 --> 00:11:50,130
因此 进一步研究

188
00:11:50,520 --> 00:11:51,990
余弦的相似性度量的定义

189
00:11:52,350 --> 00:12:05,310
因此 两个文件之间的

190
00:12:05,310 --> 00:12:13,090
分子或相似性

191
00:12:13,480 --> 00:12:21,460
是这个内积 那是K XIK

192
00:12:21,460 --> 00:12:24,890
XJK的总和

193
00:12:25,550 --> 00:12:32,840
因此 如果这两个文件中

194
00:12:33,320 --> 00:12:36,630
没有相同内容的话 这个内积就等于零

195
00:12:37,160 --> 00:12:43,020
因此 这是实际上--K的总和--

196
00:12:43,440 --> 00:12:56,650
是否文件指示器 I和J 都包含单词 K

197
00:12:57,090 --> 00:13:00,700
因为我猜XIK表示

198
00:13:01,070 --> 00:13:02,810
是否文档I字符K

199
00:13:03,260 --> 00:13:07,270
XJK表示J文件是否包含单词 K

200
00:13:07,780 --> 00:13:12,730
因此 只有当字符K出现在这两个文件中

201
00:13:13,020 --> 00:13:14,330
该内积才为1

202
00:13:14,740 --> 00:13:18,170
因此 如果这两个文件中没有相同的话

203
00:13:18,490 --> 00:13:21,210
这两个文件之间的相似性将是零

204
00:13:21,520 --> 00:13:35,860
例如 假设你的文件 XI 有单词study

205
00:13:36,240 --> 00:13:42,570
文件XJ 有单词learn 然后

206
00:13:42,950 --> 00:13:45,590
这两个文件

207
00:13:45,900 --> 00:13:48,680
可能被认为是完全不同的

208
00:13:49,180 --> 00:13:52,040
【听不清】有效的学习策略

209
00:13:52,900 --> 00:13:54,520
有时你读有关的新闻文章

210
00:13:55,160 --> 00:13:57,560
那你会问 有什么其他的文件与之类似吗?

211
00:13:58,010 --> 00:14:00,640
如果还有一堆其他文件

212
00:14:01,000 --> 00:14:02,500
关于"学习的好方法"

213
00:14:02,850 --> 00:14:04,270
而不是有共同的单词

214
00:14:04,560 --> 00:14:06,130
因此两个文件的相似性是零

215
00:14:07,010 --> 00:14:10,260
所以这里有一个卡通

216
00:14:10,630 --> 00:14:12,370
关于我们所希望或者说是我希望PCA会做的事情

217
00:14:12,820 --> 00:14:16,090
这是假设在横轴上

218
00:14:16,850 --> 00:14:22,820
我绘制单词learn 并在垂直的访问

219
00:14:23,220 --> 00:14:24,230
我绘制单词study

220
00:14:24,770 --> 00:14:28,630
因此 那个值要么是0 要么是1

221
00:14:29,000 --> 00:14:31,110
因此 如果文档包含单词learn

222
00:14:31,570 --> 00:14:34,720
但没包含study 那么它会在那里绘制该文件

223
00:14:35,610 --> 00:14:36,940
如果文档

224
00:14:37,420 --> 00:14:38,860
既没包含单词learn 也没有包含study

225
00:14:39,210 --> 00:14:41,780
那么它会在(0  0)处绘制该文件

226
00:14:42,290 --> 00:14:45,170
因此这是一个关于PCA背后做了什么的卡通

227
00:14:45,760 --> 00:14:49,310
这是我们确定的低维子空间

228
00:14:49,680 --> 00:14:54,030
这将是总和--特征向量 我们避免PCA

229
00:14:54,470 --> 00:14:59,890
现在 假定我们有一个关于learning的文件

230
00:15:00,320 --> 00:15:02,110
我们有一个关于studying的文件

231
00:15:02,490 --> 00:15:06,090
关于learning的文件指向右边

232
00:15:06,170 --> 00:15:07,410
关于studying的文件指向上边

233
00:15:07,490 --> 00:15:09,730
因此 内积

234
00:15:10,240 --> 00:15:13,020
或这两个文件之间的余弦角将是--对不起

235
00:15:13,220 --> 00:15:15,700
这两个文件之间的内积将为0

236
00:15:16,800 --> 00:15:21,800
因此 这两个文件是完全无关的

237
00:15:22,800 --> 00:15:24,830
这是不是我们想要的

238
00:15:24,830 --> 00:15:28,110
关于study的文件  关于learn的文件 它们是相关的

239
00:15:28,350 --> 00:15:31,160
但是 我们这两个文件

240
00:15:31,160 --> 00:15:33,740
我们在这个子空间设计它

241
00:15:34,290 --> 00:15:41,330
那么这两个文件现在变得紧密联系起来

242
00:15:41,920 --> 00:15:44,140
并且该算法将认识到

243
00:15:44,530 --> 00:15:47,370
当你说的这两个文件之间的内积

244
00:15:47,730 --> 00:15:49,520
结果实际上是一个正数

245
00:15:49,940 --> 00:15:53,790
因此 LSI使我们算法能够认识到

246
00:15:54,070 --> 00:15:54,740
这两个文件之间

247
00:15:55,140 --> 00:15:56,780
有一些正相似性

248
00:15:57,340 --> 00:16:02,360
所以 这只是有关PCA

249
00:16:02,900 --> 00:16:04,460
可能处理文本数据的直觉

250
00:16:04,840 --> 00:16:06,800
同样的事情 转到其他例子中

251
00:16:07,090 --> 00:16:08,110
那两个单词study和learn

252
00:16:08,460 --> 00:16:10,970
所以 你已经--你找到一个关于

253
00:16:11,400 --> 00:16:12,290
政治家的文件

254
00:16:12,730 --> 00:16:15,330
和杰出政治家名字的文件

255
00:16:15,760 --> 00:16:20,620
这也将使文件更加紧密

256
00:16:20,710 --> 00:16:26,080
或着任何相关的话题 他们最终会

257
00:16:26,180 --> 00:16:28,350
紧密联系起来 只是低维空间

258
00:16:31,940 --> 00:16:34,570
有问题吗?

259
00:16:36,590 --> 00:16:40,030
学生:那个应该是i还是k?

260
00:16:40,110 --> 00:16:42,850
教员(安德鲁吴):哪些呢?这一个?

261
00:16:43,480 --> 00:16:44,500
受访者:不 这一行

262
00:16:44,850 --> 00:16:51,790
教员(安德鲁吴):哦 这一行 哦 对了 谢谢

263
00:16:53,800 --> 00:17:07,370
【听不清】 因此 让我们现在来谈谈

264
00:17:07,670 --> 00:17:16,530
如何真正实现它 好了

265
00:17:16,980 --> 00:17:21,550
你们有多少人知道SVD(奇异值分解)

266
00:17:21,860 --> 00:17:25,640
或单值分解是什么?哇 有很多

267
00:17:26,040 --> 00:17:28,550
比我想象的多得多 挺好奇

268
00:17:28,640 --> 00:17:32,070
你们是在本科还是研究生时学习的呢?

269
00:17:34,610 --> 00:17:41,850
好的 让我讨论它

270
00:17:42,900 --> 00:17:46,870
我没想到这么多人知道什么是SVD的

271
00:17:47,240 --> 00:17:49,000
把它录下来

272
00:17:49,330 --> 00:17:51,890
只是让其他人也可以学习它

273
00:17:51,970 --> 00:17:59,020
所以 我要说一点有关如何实施PCA知识

274
00:17:59,110 --> 00:18:03,290
我刚才躲避的问题是

275
00:18:04,370 --> 00:18:06,780
当你有这些高维向量

276
00:18:07,280 --> 00:18:08,190
然后Σ是一个大矩阵

277
00:18:08,760 --> 00:18:12,210
尤其是 对于我们的文本例子

278
00:18:12,890 --> 00:18:16,620
如果向量XI是50  000维

279
00:18:17,810 --> 00:18:19,620
协方差矩阵将是

280
00:18:20,120 --> 00:18:26,930
50  000*50  000维

281
00:18:27,440 --> 00:18:29,740
它太大了 没法明确表示

282
00:18:31,080 --> 00:18:37,970
我想你们许多人已经知道这一点

283
00:18:38,270 --> 00:18:39,810
但我只想说也无妨

284
00:18:40,800 --> 00:18:43,540
原来有另一种方式来实现PCA

285
00:18:43,850 --> 00:18:51,950
那就是 如果A是任何N*N矩阵

286
00:18:52,670 --> 00:18:56,980
然后线性代数的最显着的成果之一是

287
00:18:57,370 --> 00:19:02,240
矩阵A 可以分解成

288
00:19:02,860 --> 00:19:06,080
一个奇异值分解

289
00:19:07,270 --> 00:19:12,080
这也就意味着 矩阵A 它是N*N的

290
00:19:12,700 --> 00:19:16,650
总是可以分解成三个矩阵的乘积

291
00:19:17,080 --> 00:19:20,990
U是N*N D是一个方阵 即N*N

292
00:19:21,430 --> 00:19:35,960
且V将成为对角线

293
00:19:36,470 --> 00:19:45,600
并且

294
00:19:46,210 --> 00:19:47,680
ΣI的值被称为

295
00:19:48,550 --> 00:19:55,410
矩阵A的奇异值

296
00:19:55,500 --> 00:20:04,000
几乎你们大部分都是在研究生时学过

297
00:20:04,650 --> 00:20:06,540
而不是在本科时学过  所以事实证明

298
00:20:07,130 --> 00:20:09,350
当你在本科学习线性代数课程时

299
00:20:09,730 --> 00:20:11,240
通常你们学习了很多分解

300
00:20:11,630 --> 00:20:13,930
所以通常要学习QLD成分

301
00:20:14,270 --> 00:20:15,810
可能是矩阵LU分解

302
00:20:16,940 --> 00:20:18,130
大多本科课程

303
00:20:18,530 --> 00:20:20,540
没有谈到奇异值分解(SVD)

304
00:20:20,830 --> 00:20:26,010
但至少在--在机器学习中 几乎我所做的

305
00:20:26,420 --> 00:20:29,700
你会发现 你最终使用SVD

306
00:20:29,990 --> 00:20:31,470
远远超过任何

307
00:20:31,880 --> 00:20:35,100
你在线性代数课堂上学过的分解

308
00:20:35,460 --> 00:20:37,190
就个人而言

309
00:20:37,590 --> 00:20:40,260
我在过去的一年中 应用SVD好几十次

310
00:20:41,030 --> 00:20:43,580
但LU和QRD成分

311
00:20:44,010 --> 00:20:45,010
我觉得我曾经只在去年使用过

312
00:20:45,280 --> 00:20:46,280
一次QRD成分

313
00:20:46,710 --> 00:20:48,000
和LU分解

314
00:20:48,800 --> 00:20:55,850
让我们看看 我会对此多说一点

315
00:20:56,290 --> 00:21:03,630
所以我将要画这个图片 我猜

316
00:21:04,090 --> 00:21:09,870
例如 如果A是一个N*N矩阵

317
00:21:10,240 --> 00:21:13,260
它可以分解成另一个矩阵U

318
00:21:13,560 --> 00:21:14,570
那也是N*N的

319
00:21:14,970 --> 00:21:25,390
它是一样大小的 D 是N*N的

320
00:21:25,760 --> 00:21:32,340
另一个方阵 V转置

321
00:21:32,720 --> 00:21:33,760
也是N*N的

322
00:21:34,190 --> 00:21:39,710
此外 在奇异值分解中

323
00:21:40,160 --> 00:21:44,200
矩阵U的列

324
00:21:44,620 --> 00:21:52,040
将是A转置的特征向量

325
00:21:52,400 --> 00:21:56,640
V的列

326
00:21:57,100 --> 00:22:06,750
将是A转置*A的特征向量

327
00:22:07,560 --> 00:22:13,090
要计算它

328
00:22:13,460 --> 00:22:18,220
你只需要使用Matlab或Octave的SVD命令

329
00:22:18,880 --> 00:22:25,950
今天 数值线性代数的艺术是

330
00:22:26,450 --> 00:22:29,390
SVD 奇异值分解

331
00:22:29,750 --> 00:22:32,130
矩阵可以非常稳定地计算

332
00:22:33,040 --> 00:22:37,270
我们使用Matlab或Octave来进行计算

333
00:22:38,250 --> 00:22:39,940
比方说 一个矩阵的特征向量

334
00:22:40,450 --> 00:22:46,530
因此 SVD的例程在数字上 甚至比

335
00:22:46,970 --> 00:22:48,330
用于查找

336
00:22:48,710 --> 00:22:49,630
矩阵中的矩阵特征向量例程更稳定

337
00:22:50,360 --> 00:22:52,650
所以 你可以放心地使用这样的例程

338
00:22:52,990 --> 00:22:54,530
类似的方式 他们不用思考它是如何计算

339
00:22:54,830 --> 00:22:56,090
只要使用这个平方根命令就行了

340
00:22:56,420 --> 00:22:57,890
你可以计算平方根的东西

341
00:22:58,290 --> 00:22:59,910
但是不用担心

342
00:23:00,490 --> 00:23:02,000
你知道电脑会给你正确的答案

343
00:23:02,410 --> 00:23:05,500
对于大部分合理大小的矩阵

344
00:23:05,890 --> 00:23:06,580
甚至高达

345
00:23:06,940 --> 00:23:09,000
数千*数千矩阵 SVD的例程

346
00:23:09,370 --> 00:23:11,260
我把它作为一个平方根函数

347
00:23:11,610 --> 00:23:13,770
如果你调用它 它会给你正确的答案

348
00:23:14,150 --> 00:23:16,050
您不必担心太多

349
00:23:16,420 --> 00:23:18,340
如果您有非常大的矩阵

350
00:23:18,670 --> 00:23:19,550
比如一个100万*100万的矩阵

351
00:23:20,200 --> 00:23:21,900
我可能会开始有点担心

352
00:23:22,300 --> 00:23:23,940
但几千*几千的矩阵

353
00:23:24,290 --> 00:23:26,700
现在它能很好的执行

354
00:23:28,630 --> 00:23:30,050
受访者:【听不清】

355
00:23:30,410 --> 00:23:32,140
讲师(安德鲁吴):SVD的难点是什么?

356
00:23:32,230 --> 00:23:33,090
这是一个很好的问题 其实我不知道

357
00:23:33,180 --> 00:23:35,990
我要猜测它大致和N的立方相类似

358
00:23:36,080 --> 00:23:37,440
我不知道

359
00:23:37,500 --> 00:23:44,290
【听不清】算法 所以 我认为--我不知道


360
00:23:44,710 --> 00:23:46,850
这些算法的转换是什么

361
00:23:49,270 --> 00:23:57,350
我画出来的例子是一个宽矩阵

362
00:23:57,730 --> 00:23:59,430
宽矩阵的宽比高要长

363
00:23:59,930 --> 00:24:05,630
以同样的方式 你也可以对高矩阵调用SVD

364
00:24:06,140 --> 00:24:07,630
那么它的高度比它的宽度大

365
00:24:07,960 --> 00:24:20,370
它会分解成--OK?

366
00:24:20,750 --> 00:24:22,660
像那三个矩阵那样的结果

367
00:24:23,580 --> 00:24:32,340
这样做的好处是  我们可以用它

368
00:24:32,660 --> 00:24:36,040
来非常有效地计算特征向量和PCA

369
00:24:38,530 --> 00:24:49,250
特别是 协方差矩阵Σ是这样的

370
00:24:49,990 --> 00:24:54,450
这是所有数项的总和

371
00:24:55,550 --> 00:24:57,250
因此 如果你回头想想

372
00:24:57,680 --> 00:25:01,170
设计矩阵的定义

373
00:25:01,730 --> 00:25:06,700
我觉得我在第二次演讲中描述了它

374
00:25:07,110 --> 00:25:09,850
当我们推导出密切形式的这些方阵【无声】

375
00:25:10,220 --> 00:25:11,880
这些方阵的解答方案

376
00:25:12,280 --> 00:25:14,670
设计矩阵就是这个矩阵

377
00:25:15,130 --> 00:25:26,280
我把我的例子 并把它们堆入行中

378
00:25:26,790 --> 00:25:29,570
他们称这种设计矩阵为【听不清】

379
00:25:30,220 --> 00:25:35,070
所以 如果你构造设计矩阵

380
00:25:35,500 --> 00:25:40,180
协方差矩阵Σ

381
00:25:41,010 --> 00:25:45,330
可以写成X转置

382
00:26:01,060 --> 00:26:03,060
即X^T 【听不清】

383
00:26:10,530 --> 00:26:19,710
OK?我希望你看到为什么在X^TX能够给你

384
00:26:20,120 --> 00:26:21,680
提供向量产品的总和

385
00:26:22,980 --> 00:26:24,490
如果你现在没有看到

386
00:26:24,830 --> 00:26:27,490
回家去 并确认自己【无声】这是否是真的

387
00:26:36,530 --> 00:26:42,780
要获得Σ的top K特征向量

388
00:26:50,800 --> 00:27:02,470
你会采取Σ和使用进行分解--对不起

389
00:27:03,270 --> 00:27:09,710
你会采取矩阵X 你会作为SVD而计算

390
00:27:10,290 --> 00:27:11,760
所以 你得到USV转置

391
00:27:12,370 --> 00:27:19,350
然后U的前三列是

392
00:27:19,780 --> 00:27:32,390
X^TX的top K特征向量 因此

393
00:27:32,990 --> 00:27:38,730
是协方差矩阵Σ的top K特征向量

394
00:27:39,460 --> 00:27:48,460
因此 在我们的例子中 设计矩阵 可能是 R

395
00:27:49,090 --> 00:27:51,390
如果你的字典中有50000个单词

396
00:27:52,140 --> 00:27:56,230
那么设计矩阵将是RM*50000

397
00:27:56,800 --> 00:28:01,230
如果你有100个例子 那就是100*50000

398
00:28:01,900 --> 00:28:05,660
因此X很容易被驾驭 用来表示和计算SVD

399
00:28:06,040 --> 00:28:10,420
而矩阵Σ将更难被表示

400
00:28:10,890 --> 00:28:12,430
这是50  000*50  000

401
00:28:12,840 --> 00:28:16,720
因此 这给你一种有效的方式来实现PCA

402
00:28:17,830 --> 00:28:21,330
我想谈谈这个的原因是 前几年

403
00:28:21,410 --> 00:28:24,410
我没有谈过它

404
00:28:25,280 --> 00:28:27,280
在课堂项目中 我发现很多学生用SVD

405
00:28:27,280 --> 00:28:30,540
来实现大型问题和【听不清】

406
00:28:31,170 --> 00:28:34,320
所以这是一个更好的实现PCA的方法

407
00:28:35,020 --> 00:28:36,990
如果你有非常高维数据的时候

408
00:28:37,510 --> 00:28:39,260
如果你有低维数据

409
00:28:39,670 --> 00:28:41,380
如果你有50*100维的数据

410
00:28:41,670 --> 00:28:44,620
然后计算Σ 没有问题

411
00:28:44,970 --> 00:28:46,560
你可以使用老方法来处理 否则

412
00:28:47,010 --> 00:28:48,010
使用SVD来实现它

413
00:28:48,860 --> 00:28:53,890
关于这个有问题么?

414
00:29:25,630 --> 00:29:29,120
最后我想说的是 在实践中

415
00:29:29,540 --> 00:29:31,670
当你要实现这个 我想说一个注意事项

416
00:29:32,500 --> 00:29:38,740
原来 对于许多应用程序--让我们来看看

417
00:29:39,400 --> 00:29:41,700
当对这些应用SVD--呀

418
00:29:42,370 --> 00:29:43,550
受访者:是一个快速的问题

419
00:29:43,940 --> 00:29:45,670
U或V的top k列

420
00:29:45,980 --> 00:29:50,830
是因为X^TX就是V^T 对吗?

421
00:29:51,280 --> 00:29:54,710
教导员(安德鲁吴):让我们来看看 哦对了

422
00:29:55,520 --> 00:29:59,890
我觉得你说得对 我觉得你说得对

423
00:30:00,670 --> 00:30:08,400
让我们来看看 它是U还是V的top K列?

424
00:30:09,920 --> 00:30:33,080
是啊 我觉得你说得对 是吗?

425
00:30:33,640 --> 00:30:37,110
有东西困扰我 但我认为你说得对

426
00:30:42,080 --> 00:30:44,160
受访者:【听不清】 因此 然后X^TX应该是VDD

427
00:30:45,070 --> 00:30:50,220
X是UDV 因此 X^TX将是--

428
00:30:51,080 --> 00:30:53,350
教员(安德鲁吴):【听不清】

429
00:30:59,170 --> 00:31:04,040
如果有人思考它 并且有另一种意见

430
00:31:04,600 --> 00:31:05,830
告诉我 但我认为你说得对

431
00:31:06,750 --> 00:31:08,750
我会确保我得到的细节 并且让你知道

432
00:31:15,940 --> 00:31:19,920
当时每个人都还在寻找它

433
00:31:22,090 --> 00:31:24,390
汤姆 你可以找出正确的答案 让我知道吗?

434
00:31:25,090 --> 00:31:26,400
男声:这听起来很正确的

435
00:31:27,940 --> 00:31:29,940
教员(安德鲁吴):好 酷 OK

436
00:31:30,530 --> 00:31:32,920
所以最后一个事项 一个注意事项

437
00:31:33,600 --> 00:31:34,930
事实证明 在这个例子中

438
00:31:35,540 --> 00:31:38,250
我执行宽矩阵的SVD

439
00:31:38,750 --> 00:31:44,550
因此 矩阵X是M*N(原字幕是N by N)的

440
00:31:45,280 --> 00:31:49,570
结果当你找到它的奇异值分解时

441
00:31:50,120 --> 00:31:57,190
事实证明--让我们来看看

442
00:31:57,890 --> 00:32:00,630
是啊 我觉得你绝对正确

443
00:32:01,270 --> 00:32:03,680
因此 事实证明 我们找到了这个SVD

444
00:32:04,130 --> 00:32:07,260
这个矩阵的右边的大部分

445
00:32:07,650 --> 00:32:08,830
都为0

446
00:32:09,320 --> 00:32:17,160
此外 当你计算矩阵D

447
00:32:17,790 --> 00:32:21,010
此矩阵的很大一部分都为0

448
00:32:21,890 --> 00:32:23,800
你有矩阵V^T(原字幕为D^T)

449
00:32:24,390 --> 00:32:29,900
因此 这取决于你采用什么样的公约 例如

450
00:32:30,350 --> 00:32:33,160
我认为MATLAB实际使用的

451
00:32:33,780 --> 00:32:37,770
只是斩去元素0的公约

452
00:32:47,250 --> 00:32:50,320
所以 MATLAB使用斩去U矩阵

453
00:32:50,770 --> 00:32:53,450
最右边的一半

454
00:32:53,530 --> 00:32:55,400
斩去D矩阵的底部的公约

455
00:32:55,490 --> 00:32:59,950
我不知道这是否还取决于MATLAB版本

456
00:33:00,170 --> 00:33:01,400
但是当你在Matlab

457
00:33:01,400 --> 00:33:03,740
或其他一些数值代数软件包上调用SVD

458
00:33:04,240 --> 00:33:07,510
当矩阵宽度大于高度时

459
00:33:07,870 --> 00:33:10,030
定义SVD的公约会略有不同

460
00:33:10,470 --> 00:33:12,490
所以注意堤防这个东西 并确保

461
00:33:12,850 --> 00:33:15,440
你对计算机指令

462
00:33:15,870 --> 00:33:19,960
能够使用相对应的数学代数库的公约

463
00:33:21,590 --> 00:33:24,780
事实证明 如果你使用Matlab的【听不清】

464
00:33:25,170 --> 00:33:27,530
或者你写C代码

465
00:33:27,970 --> 00:33:29,760
有很多系统库

466
00:33:30,250 --> 00:33:32,250
可以为你计算SVD

467
00:33:32,740 --> 00:33:35,500
但是他们定义的这些矩阵的维数的公约

468
00:33:36,390 --> 00:33:37,870
是略有不同

469
00:33:38,350 --> 00:33:39,040
所以需要确保

470
00:33:39,430 --> 00:33:40,760
你可以找到使用的软件包

471
00:33:41,370 --> 00:33:45,800
最后 我想谈谈之前说过的

472
00:33:46,170 --> 00:33:48,050
无监督学习算法

473
00:33:48,460 --> 00:33:51,010
把它放入稍微大点的环境中

474
00:33:51,390 --> 00:33:53,750
它部分地解答了我在办公时间时

475
00:33:54,100 --> 00:33:55,660
学生提出的

476
00:33:56,420 --> 00:34:00,300
关于如果使用每一个算法的问题

477
00:34:01,540 --> 00:34:03,040
所以我要来绘制一个2*2矩阵

478
00:34:03,460 --> 00:34:09,210
这是一个图表 我觉得是很有用的

479
00:34:09,850 --> 00:34:17,940
我们前面谈到的算法之一 在此之前

480
00:34:18,290 --> 00:34:21,340
是因子分析 它是--它是--

481
00:34:21,940 --> 00:34:25,760
我希望你还记得我画的图

482
00:34:26,110 --> 00:34:27,960
在线上 我有很多的Z点

483
00:34:28,560 --> 00:34:30,980
然后 我画了这些椭圆

484
00:34:31,350 --> 00:34:36,260
我希望你还记得那个图

485
00:34:36,650 --> 00:34:37,970
这是一个因子分析模型

486
00:34:38,390 --> 00:34:41,030
对密度效应的高斯建模 对不对?

487
00:34:41,510 --> 00:34:43,860
它也是一个主成分分析(PCA) 刚才

488
00:34:44,370 --> 00:34:47,800
所以因子分析和PCA之间的不同之处是

489
00:34:48,190 --> 00:34:50,020
我思考他的方式

490
00:34:50,280 --> 00:34:53,870
因子分析法是一种密度估计算法

491
00:34:54,510 --> 00:34:57,560
它试图模拟训练示例X的密度

492
00:34:58,180 --> 00:35:03,640
而PCA是不是一个概率算法

493
00:35:04,380 --> 00:35:07,300
特别是 它没有赋予

494
00:35:07,740 --> 00:35:11,660
任何概率分布的训练样本

495
00:35:11,940 --> 00:35:13,910
而直接去寻找子空间

496
00:35:14,210 --> 00:35:16,350
因此 依据何时使用因素分析

497
00:35:16,810 --> 00:35:17,950
何时使用PCA

498
00:35:18,230 --> 00:35:21,290
如果你的目标是降低数据的维数

499
00:35:21,810 --> 00:35:24,910
如果你的目标是找到数据的子空间

500
00:35:25,270 --> 00:35:30,330
然后PCA的直接尝试寻找子空间

501
00:35:30,730 --> 00:35:33,220
我想我会倾向于使用PCA的

502
00:35:33,580 --> 00:35:39,920
因子分析 好似假定数据在一个子空间中

503
00:35:40,520 --> 00:35:42,090
让我在这里写一个子空间

504
00:35:42,750 --> 00:35:47,830
因此 这两个算法好似数据

505
00:35:48,100 --> 00:35:51,700
可能位于或于靠近某些低维子空间

506
00:35:52,060 --> 00:35:53,780
但是从根本上说 因子分析

507
00:35:54,170 --> 00:35:56,290
我认为它是一个密度估计算法

508
00:35:56,580 --> 00:35:58,870
因此 有一些非常高的维分布

509
00:35:59,160 --> 00:36:01,430
我想建立模型P(x)

510
00:36:02,070 --> 00:36:03,270
那因子分析的算法是

511
00:36:03,270 --> 00:36:04,600
我更倾向于使用的算法

512
00:36:05,960 --> 00:36:07,560
因此 即使你可以在理论上

513
00:36:08,050 --> 00:36:11,890
我会倾向于避免使用因子分析

514
00:36:12,210 --> 00:36:14,690
以确定一个子空间中的数据集

515
00:36:15,200 --> 00:36:19,740
所以高斯 如果你想要做异常检测

516
00:36:20,100 --> 00:36:21,390
如果我想建立模型P(x)

517
00:36:21,630 --> 00:36:23,500
那么 如果你的N的概率非常低

518
00:36:23,500 --> 00:36:25,740
那么你可以分解一个异常 然后我

519
00:36:26,970 --> 00:36:29,550
会倾向于使用因子分析 来处理密度估计

520
00:36:30,490 --> 00:36:34,270
因此 因子分析和PCA是两种算法

521
00:36:34,930 --> 00:36:37,280
假设你的数据在子空间中

522
00:36:37,820 --> 00:36:40,360
我们谈到了其他算法的原因是

523
00:36:40,880 --> 00:36:47,630
算法假定数据位于团块中

524
00:36:48,010 --> 00:36:53,940
或数据有几个数组连贯性

525
00:36:54,420 --> 00:36:56,400
因此 让我填充这张照片的空白区

526
00:37:07,930 --> 00:37:10,960
所以 如果你认为你的数据位于团块中

527
00:37:11,360 --> 00:37:12,200
或位于数组中

528
00:37:12,720 --> 00:37:15,600
而且如果去【听不清】密度估计的话

529
00:37:15,920 --> 00:37:18,500
我倾向于使用【无声】算法的混合

530
00:37:18,930 --> 00:37:21,840
但同样 你不一定要赋予你的数据

531
00:37:22,200 --> 00:37:24,190
任何可能的语义

532
00:37:24,670 --> 00:37:26,390
因此 如果你只是想找到数据块

533
00:37:26,810 --> 00:37:28,540
然后我倾向于使用聚类算法

534
00:37:29,110 --> 00:37:31,910
所以之前没有看到任何人画出这样的图片

535
00:37:32,300 --> 00:37:34,320
但我倾向于在我脑子中整理这些东西

536
00:37:34,890 --> 00:37:36,190
希望这有助于引导

537
00:37:36,800 --> 00:37:39,270
当你可能使用这些算法

538
00:37:39,650 --> 00:37:42,300
取决于你是否相信这些数据

539
00:37:42,590 --> 00:37:43,650
可能位于子空间

540
00:37:44,010 --> 00:37:45,790
还是结合在数据块中

541
00:37:51,480 --> 00:37:56,190
好的 那是关于PCA的讨论

542
00:38:00,770 --> 00:38:04,940
接下来我想要做的

543
00:38:07,840 --> 00:38:13,530
是有关独立成分分析 或者说ICA 请说

544
00:38:13,650 --> 00:38:16,290
受访者:我有一个  关于右上角的问题

545
00:38:16,390 --> 00:38:19,540
那么一旦你的所有特征向量

546
00:38:19,640 --> 00:38:23,020
有没有人知道一些... 好吧

547
00:38:23,130 --> 00:38:24,530
这些特征I和特征J是有多么相似呢

548
00:38:24,650 --> 00:38:28,450
您选择了一些特征向量

549
00:38:28,550 --> 00:38:29,960
并且你在特征I和特征J之间

550
00:38:30,330 --> 00:38:34,390
采取一些点乘积和特征向量

551
00:38:35,060 --> 00:38:37,040
但也有很多可以选择的特征向量

552
00:38:38,020 --> 00:38:38,660
教导员(安德鲁吴):对

553
00:38:39,260 --> 00:38:41,430
所以Justin的问题是

554
00:38:41,930 --> 00:38:43,720
已经找到了我的特征向量

555
00:38:44,050 --> 00:38:46,630
我选择什么样的特征向量 用于测量距离

556
00:38:47,020 --> 00:38:54,140
我要开始跟进

557
00:38:55,410 --> 00:38:59,240
因此 答案是--在这个图里

558
00:38:59,760 --> 00:39:03,490
我会避免在另外一个时间中思考特征向量

559
00:39:03,850 --> 00:39:06,650
一个更好的方式来看这个图 实际上是-

560
00:39:07,240 --> 00:39:10,290
如果我决定选择100个特征向量

561
00:39:10,700 --> 00:39:13,060
这实际上是100 D的子空间

562
00:39:18,880 --> 00:39:23,360
所以我不会把数据投影到一个特征向量上

563
00:39:23,790 --> 00:39:27,250
这个箭头 这个图

564
00:39:27,640 --> 00:39:29,070
这表示

565
00:39:29,340 --> 00:39:31,150
我所有特征向量的100维子空间【听不清】

566
00:39:31,410 --> 00:39:35,290
所以 我实际上做的是

567
00:39:36,340 --> 00:39:38,170
把数据投影到跨度上 特征向量的线性跨度

568
00:39:39,500 --> 00:39:42,150
然后 我测量距离

569
00:39:42,150 --> 00:39:45,930
或者用特征向量的两点投影之间内积

570
00:39:46,830 --> 00:39:49,100
OK

571
00:39:49,830 --> 00:39:56,290
因此 让我们来谈谈ICA 独立成分分析

572
00:39:56,920 --> 00:40:01,990
然而PCA是用于查找

573
00:40:02,310 --> 00:40:05,550
数据变更的主轴的一个算法

574
00:40:05,880 --> 00:40:08,730
在ICA中 我们将尝试找到

575
00:40:09,040 --> 00:40:11,520
数据变化的独立组成部分

576
00:40:12,050 --> 00:40:14,510
那么 请切换到笔记本电脑中

577
00:40:15,010 --> 00:40:17,750
我们用花一秒钟来启动它

578
00:40:18,110 --> 00:40:30,840
我要去这样处理-即使如果你加上-好的

579
00:40:31,240 --> 00:40:34,240
其实 这是我在演讲1中显示的幻灯片

580
00:40:34,730 --> 00:40:38,760
关于鸡尾酒会的问题

581
00:40:39,150 --> 00:40:42,100
假设在一个鸡尾酒会中

582
00:40:42,620 --> 00:40:44,440
你在房间里有两个麦克风

583
00:40:44,710 --> 00:40:47,010
重叠的两段谈话

584
00:40:47,980 --> 00:40:49,860
然后你可以分离出

585
00:40:50,180 --> 00:40:51,820
原来的两个演讲者的声音吗?

586
00:40:52,070 --> 00:40:53,580
所以在第一次演讲中

587
00:40:54,520 --> 00:40:55,910
其实我已经播放了这个音频

588
00:40:56,220 --> 00:41:12,860
这假设是麦克风1的记录 【录音】

589
00:41:12,860 --> 00:41:14,140
教员(安德鲁吴):那么 问题是

590
00:41:14,630 --> 00:41:15,990
这里有两个演讲者

591
00:41:16,530 --> 00:41:17,950
彼此独立讲话

592
00:41:18,620 --> 00:41:22,910
因此 每一个演讲者是一系列声音信号

593
00:41:23,490 --> 00:41:26,250
独立于房间内其他的谈话声音

594
00:41:26,700 --> 00:41:30,040
所以这是一个监督学习问题 问题是

595
00:41:30,440 --> 00:41:32,470
我们是否可以用这两个麦克风的录音

596
00:41:33,190 --> 00:41:35,260
采用一个算法 找到

597
00:41:35,550 --> 00:41:37,760
这个数据中独立的成分?

598
00:41:38,040 --> 00:41:47,200
如果我们这样处理 这是输出 【录音】

599
00:41:47,900 --> 00:41:55,910
教员(安德鲁吴):这是另一个 【录音】

600
00:41:56,270 --> 00:41:58,040
教员(安德鲁吴):只是为了好玩

601
00:41:58,460 --> 00:41:59,920
这些都是我从【听不清】音频中剪辑的

602
00:42:00,410 --> 00:42:03,520
只是为了好玩 让我播放另外一个

603
00:42:03,910 --> 00:42:13,200
这是重叠的麦克风1 【录音】

604
00:42:13,600 --> 00:42:21,560
教员(安德鲁吴):这里是麦克风2【录音】

605
00:42:21,970 --> 00:42:23,130
教员(安德鲁吴):作为输入

606
00:42:23,470 --> 00:42:30,880
这里是输出1  【录音】

607
00:42:31,200 --> 00:42:31,990
教员(安德鲁吴):它并不完美

608
00:42:32,330 --> 00:42:33,640
但它大致上分出了音乐

609
00:42:34,000 --> 00:42:41,730
这里是第2个 【录音】

610
00:42:42,190 --> 00:42:43,710
教员(安德鲁吴):OK

611
00:42:43,710 --> 00:42:44,850
请切换回【听不清】

612
00:42:45,930 --> 00:42:48,440
所以我现在想要做的是

613
00:42:49,000 --> 00:42:50,810
描述一种算法来处理它

614
00:42:51,840 --> 00:43:00,380
在我进入这个算法之前

615
00:43:00,700 --> 00:43:04,810
我想说两分钟关于CDF的知识

616
00:43:05,150 --> 00:43:06,700
即累积分布函数

617
00:43:18,380 --> 00:43:20,140
我知道你们中的大多数人都知道这些是什么

618
00:43:20,430 --> 00:43:22,830
但我要提醒你 它们是什么

619
00:43:23,240 --> 00:43:26,820
比方说 你有一个1维的随机变量S

620
00:43:27,270 --> 00:43:33,260
假设你有一个随机变量 S

621
00:43:33,930 --> 00:43:38,850
并假设它有一个属性密度函数Ps(s)

622
00:43:39,060 --> 00:43:47,470
CDF作为一个函数而被定义

623
00:43:49,950 --> 00:43:52,620
或者更确切地说 作为F 这是概率

624
00:43:53,860 --> 00:43:57,840
随机变量 S 小于小写字母s的给定值

625
00:44:01,700 --> 00:44:05,230
例如 如果这是你的高斯密度

626
00:44:06,200 --> 00:44:07,730
然后高斯密度

627
00:44:08,730 --> 00:44:11,120
通常要注意小写的φ

628
00:44:11,610 --> 00:44:13,330
这大概一个钟形的密度

629
00:44:13,930 --> 00:44:21,240
然后CDF或高斯分布会是这个样子

630
00:44:21,900 --> 00:44:23,950
将有一个主函数PI

631
00:44:24,770 --> 00:44:30,050
所以 如果我选择一个那样的S值

632
00:44:30,490 --> 00:44:32,540
那么这个高度--这是【听不清】概率

633
00:44:33,010 --> 00:44:35,020
那样我的高斯随机变量比那个值小

634
00:44:35,850 --> 00:44:39,110
换句话说 函数在哪一点的高度

635
00:44:39,680 --> 00:44:46,840
小于高斯密度的到s点的区域

636
00:44:47,800 --> 00:44:50,640
当你越来越往右移动

637
00:44:50,990 --> 00:44:52,240
这个函数将靠近1

638
00:44:52,520 --> 00:44:54,710
当你愈加整合这个高斯区域的时候

639
00:45:28,650 --> 00:45:32,850
因此 另一种来写S的F的方法

640
00:45:33,080 --> 00:45:37,130
是积分 密度为S的负无穷大 dt

641
00:45:40,730 --> 00:45:43,690
所以有些东西以后还会再出现的

642
00:45:44,150 --> 00:45:45,630
假设我有一个随机变量 S

643
00:45:46,030 --> 00:45:48,950
我想对随机变量的分布建模

644
00:45:49,390 --> 00:45:51,230
我可以做的一件事情是

645
00:45:51,720 --> 00:45:54,820
我可以做我可以指定我认为的密度是什么

646
00:45:55,260 --> 00:46:04,270
或者 我可以指定CDF是什么

647
00:46:04,810 --> 00:46:11,220
这些都涉及这个等式  F是P(s)的一部分

648
00:46:11,700 --> 00:46:17,350
你还可以通过CDF和求导工具

649
00:46:17,770 --> 00:46:19,290
来恢复这个密度

650
00:46:19,530 --> 00:46:21,310
所以F prime 采取CDF的求导

651
00:46:21,640 --> 00:46:22,920
你得到它的密度

652
00:46:23,410 --> 00:46:26,650
因此 这个出现在 当我导出ICA的中期

653
00:46:27,040 --> 00:46:28,880
这是将是一个步骤

654
00:46:29,240 --> 00:46:31,570
他们需要假定一个随机变量S的分布

655
00:46:31,990 --> 00:46:33,890
我也可以直接为S指定密度

656
00:46:34,780 --> 00:46:38,300
或者我可以指定CDF 我选择指定CDF

657
00:46:40,730 --> 00:46:45,040
它是一个函数 使0增加到1

658
00:46:45,520 --> 00:46:49,580
所以 你可以选择任何函数 它看起来

659
00:46:49,950 --> 00:46:53,130
尤其是 从那个看起来

660
00:46:53,520 --> 00:46:54,760
像帽子的拉拖出来的函数

661
00:46:55,080 --> 00:46:57,050
例如 你可以选择一个CDF的函数

662
00:46:57,430 --> 00:47:01,250
这将是为

663
00:47:01,580 --> 00:47:04,530
随机变量s指定分布密度的一个方式

664
00:47:05,340 --> 00:47:09,780
因此这个等下将会出现 只要【听不清】

665
00:47:30,390 --> 00:47:32,480
如果你熟悉它

666
00:47:33,640 --> 00:47:34,840
如果你看到过它 就举个手 很好

667
00:47:36,580 --> 00:47:43,360
因此 让我们开始

668
00:47:43,740 --> 00:47:45,390
推导出我们的ICA(原字幕为RCA)

669
00:47:45,850 --> 00:47:47,770
或者我们的独立成分分析算法

670
00:47:48,640 --> 00:48:00,950
我们假设数据是来自N个源声音

671
00:48:01,180 --> 00:48:04,390
所以让我们假设

672
00:48:04,810 --> 00:48:06,400
在一个鸡尾酒会中有N个演讲者

673
00:48:06,860 --> 00:48:12,270
所以它的源声音我会写在RN中有一个向量S

674
00:48:13,070 --> 00:48:15,610
所以要具体了解我的意思是什么

675
00:48:16,110 --> 00:48:19,690
我要去使用Sij表示在时间I时

676
00:48:20,340 --> 00:48:31,480
从演讲者J发出的信号

677
00:48:32,620 --> 00:48:34,970
这就是我的意思 那么 声音是什么?

678
00:48:35,640 --> 00:48:38,110
当你听到的声波 声音

679
00:48:38,430 --> 00:48:41,460
通过一种扩展和压缩的模式而创建

680
00:48:41,950 --> 00:48:43,340
所以 你听到我的声音

681
00:48:43,740 --> 00:48:48,890
是因为我的嘴在空气压力中造成一定的变化

682
00:48:49,300 --> 00:48:51,420
那么你的耳朵通过检测气压的变化

683
00:48:51,920 --> 00:48:53,500
就能听到我的声音

684
00:48:53,900 --> 00:48:57,340
那么麦克风记录的 是我的嘴产生的声音

685
00:48:57,910 --> 00:49:00,670
是一个模式 我想 我要画一张图

686
00:49:01,820 --> 00:49:08,220
空气压力的变化 因此 这就是声音

687
00:49:08,580 --> 00:49:09,930
当看到一个麦克风录音

688
00:49:10,310 --> 00:49:12,780
你会看到这些大致的周期信号

689
00:49:13,380 --> 00:49:15,710
随着时间的推移 而产生的气压变化

690
00:49:16,090 --> 00:49:17,070
基于空气压力

691
00:49:17,420 --> 00:49:18,880
而上下浮动

692
00:49:19,570 --> 00:49:21,680
因此 这就是语音信号的样子

693
00:49:22,470 --> 00:49:24,210
因此 这是演讲者1

694
00:49:25,300 --> 00:49:30,280
然后我的意思是--这是一个时间 T

695
00:49:31,210 --> 00:49:33,350
我想说的是 该点的值

696
00:49:34,750 --> 00:49:38,510
我要表示为S 上标T 下标1

697
00:49:40,190 --> 00:49:45,740
同样 演讲者2 它输出一些声波

698
00:49:46,580 --> 00:49:48,180
演讲者的声音将播放它

699
00:49:48,650 --> 00:49:51,600
实际上 它会听起来像一个单频信号我认为

700
00:49:52,330 --> 00:49:55,350
因此 在同样的方式 在同一时间 T

701
00:49:55,910 --> 00:50:00,230
那个值是由两个空气压力值所产生的

702
00:50:00,740 --> 00:50:03,540
我会表示为S t 2

703
00:50:05,950 --> 00:50:39,970
因此 我们观察到:X(i)=A*S(i)

704
00:50:40,690 --> 00:50:44,960
这些X(i)是属于R^N的向量

705
00:50:45,500 --> 00:50:52,370
所以我要假设 我有N个麦克风

706
00:50:52,880 --> 00:50:55,730
并且我的每一个麦克风

707
00:50:56,250 --> 00:50:59,680
记录了演讲者声音的一些线性组合

708
00:51:00,070 --> 00:51:01,040
因此 每个麦克风记录了

709
00:51:01,440 --> 00:51:03,980
一些演讲者声音的重叠组合

710
00:51:04,360 --> 00:51:12,380
例如 Xij 这是--

711
00:51:12,770 --> 00:51:15,520
这是在时间I 麦克风J中记录的声音

712
00:51:15,950 --> 00:51:18,360
所以通过矩阵乘法的定义

713
00:51:18,950 --> 00:51:28,970
这是AIKSJ总和 哦 对不起 OK?

714
00:51:29,590 --> 00:51:34,230
那么 我的J--对不起

715
00:51:34,880 --> 00:51:39,740
那么 我的J麦克风记录的是

716
00:51:40,230 --> 00:51:44,670
所有演讲者的一些线性组合

717
00:51:44,760 --> 00:51:48,880
所以在时间I 麦克风J中 正在录制的是

718
00:51:49,390 --> 00:51:52,080
在时间I 所有演讲者声音的一些线性组合

719
00:51:52,900 --> 00:51:55,760
所以这里的K表示第N个演讲者

720
00:51:56,210 --> 00:52:03,320
因此 我们的目标是找到

721
00:52:03,760 --> 00:52:07,140
矩阵W 等于A的逆矩阵(A^(-1))

722
00:52:07,580 --> 00:52:08,930
并以这种方式定义W

723
00:52:10,000 --> 00:52:20,040
因此 我们可以录制源声音

724
00:52:20,390 --> 00:52:24,810
作为我们麦克风录音XI的线性组合

725
00:52:32,920 --> 00:52:35,040
正如一个符号的点

726
00:52:36,020 --> 00:52:38,180
我要这样写矩阵W

727
00:52:38,770 --> 00:52:53,490
我要使用小写w_12 等等

728
00:52:53,910 --> 00:52:56,610
来表示这个矩阵W的角色

729
00:53:03,220 --> 00:53:05,900
让我们来看看

730
00:53:18,310 --> 00:53:20,640
因此 让我们来看看为什么IC是可能的

731
00:53:21,140 --> 00:53:24,450
鉴于这些重叠的声音 让我们简要地想想

732
00:53:24,800 --> 00:53:29,720
为什么有可能恢复出原始声音

733
00:53:30,220 --> 00:53:32,810
因此 下一个例子 我想说的--比方说

734
00:53:33,270 --> 00:53:46,860
我的演讲者输出--

735
00:53:47,430 --> 00:53:49,200
这听起来像白噪声

736
00:53:49,670 --> 00:53:52,170
请切换到笔记本的显示器 好吗?

737
00:53:53,550 --> 00:53:55,510
对于这个例子 比方说

738
00:53:56,590 --> 00:54:00,250
我的每一个演讲者输出统一的白噪声

739
00:54:00,780 --> 00:54:02,180
因此 如果是这样的话

740
00:54:02,730 --> 00:54:04,840
这些都是我的轴 S1和S2

741
00:54:05,280 --> 00:54:07,610
这是我的两个演讲者发出的声音

742
00:54:08,040 --> 00:54:11,990
他们发声的部分

743
00:54:12,310 --> 00:54:13,640
看起来像一个方盒子中的线

744
00:54:14,000 --> 00:54:14,850
如果两个演讲者

745
00:54:15,240 --> 00:54:18,090
独立均匀输出-1随机变量

746
00:54:18,810 --> 00:54:22,400
因此 这是S1和S2 我的源声音的一部分

747
00:54:23,920 --> 00:54:27,580
这是我的麦克风记录的一个典型样本

748
00:54:27,970 --> 00:54:29,930
在这里 在轴上 是X1和X2

749
00:54:31,390 --> 00:54:35,390
因此这些是我从ICA【听不清】得到的图像

750
00:54:35,990 --> 00:54:41,120
鉴于这样的图片 你可以稍微看看这个盒子

751
00:54:41,490 --> 00:54:45,140
并且可以稍微说说 这个平行四边形的轴

752
00:54:45,870 --> 00:54:49,670
你可以想想 什么样的线性变换

753
00:54:50,160 --> 00:54:52,640
可以把平行四边形变成一个盒子(正方形)

754
00:54:53,030 --> 00:54:57,530
因此 结果在ICA中 有一些固有的含糊之处

755
00:54:57,980 --> 00:55:00,040
我只想说它们是什么

756
00:55:00,440 --> 00:55:04,050
其中之一是 你不能恢复原来的声源索引

757
00:55:04,880 --> 00:55:08,510
特别是 如果我

758
00:55:08,870 --> 00:55:09,960
为演讲者1和演讲者2产生数据

759
00:55:10,530 --> 00:55:11,810
你可以运行ICA

760
00:55:12,250 --> 00:55:13,430
然后你可能

761
00:55:13,780 --> 00:55:14,810
以演讲者的顺序颠倒而告终

762
00:55:15,290 --> 00:55:17,740
相当于 如果你使用这个图片

763
00:55:18,090 --> 00:55:21,120
并且把这张照片进行45度轴翻转

764
00:55:21,710 --> 00:55:24,220
你使用一个45度的轴

765
00:55:24,640 --> 00:55:26,410
并且通过越45度轴反射出的图片

766
00:55:26,930 --> 00:55:27,840
你仍然会得到一个盒子

767
00:55:28,460 --> 00:55:30,260
因此 有没有算法可以

768
00:55:30,620 --> 00:55:31,560
告诉哪个是演讲者1号

769
00:55:31,910 --> 00:55:32,890
哪个是演讲者2号

770
00:55:33,450 --> 00:55:35,690
演讲者的编号方式或顺序是模糊的

771
00:55:38,060 --> 00:55:39,570
其他模糊的源声音

772
00:55:39,990 --> 00:55:41,580
这些都是在这个例子中的含糊之处

773
00:55:42,110 --> 00:55:43,660
是源声音的标志

774
00:55:44,460 --> 00:55:46,590
所以我的发言录音

775
00:55:46,990 --> 00:55:52,450
你不能告诉你是得到了正的SI

776
00:55:52,930 --> 00:55:54,670
还是负的SI

777
00:55:55,310 --> 00:55:58,070
在此图片中 相对应的是

778
00:55:58,610 --> 00:55:59,770
如果你用这张照片

779
00:56:00,300 --> 00:56:02,290
沿纵轴进行投影

780
00:56:02,660 --> 00:56:04,340
如果你沿横轴进行投影

781
00:56:04,340 --> 00:56:05,850
你仍然得到一个盒子

782
00:56:06,560 --> 00:56:07,540
你仍然可以获得【无声】演讲者

783
00:56:07,750 --> 00:56:12,040
因此 在这个例子中

784
00:56:12,990 --> 00:56:15,920
你不能保证恢复的是正的SI

785
00:56:16,560 --> 00:56:17,630
还是负的SI

786
00:56:18,170 --> 00:56:20,670
因此 原来

787
00:56:21,090 --> 00:56:23,400
这是在这个例子中仅有的两个模糊之处

788
00:56:23,930 --> 00:56:25,600
什么是演讲者的置换

789
00:56:25,980 --> 00:56:27,650
而另一种是演讲者的标志

790
00:56:28,340 --> 00:56:29,460
演讲者的置换

791
00:56:29,900 --> 00:56:31,190
没有太多可以做的

792
00:56:31,570 --> 00:56:33,320
事实证明 如果你使用音频信号源

793
00:56:34,010 --> 00:56:37,720
如果翻转标志 和你采用负的S

794
00:56:38,280 --> 00:56:40,010
而且如果你通过麦克风播放它

795
00:56:40,470 --> 00:56:41,470
声音将难以区别

796
00:56:42,110 --> 00:56:47,300
因此 对于许多我们所关心的应用

797
00:56:47,690 --> 00:56:51,030
标志以及置换是模糊的

798
00:56:52,110 --> 00:56:54,600
但你真的不关心它

799
00:56:55,050 --> 00:56:57,020
请让我们切换到黑板

800
00:57:11,370 --> 00:57:14,220
事实证明 我不想花太多时间在这上面

801
00:57:14,810 --> 00:57:15,810
但我想简单地说

802
00:57:16,330 --> 00:57:17,320
真实的原因是

803
00:57:17,670 --> 00:57:20,850
为什么这些是含糊不清的唯一来源--

804
00:57:21,530 --> 00:57:22,530
这里的含糊之处

805
00:57:25,830 --> 00:57:30,270
是演讲者和标志的排列

806
00:57:31,370 --> 00:57:37,950
原来 这些含糊不清的原因

807
00:57:38,260 --> 00:57:42,010
是因为SIJ是非高斯分布的 我

808
00:57:42,570 --> 00:57:49,920
不想花太多时间在这上面 但我会简单地说

809
00:57:50,420 --> 00:57:53,360
假设我的原始来源 S1和S2 是高斯分布的

810
00:57:54,660 --> 00:58:02,440
因此 假设SI是高斯分布

811
00:58:02,890 --> 00:58:05,400
就表示零和特性协方差

812
00:58:07,250 --> 00:58:09,250
这意味着 我的每一个演讲者

813
00:58:10,240 --> 00:58:11,370
输出一个高斯随机变量

814
00:58:12,100 --> 00:58:14,280
这里有一个高斯数据的典型例子

815
00:58:17,410 --> 00:58:21,510
你会想起 特性协变量的高斯分布的轮廓

816
00:58:22,020 --> 00:58:26,200
看起来像这样的 对不对?

817
00:58:26,550 --> 00:58:29,400
高斯是一个对称的球状分布

818
00:58:30,130 --> 00:58:33,130
所以 如果我的演讲者输出高斯随机变量

819
00:58:33,640 --> 00:58:37,120
然后如果我注意到一个这样的线性组合

820
00:58:38,310 --> 00:58:41,530
实际上是没有办法来恢复原有的分布

821
00:58:41,880 --> 00:58:43,230
因为这没法告诉我

822
00:58:43,630 --> 00:58:45,170
是否这个轴是在这个角度

823
00:58:46,000 --> 00:58:48,120
还是他们是在这个角度等等

824
00:58:48,640 --> 00:58:52,560
高斯是一个旋转对称分布

825
00:58:53,090 --> 00:58:54,710
所以我想我不能够

826
00:58:55,090 --> 00:58:57,340
恢复这个旋转的方向

827
00:58:58,250 --> 00:59:00,120
所以我不想证明太多

828
00:59:00,450 --> 00:59:01,980
我不想花太多的时间在这上面

829
00:59:02,240 --> 00:59:04,500
但事实证明 如果你的源是一个高斯分布

830
00:59:04,850 --> 00:59:07,110
那么它实际上是不可能做ICA的

831
00:59:07,530 --> 00:59:11,080
ICA的关键依赖你的数据是非高斯分布的

832
00:59:11,630 --> 00:59:12,730
因为如果数据是高斯分布

833
00:59:13,100 --> 00:59:15,220
那么数据的旋转会产生歧义的

834
00:59:15,640 --> 00:59:18,530
所以 不管你有多少数据

835
00:59:18,950 --> 00:59:22,150
即使你有无穷大量的数据

836
00:59:22,450 --> 00:59:24,890
你都不能够恢复矩阵A或W

837
00:59:32,850 --> 00:59:34,990
让我们继续分解算法

838
00:59:49,380 --> 00:59:59,650
要做到这一点 我需要另外一个结果

839
00:59:59,980 --> 01:00:01,620
然后推导将会是三行

840
01:00:02,210 --> 01:00:04,660
【听不清】为N的许多变量

841
01:00:05,120 --> 01:00:07,950
这是我所有的演讲者

842
01:00:08,330 --> 01:00:09,730
在任何时候发出的声音的联合矢量

843
01:00:11,500 --> 01:00:17,920
因此 让我们说S的密度是P_S 大写S

844
01:00:19,260 --> 01:00:22,610
所以 我的麦克风录音记录的S等于AS

845
01:00:23,380 --> 01:00:28,240
等于W逆S 同样 S等于W的X的标志

846
01:00:29,070 --> 01:00:34,640
因此 让我们思考什么是X的密度

847
01:00:35,420 --> 01:00:37,420
所以我有Ps(s) 我知道S的密度

848
01:00:38,080 --> 01:00:41,030
X是S的线性组合

849
01:00:41,460 --> 01:00:44,230
因此 让我们弄清楚 X的密度是什么

850
01:00:45,260 --> 01:00:49,740
我们可以做的一件事 是计算出S是什么

851
01:00:50,260 --> 01:00:56,670
因此这仅仅是-应用到S的对于W的密度

852
01:00:57,040 --> 01:00:59,650
所以让我们看看 这是S的概率

853
01:01:00,030 --> 01:01:03,980
所以我们只要找出S是什么

854
01:01:04,590 --> 01:01:07,390
S是Wx 所以S的概率为Wx

855
01:01:08,040 --> 01:01:09,760
所以X的概率必须【听不清】

856
01:01:10,320 --> 01:01:11,310
因此 这是错误的

857
01:01:11,730 --> 01:01:14,070
结果你可以对质量函数这样处理

858
01:01:14,410 --> 01:01:15,690
但不能对连续密度这样处理

859
01:01:16,010 --> 01:01:18,880
因此 尤其是这的不正确 说X的概率是-

860
01:01:19,440 --> 01:01:21,860
好的 你只要搞清楚S是什么 

861
01:01:22,650 --> 01:01:25,860
然后你说S的概率是适用于它 这是不对的

862
01:01:26,260 --> 01:01:27,620
你不能对密度这样处理

863
01:01:28,190 --> 01:01:29,640
你不能说 S的概率是哪个

864
01:01:30,060 --> 01:01:33,090
因为它的一个属性密度函数 特别是

865
01:01:33,540 --> 01:01:39,710
正确的公式是S的密度应用到Wx

866
01:01:40,530 --> 01:01:42,800
乘以矩阵的行列式W

867
01:01:44,500 --> 01:01:46,850
让我举一个例子

868
01:01:47,370 --> 01:01:56,090
比方说S的密度是这个

869
01:01:56,570 --> 01:02:04,250
在这个例子中 S在单位时间中是间隔均匀的

870
01:02:05,430 --> 01:02:14,820
所以S的密度看起来是那样的

871
01:02:15,540 --> 01:02:19,010
它只是0 1的均匀分布的密度

872
01:02:20,670 --> 01:02:22,940
因此 让我让X等于两倍的S

873
01:02:24,280 --> 01:02:31,050
因此 这意味着A等于2  W等于二分之一

874
01:02:33,350 --> 01:02:35,910
因此 如果S是01的均匀分布

875
01:02:36,710 --> 01:02:38,490
那么X 这是两倍的它

876
01:02:38,840 --> 01:02:41,780
从零到2以上范围内的均匀分布

877
01:02:42,960 --> 01:02:45,370
因此 X的密度将是--

878
01:02:53,590 --> 01:02:59,570
那是1 这是2 这是二分之一 这是1 OK?

879
01:03:02,510 --> 01:03:09,680
X的密度将指标为零这些为X【无声】

880
01:03:10,500 --> 01:03:14,300
2乘以W 乘以二分之一

881
01:03:15,640 --> 01:03:19,190
理解了么?

882
01:03:19,560 --> 01:03:23,650
【听不清】计算X的密度 因为

883
01:03:23,990 --> 01:03:26,260
X是现在遍布范围更广

884
01:03:26,720 --> 01:03:28,970
现在的X密度较小 因此

885
01:03:29,360 --> 01:03:37,520
X的密度是一个二分之一项 OK?

886
01:03:38,230 --> 01:03:40,360
这是一个图解

887
01:03:40,930 --> 01:03:43,060
说明一维随机变量

888
01:03:43,420 --> 01:03:44,520
或S和X的1D

889
01:03:45,080 --> 01:03:46,770
我不想显示它

890
01:03:47,100 --> 01:03:48,280
但这个的

891
01:03:48,590 --> 01:03:49,520
标准化是

892
01:03:49,840 --> 01:03:51,020
矢量值随机变量X的密度

893
01:03:51,390 --> 01:03:53,530
X此时是矩阵W的行列式

894
01:03:54,070 --> 01:03:55,060
我展示了

895
01:03:55,380 --> 01:03:56,340
一维【无声】的标准化

896
01:04:15,770 --> 01:04:22,990
因此 我们差不多好了

897
01:04:25,040 --> 01:04:30,330
这里是说明我怎么实施ICA的

898
01:04:32,350 --> 01:04:51,540
所以 我的S上的分布 我要去假定

899
01:04:51,940 --> 01:04:53,800
我的S的密度是 作

900
01:04:54,280 --> 01:04:57,300
为N个演讲者的密度的乘积--

901
01:04:57,750 --> 01:05:01,950
发出某些声音的演讲者的乘积

902
01:05:02,290 --> 01:05:03,300
这是一个密度的乘积

903
01:05:03,960 --> 01:05:05,940
这是一个分布的乘积

904
01:05:06,320 --> 01:05:08,410
因为我要假定演讲者

905
01:05:08,770 --> 01:05:09,870
有独立的谈话

906
01:05:10,280 --> 01:05:12,750
因此 SI是独立于I的不同值

907
01:05:15,930 --> 01:05:18,100
所以通过我们刚刚算出的公式

908
01:05:18,480 --> 01:05:36,550
它和X的密度是相等的

909
01:05:36,950 --> 01:05:40,540
我就提醒你 W是A的逆矩阵

910
01:05:42,580 --> 01:05:46,340
这是我先前定义矩阵

911
01:05:47,700 --> 01:05:51,970
使SI等于WI转置X

912
01:05:52,900 --> 01:05:55,330
所以这就是在那里东西

913
01:05:56,780 --> 01:06:01,710
为了使这个模型完成我的构想

914
01:06:02,480 --> 01:06:11,630
我需要做的最后一件事情

915
01:06:11,980 --> 01:06:13,500
我认为是选择每一个演讲者声音的密度

916
01:06:14,250 --> 01:06:18,890
我需要假定一些

917
01:06:19,220 --> 01:06:20,500
单个演讲者发出的声音的密度

918
01:06:20,990 --> 01:06:24,610
所以下面的讨论 我正当地

919
01:06:25,030 --> 01:06:28,510
当【无声】 ICA的 我可以做的一件事情

920
01:06:29,060 --> 01:06:31,480
是我可以选择S的密度

921
01:06:32,150 --> 01:06:34,320
或等价地 我可以选择CDF

922
01:06:34,680 --> 01:06:36,070
S的累积分布函数

923
01:06:37,970 --> 01:06:39,950
在这种情况下 我会选择一个CDF

924
01:06:41,030 --> 01:06:45,050
可能由于历史的原因  可能为方便起见

925
01:06:47,210 --> 01:06:48,720
我需要为S选择CDF

926
01:06:49,540 --> 01:06:51,010
所以 这意味着

927
01:06:51,310 --> 01:06:52,590
我只需要选择一些函数

928
01:06:52,900 --> 01:06:54,060
从零增加到1

929
01:06:54,570 --> 01:06:57,330
我知道我不能选择高斯分布

930
01:06:57,720 --> 01:07:01,430
因为我们知道你不能做高斯数据的ICA

931
01:07:01,980 --> 01:07:04,220
所以 我需要一些函数 从0增加到1

932
01:07:04,930 --> 01:07:05,490
那不是一个

933
01:07:05,900 --> 01:07:07,180
高斯分布的

934
01:07:07,510 --> 01:07:09,650
累积分布函数

935
01:07:10,340 --> 01:07:12,060
那么我知道其他的

936
01:07:12,530 --> 01:07:13,330
从零到增加1的函数吗?

937
01:07:14,260 --> 01:07:19,560
我只是选择的CDF Sigmoid函数

938
01:07:21,850 --> 01:07:25,460
为方便起见

939
01:07:26,020 --> 01:07:28,140
这是一个常用的选择

940
01:07:28,720 --> 01:07:30,540
实际上这里选择Sigmoid函数

941
01:07:30,950 --> 01:07:32,030
没有什么重要原因

942
01:07:32,440 --> 01:07:33,850
这只是一个大家都知道的方便的函数

943
01:07:34,360 --> 01:07:35,860
它恰好

944
01:07:36,240 --> 01:07:37,640
从0增加到1

945
01:07:37,990 --> 01:07:41,380
当你使用Sigmoid的求导函数

946
01:07:46,110 --> 01:07:49,410
并返回你的密度

947
01:07:50,520 --> 01:07:51,940
这不是高斯分布

948
01:07:52,590 --> 01:07:54,200
这是主要凭借选择sigmoid函数

949
01:08:18,680 --> 01:08:21,350
所以真的没有理性的选择Σ的

950
01:08:21,840 --> 01:08:23,440
很多其他的东西将运行良好

951
01:08:23,910 --> 01:08:25,360
这只是一个共同的 合理的默认值

952
01:08:38,080 --> 01:08:41,860
原来 Sigma能够处理好大量的数据源的

953
01:08:42,360 --> 01:08:45,390
原因之一是 是否这是高斯分布

954
01:08:49,130 --> 01:08:51,600
如果你确实采取sigmoid和及其衍生

955
01:08:52,330 --> 01:09:03,820
你会发现 sigmoid

956
01:09:04,160 --> 01:09:05,480
比高斯的尾部延伸得更远

957
01:09:05,840 --> 01:09:07,740
我的意思是sigmoid密度逐渐

958
01:09:08,170 --> 01:09:10,880
下降到零的速度要比高斯慢得多

959
01:09:12,020 --> 01:09:14,480
这尾部的量级

960
01:09:14,900 --> 01:09:17,640
从E下降到S的平方

961
01:09:18,180 --> 01:09:21,160
对于sigmoid 看起来从E到-S 尾部的量级

962
01:09:21,630 --> 01:09:24,850
从E下降到-S sigmoid

963
01:09:25,260 --> 01:09:26,750
在E周围到负S的平方

964
01:09:27,130 --> 01:09:27,980
原来

965
01:09:28,390 --> 01:09:31,440
这有着【听不清】尾巴的大部分属性分配

966
01:09:31,800 --> 01:09:34,040
缓慢下降到0的分配

967
01:09:34,510 --> 01:09:36,510
在你的数据的处理上 比高斯运行地更好

968
01:09:39,920 --> 01:09:42,490
其实 你有时可以使用的另外一个选择是

969
01:09:42,810 --> 01:09:46,240
那就是所谓的拉普拉斯分布

970
01:09:46,950 --> 01:09:48,780
它也将很好地处理许多数据源的情况

971
01:10:06,090 --> 01:10:07,730
现在坚持使用sigmoid

972
01:10:09,350 --> 01:10:11,320
我用两个步骤写下这个算法

973
01:10:11,700 --> 01:10:17,750
给出我的训练集 并给你们看

974
01:10:18,100 --> 01:10:19,590
这是一个未标记的训练集

975
01:10:20,940 --> 01:10:23,120
我可以写下我的参数记录相似性

976
01:10:24,010 --> 01:10:24,560
所以

977
01:10:25,100 --> 01:10:42,230
这-组合了我的训练例子 日志乘以它

978
01:10:42,980 --> 01:10:44,320
所以 我的记录相似性

979
01:10:53,280 --> 01:10:56,450
要了解那些参数 此模型中的W

980
01:10:56,790 --> 01:11:06,980
我可以使用【听不清】同意 就是这一点

981
01:11:07,910 --> 01:11:11,750
事实证明 如果你做完数学 让我们来看看

982
01:11:12,180 --> 01:11:16,280
如果S的P等于sigmoid的衍生

983
01:11:19,380 --> 01:11:21,040
然后 如果你只是

984
01:11:21,650 --> 01:11:22,860
通过数学来计算【听不清】

985
01:11:24,170 --> 01:11:25,610
你已经做了很多次了

986
01:11:26,080 --> 01:11:27,540
我不会去显示它的细节

987
01:11:28,050 --> 01:11:29,720
你会发现 这是相等的

988
01:11:42,820 --> 01:11:49,220
OK?这只是 - 你可以自己算出这些

989
01:11:49,960 --> 01:11:52,150
这只是数学计算关于W的衍生

990
01:11:53,320 --> 01:11:57,430
因此 简而言之 给定的训练集

991
01:11:57,870 --> 01:12:01,090
在这里是我的【无声】更新规则

992
01:12:01,520 --> 01:12:05,200
所以你可以运行【听不清】来学习参数W

993
01:12:06,430 --> 01:12:14,620
大功告成后 你再输出SI等于WXI

994
01:12:15,050 --> 01:12:18,520
并且你已经分开了你的数据源

995
01:12:18,880 --> 01:12:20,730
返回出原来的独立的源声音

996
01:12:21,250 --> 01:12:24,420
希望只有一个置换

997
01:12:24,950 --> 01:12:28,900
和一个+/-号 OK?

998
01:12:29,760 --> 01:12:35,250
因此 切换到笔记本上 好吗?所以我们

999
01:12:35,600 --> 01:12:38,370
就换了几个ICA的应用的例子

1000
01:12:42,150 --> 01:12:45,140
其实 这是我们的助教 Katie的一张照片

1001
01:12:45,860 --> 01:12:48,990
因此 ICA的应用程序之一

1002
01:12:49,290 --> 01:12:53,550
是处理各种【听不清】记录数据

1003
01:12:54,060 --> 01:12:54,910
因此【听不清】

1004
01:12:55,250 --> 01:12:59,740
这是一个EEG(脑电图)的图片

1005
01:13:00,160 --> 01:13:02,600
其中有一些电极的地方--

1006
01:13:02,820 --> 01:13:07,570
在这种情况下  在Katie的大脑上 在Katie的头皮上

1007
01:13:08,310 --> 01:13:10,400
因此 每个电极测量

1008
01:13:10,480 --> 01:13:14,450
随时间变化的电压

1009
01:13:15,110 --> 01:13:18,440
在右边 这是一个典型的【无声】数据的例子

1010
01:13:18,770 --> 01:13:20,180
其中每个电极测量

1011
01:13:20,560 --> 01:13:21,900
随时间变化的电压

1012
01:13:22,280 --> 01:13:24,110
因此 横轴是时间

1013
01:13:24,110 --> 01:13:25,130
纵轴是电压

1014
01:13:26,500 --> 01:13:28,740
因此 这里是同样的事情 一点点加剧

1015
01:13:29,580 --> 01:13:31,310
您会注意到 数据中有产出

1016
01:13:31,610 --> 01:13:34,770
周期在哪里 数据在哪里盘旋

1017
01:13:35,060 --> 01:13:37,940
所有的电极似乎

1018
01:13:38,330 --> 01:13:40,130
测量着这些非常同步的录音

1019
01:13:40,950 --> 01:13:43,050
事实证明 我们看一下【听不清】数据

1020
01:13:43,590 --> 01:13:45,260
以及一些其他类型的数据

1021
01:13:45,610 --> 01:13:47,930
心跳的数据

1022
01:13:48,380 --> 01:13:49,740
人的眼睛闪烁的数据 等等

1023
01:13:50,590 --> 01:13:55,900
所以漫画家 如果你能想象 在我的头皮上

1024
01:13:56,330 --> 01:13:59,350
放置电极或麦克风 然后每个麦克风

1025
01:13:59,640 --> 01:14:02,140
录制一些所有事情

1026
01:14:02,450 --> 01:14:03,660
发生在我的脑子 或在我的身上的重叠组合

1027
01:14:04,160 --> 01:14:06,970
我的大脑存在很多不同的进程处理

1028
01:14:07,380 --> 01:14:08,780
我的身体【听不清】处理

1029
01:14:09,400 --> 01:14:10,740
每个电极测量

1030
01:14:11,590 --> 01:14:14,640
在我的脑子中不同声音的总和

1031
01:14:15,270 --> 01:14:17,630
这不太像我想要的方式

1032
01:14:18,210 --> 01:14:22,770
因此 我们可以借此数据

1033
01:14:23,150 --> 01:14:24,860
并且运行ICA 并找出一个独立组件

1034
01:14:25,190 --> 01:14:27,350
在我的脑子的独立进程是什么

1035
01:14:27,670 --> 01:14:29,610
这是一个运行ICA的例子

1036
01:14:30,850 --> 01:14:32,620
所以你发现少数的组件

1037
01:14:32,940 --> 01:14:34,720
类似那里出现的 它们相当于心跳

1038
01:14:35,090 --> 01:14:37,980
这里的箭头--所以这些都是周期信号

1039
01:14:38,370 --> 01:14:39,590
他们偶尔

1040
01:14:39,860 --> 01:14:42,480
对应【听不清】心跳的组件

1041
01:14:42,870 --> 01:14:45,950
你还可以找到像眨眼组成的东西

1042
01:14:46,370 --> 01:14:48,150
相应的sigmoid所产生的

1043
01:14:48,550 --> 01:14:49,320
当你眨眼的时候

1044
01:14:50,170 --> 01:14:52,850
通过这样做 可以从数据中

1045
01:14:53,250 --> 01:14:54,840
减去心跳和眨眼的数据

1046
01:14:55,280 --> 01:14:58,510
现在你得到更清晰的ICA的数据--

1047
01:14:58,850 --> 01:15:00,420
获得更清晰的脑电图(EEG)读数

1048
01:15:00,820 --> 01:15:02,780
你可以做进一步的科学研究

1049
01:15:03,320 --> 01:15:06,090
因此 这是一个相当常用的预处理步骤

1050
01:15:06,640 --> 01:15:08,840
这是一个ICA的通用应用程序

1051
01:15:10,100 --> 01:15:15,380
【无声】例子是应用程序 再从【听不清】

1052
01:15:16,110 --> 01:15:20,970
作为运行在自然小图像补丁上的ICA的结果

1053
01:15:21,380 --> 01:15:23,690
假设我采取自然图像 并且运行ICA的数据

1054
01:15:24,080 --> 01:15:25,730
然后询问数据的独立组件是什么

1055
01:15:26,510 --> 01:15:28,290
原来 这些都是你的主成分

1056
01:15:28,820 --> 01:15:31,730
因此 这是你获得源声音的一个情节

1057
01:15:33,000 --> 01:15:35,630
该算法是说 那是一个自然的图像碎片

1058
01:15:36,530 --> 01:15:42,320
显示在左边

1059
01:15:42,810 --> 01:15:44,060
作为一个总和或者线性组合

1060
01:15:44,630 --> 01:15:47,600
独立的东西组成图像

1061
01:15:48,400 --> 01:15:51,490
因此 这种模型的自然图像

1062
01:15:51,890 --> 01:15:53,150
生成图像中的

1063
01:15:53,570 --> 01:15:54,820
不同年龄的独立的对象

1064
01:15:55,500 --> 01:15:58,530
关于这个迷人的事情之一是

1065
01:15:58,850 --> 01:15:59,670
类似的神经系统科学

1066
01:16:00,050 --> 01:16:02,140
这也被视为人类的大脑

1067
01:16:02,550 --> 01:16:04,880
如何处理图像数据的方法的推测

1068
01:16:05,340 --> 01:16:08,860
原来 在许多方面 这是类似的

1069
01:16:09,270 --> 01:16:13,510
发生在早在人类大脑

1070
01:16:13,960 --> 01:16:15,940
或者哺乳动物的大脑中视觉处理的计算

1071
01:16:16,800 --> 01:16:21,260
这是相当有趣

1072
01:16:21,560 --> 01:16:23,940
来看年龄是图像的独立组件

1073
01:16:24,560 --> 01:16:27,300
是否有简单的问题 因为我有点拖堂了

1074
01:16:27,810 --> 01:16:29,910
在我关闭之前 有没有快捷的提问?

1075
01:16:30,250 --> 01:16:31,580
受访者:【无声】方阵?

1076
01:16:31,660 --> 01:16:32,460
教导员(安德鲁吴):噢 是的

1077
01:16:32,500 --> 01:16:33,660
对于我所描述的算法

1078
01:16:34,060 --> 01:16:35,010
假设A是一个方阵

1079
01:16:35,580 --> 01:16:37,460
事实证明  如果你有比演讲者更多的话筒

1080
01:16:37,840 --> 01:16:39,340
也可以适用于非常相似的算法

1081
01:16:39,790 --> 01:16:41,680
如果你有比演讲者少的麦克风

1082
01:16:42,030 --> 01:16:43,530
这有几分像开放搜索问题

1083
01:16:43,890 --> 01:16:45,520
很可能 如果你有一个男的

1084
01:16:45,850 --> 01:16:47,770
和一个女的演讲者 但只有一个麦克风

1085
01:16:48,140 --> 01:16:49,590
你有时可以把它们分开

1086
01:16:49,920 --> 01:16:50,860
因为一个是高的 一个是低的

1087
01:16:51,190 --> 01:16:53,240
如果你有两男演讲者或两个女的演讲者

1088
01:16:53,710 --> 01:16:55,810
那么要用一个麦克风来分开它们

1089
01:16:56,250 --> 01:16:57,860
是超出目前工艺水平的

1090
01:16:58,200 --> 01:16:59,400
这是一个伟大的研究问题

1091
01:17:00,150 --> 01:17:01,820
好了 我对此深感抱歉 我拖堂了

1092
01:17:02,180 --> 01:17:02,970
让我们现在关闭它

1093
01:17:03,480 --> 01:17:05,180
我们将继续

1094
01:17:05,260 --> 01:17:06,520
强化学习下面的【无声】

