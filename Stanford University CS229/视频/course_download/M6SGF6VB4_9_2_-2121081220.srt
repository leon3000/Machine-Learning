1
00:00:22,650 --> 00:00:30,030
Okay. So welcome back.

2
00:00:30,180 --> 00:00:34,750
What I want to do today is start a new chapter

3
00:00:34,890 --> 00:00:37,470
in between now and then.

4
00:00:37,580 --> 00:00:39,120
In particular, I want to talk about learning theory.

5
00:00:39,240 --> 00:00:44,240
So in the previous, I guess eight lectures so far,

6
00:00:44,370 --> 00:00:46,830
You've learned about a lot of learning algorithms,

7
00:00:46,920 --> 00:00:50,930
and yes, you now I hope understand a little about some of

8
00:00:51,010 --> 00:00:53,420
the best and most powerful tools of machine learning in the.

9
00:00:53,530 --> 00:00:57,340
And all of you are now sort of well qualified to go into

10
00:00:57,800 --> 00:01:00,310
industry and though powerful learning algorithms apply,

11
00:01:00,420 --> 00:01:02,560
really the most powerful learning algorithms

12
00:01:02,640 --> 00:01:03,810
we know to all sorts of problems,

13
00:01:03,930 --> 00:01:06,080
and in fact, I hope you start to do

14
00:01:06,170 --> 00:01:07,740
that on your projects right away as well.

15
00:01:07,830 --> 00:01:12,160
You might remember,

16
00:01:12,270 --> 00:01:14,280
I think it was in the very first lecture,

17
00:01:14,370 --> 00:01:16,240
that I made an analogy to

18
00:01:16,340 --> 00:01:18,640
if you're trying to learn to be a carpenter,

19
00:01:18,740 --> 00:01:19,820
so if you imagine

20
00:01:19,930 --> 00:01:22,760
you're going to carpentry school to learn to be a carpenter,

21
00:01:22,860 --> 00:01:26,060
then only a small part of what you need to do

22
00:01:26,170 --> 00:01:28,000
is to acquire a set of tools.

23
00:01:28,110 --> 00:01:29,500
If you learn to be a carpenter

24
00:01:29,600 --> 00:01:31,010
you don't walk in

25
00:01:31,090 --> 00:01:32,320
and pick up a tool box and

26
00:01:32,420 --> 00:01:37,470
so when you need to cut a piece of wood

27
00:01:37,560 --> 00:01:38,540
do you use a rip saw,

28
00:01:38,620 --> 00:01:40,040
or a jig saw, or a keyhole saw whatever,

29
00:01:40,160 --> 00:01:41,760
is this really mastering the tools

30
00:01:41,880 --> 00:01:44,740
there's also an essential part of becoming a good carpenter.

31
00:01:44,840 --> 00:01:48,510
And what I want to do in the next few lectures

32
00:01:48,650 --> 00:01:51,220
is actually give you a sense of the mastery

33
00:01:51,330 --> 00:01:53,310
of the machine learning tools all of you have.

34
00:01:53,430 --> 00:01:55,020
Okay?

35
00:01:55,120 --> 00:01:58,190
And so in particular, in the next few lectures

36
00:01:58,300 --> 00:02:01,560
what I want to is to talk more deeply about

37
00:02:01,670 --> 00:02:03,790
the properties of different machine learning algorithms

38
00:02:03,990 --> 00:02:05,200
so that you can get a sense of

39
00:02:05,280 --> 00:02:07,020
when it's most appropriate to use each one.

40
00:02:07,160 --> 00:02:09,150
And it turns out that

41
00:02:09,260 --> 00:02:11,540
one of the most common scenarios in machine learning

42
00:02:11,660 --> 00:02:15,940
is someday you'll be doing research or a company.

43
00:02:16,050 --> 00:02:17,630
And you'll apply

44
00:02:17,730 --> 00:02:19,280
one of the learning algorithms you learned about,

45
00:02:19,360 --> 00:02:20,660
you may apply logistic regression,

46
00:02:20,770 --> 00:02:21,870
or support vector machines,

47
00:02:21,940 --> 00:02:22,840
or Na.ve Bayes or something,

48
00:02:22,940 --> 00:02:25,280
and for whatever bizarre reason,

49
00:02:25,390 --> 00:02:27,620
it won't work as well as you were hoping

50
00:02:27,700 --> 00:02:29,650
or it won't quite do what you were hoping it to.

51
00:02:29,770 --> 00:02:36,250
To me what really separates the people

52
00:02:36,360 --> 00:02:38,800
from what really separates the people

53
00:02:38,940 --> 00:02:41,020
that really understand and really get machine learning,

54
00:02:41,130 --> 00:02:44,160
compared to people that maybe read the textbook

55
00:02:44,270 --> 00:02:46,270
and so they'll work through the math,

56
00:02:46,380 --> 00:02:47,700
will be what you do next.

57
00:02:47,800 --> 00:02:49,290
Will be in your decisions of

58
00:02:49,370 --> 00:02:51,410
when you apply a support vector machine

59
00:02:51,520 --> 00:02:53,110
and it doesn't quite do what you wanted,

60
00:02:54,960 --> 00:02:55,510
do you really understand enough

61
00:02:55,630 --> 00:02:57,140
about support vector machines to know

62
00:02:57,260 --> 00:02:58,870
what to do nextand how to modify the algorithm?

63
00:02:58,990 --> 00:03:00,310
And to me that's often

64
00:03:00,430 --> 00:03:02,080
what really separates the great people

65
00:03:02,190 --> 00:03:03,480
in machine learning versus the people

66
00:03:03,590 --> 00:03:05,460
that like read the text book and so they'll the math,

67
00:03:05,570 --> 00:03:07,030
and so they'll have just understood that.

68
00:03:07,170 --> 00:03:08,480
Okay?

69
00:03:08,620 --> 00:03:10,830
So what I want to do today

70
00:03:10,940 --> 00:03:14,370
is lecture will mainly be on learning theory

71
00:03:14,480 --> 00:03:16,880
and we'll start to talk about some of the theoretical results

72
00:03:16,980 --> 00:03:17,870
of machine learning.

73
00:03:17,980 --> 00:03:20,970
The next lecture, later this week

74
00:03:21,070 --> 00:03:24,070
will be on algorithms for sort of,

75
00:03:24,200 --> 00:03:27,550
or fixing some of the problems that the learning theory

76
00:03:27,670 --> 00:03:29,020
will point out to us and help us understand.

77
00:03:29,120 --> 00:03:31,500
And then two lectures from now,

78
00:03:31,800 --> 00:03:35,090
that lecture will be almost entirely focused on the practical

79
00:03:35,230 --> 00:03:38,800
advice for how to apply learning algorithms. Okay?

80
00:03:38,910 --> 00:03:42,160
So you have any questions about this before I start?

81
00:03:42,260 --> 00:03:49,730
Okay.

82
00:03:49,840 --> 00:03:53,540
So the very first thing we're gonna talk about is

83
00:03:53,660 --> 00:03:56,560
something that you've probably already seen

84
00:03:56,670 --> 00:03:57,560
on the first homework,

85
00:03:58,010 --> 00:03:59,740
and something that alluded to previously,

86
00:03:59,850 --> 00:04:03,380
which is the bias variance trade-off.

87
00:04:03,510 --> 00:04:08,020
So take ordinary least squares,

88
00:04:08,130 --> 00:04:09,760
the first learning algorithm we learned about,

89
00:04:09,920 --> 00:04:13,120
if you  a straight line through these datas,

90
00:04:13,220 --> 00:04:14,720
this is not a very good model.

91
00:04:14,810 --> 00:04:21,990
Right. And if this happens,

92
00:04:22,130 --> 00:04:25,100
we say it has underfit the data,

93
00:04:25,490 --> 00:04:26,530
or we say that

94
00:04:26,630 --> 00:04:28,450
this is a learning algorithm with a very high bias,

95
00:04:28,580 --> 00:04:30,410
because it is failing

96
00:04:30,540 --> 00:04:33,170
to fit the evident quadratic structure in the data.

97
00:04:33,290 --> 00:04:41,680
And for the prefaces of you can formally think of the bias

98
00:04:41,790 --> 00:04:45,100
of the learning algorithm as representing the fact that

99
00:04:45,210 --> 00:04:47,450
even if you had an infinite amount of training data,

100
00:04:47,590 --> 00:04:49,410
even if you had tons of training data,

101
00:04:49,520 --> 00:04:53,850
this algorithm would still fail to fit the quadratic function

102
00:04:53,960 --> 00:04:56,050
the quadratic structure in the data.

103
00:04:56,170 --> 00:04:57,100
And so we think of

104
00:04:57,210 --> 00:04:59,210
this as a learning algorithm with high bias.

105
00:04:59,330 --> 00:05:02,060
Then

106
00:05:02,180 --> 00:05:03,290
there's the opposite problem,

107
00:05:03,400 --> 00:05:06,450
so that's the same dataset.

108
00:05:06,560 --> 00:05:16,120
If you fit a fourth of the polynomials into this dataset,

109
00:05:16,230 --> 00:05:20,300
then you have

110
00:05:20,410 --> 00:05:21,370
you'll be able to

111
00:05:21,480 --> 00:05:23,140
interpolate the five data points exactly,

112
00:05:23,250 --> 00:05:24,420
but clearly,

113
00:05:24,540 --> 00:05:28,280
this is also not a great model to the structure that you

114
00:05:28,380 --> 00:05:29,860
and I probably see in the data.

115
00:05:29,980 --> 00:05:35,430
And we say that this algorithm has a problem

116
00:05:35,550 --> 00:05:38,690
excuse me, is overfitting the data

117
00:05:38,800 --> 00:05:45,990
or alternatively that this algorithm has high variance.

118
00:05:46,100 --> 00:05:46,820
Okay?

119
00:05:46,940 --> 00:05:49,870
And the intuition behind overfitting a high variance is that

120
00:05:49,980 --> 00:05:53,590
the algorithm is fitting serious patterns in the data,

121
00:05:53,710 --> 00:05:57,020
or is fitting idiosyncratic properties of this specific dataset,

122
00:05:57,130 --> 00:05:59,090
be it the dataset of housing prices

123
00:05:59,180 --> 00:06:01,360
or whatever.

124
00:06:01,470 --> 00:06:02,170
And quite often,

125
00:06:02,280 --> 00:06:03,280
They'll be some happy medium

126
00:06:03,400 --> 00:06:09,890
of fitting a quadratic function

127
00:06:10,010 --> 00:06:11,560
that maybe won't interpolate

128
00:06:11,640 --> 00:06:12,760
your data points perfectly

129
00:06:12,860 --> 00:06:15,250
but also captures multi-structure

130
00:06:15,370 --> 00:06:17,350
in your data than a simple

131
00:06:17,460 --> 00:06:19,360
model which under fits.

132
00:06:19,470 --> 00:06:23,390
I say that you can sort of have the exactly

133
00:06:23,500 --> 00:06:25,210
the same picture of classification problems as well,

134
00:06:25,360 --> 00:06:35,970
so lets say this is my training set, right,

135
00:06:36,120 --> 00:06:38,730
of positive and negative examples,

136
00:06:38,890 --> 00:06:47,480
and so you can fit logistic regression

137
00:06:47,600 --> 00:06:49,170
with a very high order polynomial [inaudible],

138
00:06:49,330 --> 00:07:02,000
or [inaudible] of X equals the sigmoid function of whatever.

139
00:07:02,130 --> 00:07:03,190
Sigmoid function applied to

140
00:07:03,290 --> 00:07:04,220
a tenth of the polynomial.

141
00:07:04,370 --> 00:07:05,210
And you do that,

142
00:07:05,340 --> 00:07:13,860
maybe you get a decision boundary like this. Right.

143
00:07:14,000 --> 00:07:15,520
That does indeed perfectly separate the positive

144
00:07:15,640 --> 00:07:17,750
and negative classes, this is another example of

145
00:07:17,850 --> 00:07:23,610
how overfitting, and in contrast you fit logistic regression

146
00:07:23,720 --> 00:07:25,320
into this model with just the linear features,

147
00:07:25,440 --> 00:07:26,440
with none of the quadratic features,

148
00:07:26,570 --> 00:07:28,520
then maybe you get a decision boundary like that,

149
00:07:29,440 --> 00:07:31,610
which can also underfit. Okay.

150
00:07:31,700 --> 00:07:35,770
So what I want to do now is

151
00:07:35,880 --> 00:07:38,870
understand this problem

152
00:07:38,980 --> 00:07:41,100
of overfitting versus underfitting,

153
00:07:41,240 --> 00:07:42,880
of high bias versus high variance,

154
00:07:42,970 --> 00:07:47,230
more explicitly, I will do that by posing a more formal

155
00:07:47,340 --> 00:07:49,450
model of machine learning and so trying to prove

156
00:07:49,590 --> 00:07:52,820
when these two twin problems

157
00:07:52,940 --> 00:07:56,210
when each of these two problems come up.

158
00:07:56,470 --> 00:08:03,550
And as I'm modeling the example for

159
00:08:03,660 --> 00:08:04,420
our initial foray into learning theory,

160
00:08:04,550 --> 00:08:06,870
I want to talk about learning classification,

161
00:08:06,980 --> 00:08:24,570
in which H of X is equal to G of data transpose X. Okay?

162
00:08:24,650 --> 00:08:26,030
So the learning classifier.

163
00:08:26,110 --> 00:08:27,930
And for this class I'm going to use, Z

164
00:08:28,040 --> 00:08:29,440
excuse me

165
00:08:29,520 --> 00:08:30,740
I'm gonna use G as indicator Z grading with zero.

166
00:08:30,850 --> 00:08:41,190
With apologies

167
00:08:41,310 --> 00:08:43,800
in advance for changing the notation yet again,

168
00:08:43,900 --> 00:08:46,400
for the support vector machine lectures

169
00:08:46,500 --> 00:08:48,020
we use Y equals minus one or plus one.

170
00:08:48,110 --> 00:08:50,310
For learning theory lectures,

171
00:08:50,420 --> 00:08:52,750
turns out it'll be a bit cleaner if I switch back to

172
00:08:52,990 --> 00:08:54,300
Y equals zero-one again,

173
00:08:54,430 --> 00:08:56,230
so I'm gonna switch back to my original notation.

174
00:08:56,350 --> 00:09:03,620
And so you think of this model as a model forum as logistic

175
00:09:03,690 --> 00:09:05,180
regressions, say, and think of this

176
00:09:05,260 --> 00:09:06,560
as being similar to logistic regression,

177
00:09:06,660 --> 00:09:07,660
except that now we're going to

178
00:09:07,740 --> 00:09:09,910
force the logistic regression algorithm,

179
00:09:10,020 --> 00:09:14,170
to opt for labels that are either zero or one.

180
00:09:14,280 --> 00:09:14,980
Okay?

181
00:09:15,060 --> 00:09:16,920
So you can think of this as a classifier

182
00:09:17,030 --> 00:09:18,960
to opt for labels zero or one involved in the probabilities.

183
00:09:19,080 --> 00:09:25,800
And so as usual

184
00:09:25,890 --> 00:09:28,480
Let's say we're given a training set of M examples.

185
00:09:35,430 --> 00:09:35,930
That's just my notation

186
00:09:36,040 --> 00:09:38,460
for writing a set of M examples

187
00:09:38,550 --> 00:09:39,920
ranging from I equals one through M.

188
00:09:40,030 --> 00:09:42,990
And I'm going to assume that

189
00:09:43,100 --> 00:09:45,650
the training example is XIYI.

190
00:09:45,760 --> 00:09:47,590
I've drawn IID,

191
00:09:47,720 --> 00:09:50,880
from sum distribution, scripts D.

192
00:09:51,000 --> 00:09:51,760
Okay? [Inaudible].

193
00:09:51,830 --> 00:09:53,740
Identically and definitively distributed

194
00:09:53,830 --> 00:09:57,390
and if you have you have running

195
00:09:57,500 --> 00:09:58,390
a classification problem on houses,

196
00:09:58,500 --> 00:10:01,250
like features of the house comma,

197
00:10:01,360 --> 00:10:03,470
whether the house will be sold in the next six months,

198
00:10:03,560 --> 00:10:06,830
then this is just the priority distribution over features of

199
00:10:06,940 --> 00:10:10,190
houses and whether or not they'll be sold.

200
00:10:10,350 --> 00:10:10,940
Okay?

201
00:10:11,050 --> 00:10:12,330
So I'm gonna assume that

202
00:10:12,410 --> 00:10:13,750
training examples we've drawn IID

203
00:10:13,860 --> 00:10:16,650
from some probability distributions,

204
00:10:16,760 --> 00:10:19,410
scripts D. Well, same thing for spam,

205
00:10:19,520 --> 00:10:21,730
if you're trying to build a spam classifier then this would be

206
00:10:21,840 --> 00:10:25,560
the distribution of what emails look like comma, whether

207
00:10:25,670 --> 00:10:30,590
they are spam or not. And in particular,

208
00:10:30,720 --> 00:10:35,060
to understand or simplify to understand the phenomena of

209
00:10:35,160 --> 00:10:38,170
bias invariance, I'm actually going to use a simplified

210
00:10:38,280 --> 00:10:39,760
model of machine learning.

211
00:10:39,900 --> 00:10:46,630
And in particular, logistic regression fits this parameters

212
00:10:46,710 --> 00:10:49,390
the model like this for maximizing the law of likelihood.

213
00:10:49,460 --> 00:10:53,900
But in order to understand learning algorithms more deeply,

214
00:10:54,010 --> 00:10:54,790
I'm just going to assume

215
00:10:54,870 --> 00:10:56,170
a simplified model of machine learning,

216
00:10:56,280 --> 00:10:57,880
let me just write that down.

217
00:10:57,960 --> 00:11:04,220
So I'm going to define training error

218
00:11:04,330 --> 00:11:10,850
as so this is a training error of

219
00:11:10,850 --> 00:11:14,110
a hypothesis X subscript  data.

220
00:11:14,220 --> 00:11:17,170
Write this epsilon hat of subscript data.

221
00:11:17,280 --> 00:11:20,680
If I want to make the dependence on

222
00:11:20,790 --> 00:11:21,750
a training set explicit,

223
00:11:21,860 --> 00:11:24,840
I'll write this with a subscript S there where S is a training set.

224
00:11:24,970 --> 00:11:26,260
And I'll define this as,

225
00:11:26,380 --> 00:11:42,810
I hope the notation is clear.

226
00:11:42,900 --> 00:11:45,720
This is a sum of indicator functions

227
00:11:45,830 --> 00:11:49,440
for whether your hypothesis correctly classifies the Y the

228
00:11:49,550 --> 00:11:53,900
IFE example. And so when you divide by M,

229
00:11:54,010 --> 00:11:56,800
this is just in your training set what's the fraction of

230
00:11:56,910 --> 00:12:00,380
training examples your hypothesis classifies

231
00:12:00,490 --> 00:12:02,630
so defined as a training error.

232
00:12:02,740 --> 00:12:06,810
And training error is also called risk.

233
00:12:06,970 --> 00:12:12,960
The simplified model of machine learning I'm gonna talk

234
00:12:13,100 --> 00:12:15,880
about is called empirical risk minimization.

235
00:12:16,030 --> 00:12:19,690
And in particular, I'm going to assume that

236
00:12:19,840 --> 00:12:22,750
the way my learning algorithm works is it will choose

237
00:12:22,900 --> 00:12:36,240
parameters data, that minimize my training error.  Okay?

238
00:12:36,320 --> 00:12:40,780
And it will be this learning algorithm

239
00:12:40,870 --> 00:12:42,980
that we'll prove properties about.

240
00:12:43,060 --> 00:12:46,570
And it turns out that you can think of

241
00:12:46,650 --> 00:12:48,850
this as the most basic learning algorithm,

242
00:12:48,960 --> 00:12:50,840
the algorithm that minimizes your training error.

243
00:12:50,930 --> 00:12:53,890
It turns out that logistic regression and support vector

244
00:12:53,960 --> 00:12:57,590
machines can be formally viewed as approximation cities,

245
00:12:57,710 --> 00:13:00,590
so it turns out that if you actually want to do this,

246
00:13:00,700 --> 00:13:03,090
this is a nonconvex optimization problem.

247
00:13:03,220 --> 00:13:04,920
This is actually it actually [inaudible]

248
00:13:05,050 --> 00:13:06,420
hard to solve this optimization problem.

249
00:13:06,540 --> 00:13:11,480
And logistic regression and support vector machines

250
00:13:11,600 --> 00:13:14,900
can both be viewed as approximations to

251
00:13:15,010 --> 00:13:16,610
this nonconvex optimization problem

252
00:13:16,730 --> 00:13:19,370
by finding the convex approximation to it.

253
00:13:19,500 --> 00:13:23,070
Think of this as similar to what algorithms

254
00:13:23,180 --> 00:13:25,270
like logistic regression are doing.

255
00:13:25,390 --> 00:13:33,600
So let me take that definition of empirical risk minimization

256
00:13:33,710 --> 00:13:38,600
and actually just rewrite it in a different equivalent way.

257
00:13:38,710 --> 00:13:43,780
For the results I want to prove today,

258
00:13:43,860 --> 00:13:48,320
it turns out that it will be useful to think of our learning

259
00:13:48,440 --> 00:13:51,420
algorithm as not choosing a set of parameters,

260
00:13:51,560 --> 00:13:53,910
but as choosing a function.

261
00:13:54,020 --> 00:13:58,100
So let me say what I mean by that.

262
00:13:58,210 --> 00:14:03,980
Let me define the hypothesis class, script h,

263
00:14:04,100 --> 00:14:06,610
as the class of all hypotheses of

264
00:14:06,760 --> 00:14:08,110
in other words

265
00:14:08,220 --> 00:14:09,450
as the class of

266
00:14:09,560 --> 00:14:12,260
all linear classifiers,

267
00:14:12,370 --> 00:14:20,280
that your learning algorithm  is choosing from.

268
00:14:20,390 --> 00:14:24,070
Okay? So H subscript data

269
00:14:24,180 --> 00:14:30,820
is a specific linear classifier,

270
00:14:30,930 --> 00:14:32,690
so H subscript data

271
00:14:32,810 --> 00:14:37,510
in each of these functions each of these is a function

272
00:14:37,610 --> 00:14:42,240
mapping from the input domain X

273
00:14:42,300 --> 00:14:43,350
is the class zero-one.

274
00:14:43,450 --> 00:14:44,170
Each of these is a function,

275
00:14:44,260 --> 00:14:45,450
and as you vary the parameter's data,

276
00:14:45,550 --> 00:14:47,110
you get different functions.

277
00:14:47,200 --> 00:14:48,570
And so let me define the hypothesis class script H

278
00:14:48,680 --> 00:14:50,690
to be the class of all functions

279
00:14:50,780 --> 00:14:53,120
that say logistic regression can choose from.

280
00:14:53,200 --> 00:14:54,440
Okay.

281
00:14:54,570 --> 00:14:58,690
So this is the class of all linear classifiers

282
00:14:58,820 --> 00:15:01,920
and so I'm going to define, or maybe redefine

283
00:15:02,020 --> 00:15:06,140
empirical risk minimization as instead of writing this

284
00:15:06,240 --> 00:15:08,200
choosing a set of parameters, I want to think of it

285
00:15:08,310 --> 00:15:14,740
as choosing a function into hypothesis class of script H

286
00:15:14,850 --> 00:15:22,940
that minimizes my training error.

287
00:15:23,050 --> 00:15:24,560
Okay?

288
00:15:24,670 --> 00:15:32,150
So actually can you raise your hand

289
00:15:32,260 --> 00:15:33,510
if it makes sense to you

290
00:15:33,620 --> 00:15:35,050
why this is equivalent to the previous formulation?

291
00:15:35,150 --> 00:15:37,620
Okay, cool. Thanks.

292
00:15:37,740 --> 00:15:44,230
So for development of the use of think of algorithms

293
00:15:44,310 --> 00:15:46,880
as choosing from function from the class instead,

294
00:15:46,990 --> 00:15:50,120
because in a more general case this set,

295
00:15:50,220 --> 00:15:53,560
script H, can be some other class of functions.

296
00:15:53,670 --> 00:15:56,460
Maybe is a class of all functions represented by viewer

297
00:15:56,590 --> 00:16:01,980
network, or the class of all some other class of functions the

298
00:16:02,090 --> 00:16:03,130
learning algorithm wants to choose from.

299
00:16:03,250 --> 00:16:07,900
And this definition for empirical risk minimization

300
00:16:08,010 --> 00:16:09,680
will still apply.

301
00:16:09,750 --> 00:16:10,640
Okay?

302
00:16:10,720 --> 00:16:16,770
So what we'd like to do is understand whether

303
00:16:16,880 --> 00:16:20,620
empirical risk minimization is a reasonable algorithm.

304
00:16:20,730 --> 00:16:21,420
Alex?

305
00:16:21,540 --> 00:16:27,090
Student: a function that's defined by G of data TX,

306
00:16:27,200 --> 00:16:28,970
or is it now more general?

307
00:16:29,090 --> 00:16:31,030
Instructor (Andrew Ng): I see, right

308
00:16:31,150 --> 00:16:37,070
so lets see I guess this the question is

309
00:16:37,190 --> 00:16:41,450
H data still defined by G of phase transpose X,

310
00:16:41,560 --> 00:16:42,290
is this more general?

311
00:16:42,370 --> 00:16:43,810
Student:[Inaudible]

312
00:16:43,890 --> 00:16:49,040
Instructor (Andrew Ng): Oh, yeah so very, two answers

313
00:16:49,110 --> 00:16:50,860
to that. One is, this framework is general,

314
00:16:50,940 --> 00:16:52,440
so for the purpose of this lecture

315
00:16:52,560 --> 00:16:54,900
it may be useful to you to keep in mind a model

316
00:16:55,010 --> 00:16:58,470
of the example of when H subscript data is the class of all

317
00:16:58,580 --> 00:16:59,710
linear classifiers

318
00:16:59,820 --> 00:17:01,590
such as those used by like a visectron algorithm

319
00:17:01,670 --> 00:17:06,250
or logistic regression. This everything on this board,

320
00:17:06,360 --> 00:17:08,500
however, is actually more general.

321
00:17:08,600 --> 00:17:10,530
H can be any set of functions,

322
00:17:10,610 --> 00:17:13,330
mapping from the INFA domain to the center

323
00:17:13,420 --> 00:17:14,780
of class label zero and one,

324
00:17:14,890 --> 00:17:16,420
and then you can perform

325
00:17:16,540 --> 00:17:20,120
empirical risk minimization over any hypothesis class.

326
00:17:20,230 --> 00:17:22,270
For the purpose of today's lecture,

327
00:17:22,380 --> 00:17:25,360
I am going to restrict myself to talking about

328
00:17:25,470 --> 00:17:26,970
binary classification,

329
00:17:27,080 --> 00:17:28,240
but it turns out everything

330
00:17:28,340 --> 00:17:30,090
I say generalizes to regression in other problem as well.

331
00:17:30,190 --> 00:17:31,710
Does that answer your question?

332
00:17:31,820 --> 00:17:32,960
Student:Yes.

333
00:17:33,060 --> 00:17:34,850
Instructor (Andrew Ng): Cool. All right.

334
00:17:34,950 --> 00:17:37,680
So I wanna understand if empirical risk minimization

335
00:17:37,800 --> 00:17:39,140
is a reasonable algorithm.

336
00:17:39,240 --> 00:17:42,000
In particular, what are the things we can prove about it?

337
00:17:44,280 --> 00:17:47,410
So clearly we don't actually care about training error,

338
00:17:47,520 --> 00:17:49,450
we don't really care about

339
00:17:49,720 --> 00:17:51,180
making accurate predictions on the training set,

340
00:17:51,300 --> 00:17:52,390
or at a least that's not the ultimate goal.

341
00:17:52,480 --> 00:17:53,910
The ultimate goal is

342
00:17:54,040 --> 00:18:03,090
how well it makes generalizationhow well it makes

343
00:18:03,180 --> 00:18:04,230
predictions on examples that we haven't seen before.

344
00:18:04,300 --> 00:18:05,420
How well it predicts prices

345
00:18:05,530 --> 00:18:08,460
or sale or no sale outcomes of

346
00:18:08,530 --> 00:18:09,370
houses you haven't seen before.

347
00:18:09,460 --> 00:18:11,470
So what we really care about

348
00:18:11,590 --> 00:18:14,440
is generalization error,

349
00:18:14,550 --> 00:18:16,930
which I write as epsilon of H.

350
00:18:17,050 --> 00:18:19,140
And this is defined as

351
00:18:19,290 --> 00:18:25,050
the probability that if I sample a new example,

352
00:18:25,220 --> 00:18:31,620
X comma Y, from that distribution scripts D,

353
00:18:31,710 --> 00:18:43,920
my hypothesis mislabels that example.

354
00:18:44,040 --> 00:18:48,350
And in terms of notational convention,

355
00:18:48,470 --> 00:18:49,490
usually

356
00:18:49,570 --> 00:18:52,970
if I use if I place a hat on top of something,

357
00:18:53,080 --> 00:18:54,050
it usually means

358
00:18:54,190 --> 00:18:55,130
not always

359
00:18:55,210 --> 00:18:57,210
but it usually means that

360
00:18:57,310 --> 00:18:58,660
it is an attempt to estimate something about the hat.

361
00:18:58,770 --> 00:19:02,100
So for example, epsilon hat here

362
00:19:02,210 --> 00:19:04,110
this is something that we're trying

363
00:19:04,190 --> 00:19:05,340
think of epsilon hat training error

364
00:19:05,430 --> 00:19:08,500
as an attempt to approximate generalization error.

365
00:19:08,630 --> 00:19:11,790
Okay, so the notation convention is usually the things

366
00:19:11,910 --> 00:19:15,250
with the hats on top are things we're using to estimate other quantities.

367
00:19:15,360 --> 00:19:17,870
So the notion is

368
00:19:17,940 --> 00:19:20,190
Another qauantities estimate

369
00:19:20,270 --> 00:19:21,420
And H hat is a hypothesis output by learning algorithm

370
00:19:21,480 --> 00:19:22,620
to try to estimate what the functions from H to Y, X to Y.

371
00:19:22,710 --> 00:19:26,450
So let's actually prove some things about

372
00:19:26,540 --> 00:19:30,870
when empirical risk minimization will do well in a sense of

373
00:19:30,990 --> 00:19:32,570
giving us low generalization error,

374
00:19:32,680 --> 00:19:33,910
which is what we really care about.

375
00:19:33,990 --> 00:19:49,320
In order to prove our first learning theory result,

376
00:19:49,440 --> 00:19:51,170
I'm going to have to state two lemmas,

377
00:19:51,290 --> 00:19:54,460
the first is the union vowel,

378
00:19:54,600 --> 00:20:01,770
which is the following,

379
00:20:01,880 --> 00:20:10,910
let A1 through AK be K event.

380
00:20:11,020 --> 00:20:13,590
And when I say events,

381
00:20:13,670 --> 00:20:15,800
I mean events in a sense of a probabilistic

382
00:20:15,870 --> 00:20:17,290
event that either happens or not.

383
00:20:17,370 --> 00:20:20,600
And these are not necessarily independent.

384
00:20:20,720 --> 00:20:31,520
So there's some current distribution over the events A one

385
00:20:31,640 --> 00:20:34,040
through AK,

386
00:20:34,140 --> 00:20:35,310
and maybe they're independent

387
00:20:35,410 --> 00:20:36,210
maybe not,

388
00:20:36,290 --> 00:20:37,120
no assumption on that.

389
00:20:37,200 --> 00:20:49,030
Then the probability of A one or A two or dot, dot, dot, up

390
00:20:49,140 --> 00:20:51,750
to AK, this union symbol,

391
00:20:51,860 --> 00:20:52,950
this hat,

392
00:20:53,030 --> 00:20:55,920
this just means this sort of just set notation

393
00:20:55,920 --> 00:20:56,920
for probability just means ?°or.?±

394
00:20:57,540 --> 00:21:00,310
So the probability of at least one of these events occurring,

395
00:21:00,420 --> 00:21:03,110
of A one or A two, or up to AK,

396
00:21:03,230 --> 00:21:06,880
this is S equal to the probability of A one plus probability of

397
00:21:06,990 --> 00:21:13,920
A two plus dot, dot, dot, plus probability of AK. Okay?

398
00:21:13,920 --> 00:21:14,920
So the intuition behind this is just that ¨

399
00:21:20,050 --> 00:21:22,510
I'm not sure if you've seen

400
00:21:22,620 --> 00:21:25,750
Venn diagrams depictions of probability before,

401
00:21:25,840 --> 00:21:26,580
if you haven't,

402
00:21:26,660 --> 00:21:28,430
what I'm about to do may be a little cryptic,

403
00:21:28,510 --> 00:21:29,340
so just ignore that.

404
00:21:29,440 --> 00:21:30,500
Just ignore what I'm about to do

405
00:21:30,590 --> 00:21:31,510
if you haven't seen it before.

406
00:21:31,610 --> 00:21:33,040
But if you have seen it before then

407
00:21:33,160 --> 00:21:33,960
this is really

408
00:21:34,040 --> 00:21:40,050
this is really great the probability of

409
00:21:40,130 --> 00:21:43,320
A one, union A two, union A three,

410
00:21:43,400 --> 00:21:48,290
is less than the P of A one,

411
00:21:48,370 --> 00:21:52,540
plus P of A two, plus P of A three.

412
00:21:52,620 --> 00:21:54,960
Right. So that the total mass in the union of

413
00:21:55,040 --> 00:21:56,990
these three things [inaudible] to the sum of the masses in

414
00:21:57,090 --> 00:21:59,240
the three individual sets, it's not very surprising.

415
00:21:59,320 --> 00:22:02,300
It turns out that depending on

416
00:22:02,370 --> 00:22:03,880
how you define your axioms of probability,

417
00:22:03,960 --> 00:22:07,880
this is actually one of the axioms that probably varies,

418
00:22:07,950 --> 00:22:09,540
so I won't actually try to prove this.

419
00:22:09,610 --> 00:22:13,680
This is usually written as an axiom.

420
00:22:13,770 --> 00:22:16,880
So sigmas of avitivity are probably measured as this

421
00:22:16,960 --> 00:22:18,550
what is sometimes called as well.

422
00:22:18,630 --> 00:22:27,640
But in learning theory

423
00:22:27,720 --> 00:22:29,260
it's commonly called the union balance

424
00:22:29,330 --> 00:22:30,240
I just call it that.

425
00:22:30,320 --> 00:22:34,880
The other lemma I need is called

426
00:22:34,950 --> 00:22:35,810
the Hufting inequality.

427
00:22:35,920 --> 00:22:41,710
And again, I won't actually prove this,

428
00:22:41,820 --> 00:22:42,690
I'll just state it,

429
00:22:42,750 --> 00:22:52,020
which is let's let Z1 up to ZM, BM, IID,

430
00:22:52,090 --> 00:22:58,470
there may be random variables with mean Phi.

431
00:22:58,570 --> 00:23:10,280
So the probability of ZI equals 1 is equal to Phi.

432
00:23:10,440 --> 00:23:20,460
So let's say you observe M IID

433
00:23:20,540 --> 00:23:21,890
for newly random variables

434
00:23:21,970 --> 00:23:24,070
and you want to estimate their mean.

435
00:23:24,150 --> 00:23:25,460
So let me define Phi hat,

436
00:23:25,550 --> 00:23:27,630
and this is again that notation, no convention,

437
00:23:27,720 --> 00:23:28,480
Phi hat means

438
00:23:28,550 --> 00:23:32,870
does not attempt is an estimate or something else.

439
00:23:32,960 --> 00:23:35,910
So when we define Phi hat to be 1 over M,

440
00:23:35,990 --> 00:23:38,020
semper my equals one through MZI.

441
00:23:38,100 --> 00:23:39,660
Okay?

442
00:23:39,730 --> 00:23:42,100
So this is our attempt to

443
00:23:42,180 --> 00:23:43,870
estimate the mean of these Benuve random variables

444
00:23:43,940 --> 00:23:45,490
by sort of taking its average.

445
00:23:45,580 --> 00:23:53,510
And let any gamma be fixed.

446
00:23:53,620 --> 00:24:04,990
Then, the Hufting inequality is that

447
00:24:05,070 --> 00:24:16,860
the probability your estimate of Phi

448
00:24:17,020 --> 00:24:20,060
is more than gamma away from the true value of Phi,

449
00:24:20,170 --> 00:24:22,630
that this is bounded by two E

450
00:24:22,740 --> 00:24:23,980
to the next of two gamma squared.

451
00:24:24,060 --> 00:24:24,810
Okay?

452
00:24:24,810 --> 00:24:25,810
So just in pictures ¨

453
00:24:27,080 --> 00:24:35,050
so this theorem holds his lemma,

454
00:24:35,160 --> 00:24:36,000
the Hufting inequality,

455
00:24:36,100 --> 00:24:36,960
this is just a statement of fact,

456
00:24:37,040 --> 00:24:37,850
this just holds true.

457
00:24:37,930 --> 00:24:40,390
But let me now draw a cartoon to describe

458
00:24:40,470 --> 00:24:42,330
some of the intuition behind this, I guess.

459
00:24:42,440 --> 00:24:45,430
So lets say

460
00:24:45,540 --> 00:24:47,190
this is a real number line

461
00:24:47,280 --> 00:24:48,440
from zero to one.

462
00:24:48,550 --> 00:24:51,860
And so Phi is the mean of your Benuve random variables.

463
00:24:51,930 --> 00:24:56,730
You will remember from you know,

464
00:24:56,800 --> 00:24:59,700
whatever some undergraduate probability or statistics class,

465
00:24:59,810 --> 00:25:02,250
we told you

466
00:25:02,330 --> 00:25:03,420
the central limit theorem that says

467
00:25:03,500 --> 00:25:04,880
that when you average all the things together,

468
00:25:04,990 --> 00:25:05,870
you tend to get a Gaussian distribution.

469
00:25:05,970 --> 00:25:08,300
And so when you toss M coins

470
00:25:08,410 --> 00:25:09,860
with bias Phi,

471
00:25:09,940 --> 00:25:11,960
we observe these M Benuve random variables,

472
00:25:12,080 --> 00:25:13,720
and we average them,

473
00:25:13,800 --> 00:25:18,390
then the probability distribution of Phi hat

474
00:25:18,480 --> 00:25:28,040
will roughly be a Gaussian lets say.

475
00:25:28,150 --> 00:25:29,450
Okay?

476
00:25:29,540 --> 00:25:31,850
It turns out if you haven't seen this up before,

477
00:25:31,960 --> 00:25:33,330
this is actually that

478
00:25:33,430 --> 00:25:34,660
the cumulative distribution function of Phi hat

479
00:25:34,730 --> 00:25:35,750
will converse with that of the Gaussian.

480
00:25:36,330 --> 00:25:38,110
Technically Phi hat can only take on

481
00:25:38,190 --> 00:25:39,630
a discreet set of values

482
00:25:39,710 --> 00:25:42,310
because these are factions one over Ms.

483
00:25:42,400 --> 00:25:43,750
It doesn't really have an entity

484
00:25:43,840 --> 00:25:45,500
but just as a cartoon think of it

485
00:25:45,500 --> 00:25:47,160
as a converse roughly to a

486
00:25:47,280 --> 00:25:51,240
Gaussian. So what the Hufting inequality says is that

487
00:25:51,350 --> 00:25:52,630
if you pick a value of gamma,

488
00:25:52,760 --> 00:25:58,460
let me put S one interval gamma

489
00:25:58,580 --> 00:25:59,740
there's another interval gamma.

490
00:25:59,860 --> 00:26:02,680
Then the saying that the probability mass of the details

491
00:26:02,790 --> 00:26:07,840
the probability that my value of Phi hat is more than

492
00:26:07,930 --> 00:26:10,060
a gamma away from the true value,

493
00:26:10,160 --> 00:26:16,610
that the total mass that the total probability mass

494
00:26:16,690 --> 00:26:24,100
in these tails is at most two E to the negative two gamma

495
00:26:24,220 --> 00:26:25,850
squared M. Okay?

496
00:26:25,930 --> 00:26:27,130
That's what the Hufting inequality

497
00:26:27,490 --> 00:26:29,050
so if you can't read that this just says

498
00:26:29,160 --> 00:26:29,950
this is just

499
00:26:30,050 --> 00:26:31,760
the right hand side of the bound, two E to negative two

500
00:26:31,870 --> 00:26:34,410
gamma squared. So balance the probability that

501
00:26:34,520 --> 00:26:36,840
you make a mistake in estimating the mean of a Benuve

502
00:26:36,950 --> 00:26:37,720
random variable.

503
00:26:37,810 --> 00:26:45,000
And the cool thing about

504
00:26:45,090 --> 00:26:45,780
this bound the interesting thing

505
00:26:45,860 --> 00:26:51,930
behind this bound is that the exponentially in M, so it

506
00:26:52,020 --> 00:26:53,500
says that for a fixed value of gamma,

507
00:26:53,610 --> 00:26:57,230
as you increase the size of your training set,

508
00:26:57,310 --> 00:26:58,310
as you toss a coin more and more,

509
00:26:58,390 --> 00:27:00,340
then the worth of this Gaussian will shrink.

510
00:27:00,450 --> 00:27:03,470
The worth of this Gaussian will actually

511
00:27:03,570 --> 00:27:04,820
shrink like one over root to M.

512
00:27:04,900 --> 00:27:08,760
And that will cause the probability mass left

513
00:27:08,840 --> 00:27:11,000
in the tails to decrease exponentially,

514
00:27:11,070 --> 00:27:13,980
quickly, as a function of that.

515
00:27:14,080 --> 00:27:15,690
And this will be important later.

516
00:27:15,770 --> 00:27:17,260
Yeah?

517
00:27:17,350 --> 00:27:20,780
Student: Does this come from the central limit theorem.

518
00:27:20,870 --> 00:27:22,110
Instructor (Andrew Ng): No it doesn't.

519
00:27:22,190 --> 00:27:23,460
So this is proved by a different

520
00:27:23,560 --> 00:27:26,400
this is proved no so the central limit theorem

521
00:27:26,520 --> 00:27:28,570
there may be a version of the central limit theorem,

522
00:27:28,680 --> 00:27:30,450
but the versions I'm familiar with tend

523
00:27:30,560 --> 00:27:32,180
are sort of asymptotic,

524
00:27:32,290 --> 00:27:34,490
but this works for any finer value of M.

525
00:27:34,620 --> 00:27:38,330
Oh, and for your this bound holds

526
00:27:38,410 --> 00:27:39,220
even if M is equal to two,

527
00:27:39,290 --> 00:27:40,490
or M is [inaudible],

528
00:27:40,560 --> 00:27:41,440
if M is very small,

529
00:27:41,540 --> 00:27:43,490
the central limit theorem approximation is not gonna hold,

530
00:27:43,600 --> 00:27:45,910
but this theorem holds regardless.

531
00:27:45,990 --> 00:27:46,800
Okay?

532
00:27:46,880 --> 00:27:49,230
I'm drawing this just

533
00:27:49,340 --> 00:27:50,930
as a cartoon to help explain the intuition,

534
00:27:51,040 --> 00:27:53,040
but this theorem just holds true,

535
00:27:53,140 --> 00:27:55,040
without reference to central limit theorem.

536
00:27:55,150 --> 00:28:03,500
All right.

537
00:28:03,610 --> 00:28:11,200
So lets start to understand empirical risk minimization,

538
00:28:11,310 --> 00:28:23,810
and what I want to do is begin

539
00:28:23,920 --> 00:28:31,990
with studying empirical risk minimization for a  case

540
00:28:32,100 --> 00:28:34,810
that's a logistic regression, and in particular

541
00:28:34,890 --> 00:28:36,000
I want to start with studying

542
00:28:36,090 --> 00:28:38,200
the case of finite hypothesis classes.

543
00:28:38,300 --> 00:28:50,460
So let's say script H is a class of K hypotheses.

544
00:28:50,620 --> 00:28:58,380
Right. So this is K functions w

545
00:28:58,500 --> 00:29:00,010
ith no each of these is just a function mapping

546
00:29:00,120 --> 00:29:01,050
from inputs to outputs,

547
00:29:01,170 --> 00:29:02,110
there's no parameters in this.

548
00:29:02,220 --> 00:29:07,860
And so what the empirical risk minimization would do is

549
00:29:07,980 --> 00:29:10,070
it would take the training set

550
00:29:10,180 --> 00:29:16,460
and it'll then look at each of these K functions,

551
00:29:16,580 --> 00:29:20,170
and it'll pick whichever of these functions has the lowest

552
00:29:20,270 --> 00:29:21,240
training error. Okay?

553
00:29:21,370 --> 00:29:23,150
So now that the logistic regression

554
00:29:23,260 --> 00:29:26,090
uses an infinitely large a continuous infinitely large class of

555
00:29:26,200 --> 00:29:27,740
hypotheses, script H,

556
00:29:27,830 --> 00:29:31,910
but to prove the first row I actually want to just describe

557
00:29:31,990 --> 00:29:34,680
our first learning theorem is all for the case of

558
00:29:34,760 --> 00:29:36,470
when you have a finite hypothesis class,

559
00:29:36,580 --> 00:29:39,730
then we'll later generalize that into the hypothesis classes.

560
00:29:39,850 --> 00:29:54,820
So empirical risk minimization

561
00:29:54,920 --> 00:29:56,790
takes the hypothesis of the lowest training error,

562
00:29:56,870 --> 00:30:01,620
and what I'd like to do is prove a bound

563
00:30:01,720 --> 00:30:05,270
on the generalization error of H hat.

564
00:30:05,350 --> 00:30:06,790
All right. So in other words

565
00:30:06,870 --> 00:30:07,970
I'm gonna prove that

566
00:30:08,050 --> 00:30:08,980
somehow minimizing training error

567
00:30:09,060 --> 00:30:10,370
allows me to do well on generalization error.

568
00:30:10,470 --> 00:30:11,430
And here's the strategy,

569
00:30:11,520 --> 00:30:22,770
the first step in this prove I'm going to show that

570
00:30:22,840 --> 00:30:27,240
training error is a good approximation to generalization

571
00:30:27,310 --> 00:30:34,920
error, and then I'm going to show that

572
00:30:35,020 --> 00:30:42,800
this implies a bound on the generalization error of

573
00:30:42,900 --> 00:30:44,770
the hypothesis of  empirical risk minimization.

574
00:30:44,860 --> 00:30:50,660
And I just realized, this class I guess

575
00:30:50,750 --> 00:30:55,140
is also maybe slightly notation heavy class round, instead of

576
00:30:55,210 --> 00:30:57,760
just introducing a reasonably large set of new symbols,

577
00:30:57,840 --> 00:31:00,910
so if again, in the course of today's lecture,

578
00:31:01,020 --> 00:31:02,000
you're looking at some symbol

579
00:31:02,070 --> 00:31:03,420
and you don't quite remember what it is,

580
00:31:03,520 --> 00:31:04,510
please raise your hand and ask.

581
00:31:04,580 --> 00:31:05,790
what's that, what was that,

582
00:31:05,870 --> 00:31:07,670
was that a generalization error

583
00:31:07,760 --> 00:31:09,040
or was it something else?

584
00:31:09,120 --> 00:31:12,850
So raise your hand and ask if you don't understand what

585
00:31:12,930 --> 00:31:13,950
the notation I was defining.

586
00:31:14,030 --> 00:31:20,010
Okay. So let me introduce this in two steps.

587
00:31:20,130 --> 00:31:21,700
And the empirical risk strategy is

588
00:31:21,790 --> 00:31:23,310
I'm gonna show training errors that

589
00:31:23,420 --> 00:31:27,140
give approximation generalization error,

590
00:31:27,220 --> 00:31:28,590
and this will imply that minimizing training error

591
00:31:28,670 --> 00:31:29,400
will also do pretty well

592
00:31:29,470 --> 00:31:30,330
in terms of minimizing generalization error.

593
00:31:30,410 --> 00:31:32,580
And this will give us a bound on the generalization error of

594
00:31:32,660 --> 00:31:35,820
the hypothesis output by empirical risk minimization.

595
00:31:35,940 --> 00:31:37,990
Okay?

596
00:31:38,120 --> 00:31:40,790
So here's the idea.

597
00:31:40,870 --> 00:31:54,510
So lets even not consider all the hypotheses at once,

598
00:31:54,630 --> 00:31:56,660
lets pick any hypothesis,

599
00:31:56,740 --> 00:31:59,480
HJ in the class script H,

600
00:31:59,560 --> 00:32:03,330
and so until further notice lets just consider there one fixed

601
00:32:03,420 --> 00:32:05,430
hypothesis. So pick any one hypothesis

602
00:32:05,530 --> 00:32:07,680
and let's talk about that one.

603
00:32:07,760 --> 00:32:14,800
Let me define ZI

604
00:32:14,880 --> 00:32:25,880
to be indicator function for

605
00:32:25,960 --> 00:32:30,170
whether this hypothesis misclassifies the IFE example

606
00:32:30,270 --> 00:32:31,840
excuse me or Z subscript I. Okay?

607
00:32:31,920 --> 00:32:45,150
So ZI would be zero or one depending on whether this one

608
00:32:45,230 --> 00:32:47,860
hypothesis which is the only one I'm gonna even consider

609
00:32:47,950 --> 00:32:51,110
whether this hypothesis was classified as an example.

610
00:32:51,210 --> 00:32:59,310
And so my training set is

611
00:32:59,390 --> 00:33:00,890
drawn randomly from sum distribution scripts d,

612
00:33:00,970 --> 00:33:06,450
and depending on what training examples I've got,

613
00:33:06,560 --> 00:33:08,900
these ZIs would be either zero or one.

614
00:33:08,980 --> 00:33:14,270
So let's figure out what the probability distribution ZI is.

615
00:33:14,350 --> 00:33:18,680
Well, so ZI takes on the value of either zero or one

616
00:33:18,760 --> 00:33:19,610
so clearly is

617
00:33:19,690 --> 00:33:20,650
a Benuve random variable,

618
00:33:20,730 --> 00:33:21,890
it can only take on these values.

619
00:33:21,970 --> 00:33:33,890
Well, what's the probability that ZI is equal to one?

620
00:33:33,980 --> 00:33:34,700
In other words,

621
00:33:34,780 --> 00:33:38,950
what's the probability that from a fixed hypothesis HJ,

622
00:33:39,030 --> 00:33:44,170
when I sample my training set IID from distribution D,

623
00:33:44,240 --> 00:33:48,080
what is the chance that my hypothesis will misclassify it?

624
00:33:48,190 --> 00:33:51,240
Well, by definition,

625
00:33:51,350 --> 00:33:57,780
that's just a generalization error of my hypothesis HJ.

626
00:33:57,890 --> 00:34:05,980
So ZI is a Benuve random variable with mean given by

627
00:34:06,060 --> 00:34:07,940
the generalization error of this hypothesis.

628
00:34:08,050 --> 00:34:17,780
Raise your hand if that made sense. Oh, cool. Great.

629
00:34:17,860 --> 00:34:23,650
And moreover, all the ZIs

630
00:34:23,730 --> 00:34:25,190
have the same probability of being one,

631
00:34:25,280 --> 00:34:27,510
and all my training examples I've drawn are IID,

632
00:34:27,590 --> 00:34:29,920
and so the ZIs are also independent

633
00:34:29,990 --> 00:34:43,370
and therefore the ZIs themselves are IID random variables.

634
00:34:43,500 --> 00:34:44,460
Okay?

635
00:34:44,540 --> 00:34:45,520
Because my training examples

636
00:34:45,600 --> 00:34:46,750
were drawn independently of each other, by assumption.

637
00:34:46,830 --> 00:34:59,180
If you read this as the definition of training error,

638
00:34:59,260 --> 00:35:07,320
the training error of my hypothesis HJ, that's just that.

639
00:35:07,410 --> 00:35:10,560
That's just the average of my ZIs,

640
00:35:10,640 --> 00:35:14,210
which was , well I previously defined it like this.

641
00:35:14,290 --> 00:35:22,100
Okay?

642
00:35:22,180 --> 00:35:33,860
And so epsilon hat of HJ is exactly the average of MIID,

643
00:35:33,940 --> 00:35:35,080
Benuve random variables,

644
00:35:35,160 --> 00:35:37,690
drawn from Benuve distribution

645
00:35:37,770 --> 00:35:41,210
with mean given by the generalization error,

646
00:35:41,300 --> 00:35:46,180
so well this is the average of MIID

647
00:35:46,280 --> 00:35:47,160
Benuve random variables,

648
00:35:47,240 --> 00:35:50,110
each of which has meaning

649
00:35:50,460 --> 00:35:56,650
given by the generalization error of HJ.

650
00:35:56,750 --> 00:36:07,940
And therefore, by the Hufting inequality

651
00:36:08,020 --> 00:36:17,570
we have to add the probability that the difference

652
00:36:17,640 --> 00:36:18,870
between training and generalization error,

653
00:36:18,950 --> 00:36:20,620
the probability that this is greater than gamma is less than

654
00:36:20,700 --> 00:36:26,610
to two, E to the negative two, gamma squared M. Okay?

655
00:36:26,690 --> 00:36:28,150
Exactly by the Hufting inequality.

656
00:36:28,230 --> 00:36:33,990
And what this proves is that,

657
00:36:34,080 --> 00:36:36,310
for my fixed hypothesis HJ,

658
00:36:36,380 --> 00:36:38,250
my training error,

659
00:36:38,320 --> 00:36:41,910
epsilon hat will with high probability,

660
00:36:41,980 --> 00:36:44,120
assuming M is large,

661
00:36:44,200 --> 00:36:45,230
if M is large than

662
00:36:45,310 --> 00:36:46,640
this thing on the right hand side will be small,

663
00:36:46,710 --> 00:36:48,960
because this is two Es

664
00:36:49,060 --> 00:36:52,080
and a negative two gamma squared M.

665
00:36:52,160 --> 00:36:53,160
So this says that if my training set is large enough,

666
00:36:53,240 --> 00:36:55,450
then the probability my training error

667
00:36:55,530 --> 00:36:57,110
is far from generalization error,

668
00:36:57,190 --> 00:36:58,880
meaning that it is more than gamma,

669
00:36:58,990 --> 00:37:00,150
will be small,

670
00:37:00,230 --> 00:37:03,180
will be bounded by this thing on the right hand side. Okay?

671
00:37:03,290 --> 00:37:10,860
Now, here's the tricky part,

672
00:37:10,930 --> 00:37:13,560
what we've done is approve this bound

673
00:37:13,670 --> 00:37:14,740
for one fixed hypothesis,

674
00:37:14,820 --> 00:37:15,800
for HJ.

675
00:37:15,890 --> 00:37:18,580
What I want to prove is that training error

676
00:37:18,650 --> 00:37:20,430
will be a good estimate for generalization error,

677
00:37:20,540 --> 00:37:22,810
not just for this one hypothesis HJ,

678
00:37:22,920 --> 00:37:25,800
but actually for all K hypotheses

679
00:37:25,910 --> 00:37:28,670
in my hypothesis class script H.

680
00:37:28,780 --> 00:37:37,330
So let's do it, well, better do it on a new board.

681
00:37:37,400 --> 00:37:56,350
So in order to show that,

682
00:37:56,420 --> 00:37:59,190
let me define a random event,

683
00:37:59,270 --> 00:38:02,420
let me define AJ to be the event

684
00:38:02,520 --> 00:38:24,820
that to be the event that you know,

685
00:38:24,900 --> 00:38:27,280
the difference between training and generalization error

686
00:38:27,360 --> 00:38:29,260
is more than gamma on a hypothesis HJ.

687
00:38:29,340 --> 00:38:34,260
And so what we put on the previous board was that

688
00:38:34,360 --> 00:38:36,240
the probability of AJ is less equal to

689
00:38:36,320 --> 00:38:39,450
two E to the negative two, gamma squared M,

690
00:38:39,530 --> 00:38:40,450
and this is pretty small.

691
00:38:40,530 --> 00:38:46,510
Now, What I want to bound is the probability that

692
00:38:46,600 --> 00:38:53,120
there exists some hypothesis in my class script H,

693
00:38:53,240 --> 00:39:05,310
such that I make a large error in my estimate of

694
00:39:05,390 --> 00:39:07,300
generalization error. Okay?

695
00:39:07,380 --> 00:39:10,730
Such that this holds true.

696
00:39:10,800 --> 00:39:16,270
So this is really just that the probability that

697
00:39:16,350 --> 00:39:17,820
there exists a hypothesis for which this holds.

698
00:39:17,910 --> 00:39:19,310
This is really the probability

699
00:39:19,390 --> 00:39:26,510
that A one or A two, or up to AK holds.

700
00:39:26,590 --> 00:39:31,910
The chance there exists a hypothesis is just well the priority

701
00:39:32,010 --> 00:39:35,020
that for hypothesis one and make a large error

702
00:39:35,120 --> 00:39:36,570
in estimating the generalization error,

703
00:39:36,640 --> 00:39:39,560
or for hypothesis two and make a large error in estimating

704
00:39:39,660 --> 00:39:41,070
generalization error, and so on.

705
00:39:41,170 --> 00:39:45,070
And so by the union bound,

706
00:39:45,180 --> 00:39:46,550
this is less than equal to that,

707
00:39:46,660 --> 00:39:58,910
which is therefore less than

708
00:39:59,000 --> 00:40:16,330
equal to is equal to that. Okay?

709
00:40:16,440 --> 00:40:47,990
So let me just take one minus both sides of the equation

710
00:40:48,070 --> 00:40:48,880
on the previous board

711
00:40:48,960 --> 00:40:50,070
let me take one minus both sides,

712
00:40:50,140 --> 00:40:57,450
so the probability that there does not exist for hypothesis

713
00:40:57,530 --> 00:41:09,910
such that  . The probability that there does not exist a

714
00:41:10,020 --> 00:41:12,840
hypothesis on which I make a large error in this estimate

715
00:41:12,950 --> 00:41:14,290
while this is equal to

716
00:41:14,370 --> 00:41:16,050
the probability that for all hypotheses,

717
00:41:16,170 --> 00:41:25,630
I make a small error,

718
00:41:25,750 --> 00:41:28,640
or at most gamma, in my estimate of generalization error.

719
00:41:28,760 --> 00:41:33,520
In taking one minus on the right hand side

720
00:41:33,630 --> 00:41:41,960
I get two KE to the negative two gamma squared M.

721
00:41:42,050 --> 00:41:42,950
Okay?

722
00:41:43,040 --> 00:41:45,010
And so

723
00:41:45,080 --> 00:41:47,930
and the sign of the inequality flipped

724
00:41:48,010 --> 00:41:49,340
because I took one minus both sides.

725
00:41:49,420 --> 00:41:51,410
The minus sign flips the sign of the equality.

726
00:41:51,490 --> 00:41:57,030
So what we're shown is that

727
00:41:57,140 --> 00:42:02,900
with probability which abbreviates to

728
00:42:03,030 --> 00:42:03,960
WP with probability

729
00:42:04,080 --> 00:42:07,590
one minus two KE to the negative two gamma squared M.

730
00:42:07,700 --> 00:42:17,090
We have that, epsilon hat of H will be

731
00:42:17,200 --> 00:42:23,440
then gamma of epsilon of H,

732
00:42:23,560 --> 00:42:36,680
simultaneously for all hypotheses in our class script H.

733
00:42:36,760 --> 00:42:50,620
And so just to give this result a name,

734
00:42:50,730 --> 00:42:58,330
this is called, one instance of what's called a uniform

735
00:42:58,440 --> 00:43:03,750
conversions result, and the term uniform conversions this

736
00:43:03,850 --> 00:43:05,310
sort of alludes to the fact that

737
00:43:05,660 --> 00:43:07,620
this shows that as M becomes large,

738
00:43:07,700 --> 00:43:15,970
then these epsilon hats will all simultaneously converge

739
00:43:16,040 --> 00:43:17,730
to epsilon of H.

740
00:43:17,810 --> 00:43:20,010
That training error will become very close to

741
00:43:20,140 --> 00:43:22,200
generalization error simultaneously for all hypotheses H.

742
00:43:22,280 --> 00:43:26,760
That's what the term uniform refers to,

743
00:43:26,840 --> 00:43:28,880
is the fact that this converges for all hypotheses H

744
00:43:28,950 --> 00:43:30,170
and not just for one hypothesis.

745
00:43:30,250 --> 00:43:33,190
And so what we're shown is one example of

746
00:43:33,270 --> 00:43:34,350
a uniform conversions result.

747
00:43:34,420 --> 00:43:35,280
Okay?

748
00:43:35,360 --> 00:43:37,870
So let me clean a couple more boards.

749
00:43:37,950 --> 00:43:38,790
I'll come back

750
00:43:38,870 --> 00:43:39,900
and ask what questions you have about this.

751
00:43:39,960 --> 00:43:41,910
We should take another look at this and make sure it all

752
00:43:41,990 --> 00:43:42,980
makes sense. Yeah, okay.

753
00:43:43,050 --> 00:44:22,160
What questions do you have about this?

754
00:44:22,240 --> 00:44:28,800
Student: How the is the value of gamma computed?

755
00:44:28,920 --> 00:44:33,080
Instructor (Andrew Ng): Right. Yeah. So let's see,

756
00:44:33,160 --> 00:44:36,180
the question is how is the value of gamma computed?

757
00:44:36,300 --> 00:44:37,860
So for these purposes,

758
00:44:37,940 --> 00:44:40,180
gamma is a constant.

759
00:44:40,260 --> 00:44:41,100
Imagine

760
00:44:41,160 --> 00:44:42,100
a gamma is some constant

761
00:44:42,180 --> 00:44:44,300
that we chose in advance  and this is a bound

762
00:44:44,380 --> 00:44:47,660
that holds true for any fixed value of gamma.

763
00:44:47,770 --> 00:44:51,480
Later on as we take this bound

764
00:44:51,560 --> 00:44:53,680
and then sort of develop this result further,

765
00:44:53,770 --> 00:44:56,780
we'll choose specific values of gamma as  of this bound.

766
00:44:56,900 --> 00:44:59,130
For now we'll just imagine that

767
00:44:59,200 --> 00:45:00,330
when we're proved

768
00:45:00,400 --> 00:45:01,760
this holds true for any value of gamma.

769
00:45:01,840 --> 00:45:05,650
Any questions? Yeah?

770
00:45:05,730 --> 00:45:08,790
Student:[Inaudible] hypothesis phase is infinite [inaudible]?

771
00:45:08,870 --> 00:45:09,610
Instructor (Andrew Ng):Yes,

772
00:45:09,670 --> 00:45:10,980
the labs in the hypothesis phase is infinite,

773
00:45:11,060 --> 00:45:15,170
so this simple result won't work in this present form,

774
00:45:15,250 --> 00:45:18,360
but we'll generalize this probably won't get to it today

775
00:45:18,440 --> 00:45:20,900
but we'll generalize this at the beginning

776
00:45:20,990 --> 00:45:21,860
of the next lecture

777
00:45:21,940 --> 00:45:23,300
to infinite hypothesis classes.

778
00:45:23,380 --> 00:45:28,560
Student:How do we use this theory ?

779
00:45:28,670 --> 00:45:30,870
Instructor (Andrew Ng):How do you use theorem factors?

780
00:45:30,950 --> 00:45:34,710
So let me, I might get to a little of that later today,

781
00:45:34,820 --> 00:45:36,720
we'll talk concretely about algorithms,

782
00:45:36,810 --> 00:45:39,030
the consequences of the understanding

783
00:45:39,150 --> 00:45:41,910
of these things in the next lecture as well.

784
00:45:41,990 --> 00:45:44,780
Yeah, okay? Cool.

785
00:45:44,860 --> 00:45:47,550
Can you just raise your hand

786
00:45:47,630 --> 00:45:49,660
if the things I've proved so far make sense?

787
00:45:49,740 --> 00:45:51,640
Okay. Cool. Great. Thanks.

788
00:45:51,720 --> 00:45:57,300
All right. Let me just take this uniform conversions

789
00:45:57,390 --> 00:46:01,320
bound and rewrite it in a couple of other forms.

790
00:46:01,410 --> 00:46:06,920
So this is a sort of a bound on probability,

791
00:46:07,000 --> 00:46:08,170
this is saying suppose

792
00:46:08,250 --> 00:46:10,580
I fix my training set and then fix my training set

793
00:46:10,670 --> 00:46:14,420
And fix my threshold, my error threshold gamma,

794
00:46:14,500 --> 00:46:17,740
what is the probability that uniform conversions holds,

795
00:46:17,850 --> 00:46:21,930
and well, that's my formula that gives the answer.

796
00:46:22,010 --> 00:46:23,390
This is the probability of something happening.

797
00:46:23,550 --> 00:46:28,420
So there are actually three parameters of interest.

798
00:46:28,420 --> 00:46:29,420
One is, ?°What is this probability'±

799
00:46:30,840 --> 00:46:32,400
The other parameter is,

800
00:46:32,470 --> 00:46:33,780
What's the training set size M?

801
00:46:33,860 --> 00:46:35,440
And the third parameter is,

802
00:46:35,510 --> 00:46:39,060
What is the value of this error threshold gamma?

803
00:46:39,150 --> 00:46:43,630
I'm not gonna vary K for these purposes.

804
00:46:43,710 --> 00:46:46,720
So other two other equivalent forms of the bounds,

805
00:46:46,800 --> 00:46:48,440
which  so you can ask,

806
00:46:48,440 --> 00:46:49,440
Given gamma ¨so what we proved was given gamma

807
00:46:58,700 --> 00:46:59,440
and given M,

808
00:46:59,510 --> 00:47:00,420
what is the probability of uniform conversions?

809
00:47:00,520 --> 00:47:03,570
The other equivalent forms are,

810
00:47:03,650 --> 00:47:09,690
so that given gamma and the probability delta of

811
00:47:09,770 --> 00:47:10,970
making a large error,

812
00:47:11,060 --> 00:47:18,880
how large a training set size do you need in order to give a

813
00:47:18,960 --> 00:47:26,040
bound on how large a training set size do you need to give a

814
00:47:26,120 --> 00:47:28,370
uniform conversions bound with parameters gamma

815
00:47:28,450 --> 00:47:29,190
and delta?

816
00:47:29,270 --> 00:47:30,340
And well,

817
00:47:30,440 --> 00:47:36,050
so if you set delta to be

818
00:47:36,130 --> 00:47:38,460
two KE so negative two gamma squared M.

819
00:47:38,540 --> 00:47:40,330
This is that form that I had on the left.

820
00:47:40,410 --> 00:47:44,170
And if you solve for M,

821
00:47:44,250 --> 00:47:49,840
what you find is that there's an equivalent form of this

822
00:47:49,920 --> 00:47:58,100
result that says that so long as your training set assigns M

823
00:47:58,180 --> 00:48:00,170
as greater than this.

824
00:48:00,250 --> 00:48:02,290
And this is the formula that I get by solving for M.

825
00:48:02,360 --> 00:48:06,440
Okay?

826
00:48:06,520 --> 00:48:07,950
So long as M is greater than equal to this,

827
00:48:08,420 --> 00:48:09,890
then with probability,

828
00:48:09,960 --> 00:48:12,390
which I'm abbreviating to WP again,

829
00:48:12,470 --> 00:48:14,710
with probability at least one minus delta

830
00:48:14,790 --> 00:48:24,600
we have for all.

831
00:48:24,680 --> 00:48:30,680
Okay?

832
00:48:30,790 --> 00:48:34,430
So this says how large a training set size that

833
00:48:34,580 --> 00:48:35,530
I need to guarantee that

834
00:48:35,640 --> 00:48:36,710
with probability

835
00:48:36,850 --> 00:48:38,080
at least one minus delta,

836
00:48:38,230 --> 00:48:39,400
we have the training error is

837
00:48:39,500 --> 00:48:41,750
within gamma of generalization error for all my

838
00:48:41,920 --> 00:48:43,360
hypotheses, and this gives an answer.

839
00:48:43,500 --> 00:48:47,880
And just to give this another name,

840
00:48:47,990 --> 00:48:50,040
this is an example of a sample complexity bound.

841
00:48:50,110 --> 00:49:02,440
So from undergrad computer science classes

842
00:49:02,490 --> 00:49:04,180
you may have heard of computational complexity,

843
00:49:04,250 --> 00:49:05,150
which is how much computations

844
00:49:05,220 --> 00:49:06,260
you need to achieve something.

845
00:49:06,340 --> 00:49:09,040
So sample complexity just means

846
00:49:09,130 --> 00:49:10,010
how large a training example

847
00:49:10,080 --> 00:49:11,860
how large a sample of examples do you need

848
00:49:11,940 --> 00:49:15,830
in order to achieve a certain bound and error.

849
00:49:15,910 --> 00:49:19,090
And it turns out that in many of the theorems

850
00:49:19,180 --> 00:49:21,450
we write out you can pose them

851
00:49:21,560 --> 00:49:22,780
in sort of a form of probability bound

852
00:49:22,860 --> 00:49:25,240
or a sample complexity bound or in some other form.

853
00:49:25,380 --> 00:49:26,510
I personally often find

854
00:49:26,660 --> 00:49:29,080
the sample complexity bounds the most easy to interpret

855
00:49:29,220 --> 00:49:30,090
because it says

856
00:49:30,190 --> 00:49:31,340
how large a training set do

857
00:49:31,490 --> 00:49:32,690
you need to give a certain bound on the errors.

858
00:49:32,800 --> 00:49:37,950
And in fact well, we'll see this later,

859
00:49:38,030 --> 00:49:40,490
sample complexity bounds often sort of

860
00:49:40,560 --> 00:49:43,240
help to give guidance for really

861
00:49:43,310 --> 00:49:45,230
if you're trying to achieve something

862
00:49:45,310 --> 00:49:46,130
on a machine learning problem,

863
00:49:46,210 --> 00:49:47,750
this really is trying to give guidance on

864
00:49:47,820 --> 00:49:50,680
how much training data you need to prove something.

865
00:49:50,760 --> 00:49:57,370
The one thing I want to note here is that

866
00:49:57,460 --> 00:50:00,830
M grows like the log of K, right,

867
00:50:00,910 --> 00:50:03,080
so the log of K grows

868
00:50:03,190 --> 00:50:04,980
extremely slowly as a function of K.

869
00:50:05,110 --> 00:50:07,550
The log is one of the slowest growing functions, right.

870
00:50:07,730 --> 00:50:12,790
It's one of well, some of you may have heard this, right?

871
00:50:12,960 --> 00:50:14,370
That for all values of K,

872
00:50:14,510 --> 00:50:19,600
right, I learned this

873
00:50:19,770 --> 00:50:24,060
from a colleague, Andrew Moore, at Carnegie Mellon that

874
00:50:24,200 --> 00:50:25,470
in computer science

875
00:50:25,610 --> 00:50:27,950
for all practical purposes for all values of K,

876
00:50:28,090 --> 00:50:31,230
log K is less, 30 this is almost true.

877
00:50:31,380 --> 00:50:34,690
So log K is, logs is one of the slowest growing functions,

878
00:50:34,830 --> 00:50:43,270
and so the fact that M sample complexity grows

879
00:50:43,320 --> 00:50:44,440
like the log of K,

880
00:50:44,530 --> 00:50:45,710
means that you can increase this number of hypotheses

881
00:50:45,790 --> 00:50:46,840
in your hypothesis class quite a lot

882
00:50:46,920 --> 00:50:48,510
and the number of the training examples you need

883
00:50:48,590 --> 00:50:51,400
won't grow very much. [Inaudible].

884
00:50:51,470 --> 00:50:57,830
This property

885
00:50:57,900 --> 00:50:59,130
will be important later

886
00:50:59,210 --> 00:51:00,630
when we talk about infinite hypothesis classes.

887
00:51:00,700 --> 00:51:04,660
The final form is the

888
00:51:04,780 --> 00:51:08,470
I guess is sometimes called the error bound,

889
00:51:08,550 --> 00:51:11,790
which is when you hold M and delta fixed and

890
00:51:11,940 --> 00:51:12,690
solved for gamma.

891
00:51:12,770 --> 00:51:27,200
And so and what do you do what you get then is that

892
00:51:27,320 --> 00:51:31,680
the probability at least one minus delta, we have that.

893
00:51:31,790 --> 00:51:43,820
For all hypotheses in my hypothesis class,

894
00:51:43,900 --> 00:51:47,140
the difference in the training generalization error

895
00:51:47,220 --> 00:51:48,460
would be less than equal to that.

896
00:51:48,540 --> 00:51:53,770
Okay?

897
00:51:53,890 --> 00:51:55,940
And that's just solving for gamma

898
00:51:56,020 --> 00:51:59,370
and plugging the value I get in there. Okay?

899
00:51:59,450 --> 00:52:01,240
All right.

900
00:52:01,350 --> 00:52:38,010
So the second step of the overall proof

901
00:52:38,090 --> 00:52:39,680
I want to execute is the following.

902
00:52:39,770 --> 00:52:47,140
The result of the training error is

903
00:52:47,230 --> 00:52:49,450
essentially that uniform conversions will hold true with

904
00:52:49,540 --> 00:52:52,250
high probability. What I want to show now is

905
00:52:52,330 --> 00:52:54,960
let's assume that uniform conversions hold.

906
00:52:55,070 --> 00:52:58,590
So let's assume that for all hypotheses H,

907
00:52:58,670 --> 00:53:01,810
we have that epsilon of

908
00:53:01,920 --> 00:53:06,150
H minus epsilon hat of H, is less than of the gamma.

909
00:53:06,270 --> 00:53:07,140
Okay?

910
00:53:07,220 --> 00:53:12,720
What I want to do now is

911
00:53:12,890 --> 00:53:15,220
use this to see what we can prove about the bound of

912
00:53:15,370 --> 00:53:34,950
see what we can prove about the generalization error.

913
00:53:35,090 --> 00:53:35,770
So I want to know

914
00:53:35,950 --> 00:53:36,710
suppose this holds true

915
00:53:36,850 --> 00:53:38,520
I want to know can we prove something about

916
00:53:38,610 --> 00:53:41,280
the generalization error of H hat, where again,

917
00:53:41,390 --> 00:53:48,310
H hat was the hypothesis selected by empirical risk

918
00:53:48,430 --> 00:53:49,930
minimization. Okay?

919
00:53:50,050 --> 00:53:54,610
So in order to show this, let me make one more definition,

920
00:53:54,690 --> 00:53:56,020
let me define H star,

921
00:53:56,090 --> 00:54:07,430
to be the hypothesis in my class script H

922
00:54:07,540 --> 00:54:08,980
that has the smallest generalization error.

923
00:54:09,060 --> 00:54:14,390
So this is if I had an infinite amount of training data

924
00:54:14,450 --> 00:54:16,060
or if I really I could go in

925
00:54:16,140 --> 00:54:18,290
and find the best possible hypothesis

926
00:54:18,570 --> 00:54:20,500
the best possible hypothesis

927
00:54:20,610 --> 00:54:22,700
in the sense of minimizing generalization error

928
00:54:22,780 --> 00:54:26,670
what's the hypothesis I would get? Okay?

929
00:54:26,780 --> 00:54:29,660
So in some sense,

930
00:54:29,730 --> 00:54:32,080
it sort of makes sense to compare the performance

931
00:54:32,170 --> 00:54:34,550
of our learning algorithm to the performance of H star,

932
00:54:34,630 --> 00:54:37,970
because we sort of, we clearly can't hope to do better

933
00:54:38,090 --> 00:54:38,870
than H star.

934
00:54:38,940 --> 00:54:45,170
Another way of saying that is that if your hypothesis class

935
00:54:45,280 --> 00:54:46,600
is a class of all linear decision boundaries,

936
00:54:46,670 --> 00:54:49,170
that the data just can't be separated

937
00:54:49,280 --> 00:54:50,450
by any linear functions.

938
00:54:50,520 --> 00:54:52,910
So if even H star is really bad,

939
00:54:52,990 --> 00:54:57,650
then there's sort of it's unlikely then there's just not

940
00:54:57,730 --> 00:55:00,160
much hope that your learning algorithm could do even

941
00:55:00,240 --> 00:55:01,230
better than H star.

942
00:55:01,310 --> 00:55:06,090
So I actually prove this result in three steps.

943
00:55:06,210 --> 00:55:11,000
So the generalization error of H hat,

944
00:55:11,100 --> 00:55:11,940
the hypothesis I chose,

945
00:55:12,010 --> 00:55:15,610
this is going to be less than equal to that,

946
00:55:15,680 --> 00:55:24,830
actually let me number these equations, right.

947
00:55:24,880 --> 00:55:28,150
This is  because of equation one,

948
00:55:28,250 --> 00:55:33,320
because I see that epsilon of H hat and epsilon hat of H hat

949
00:55:33,400 --> 00:55:39,350
will then gamma of each other. Now because H star,

950
00:55:39,730 --> 00:55:46,630
excuse me, now by the definition

951
00:55:46,680 --> 00:55:52,180
of empirical risk minimization, H hat was chosen to

952
00:55:52,250 --> 00:55:54,860
minimize training error and so there can't be any

953
00:55:54,940 --> 00:55:57,300
hypothesis with lower training error than H hat.

954
00:55:57,370 --> 00:56:00,980
So the training error of H hat must be less than

955
00:56:01,060 --> 00:56:05,630
the equal to the training error of H star.

956
00:56:05,800 --> 00:56:08,810
So this is sort of by two,

957
00:56:08,920 --> 00:56:10,480
or by the definition of H hat,

958
00:56:10,590 --> 00:56:13,910
as the hypothesis that minimizes training error H hat.

959
00:56:14,050 --> 00:56:19,560
And the final step is

960
00:56:19,650 --> 00:56:22,640
I'm going to apply this uniform conversions result again.

961
00:56:22,750 --> 00:56:23,570
We know that

962
00:56:23,660 --> 00:56:26,270
epsilon hat of H star must be

963
00:56:26,390 --> 00:56:27,690
moving gamma of epsilon of H star.

964
00:56:27,770 --> 00:56:35,100
And so this is at most plus gamma.

965
00:56:35,210 --> 00:56:38,630
Then I have my original gamma there. Okay?

966
00:56:38,750 --> 00:56:41,990
And so this is by equation one again

967
00:56:41,990 --> 00:56:42,990
because, oh, excuse me ¨

968
00:56:43,440 --> 00:56:46,360
because I know the training error of H star

969
00:56:46,470 --> 00:56:47,360
must be moving gamma

970
00:56:47,470 --> 00:56:48,960
of the generalization error of H star.

971
00:56:49,070 --> 00:57:00,390
And so well, I'll just write this as plus two gamma.

972
00:57:00,460 --> 00:57:12,590
Okay? Yeah?

973
00:57:12,700 --> 00:57:25,010
Student:[inaudible]

974
00:57:25,120 --> 00:57:26,660
Instructor (Andrew Ng): Oh, okay. Let me just, well,

975
00:57:26,740 --> 00:57:28,350
let me write that down on this board.

976
00:57:28,430 --> 00:57:46,010
So actuallyactually let me think fit this in here.

977
00:57:46,320 --> 00:57:52,160
So epsilon hat of H is the training error of the hypothesis H.

978
00:57:52,270 --> 00:57:54,020
In other words, given the hypothesisa

979
00:57:54,100 --> 00:57:55,590
hypothesis is just a function

980
00:57:55,670 --> 00:57:57,130
right mapped from X or Y

981
00:57:57,210 --> 00:57:58,840
so epsilon hat of H

982
00:57:58,910 --> 00:58:00,570
is given the hypothesis H,

983
00:58:00,650 --> 00:58:03,890
what's the fraction of training examples it misclassifies?

984
00:58:03,960 --> 00:58:07,200
And generalization error of H,

985
00:58:07,310 --> 00:58:16,540
is given the hypothesis H

986
00:58:16,650 --> 00:58:22,230
if I sample another example from my distribution scripts D,

987
00:58:22,310 --> 00:58:24,850
what's the probability

988
00:58:24,920 --> 00:58:26,740
that H will misclassify that example?

989
00:58:26,810 --> 00:58:29,200
Does that make sense?

990
00:58:29,270 --> 00:58:32,060
Student:[Inaudible]?

991
00:58:32,130 --> 00:58:33,020
Instructor (Andrew Ng): Oh, okay.

992
00:58:33,100 --> 00:58:35,770
And H hat is the hypothesis that's chosen by empirical

993
00:58:35,850 --> 00:58:41,760
risk minimization. So when I talk about empirical risk

994
00:58:41,830 --> 00:58:44,140
minimization, is the algorithm that minimizes training error,

995
00:58:44,210 --> 00:58:46,170
and so epsilon hat of H

996
00:58:46,240 --> 00:58:47,330
is the training error of H,

997
00:58:47,410 --> 00:58:49,760
and so H hat is defined as the hypothesis

998
00:58:49,840 --> 00:58:54,970
that out of all hypotheses in my class script H, t

999
00:58:55,040 --> 00:58:58,770
he one that minimizes training error epsilon hat of H.

1000
00:58:58,880 --> 00:59:06,450
Okay? All right. Yeah?

1001
00:59:06,530 --> 00:59:12,640
Student:

1002
00:59:12,720 --> 00:59:13,410
Instructor (Andrew Ng): Yes it is.

1003
00:59:13,490 --> 00:59:16,750
Student:[Inaudible]

1004
00:59:16,820 --> 00:59:20,780
Instructor (Andrew Ng): I'll talk about that later.

1005
00:59:20,860 --> 00:59:28,760
So let me tie all these things together into a theorem.

1006
00:59:28,840 --> 00:59:44,080
Let there be a hypothesis class

1007
00:59:44,200 --> 00:59:52,230
Given with a finite set

1008
00:59:52,310 --> 00:59:53,470
of K hypotheses

1009
00:59:53,560 --> 00:59:54,800
and let any M delta be fixed.

1010
00:59:54,880 --> 01:00:00,340
Then  so I fixed M and delta,

1011
01:00:00,420 --> 01:00:03,240
so this will be the error bound form of the theorem

1012
01:00:03,310 --> 01:00:04,300
right?

1013
01:00:04,380 --> 01:00:08,350
Then with probability at least one minus delta.

1014
01:00:08,430 --> 01:00:10,350
We have that.

1015
01:00:10,440 --> 01:00:12,710
The generalization error of H hat

1016
01:00:12,810 --> 01:00:21,320
is less than or equal to the minimum over all hypotheses

1017
01:00:21,380 --> 01:00:35,730
in set H epsilon of H, plus two times, plus that. Okay?

1018
01:00:35,820 --> 01:00:40,300
So to prove this, well,

1019
01:00:40,380 --> 01:00:42,390
this term of course is just epsilon of H star.

1020
01:00:42,460 --> 01:00:44,030
And so to prove this

1021
01:00:44,090 --> 01:00:46,670
we set gamma to equal to that

1022
01:00:46,740 --> 01:00:50,200
this is two times the square root term.

1023
01:00:50,270 --> 01:00:54,220
To prove this theorem

1024
01:00:54,300 --> 01:00:56,650
we set gamma to equal to that square root term.

1025
01:00:56,720 --> 01:00:58,160
Say that again?

1026
01:00:58,230 --> 01:01:04,460
Student:[Inaudible].

1027
01:01:04,540 --> 01:01:08,380
Instructor (Andrew Ng): Wait. Say that again?

1028
01:01:08,440 --> 01:01:11,770
Student:[Inaudible].

1029
01:01:11,830 --> 01:01:14,990
Instructor (Andrew Ng): Oh, yes. Thank you.

1030
01:01:15,100 --> 01:01:17,990
That didn't make sense at all.

1031
01:01:18,060 --> 01:01:20,710
Thanks. Great.

1032
01:01:20,820 --> 01:01:22,040
So set gamma to that square root term,

1033
01:01:22,110 --> 01:01:28,550
and so we know equation one, right

1034
01:01:28,660 --> 01:01:31,080
from the previous board holds

1035
01:01:31,160 --> 01:01:32,470
with probability one minus delta.

1036
01:01:32,580 --> 01:01:33,800
Right. Equation one was the uniform conversions result

1037
01:01:33,880 --> 01:01:36,850
right, that well, IE.

1038
01:01:36,920 --> 01:01:38,550
This is equation one from the previous board, right

1039
01:01:38,620 --> 01:01:50,990
so set gamma equal to this we know that

1040
01:01:51,070 --> 01:01:54,030
we'll probably use one minus delta

1041
01:01:54,100 --> 01:01:55,570
this uniform conversions holds,

1042
01:01:55,640 --> 01:01:57,150
and whenever that holds,

1043
01:01:57,220 --> 01:01:59,050
that implies

1044
01:01:59,130 --> 01:02:01,550
you know,

1045
01:02:01,620 --> 01:02:04,790
I guess if we call this equation star

1046
01:02:04,870 --> 01:02:06,630
And whenever uniform conversions holds,

1047
01:02:06,710 --> 01:02:09,030
we showed again,

1048
01:02:09,130 --> 01:02:11,310
on the previous boards

1049
01:02:11,380 --> 01:02:13,070
that this result holds,

1050
01:02:13,150 --> 01:02:14,880
that generalization error of H hat

1051
01:02:14,940 --> 01:02:19,700
is less than two, generalization error of H star  plus two times gamma.

1052
01:02:19,780 --> 01:02:20,770
Okay?

1053
01:02:20,840 --> 01:02:23,450
And so that proves this theorem.

1054
01:02:23,530 --> 01:02:43,800
So this result sort of helps us

1055
01:02:43,910 --> 01:02:46,860
to quantify a little bit that bias variance tradeoff

1056
01:02:46,950 --> 01:02:52,680
that I talked about at the beginning of actually

1057
01:02:52,800 --> 01:02:54,030
near the very start of this lecture.

1058
01:02:54,110 --> 01:02:58,960
And in particular let's say

1059
01:02:59,040 --> 01:03:01,860
I have some hypothesis class script H,

1060
01:03:01,940 --> 01:03:05,670
that I'm using, maybe as a class of all linear functions

1061
01:03:05,740 --> 01:03:09,240
and linear regression, and logistic regression with just the

1062
01:03:09,320 --> 01:03:12,920
linear features. And let's say I'm considering

1063
01:03:13,000 --> 01:03:16,510
switching to some new class H prime

1064
01:03:16,590 --> 01:03:18,770
by having more features.

1065
01:03:18,820 --> 01:03:20,030
So lets say this is linear

1066
01:03:20,100 --> 01:03:24,110
and this is quadratic

1067
01:03:24,190 --> 01:03:29,130
so the class of all linear functions

1068
01:03:29,210 --> 01:03:32,990
and the subset of the class of all quadratic functions,

1069
01:03:33,100 --> 01:03:38,370
and so H is the subset of H prime.

1070
01:03:38,440 --> 01:03:42,800
And let's say I'm considering, instead of using my

1071
01:03:42,870 --> 01:03:44,890
linear hypothesis class, let's say I'm considering

1072
01:03:44,960 --> 01:03:48,760
switching to a quadratic hypothesis class

1073
01:03:48,830 --> 01:03:50,180
or switching to a larger hypothesis class.

1074
01:03:50,250 --> 01:03:51,310
Then what are the tradeoffs involved?

1075
01:03:51,410 --> 01:03:52,210
Well, I proved this only for finite hypothesis classes,

1076
01:03:52,320 --> 01:03:54,310
but we'll see that something very similar holds for

1077
01:03:54,380 --> 01:03:55,270
infinite hypothesis classes too.

1078
01:03:55,350 --> 01:03:56,670
But the tradeoff is

1079
01:03:56,740 --> 01:03:59,650
what if I switch from H to H prime,

1080
01:03:59,720 --> 01:04:00,600
or I switch from

1081
01:04:00,680 --> 01:04:05,650
linear to quadratic functions.

1082
01:04:05,730 --> 01:04:06,480
Then epsilon of H star will become better

1083
01:04:06,560 --> 01:04:11,680
because the best hypothesis in my hypothesis class will

1084
01:04:11,760 --> 01:04:13,460
become better. The best quadratic function

1085
01:04:13,560 --> 01:04:17,340
by best I mean in the sense of generalization error  the

1086
01:04:17,420 --> 01:04:18,390
hypothesis function

1087
01:04:18,470 --> 01:04:24,210
the quadratic function with the lowest generalization error

1088
01:04:24,290 --> 01:04:29,130
has to have equal or more likely lower generalization error

1089
01:04:29,240 --> 01:04:30,800
than the best linear function.

1090
01:04:30,920 --> 01:04:33,570
So by switching to a more complex hypothesis class

1091
01:04:33,640 --> 01:04:35,490
you can get this first term as you go down.

1092
01:04:35,570 --> 01:04:41,380
But what I pay for then is that K will increase.

1093
01:04:41,450 --> 01:04:45,380
By switching to a larger hypothesis class,

1094
01:04:45,490 --> 01:04:48,650
the first term will go down,

1095
01:04:48,760 --> 01:04:49,900
but the second term will increase

1096
01:04:49,970 --> 01:04:51,770
because I now have a larger class of hypotheses

1097
01:04:51,880 --> 01:04:55,490
and so the second term K will increase.

1098
01:04:55,600 --> 01:05:00,030
And so this is sometimes called the bias

1099
01:05:00,110 --> 01:05:01,960
this is usually called the bias variance tradeoff.

1100
01:05:02,050 --> 01:05:04,600
Whereby going to larger hypothesis class

1101
01:05:04,670 --> 01:05:07,250
maybe I have the hope for finding a better function,

1102
01:05:07,330 --> 01:05:12,730
that my risk of sort of not fitting my model so accurately

1103
01:05:12,840 --> 01:05:14,380
also increases, and that's because

1104
01:05:14,460 --> 01:05:18,150
illustrated by the second term going up

1105
01:05:18,230 --> 01:05:23,150
when the size of your hypothesis, when K goes up.

1106
01:05:23,230 --> 01:05:33,670
And so speaking very loosely,

1107
01:05:33,780 --> 01:05:41,290
we can think of this first term as corresponding maybe to

1108
01:05:41,370 --> 01:05:43,650
the bias of the learning algorithm, or the bias of the

1109
01:05:43,730 --> 01:05:46,810
hypothesis class. And you canagain speaking very loosely, t

1110
01:05:46,890 --> 01:05:54,100
hink of the second term as corresponding to the variance in  your hypothesis,

1111
01:05:54,190 --> 01:05:55,170
in other words

1112
01:05:55,240 --> 01:05:57,090
how well you can actually fit a hypothesis in the

1113
01:05:57,170 --> 01:06:01,940
how well you actually fit this hypothesis class to the data.

1114
01:06:02,020 --> 01:06:04,750
And by switching to a more complex hypothesis class,

1115
01:06:04,890 --> 01:06:06,440
your variance increases

1116
01:06:06,590 --> 01:06:07,460
and your bias decreases.

1117
01:06:07,600 --> 01:06:10,450
As a note of warning,

1118
01:06:10,600 --> 01:06:12,930
it turns out that if you take like a statistics class

1119
01:06:13,080 --> 01:06:14,630
you've seen definitions of bias and variance,

1120
01:06:14,780 --> 01:06:16,870
which are often defined in terms of squared error

1121
01:06:17,010 --> 01:06:20,320
or something. It turns out that for classification problems,

1122
01:06:20,480 --> 01:06:23,200
there actually is no universally accepted formal definition

1123
01:06:23,350 --> 01:06:27,700
of bias and variance for classification problems.

1124
01:06:27,850 --> 01:06:28,970
For regression problems,

1125
01:06:29,070 --> 01:06:35,840
there is this square error definition.

1126
01:06:35,950 --> 01:06:36,720
For classification problems

1127
01:06:36,800 --> 01:06:41,670
it turns out there've been several competing proposals

1128
01:06:41,750 --> 01:06:43,010
for definitions of bias and variance.

1129
01:06:43,100 --> 01:06:44,560
So when I say bias and variance here,

1130
01:06:44,650 --> 01:06:46,580
think of these as very loose,

1131
01:06:46,660 --> 01:06:48,360
informal, intuitive definitions,

1132
01:06:48,450 --> 01:06:49,960
and not formal definitions.

1133
01:06:50,060 --> 01:07:17,520
Okay. The cartoon associated with intuition

1134
01:07:17,590 --> 01:07:20,090
I just said would be as follows:

1135
01:07:20,170 --> 01:07:28,570
Let's say and everything about the plot

1136
01:07:28,680 --> 01:07:30,310
will be for a fixed value of M,

1137
01:07:30,400 --> 01:07:31,920
for a fixed training set size M.

1138
01:07:32,000 --> 01:07:36,120
Vertical axis I'll plot ever

1139
01:07:36,230 --> 01:07:38,240
and on the horizontal axis I'll plot model complexity.

1140
01:07:38,320 --> 01:07:47,480
And by model complexity I mean sort of degree of

1141
01:07:47,590 --> 01:08:01,760
polynomial, size of your hypothesis class script H etc.

1142
01:08:01,870 --> 01:08:12,510
It actually turns out, you remember the bandwidth

1143
01:08:12,590 --> 01:08:13,960
parameter from locally weighted linear regression,

1144
01:08:14,040 --> 01:08:15,700
that also has a similar effect in controlling

1145
01:08:15,780 --> 01:08:16,690
how complex your model is.

1146
01:08:16,770 --> 01:08:19,280
Model complexity [inaudible] polynomial I guess.

1147
01:08:19,350 --> 01:08:25,150
So the more complex your model,

1148
01:08:25,230 --> 01:08:26,220
the better your training error,

1149
01:08:26,310 --> 01:08:31,860
and so your training error will tend to zero

1150
01:08:31,960 --> 01:08:33,820
as you increase the complexity of your model

1151
01:08:33,900 --> 01:08:35,450
because the more complete your model

1152
01:08:35,530 --> 01:08:39,850
the better you can fit your training set.

1153
01:08:39,920 --> 01:08:45,910
But because of this bias variance tradeoff,

1154
01:08:45,990 --> 01:08:54,670
you find that generalization error will come down

1155
01:08:54,790 --> 01:08:56,330
for a while and then it will go back up.

1156
01:08:56,410 --> 01:08:58,660
And this regime on the left is

1157
01:08:58,740 --> 01:09:00,240
when you're underfitting the data

1158
01:09:00,330 --> 01:09:10,690
or when you have high bias. And this regime on the right is

1159
01:09:10,800 --> 01:09:18,440
when you have high variance or you're overfitting the data.

1160
01:09:18,510 --> 01:09:20,230
Okay?

1161
01:09:20,320 --> 01:09:21,620
And this is why

1162
01:09:21,690 --> 01:09:24,180
a model of sort of intermediate complexity,

1163
01:09:24,270 --> 01:09:28,650
somewhere here if often preferable to if

1164
01:09:28,770 --> 01:09:34,530
and minimize generalization error. Okay?

1165
01:09:34,610 --> 01:09:36,700
So that's just a cartoon.

1166
01:09:36,780 --> 01:09:38,900
In the next lecture we'll actually talk about

1167
01:09:38,990 --> 01:09:40,690
the number of algorithms for trying

1168
01:09:40,780 --> 01:09:42,460
to automatically select model complexities,

1169
01:09:42,530 --> 01:09:46,210
say to get you as close as possible to this minimum

1170
01:09:46,280 --> 01:09:50,290
to this area of minimized generalization error.

1171
01:09:50,410 --> 01:10:12,180
The last thing I want to do is

1172
01:10:12,300 --> 01:10:16,820
actually going back to the theorem I wrote out,

1173
01:10:16,900 --> 01:10:19,830
I just want to take that theorem well,

1174
01:10:19,930 --> 01:10:21,400
so the theorem I wrote out was

1175
01:10:21,480 --> 01:10:22,440
about the error theorem

1176
01:10:22,520 --> 01:10:23,380
it says

1177
01:10:23,460 --> 01:10:29,560
an error bound theorem this says for fixed M

1178
01:10:29,640 --> 01:10:30,770
delta where probability one minus delta,

1179
01:10:30,850 --> 01:10:31,760
I get a bound on gamma,

1180
01:10:31,840 --> 01:10:32,620
which is what this term is.

1181
01:10:32,700 --> 01:10:34,010
So the very last thing I wanna do today is

1182
01:10:34,080 --> 01:10:35,430
just come back to this theorem and write out a corollary

1183
01:10:35,540 --> 01:10:36,570
where I'm gonna fix gamma,

1184
01:10:36,680 --> 01:10:37,930
I'm gonna fix my error bound,

1185
01:10:38,040 --> 01:10:40,260
and fix delta and solve for M.

1186
01:10:40,350 --> 01:10:42,160
And if you do that,

1187
01:10:42,250 --> 01:10:49,210
you get the following corollary:

1188
01:10:49,290 --> 01:10:50,140
Let H

1189
01:10:50,220 --> 01:10:54,460
be fixed with K hypotheses

1190
01:10:54,560 --> 01:11:02,680
and let any delta and gamma be fixed.

1191
01:11:02,790 --> 01:11:13,830
Then in order to guarantee that,

1192
01:11:13,910 --> 01:11:26,720
let's say I want a guarantee that the generalization error of

1193
01:11:26,830 --> 01:11:29,870
the hypothesis I choose with empirical risk minimization,

1194
01:11:29,980 --> 01:11:35,320
that this is at most two times gamma worse than the best

1195
01:11:35,390 --> 01:11:37,080
possible error I could obtain with this hypothesis class.

1196
01:11:37,150 --> 01:11:41,220
Lets say I want this to hold true with probability

1197
01:11:41,300 --> 01:11:55,490
at least one minus delta, then it suffices that M is to that.

1198
01:11:55,580 --> 01:12:00,660
Okay?

1199
01:12:00,750 --> 01:12:03,000
And this is sort of solving for the error bound for M.

1200
01:12:03,080 --> 01:12:07,350
One thing we're going to convince yourselves

1201
01:12:07,420 --> 01:12:12,450
of the easy part of this is if you set that term gamma

1202
01:12:12,530 --> 01:12:13,930
and solve for M you will get this.

1203
01:12:13,990 --> 01:12:17,100
One thing I want you to go home and sort of convince

1204
01:12:17,170 --> 01:12:19,690
yourselves of is that this result really holds true.

1205
01:12:19,760 --> 01:12:21,860
That this really logically follows

1206
01:12:21,930 --> 01:12:22,990
from the theorem

1207
01:12:23,100 --> 01:12:24,280
we've proved.

1208
01:12:24,350 --> 01:12:24,970
In other words,

1209
01:12:25,040 --> 01:12:27,640
you can take that formula we wrote and solve for M

1210
01:12:27,720 --> 01:12:28,970
because this is the formula you get for M,

1211
01:12:29,040 --> 01:12:30,820
that's just that's the easy part.

1212
01:12:30,930 --> 01:12:33,170
That once you go back and convince yourselves

1213
01:12:33,250 --> 01:12:36,510
that this theorem is a true fact and that it does indeed

1214
01:12:36,570 --> 01:12:37,820
logically follow from the other one.

1215
01:12:37,870 --> 01:12:38,800
In particular,

1216
01:12:38,910 --> 01:12:41,160
make sure that if you solve for that

1217
01:12:41,230 --> 01:12:42,640
you really get M grading equals this,

1218
01:12:42,720 --> 01:12:45,150
and why is this M grading that

1219
01:12:45,260 --> 01:12:46,270
and not M less equal two,

1220
01:12:46,310 --> 01:12:48,300
and just make sure, I can write this down a

1221
01:12:48,400 --> 01:12:49,770
nd it sounds plausible why don't you just go back

1222
01:12:49,870 --> 01:12:51,840
and convince yourself this is really true. Okay?

1223
01:12:51,910 --> 01:12:55,400
And it turns out that

1224
01:12:55,510 --> 01:12:57,840
when we prove these bounds in learning theory

1225
01:12:57,910 --> 01:13:00,990
it turns out that very often the constants are sort of loose.

1226
01:13:01,050 --> 01:13:02,050
So it turns out that

1227
01:13:02,160 --> 01:13:04,780
when we prove these bounds usually we're interested

1228
01:13:04,850 --> 01:13:08,040
usually we're not very interested in the constants,

1229
01:13:08,120 --> 01:13:11,220
and so I write this

1230
01:13:11,320 --> 01:13:16,920
as big O of one over gamma squared, log K over delta,

1231
01:13:16,990 --> 01:13:22,600
and again, the key step in this is that the dependence on

1232
01:13:22,670 --> 01:13:25,620
M with the size of the hypothesis class is logarithmic.

1233
01:13:25,720 --> 01:13:28,930
And this will be very important later

1234
01:13:29,000 --> 01:13:32,760
when we talk about infinite hypothesis classes.

1235
01:13:32,840 --> 01:13:34,280
Okay?

1236
01:13:34,360 --> 01:13:49,830
Any questions about this? No? Okay, cool.

1237
01:13:49,910 --> 01:13:52,460
So next lecture we'll come back,

1238
01:13:52,540 --> 01:13:54,110
we'll actually start from this result again.

1239
01:13:54,190 --> 01:13:54,890
Remember this.

1240
01:13:54,970 --> 01:13:56,560
I'll write this down as the first thing I do in the next

1241
01:13:56,650 --> 01:13:59,680
lecture and we'll generalize these to infinite hypothesis

1242
01:13:59,800 --> 01:14:01,070
classes and then talk about

1243
01:14:01,150 --> 01:14:02,280
practical algorithms for model spectrum.

1244
01:14:02,380 --> 01:14:03,610
So I'll see you guys in a couple days.

