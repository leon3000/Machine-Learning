1
00:00:23,040 --> 00:00:25,210
MachineLearning-Lecture18

2
00:00:25,360 --> 00:00:29,190
Okay. Welcome back. What I want to do today

3
00:00:29,260 --> 00:00:32,710
is talk about one of my favorite algorithms

4
00:00:32,910 --> 00:00:36,270
for controlling MDPs that I think is one of

5
00:00:36,380 --> 00:00:38,330
the more elegant and efficient

6
00:00:38,450 --> 00:00:40,040
and powerful algorithms that I know of.

7
00:00:40,120 --> 00:00:45,670
So what I'll do is I'll first start by

8
00:00:45,760 --> 00:00:47,860
talking about a couple variations of MDPs that

9
00:00:47,940 --> 00:00:50,680
are slightly different from the MDP

10
00:00:50,740 --> 00:00:52,500
definition you've seen so far.

11
00:00:52,590 --> 00:00:54,650
These are pretty common variations.

12
00:00:54,730 --> 00:00:56,650
One is state action rewards, and the other is

13
00:00:56,720 --> 00:01:01,700
horizon MDPs. Using this semi-modified

14
00:01:01,780 --> 00:01:04,810
definition of an MDP, I'll talk about linear

15
00:01:04,890 --> 00:01:07,960
dynamical systems. I'll spend a little bit of time

16
00:01:08,050 --> 00:01:09,860
talking about models within dynamical

17
00:01:09,940 --> 00:01:11,830
systems, and then talk about LQR, or linear

18
00:01:11,900 --> 00:01:14,880
quadratic regulation control, which will lead us

19
00:01:14,980 --> 00:01:17,460
to some kind of [inaudible] equation,

20
00:01:17,540 --> 00:01:20,410
which is something we will solve in order to do

21
00:01:20,490 --> 00:01:23,480
LQR controls.

22
00:01:25,790 --> 00:01:30,310
So just to recap, and we've seen this definition

23
00:01:30,400 --> 00:01:33,810
many times now. We've been defining an

24
00:01:33,890 --> 00:01:43,570
MDP as [inaudible] states actions, states

25
00:01:43,650 --> 00:01:45,720
transition probabilities, discount factor reward function

26
00:01:45,810 --> 00:01:54,750
where the discount factor's a gamma

27
00:01:54,810 --> 00:01:57,710
number between zero and one. And R, the

28
00:01:57,780 --> 00:02:01,340
reward function, was the function mapping

29
00:02:01,430 --> 00:02:03,510
from the states, the rewards was the

30
00:02:03,590 --> 00:02:05,670
function mapping from the states, the real

31
00:02:05,750 --> 00:02:07,310
numbers.

32
00:02:07,390 --> 00:02:11,580
So we had value iteration, which would do this.

33
00:02:11,660 --> 00:02:23,550
So after a while, the value of the iteration

34
00:02:23,650 --> 00:02:29,030
will cause V to convert to V star. Then having

35
00:02:29,110 --> 00:02:31,240
found the optimal value function, if you

36
00:02:31,330 --> 00:02:33,500
compute the optimal policy by taking

37
00:02:33,590 --> 00:02:35,710
essentially the orgmax of this equation above.

38
00:02:35,790 --> 00:02:45,580
orgmax of A of that thing.

39
00:02:45,670 --> 00:02:53,670
So in value iteration, as you iterate of this

40
00:02:53,740 --> 00:02:55,610
you know, perform this update, the function

41
00:02:55,670 --> 00:02:58,830
V will [inaudible] convert to V star.

42
00:02:58,910 --> 00:03:01,570
So there won't be so without defining

43
00:03:01,630 --> 00:03:03,350
the number of iterations, you get closer

44
00:03:03,420 --> 00:03:05,270
and closer to V star. This actually converge

45
00:03:05,350 --> 00:03:08,950
exponentially quickly to V star.

46
00:03:09,050 --> 00:03:11,470
We will never exactly convert to V star

47
00:03:11,550 --> 00:03:13,310
and define the number of iterations.

48
00:03:13,400 --> 00:03:17,640
So what I want to do now is describe a couple

49
00:03:17,750 --> 00:03:19,770
of common variations of MDPs that we

50
00:03:19,860 --> 00:03:23,970
slightly different definitions of. First the reward

51
00:03:24,060 --> 00:03:25,960
function and then second, we'll do

52
00:03:26,050 --> 00:03:29,540
something slightly different from just counting.

53
00:03:29,630 --> 00:03:34,070
Then remember in the last lecture, I said

54
00:03:34,170 --> 00:03:37,090
that for infinite state of continuously in MDPs,

55
00:03:37,170 --> 00:03:39,950
we couldn't apply the most straightforward

56
00:03:40,030 --> 00:03:42,330
version of value iteration because if you have a

57
00:03:42,410 --> 00:03:44,530
continuous state MDP, we need to use

58
00:03:44,640 --> 00:03:47,750
some approximations of the optimal value function.

59
00:03:47,840 --> 00:03:51,970
The [inaudible] later in this lecture, I'll talk

60
00:03:52,050 --> 00:03:54,650
about a special case of MDPs, where you can

61
00:03:54,750 --> 00:03:56,850
actually represent the value function exactly,

62
00:03:56,930 --> 00:03:59,790
even if you have an infinite-state space or

63
00:03:59,880 --> 00:04:02,590
even if you have a continuous-state space. I'll

64
00:04:02,690 --> 00:04:04,800
actually do that, talk about these special

65
00:04:04,900 --> 00:04:07,450
constants of infinite-state MDPs, using this new

66
00:04:07,540 --> 00:04:10,050
variation of the reward function and the

67
00:04:10,170 --> 00:04:12,170
alternative to just counting, so start to make the

68
00:04:12,260 --> 00:04:14,050
formulation a little easier.

69
00:04:14,150 --> 00:04:18,600
So the first variation I want to talk about is

70
00:04:18,710 --> 00:04:26,450
State-action Rewards. So I'm going to change the

71
00:04:26,550 --> 00:04:28,890
definition of the reward function. If this turns

72
00:04:28,990 --> 00:04:31,700
out, it won't be a huge deal. In particular,

73
00:04:31,810 --> 00:04:34,100
I change reward function to be a function

74
00:04:34,210 --> 00:04:41,740
mapping from a state action pair to the real numbers.

75
00:04:41,820 --> 00:04:43,340
What I mean by this is just the following.

76
00:04:43,400 --> 00:04:49,410
You sell off in some state in zero. You take an

77
00:04:49,490 --> 00:04:51,960
action A zero as a result of your state of action

78
00:04:52,040 --> 00:04:53,690
choice. You transition to some new state,

79
00:04:53,770 --> 00:04:56,700
S1. You take some action, A1. You transition to

80
00:04:56,770 --> 00:04:58,350
some new state, S2. You take some

81
00:04:58,410 --> 00:05:00,690
action, A2, and so on. so this is a,  it's a

82
00:05:00,770 --> 00:05:02,910
state action sequence that you see.

83
00:05:02,990 --> 00:05:06,680
So in an MPP where you have a state action

84
00:05:06,770 --> 00:05:10,300
reward, your total payoff is now defined as

85
00:05:10,380 --> 00:05:15,940
this, where your reward function is now a

86
00:05:16,030 --> 00:05:18,780
function both of the current state and of the

87
00:05:18,880 --> 00:05:21,010
action you took in the current state.

88
00:05:21,090 --> 00:05:27,470
So this is my total payoff.

89
00:05:27,540 --> 00:05:31,340
Then as usual, my goal will be to find a policy

90
00:05:31,420 --> 00:05:33,680
to find the function mapping from the

91
00:05:33,730 --> 00:05:36,580
state's actions, so that when I execute that

92
00:05:36,660 --> 00:05:39,580
policy, I can maximize the expected value of

93
00:05:39,680 --> 00:05:44,900
my total payoff. So this definition, it actually

94
00:05:44,960 --> 00:05:46,940
turns out that given an MDP with state

95
00:05:47,010 --> 00:05:48,620
action rewards, you can actually

96
00:05:48,700 --> 00:05:50,720
so by filling in with the definitions

97
00:05:50,810 --> 00:05:52,340
of the states, you can actually reduce

98
00:05:52,420 --> 00:05:53,520
this back to an MDP

99
00:05:53,610 --> 00:05:55,850
with only rewards that function in the states.

100
00:05:55,950 --> 00:05:57,510
that may or may not be obvious.

101
00:05:57,590 --> 00:06:01,290
Don't worry if it isn't. But using state action

102
00:06:01,380 --> 00:06:03,400
rewards allows you to more directly model

103
00:06:03,480 --> 00:06:06,660
problems in which different actions,

104
00:06:06,730 --> 00:06:08,150
we have different costs.

105
00:06:08,250 --> 00:06:10,390
So a running example is the robot.

106
00:06:10,450 --> 00:06:14,120
so maybe for a robot, and it's more costly for

107
00:06:14,200 --> 00:06:16,250
the robot to move than for it to stay still.

108
00:06:16,330 --> 00:06:19,760
If you give an action to stay still, and the action

109
00:06:19,850 --> 00:06:22,260
to stay still may have a lower cost because

110
00:06:22,340 --> 00:06:24,060
you're not using a battery power as you would

111
00:06:24,140 --> 00:06:25,500
recharge it for that action.

112
00:06:25,590 --> 00:06:30,250
Another example would be actually, another

113
00:06:30,330 --> 00:06:34,010
navigation example would be if you have

114
00:06:34,100 --> 00:06:37,720
an outdoor vehicle. You need to drive over

115
00:06:37,810 --> 00:06:41,870
some sort of outdoor terrain, like very rough

116
00:06:41,970 --> 00:06:44,380
rocks or driving over grass. It may be costly,

117
00:06:44,480 --> 00:06:46,580
more difficult, than driving over, say, a

118
00:06:46,660 --> 00:06:49,780
paved road. So you may assign an action that

119
00:06:49,880 --> 00:06:54,600
requires driving over grass or driving over

120
00:06:54,680 --> 00:06:56,620
rocks to be more costly

121
00:06:56,710 --> 00:06:58,340
than driving over paved road.

122
00:07:05,060 --> 00:07:06,800
So this really isn't a huge change to the

123
00:07:06,860 --> 00:07:11,660
definition of an MDP. I won't really bother to

124
00:07:11,740 --> 00:07:16,110
justify this a lot, but [inaudible] equations is

125
00:07:16,200 --> 00:07:21,000
generalizing the way that you probably

126
00:07:21,090 --> 00:07:27,010
expect it. V star of S is now equal to that.

127
00:07:36,900 --> 00:07:39,100
So previously, when the reward function was

128
00:07:39,170 --> 00:07:41,090
just a function of the state, S, we could take

129
00:07:41,140 --> 00:07:46,080
the max and push it in here. But now that the

130
00:07:46,170 --> 00:07:47,800
rewards is a function of the action you're

131
00:07:47,880 --> 00:07:49,930
taking as well, the max comes outside. So this

132
00:07:50,030 --> 00:07:52,210
says that your expected total payoff,

133
00:07:52,300 --> 00:07:54,360
starting from the state, as executing the auto policy, is

134
00:07:54,450 --> 00:07:58,280
equal to first your immediate reward,

135
00:07:58,380 --> 00:08:01,410
RFSA, for executing some action, A, in state S.

136
00:08:01,500 --> 00:08:04,980
Then plus gamma times your future expected

137
00:08:05,070 --> 00:08:10,040
total payoff. So this is your expected total

138
00:08:10,140 --> 00:08:12,500
payoff if you take the action, A, from the

139
00:08:12,580 --> 00:08:16,250
current state. So while these [inaudible] optimal

140
00:08:16,340 --> 00:08:18,180
value functions. So your actually optimal

141
00:08:18,260 --> 00:08:20,520
expected total payoff is the max of all actions

142
00:08:20,600 --> 00:08:22,560
of this thing on the right.

143
00:08:22,660 --> 00:08:28,650
Let's see. Value iteration,

144
00:08:28,740 --> 00:08:31,660
which I'm abbreviating VI,

145
00:08:34,660 --> 00:08:36,180
is really the same algorithm. B of S

146
00:08:36,260 --> 00:08:42,290
is updated as max over A, RFSA, same thing.

147
00:08:42,380 --> 00:08:44,350
just lay it on the right-hand side of

148
00:08:44,440 --> 00:08:48,570
those equations be updating V of S using

149
00:08:48,650 --> 00:08:50,330
[inaudible] equations. Again, you get value

150
00:08:50,420 --> 00:08:52,260
iteration, exactly the same way.

151
00:08:52,380 --> 00:08:58,720
Then finally, having found the optimal value

152
00:08:58,820 --> 00:09:02,210
function, V star, using the value iteration

153
00:09:02,300 --> 00:09:04,900
algorithm, you can then compute the optimal

154
00:09:04,980 --> 00:09:08,830
policy, pi star of S as same as before. The

155
00:09:08,940 --> 00:09:12,470
best action to take in the state, S, is the action,

156
00:09:12,570 --> 00:09:14,640
A, that maximizes the thing on the right-

157
00:09:14,730 --> 00:09:30,660
hand side. So having used value iteration to

158
00:09:30,740 --> 00:09:34,600
compute the optimal value function, you can

159
00:09:34,700 --> 00:09:36,880
then find pi star using that.

160
00:09:36,980 --> 00:09:46,000
So this was the easier of the two variations of

161
00:09:46,110 --> 00:09:49,560
MDPs so far. Any questions?

162
00:09:49,650 --> 00:09:51,290
Actually, are there questions about this?

163
00:10:00,970 --> 00:10:05,230
So the other variation, the other alternative

164
00:10:05,320 --> 00:10:12,060
definition, will be finite horizon MDPs.

165
00:10:15,710 --> 00:10:22,290
So finite horizon MDP comprises of the

166
00:10:22,380 --> 00:10:25,580
[inaudible] SA state transition probabilities

167
00:10:25,670 --> 00:10:31,060
with these, and the parameter T and the

168
00:10:31,170 --> 00:10:38,710
reward function. Here, T is a parameter called

169
00:10:38,810 --> 00:10:43,470
the horizon time. Concretely, what this

170
00:10:43,560 --> 00:10:47,220
really means is that we'll be taking actions in

171
00:10:47,330 --> 00:10:51,100
the MDP only for a total of capital T times

172
00:10:51,190 --> 00:10:53,590
this. So we won't use this counting anymore.

173
00:10:53,680 --> 00:10:59,780
[Audio cuts out]

174
00:10:59,860 --> 00:11:04,060
In some state as zero, take action A0. Get to some

175
00:11:04,150 --> 00:11:07,620
other state S1, take action A1 and so on.

176
00:11:07,710 --> 00:11:11,420
Eventually, you get to some state, STAT

177
00:11:11,500 --> 00:11:16,950
after T times [inaudible]. So my total payoff, now, will

178
00:11:17,050 --> 00:11:23,270
be given by this sum from time zero up to time

179
00:11:23,360 --> 00:11:25,490
T of my sum over rewards. Okay?

180
00:11:25,590 --> 00:11:33,350
My goal, as usual so this is my total payoff.

181
00:11:33,460 --> 00:11:36,160
My goal, as usual, is to maximize the expected

182
00:11:36,280 --> 00:11:38,000
value of my total payoff. We want to

183
00:11:38,100 --> 00:11:40,090
come up with a policy to maximize the

184
00:11:40,200 --> 00:11:42,830
expected value of this total payoff. The key

185
00:11:42,940 --> 00:11:46,060
difference is that the world only will exist

186
00:11:46,170 --> 00:11:48,540
[inaudible], and after that, there's no more

187
00:11:48,640 --> 00:11:50,590
rewards to be corrected.

188
00:11:50,720 --> 00:11:53,990
So this turns out to be [inaudible] of a

189
00:11:54,090 --> 00:11:55,960
difference because it turns out that the optimal

190
00:11:56,070 --> 00:12:07,860
policy may be non-stationary. The term,

191
00:12:07,950 --> 00:12:10,820
stationary, means that it doesn't depend on time.

192
00:12:10,930 --> 00:12:13,870
Non-stationary means that it may depend on

193
00:12:13,960 --> 00:12:18,350
time. So non-stationary roughly means that

194
00:12:18,450 --> 00:12:21,360
my optimal action to take will be different for

195
00:12:21,440 --> 00:12:23,240
different time steps. That's what non-

196
00:12:23,340 --> 00:12:24,640
stationary means.

197
00:12:24,740 --> 00:12:29,310
Just as an example of that, imagine that we

198
00:12:29,400 --> 00:12:32,280
have some robot. Let's say the robot is here.

199
00:12:32,380 --> 00:12:38,620
Let's say that there's a great sell over

200
00:12:38,710 --> 00:12:43,470
there with a plus one reward. Much further

201
00:12:43,590 --> 00:12:47,820
away, there's a plus ten reward. So depending

202
00:12:47,920 --> 00:12:49,650
on how much time you have left on the

203
00:12:49,750 --> 00:12:52,370
clock, it may be better to go after the plus one

204
00:12:52,470 --> 00:12:54,120
or the plus ten reward.

205
00:12:54,220 --> 00:12:57,360
If it's still early in the game, you still have a lot

206
00:12:57,440 --> 00:12:59,230
of time, it may be better to head toward

207
00:12:59,320 --> 00:13:01,190
the plus-ten rewards junction and get a much

208
00:13:01,270 --> 00:13:04,220
larger reward. If you only have a couple of

209
00:13:04,320 --> 00:13:06,650
time sets left, if the clock has nearly reached

210
00:13:06,740 --> 00:13:09,060
the time, capital T, then you may not have

211
00:13:09,160 --> 00:13:11,040
enough time to get to a plus ten reward. You've

212
00:13:11,120 --> 00:13:13,340
be better off heading for the plus one

213
00:13:13,430 --> 00:13:14,830
reward that's much more close by.

214
00:13:14,930 --> 00:13:17,910
So what this example illustrates is that when

215
00:13:18,030 --> 00:13:21,300
you're in that state, the best action to take

216
00:13:21,390 --> 00:13:23,610
could be to go left or to go right, depending on

217
00:13:23,720 --> 00:13:25,540
what time it is. So just as an example,

218
00:13:25,640 --> 00:13:28,470
illustrating how the actually policy can be

219
00:13:28,580 --> 00:13:29,900
non-stationary.

220
00:13:30,020 --> 00:13:39,160
In fact, since we have non-stationary policies

221
00:13:39,280 --> 00:13:44,680
do next, I'm going to allow non-stationary

222
00:13:44,780 --> 00:13:48,530
transition probabilities as well. So I'll just

223
00:13:48,640 --> 00:13:52,640
write that up there. What I mean is that so far,

224
00:13:52,770 --> 00:13:55,600
assuming that the state ST plus one, is

225
00:13:55,750 --> 00:13:57,760
joined from the state transition probabilities

226
00:13:57,870 --> 00:14:01,150
index by the previous states and the

227
00:14:01,250 --> 00:14:03,120
previous action.

228
00:14:03,230 --> 00:14:04,950
I've been assuming that these state transition

229
00:14:05,060 --> 00:14:06,710
probabilities are the same for all times.

230
00:14:06,840 --> 00:14:08,390
So I want to say in some state and take some

231
00:14:08,520 --> 00:14:11,520
action, the distribution of an innate state doesn't

232
00:14:11,630 --> 00:14:16,940
matter. It doesn't depend on time. So I'm going

233
00:14:17,020 --> 00:14:19,550
to allow a study more general definition

234
00:14:19,690 --> 00:14:23,090
as well, in which we have non-stationary state

235
00:14:23,200 --> 00:14:25,930
transition probabilities so that the chance

236
00:14:26,060 --> 00:14:28,600
of where you end up [inaudible] may also

237
00:14:28,720 --> 00:14:32,310
depend on what time it is.

238
00:14:32,430 --> 00:14:35,510
So as examples of this non-stationary state

239
00:14:35,630 --> 00:14:39,790
transition probabilities, one example would be

240
00:14:39,900 --> 00:14:43,830
if you model flying an aircraft over a long

241
00:14:43,940 --> 00:14:46,250
distance. Then as the aircraft flies, you burn

242
00:14:46,370 --> 00:14:48,970
fuel and become lighter. So the dynamics of the

243
00:14:49,100 --> 00:14:50,910
aircraft actually change over time. The

244
00:14:51,020 --> 00:14:53,240
mass of the aircraft can change significantly

245
00:14:53,360 --> 00:14:55,920
over time as you burn fuel. So depending on

246
00:14:56,060 --> 00:14:58,180
what time it is, your mixed state could actually

247
00:14:58,300 --> 00:15:02,690
depend on not only your current state and

248
00:15:02,820 --> 00:15:05,030
your action, but also on how much fuel you

249
00:15:05,150 --> 00:15:06,820
burn, therefore, what time it is.

250
00:15:06,940 --> 00:15:09,230
Other examples, another aerospace one, is if

251
00:15:09,370 --> 00:15:12,230
you have the weather forecast for the next

252
00:15:12,380 --> 00:15:16,210
24 hours, say, you know what the winds and

253
00:15:16,350 --> 00:15:17,970
precipitation are going to be like over the

254
00:15:18,100 --> 00:15:20,020
next 24 hours. Then again, if you fly the aircraft

255
00:15:20,180 --> 00:15:22,720
from, say, here to New York, it may cost

256
00:15:22,840 --> 00:15:25,660
different amounts to fly different routes at

257
00:15:25,820 --> 00:15:28,130
different times. Maybe flying over the

258
00:15:28,250 --> 00:15:31,120
Rockies may cost different amounts, depending

259
00:15:31,250 --> 00:15:33,520
on whether you do it now, when there's

260
00:15:33,640 --> 00:15:35,190
really great weather, or if you do it a few hours

261
00:15:35,320 --> 00:15:36,940
from now, when the weather may be

262
00:15:37,090 --> 00:15:38,860
forecast really bad.

263
00:15:38,990 --> 00:15:42,920
For an example you see everyday, same thing

264
00:15:43,050 --> 00:15:45,290
for traffic, right?

265
00:15:45,420 --> 00:15:48,820
There's at least depending on where you live,

266
00:15:48,950 --> 00:15:50,590
certainly here in California,

267
00:15:50,720 --> 00:15:52,230
there are times of day

268
00:15:52,350 --> 00:15:53,550
where traffic is really bad in lots of places.

269
00:15:53,690 --> 00:15:55,770
So the costs of driving certain roads

270
00:15:55,910 --> 00:15:57,570
may vary, depending on

271
00:15:57,700 --> 00:15:59,200
what time of day it is.

272
00:15:59,340 --> 00:16:01,700
Lots of other examples. Industrial automation,

273
00:16:01,830 --> 00:16:03,870
different machines in the factory may be

274
00:16:04,010 --> 00:16:06,200
available to different degrees at different times

275
00:16:06,320 --> 00:16:08,060
of day. They cost different amounts to hire

276
00:16:08,190 --> 00:16:09,990
different workers, depending on whether you

277
00:16:10,110 --> 00:16:13,200
pay overtime for pretty late in night or whatever. So the

278
00:16:13,330 --> 00:16:14,770
cost of doing different things in the factory

279
00:16:14,890 --> 00:16:16,820
can also be a function of time.

280
00:16:16,950 --> 00:16:20,450
The state transition probabilities can also be a

281
00:16:20,540 --> 00:16:28,150
function of time. Lastly, while we're doing

282
00:16:28,280 --> 00:16:31,010
this as well, to make this fully general, we

283
00:16:31,150 --> 00:16:33,510
might as well have non-stationary rewards

284
00:16:33,650 --> 00:16:39,130
as well, where you might also index the reward

285
00:16:39,260 --> 00:16:41,890
function of these times and prescripts,

286
00:16:42,040 --> 00:16:44,740
where the cost of doing different things may

287
00:16:44,900 --> 00:16:46,490
depend on the time as well.

288
00:16:46,640 --> 00:16:51,380
Actually, there's more examples of

289
00:16:51,510 --> 00:16:53,600
non-stationary MDPs, so let's

290
00:16:53,730 --> 00:16:56,880
so now we have a non-stationary policy.

291
00:16:57,020 --> 00:16:58,410
Let's talk about an algorithm

292
00:16:58,550 --> 00:17:00,120
to actually try to find the optimal policy.

293
00:17:00,250 --> 00:17:05,600
So let me define the following.

294
00:17:05,730 --> 00:17:15,440
This is now a slightly modified definition

295
00:17:15,550 --> 00:17:17,130
of the optimal value function.

296
00:17:17,290 --> 00:17:20,570
I'll just write this down, I guess.

297
00:17:34,510 --> 00:17:40,510
So I'm going to define the optimal value

298
00:17:40,630 --> 00:17:43,050
function, and this going to be indexed by T,

299
00:17:43,220 --> 00:17:45,570
with a subscript T. The optimal value of a state

300
00:17:45,710 --> 00:17:49,860
for time, T, we're going to define as your

301
00:17:49,980 --> 00:17:55,070
optimal sum of rewards for if you start the MDP

302
00:17:55,200 --> 00:17:58,360
at that state, S, and if the clock starts off

303
00:17:58,500 --> 00:18:03,620
at time, lowercase T.

304
00:18:03,730 --> 00:18:05,560
So the optimal value of a state will depend on

305
00:18:05,680 --> 00:18:07,010
what time it is and how much time you

306
00:18:07,120 --> 00:18:10,830
have lest to run this MDP. Therefore, the sum

307
00:18:10,950 --> 00:18:14,370
on the right sums only for time T, time T

308
00:18:14,490 --> 00:18:18,020
plus one, time T plus two up to time, capital T.

309
00:18:18,170 --> 00:18:23,990
I'll just state in English again, this is your

310
00:18:24,120 --> 00:18:27,370
expected optimal total payoff if you start your

311
00:18:27,500 --> 00:18:32,780
system in a state, S, and if the clock is

312
00:18:32,900 --> 00:18:35,550
already at time, lowercase T.

313
00:18:35,700 --> 00:18:40,050
So it turns out then there's a value iteration,

314
00:18:40,160 --> 00:18:43,110
you can value that [inaudible]. Let me just write

315
00:18:43,250 --> 00:18:45,320
out the value [inaudible] algorithm for this. It

316
00:18:45,450 --> 00:18:50,130
turns out you can well, let me just write

317
00:18:50,260 --> 00:18:57,340
this out. I'll write this below. It turns out you

318
00:18:57,490 --> 00:18:59,350
can compute the optimal value function for

319
00:18:59,490 --> 00:19:01,550
the MDP using the following recursion, which

320
00:19:01,690 --> 00:19:04,570
is very similar to what we have for value

321
00:19:04,710 --> 00:19:08,040
iteration. We're going to set V of S to be equal

322
00:19:08,190 --> 00:19:12,930
to [inaudible] over A,

323
00:19:13,070 --> 00:19:16,500
same as before, right?

324
00:19:38,770 --> 00:19:43,980
Okay? So if I start the clock at time T and from

325
00:19:44,110 --> 00:19:48,140
state S, my expected total payoff is equal

326
00:19:48,390 --> 00:19:50,600
to the maximum [inaudible] actions I may take

327
00:19:50,720 --> 00:19:53,190
of my immediate reward. Taking that

328
00:19:53,330 --> 00:19:55,590
action, A, in that state, S. Them plus my

329
00:19:55,740 --> 00:19:58,530
expected future payoff. So if I take action, A,

330
00:19:58,700 --> 00:20:02,660
I would transition with [inaudible] P, subscript

331
00:20:02,780 --> 00:20:04,910
SA, S prime to some new state, S prime.

332
00:20:05,030 --> 00:20:07,850
If I get to the state, S prime,

333
00:20:07,990 --> 00:20:10,940
my total expected payoff from

334
00:20:11,090 --> 00:20:12,850
the state S prime would be these [inaudible]

335
00:20:12,970 --> 00:20:16,180
now subscript T plus one, that's prime.

336
00:20:16,330 --> 00:20:19,690
Subscript T plus one reflects that

337
00:20:19,820 --> 00:20:21,720
after I've taken one action, my clock will have

338
00:20:21,850 --> 00:20:24,380
advanced from time T to time T plus one.

339
00:20:24,510 --> 00:20:27,980
So this is now V star subscript T plus one.

340
00:20:28,140 --> 00:20:32,770
So this expresses V star of T in terms of V star

341
00:20:32,910 --> 00:20:37,450
T plus one. Then lastly, to start off this

342
00:20:37,590 --> 00:20:42,970
recursion, you would have V star, capital T is

343
00:20:43,080 --> 00:20:52,330
equal to it's just equal to that. If you're

344
00:20:52,450 --> 00:20:56,120
already at time, capital T, then you just get to

345
00:20:56,190 --> 00:20:57,850
take one action, and then the clock runs out.

346
00:20:57,970 --> 00:21:00,820
So this is V star capital T. Your value of starting

347
00:21:00,970 --> 00:21:03,640
in some state, S, with no time with just

348
00:21:03,760 --> 00:21:06,090
one time step, with no time left on the clock.

349
00:21:06,220 --> 00:21:10,640
So in the case of finite horizon MDP,

350
00:21:10,780 --> 00:21:14,030
this actually gives up a very nice dynamic

351
00:21:14,180 --> 00:21:16,870
programming algorithm in which you can start

352
00:21:17,010 --> 00:21:18,930
off by computing V star of T. Then you

353
00:21:19,060 --> 00:21:25,060
use this backward [inaudible] to compute V star

354
00:21:25,180 --> 00:21:27,970
of capital T minus one, capital T minus

355
00:21:28,080 --> 00:21:29,980
two and so on. We compute V star of T and T

356
00:21:30,110 --> 00:21:31,390
minus one and so on. It recurs backwards

357
00:21:31,490 --> 00:21:33,330
onto your computer,

358
00:21:33,460 --> 00:21:36,450
V star for all of your time steps.

359
00:21:36,580 --> 00:21:44,320
Can you see this board? Cool.

360
00:21:44,430 --> 00:21:48,170
Then the final step is previously,

361
00:21:48,300 --> 00:21:50,180
we said that pi star of S

362
00:21:50,300 --> 00:21:51,850
I'm going to come back and change this a bit

363
00:21:51,970 --> 00:21:54,090
was the [inaudible] A of R plus A plus

364
00:21:54,200 --> 00:22:05,100
[inaudible] PSA this is sort of what we had. In

365
00:22:05,210 --> 00:22:10,400
the finite horizon case, the ultimate

366
00:22:10,530 --> 00:22:12,500
action may depend on what time it is. So the

367
00:22:12,630 --> 00:22:14,030
ultimate action to take it, time T, is

368
00:22:14,170 --> 00:22:15,490
[inaudible] actions, A.

369
00:22:15,630 --> 00:22:24,370
This is basically the augmat of exactly that

370
00:22:24,500 --> 00:22:25,790
same thing on the right-hand side as we had in

371
00:22:25,910 --> 00:22:28,890
our dynamic programming algorithm. So you

372
00:22:29,000 --> 00:22:32,170
do this for every time step, and now you

373
00:22:32,300 --> 00:22:35,360
compute it, your optimal policy for different

374
00:22:35,520 --> 00:22:38,920
time steps. Again, this is a non-stationary

375
00:22:39,050 --> 00:22:43,440
policy because pi star of S my depend on what

376
00:22:43,570 --> 00:22:45,280
time it is.

377
00:22:45,410 --> 00:22:48,770
So one minor difference the difference between this and

378
00:22:48,910 --> 00:22:52,500
the early version of the earlier version of

379
00:22:52,610 --> 00:22:55,000
value iteration is that so what you do is you

380
00:22:55,130 --> 00:22:57,420
complete V star of T. Then using the

381
00:22:57,550 --> 00:22:59,410
backwards recursion of that [inaudible]

382
00:22:59,520 --> 00:23:01,380
algorithm, you computer V star T minus one.

383
00:23:01,500 --> 00:23:04,500
Then V star T minus two and so on down to V

384
00:23:04,620 --> 00:23:07,610
star of zero. Then from these, you

385
00:23:07,730 --> 00:23:09,590
compute pi star.

386
00:23:09,710 --> 00:23:15,890
So one there's not a huge difference, but one

387
00:23:16,020 --> 00:23:18,870
minus difference [inaudible] the infinite

388
00:23:18,980 --> 00:23:23,710
horizon discounted case is that by running this

389
00:23:23,850 --> 00:23:26,090
recursion once, you now have exactly the

390
00:23:26,210 --> 00:23:28,800
right value function. So this just computes the

391
00:23:29,140 --> 00:23:31,020
value function, rather than merely

392
00:23:31,130 --> 00:23:33,670
converging [inaudible]. This just gives you the

393
00:23:33,790 --> 00:23:36,660
right value function with one pass.

394
00:23:36,790 --> 00:23:42,550
Cool. Any questions there? Yeah. Interviewee:

395
00:23:42,660 --> 00:23:44,570
[Inaudible].

396
00:23:44,690 --> 00:23:53,170
This computation's much shorter than

397
00:23:53,300 --> 00:23:55,220
valuations. So sort of yes and no. It depends on

398
00:23:55,330 --> 00:23:57,270
how large capital T is.

399
00:23:57,400 --> 00:24:01,060
Interviewee: [Inaudible] the normal MDP,

400
00:24:01,170 --> 00:24:03,590
could [inaudible] and then use

401
00:24:03,700 --> 00:24:05,630
this case for that case?

402
00:24:05,740 --> 00:24:07,180
I see. So for the normal MDP,

403
00:24:07,290 --> 00:24:10,640
can you assume capital T and then assume this?

404
00:24:10,750 --> 00:24:16,590
So it actually turns out that that's a great

405
00:24:16,700 --> 00:24:18,030
question. Let me just answer this

406
00:24:18,150 --> 00:24:19,730
in a hand-wavy way. So it actually turns out

407
00:24:19,860 --> 00:24:22,340
for a discounted infinite horizon MDP

408
00:24:22,470 --> 00:24:25,950
where some discount factor gamma. So what

409
00:24:26,100 --> 00:24:30,210
you can do is you can see, after T times X,

410
00:24:30,350 --> 00:24:33,830
gamma to the T would be really small. It would

411
00:24:33,960 --> 00:24:36,130
be like [inaudible] something. I don't

412
00:24:36,270 --> 00:24:38,640
really care what happens after that many times

413
00:24:38,780 --> 00:24:40,510
because the rewards are multiplied by

414
00:24:40,630 --> 00:24:42,440
gamma to the T. After that, I don't really care.

415
00:24:42,570 --> 00:24:44,510
So you can ask, can I take

416
00:24:44,640 --> 00:24:47,330
my infinite horizon discounted MDP

417
00:24:47,450 --> 00:24:50,910
and approximate that with a finite horizon MDP

418
00:24:51,050 --> 00:24:53,570
where the number of times, steps T,

419
00:24:53,690 --> 00:24:55,280
is chosen so that holds true.

420
00:24:55,400 --> 00:24:56,520
So it turns out you can do that.

421
00:24:56,660 --> 00:24:59,870
Then you end up with some value for T.

422
00:24:59,990 --> 00:25:03,030
You can solve for T so that this holds true.

423
00:25:03,140 --> 00:25:05,270
It turns out you can prove

424
00:25:05,420 --> 00:25:07,380
that if you took the original value

425
00:25:07,520 --> 00:25:10,490
iteration algorithm and if you run the

426
00:25:10,610 --> 00:25:13,260
original value of the iteration algorithm

427
00:25:13,400 --> 00:25:14,710
the version for discounted MDPs.

428
00:25:14,850 --> 00:25:17,770
If you run that for this same number

429
00:25:17,880 --> 00:25:20,340
of time steps, you will end up

430
00:25:20,500 --> 00:25:22,080
with an approximation to

431
00:25:22,220 --> 00:25:24,810
the value function that is about this close,

432
00:25:24,960 --> 00:25:26,420
up to some small constant factors.

433
00:25:26,590 --> 00:25:29,340
So to do that, you end up with roughly the same

434
00:25:29,470 --> 00:25:32,440
amounts of computation anyway. Then

435
00:25:32,590 --> 00:25:34,220
you actually end up with a non-stationary

436
00:25:34,330 --> 00:25:35,910
policy, which is more expensive to keep

437
00:25:36,050 --> 00:25:38,790
around. You need to keep around the different

438
00:25:38,930 --> 00:25:41,550
policy every time step, which is not as nice

439
00:25:41,670 --> 00:25:43,930
as if you had the stationary policy, same policy

440
00:25:44,040 --> 00:25:45,920
for all times.

441
00:25:46,010 --> 00:25:49,860
So there are other reasons, but sometimes you

442
00:25:49,980 --> 00:25:52,100
might take an infinite horizon discounted

443
00:25:52,200 --> 00:25:54,140
problem and approximate it to a finite horizon

444
00:25:54,350 --> 00:25:58,650
problem. But this particular reason is not

445
00:25:58,750 --> 00:26:04,560
the one. That makes sense. More questions?

446
00:26:04,690 --> 00:26:14,810
Interviewee: [Inaudible]?

447
00:26:14,930 --> 00:26:16,260
Is there a gamma in this?

448
00:26:16,410 --> 00:26:19,070
So if you want, you can actually

449
00:26:19,170 --> 00:26:21,790
change the definition of an MDP and use a

450
00:26:21,910 --> 00:26:26,470
finite horizon discounted MDP. If you want,

451
00:26:26,580 --> 00:26:29,690
you can do that. You can actually come in and

452
00:26:29,820 --> 00:26:34,010
put a gamma there, and use this counting

453
00:26:34,130 --> 00:26:36,910
the finite horizon. It turns out that usually, for

454
00:26:37,010 --> 00:26:39,180
most problems that people deal with, you

455
00:26:39,300 --> 00:26:40,270
either use discounting or

456
00:26:40,420 --> 00:26:41,830
you use the finite horizon.

457
00:26:41,930 --> 00:26:45,150
It's been less common to do both, but you can

458
00:26:45,250 --> 00:26:47,630
certainly do as well. One of the nice things

459
00:26:47,750 --> 00:26:50,370
about discounting, it makes such your value

460
00:26:50,490 --> 00:26:53,740
function is finite. Algorithmically and

461
00:26:53,860 --> 00:26:56,000
mathematically, one of the reasons to use

462
00:26:56,120 --> 00:26:59,230
discounting is because you're multiplying your

463
00:26:59,350 --> 00:27:01,450
rewards exponentially. It's a geometrically

464
00:27:01,570 --> 00:27:03,730
[inaudible] series. It shows that the value

465
00:27:03,880 --> 00:27:05,460
function is always finite. This is a really nice

466
00:27:05,570 --> 00:27:06,790
mathematical properties when you do

467
00:27:06,920 --> 00:27:08,710
discounting.

468
00:27:08,830 --> 00:27:11,180
So when you have a finite horizon anyway, then

469
00:27:11,320 --> 00:27:13,550
the value function's also guaranteed to

470
00:27:13,660 --> 00:27:14,820
be finite. So with that, you don't have to use

471
00:27:14,930 --> 00:27:16,860
discounting. But if you want, you can

472
00:27:16,970 --> 00:27:18,540
actually discount as well.

473
00:27:18,690 --> 00:27:23,790
Interviewee: [Inaudible].

474
00:27:23,880 --> 00:27:25,700
Yeah, yes, you're right. If you want,

475
00:27:25,810 --> 00:27:28,110
you can redefine the reward function

476
00:27:28,250 --> 00:27:30,310
to go downward into the to the reward function,

477
00:27:30,450 --> 00:27:32,230
since we have non-stationary rewards as well.

478
00:27:36,970 --> 00:27:45,940
So that was finite horizon MDPs. What I want

479
00:27:46,060 --> 00:27:49,800
to do now is actually use both of these

480
00:27:49,920 --> 00:27:51,690
ideas, your state action rewards and your finite

481
00:27:51,820 --> 00:27:54,920
horizon MDPs to describe a special case

482
00:27:55,050 --> 00:27:58,560
of MDPs that makes very strong assumptions

483
00:27:58,680 --> 00:28:01,690
about the problem. But these assumptions

484
00:28:01,830 --> 00:28:04,550
are reasonable for many systems. With these

485
00:28:04,650 --> 00:28:06,390
assumptions, what we come up with, I think,

486
00:28:06,530 --> 00:28:08,590
are very nice and very elegant algorithms for

487
00:28:08,730 --> 00:28:10,380
solving even very large MDPs.

488
00:28:10,540 --> 00:28:33,930
So let's talk about linear quadratic regulation.

489
00:28:45,320 --> 00:28:53,410
We just talked about dynamic

490
00:28:53,530 --> 00:28:55,980
programming for finite horizon MDPs, so just

491
00:28:56,100 --> 00:28:57,850
remember that algorithm. When I come

492
00:28:58,010 --> 00:29:00,520
back to talk about an algorithm for solving

493
00:29:00,610 --> 00:29:02,550
LQR problems, I'm actually going to use

494
00:29:02,700 --> 00:29:04,240
exactly that dynamic programming algorithm

495
00:29:04,350 --> 00:29:07,420
that you just saw for finite horizon MDPs.

496
00:29:07,550 --> 00:29:10,750
I'll be using exactly that algorithm again.

497
00:29:10,860 --> 00:29:12,180
So just remember that for now.

498
00:29:12,330 --> 00:29:17,330
So let's talk about LQR. So I want to take these

499
00:29:17,460 --> 00:29:19,930
ideas and apply them to MDPs with

500
00:29:20,070 --> 00:29:22,970
continuous state spaces and maybe even

501
00:29:23,100 --> 00:29:26,990
continuous action spaces. So to specify and

502
00:29:27,120 --> 00:29:34,000
MDPs, I need to give you this fivetuple of state

503
00:29:34,120 --> 00:29:35,930
actions, transition probabilities in the reward. I'm

504
00:29:36,050 --> 00:29:38,840
going to use the finite horizon, capital T, rather

505
00:29:38,950 --> 00:29:41,370
than discounting.

506
00:29:41,500 --> 00:29:45,770
So in LQR problems, I'm going to assume the

507
00:29:45,930 --> 00:29:48,670
following. I'm going to assume that the

508
00:29:48,810 --> 00:29:52,800
state space is in dimensional is RN. And I'm

509
00:29:52,920 --> 00:29:57,310
going to assume, also, a continuous set of

510
00:29:57,460 --> 00:30:00,970
actions lie in RT. To specify the state transition

511
00:30:01,100 --> 00:30:07,680
probabilities, PSA, I need to tell you

512
00:30:07,800 --> 00:30:10,670
what the distribution of the mixed state is,

513
00:30:10,790 --> 00:30:13,240
given the current state and the current action.

514
00:30:13,360 --> 00:30:15,870
So we actually saw a little bit of this in the last

515
00:30:16,010 --> 00:30:18,290
lecture. I want to assume the next state,

516
00:30:18,430 --> 00:30:21,250
ST plus one, is going to be a linear function of

517
00:30:21,430 --> 00:30:31,550
WT oh, excuse me. I meant to subscript that.

518
00:30:53,680 --> 00:30:57,610
Where WT is Gaussian [inaudible] would mean

519
00:30:57,730 --> 00:31:00,270
zero and some covariance given by sigma W.

520
00:31:00,390 --> 00:31:07,010
Subscripts at A and B here with subscripts T to

521
00:31:07,140 --> 00:31:10,290
show that these matrixes could change

522
00:31:10,410 --> 00:31:12,710
over time. So this would be non-stationary

523
00:31:12,830 --> 00:31:17,550
dynamics. As a point of notation, unfortunately

524
00:31:17,660 --> 00:31:21,110
compiling ideas from multiple literatures,

525
00:31:21,210 --> 00:31:23,870
so it's sort of unfortunately that capital A

526
00:31:24,000 --> 00:31:28,240
denotes both a set of actions as well as a matrix.

527
00:31:28,370 --> 00:31:31,840
When you see A later on, A will usually be used

528
00:31:31,930 --> 00:31:34,120
to denote a matrix, rather than a set of

529
00:31:34,230 --> 00:31:36,120
actions. So [inaudible] overload notation again,

530
00:31:36,240 --> 00:31:39,670
but unfortunately the notational

531
00:31:39,790 --> 00:31:42,680
conventions when you have research ideas in

532
00:31:42,820 --> 00:31:44,610
multiple research communities, often they

533
00:31:44,730 --> 00:31:48,600
share the same symbol. So just to be concrete,

534
00:31:48,710 --> 00:31:51,720
AT is a matrix that's N by N. [Inaudible]

535
00:31:51,840 --> 00:32:03,180
matrixes that are N by D. Just to be completely

536
00:32:03,310 --> 00:32:05,860
clear, right, the matrixes A and B, I'm

537
00:32:06,000 --> 00:32:07,780
going to assume, are fixed and known in

538
00:32:07,900 --> 00:32:10,020
advance. So I'm going to give you the matrixes,

539
00:32:10,130 --> 00:32:13,010
A and B, and I'm going to give you sigma W.

540
00:32:13,130 --> 00:32:15,300
Your job is to find a good policy

541
00:32:15,450 --> 00:32:17,440
for this MDP.

542
00:32:17,530 --> 00:32:20,620
So in other words, this is my specification of

543
00:32:20,730 --> 00:32:25,540
the state transition probabilities. Looking

544
00:32:25,620 --> 00:32:30,160
ahead, we see this later, it turns out this noise

545
00:32:30,290 --> 00:32:39,870
term is not very important. So it turns out

546
00:32:39,970 --> 00:32:43,110
that the treatment of the noise term is not very

547
00:32:43,230 --> 00:32:45,170
important. We'll see this later. We can

548
00:32:45,280 --> 00:32:49,490
pretty much ignore the noise term, and we'll

549
00:32:49,610 --> 00:32:52,560
still do fine. This is just a warning in the

550
00:32:52,680 --> 00:32:56,020
sequel, what I do later, I might be slightly

551
00:32:56,150 --> 00:32:58,930
sloppy in my treatment of the noise term.

552
00:32:59,070 --> 00:33:01,320
In this very special case,

553
00:33:01,490 --> 00:33:03,800
it would be unimportant.

554
00:33:03,940 --> 00:33:08,120
The last thing I have to specify is some horizon

555
00:33:08,250 --> 00:33:14,050
time, and then I also have some reward

556
00:33:14,170 --> 00:33:18,290
function. For LQR control, I'm going to assume

557
00:33:18,390 --> 00:33:20,840
that a reward function can be written as

558
00:33:20,980 --> 00:33:36,730
this, where UT is a matrix that's N by N. VT is

559
00:33:36,870 --> 00:33:42,870
a matrix that's D by D. I'll assume that UT

560
00:33:43,010 --> 00:33:49,390
and VT are both positive semi-definite. Are

561
00:33:49,510 --> 00:33:54,320
both PSD. So the fact that UT and VT are

562
00:33:54,430 --> 00:33:57,730
both positive semi-definite matrixes, that

563
00:33:57,840 --> 00:34:03,470
implies that ST transpose, UT, ST [inaudible]

564
00:34:03,600 --> 00:34:07,950
zero. Similarly, ST transpose are VT, AT,

565
00:34:08,070 --> 00:34:16,690
[inaudible] zero. So this implies that your

566
00:34:16,820 --> 00:34:19,060
rewards are always negative. This is a

567
00:34:19,170 --> 00:34:22,090
somewhat depressing MDP in which there are

568
00:34:22,240 --> 00:34:25,090
only costs and no positive rewards,

569
00:34:25,190 --> 00:34:29,240
right, because of the minus sign there.

570
00:34:43,630 --> 00:34:48,280
So as a complete example for how you might

571
00:34:48,410 --> 00:34:50,860
want to apply this, you've seen my helicopter

572
00:34:50,980 --> 00:34:52,400
videos, right? So one thing is, for example,

573
00:34:52,500 --> 00:34:58,040
suppose you have a helicopter, and

574
00:34:58,170 --> 00:35:02,710
you want the state ST to be as close to zero as

575
00:35:02,800 --> 00:35:10,450
possible. Then you might choose UT to be

576
00:35:10,580 --> 00:35:13,360
equal to the identity matrix. This will make R

577
00:35:13,490 --> 00:35:19,300
of STAT be equal to ST transpose ST. But

578
00:35:19,430 --> 00:35:25,210
that's just I'll just write that down. [Inaudible]

579
00:35:25,310 --> 00:35:34,370
oh, excuse me minus. The squared

580
00:35:34,470 --> 00:35:36,940
negative of the squared [inaudible] vector.

581
00:35:37,040 --> 00:35:39,850
So this would be penalizing the system

582
00:35:39,940 --> 00:35:42,310
quadratically for having a state that's half of

583
00:35:42,430 --> 00:35:45,200
zero, assuming that zero's the origin state. So if

584
00:35:45,290 --> 00:35:48,010
it goes to make a helicopter hover around the

585
00:35:48,110 --> 00:35:49,620
state zero, then you might choose this sort of

586
00:35:49,730 --> 00:35:51,630
reward function.

587
00:35:51,780 --> 00:35:56,690
It turns out it's also very common for action to

588
00:35:56,810 --> 00:35:59,890
choose a cost for the action. So suppose I

589
00:36:00,010 --> 00:36:02,380
choose VT to be equal to an identity matrix.

590
00:36:02,490 --> 00:36:04,070
I get minus AT transpose AT here.

591
00:36:04,190 --> 00:36:11,100
Then minus [inaudible] actions as well.

592
00:36:11,240 --> 00:36:15,350
Including a quadratic cost for actions,

593
00:36:15,450 --> 00:36:17,530
it's also a fairly common thing to do.

594
00:36:17,640 --> 00:36:19,230
In practice, this tends to be effective of

595
00:36:19,340 --> 00:36:22,840
discouraging your system from jerking

596
00:36:22,980 --> 00:36:24,590
the controls around. This discourages

597
00:36:24,710 --> 00:36:26,320
making very huge control commands.

598
00:36:26,410 --> 00:36:29,300
Having a term like this reward function often

599
00:36:29,400 --> 00:36:31,060
makes many systems behave better.

600
00:36:31,170 --> 00:36:33,890
Of course, [inaudible] choose different values,

601
00:36:34,030 --> 00:36:35,600
we have different values on the diagonal to

602
00:36:35,730 --> 00:36:38,320
give different state variables, different weight

603
00:36:38,450 --> 00:36:41,150
and so on. So lots of possible choices for U

604
00:36:41,300 --> 00:36:43,110
and B. This is one example.

605
00:36:43,240 --> 00:37:00,200
So for the next few steps, I'm going to write out

606
00:37:00,340 --> 00:37:02,860
things, I'm going to derive things,

607
00:37:03,000 --> 00:37:06,330
for the general case of non-stationary dynamics.

608
00:37:06,480 --> 00:37:09,740
I'm going as I write out more math and more

609
00:37:09,860 --> 00:37:12,630
equations for LQR, I'm going to try write it out

610
00:37:12,770 --> 00:37:15,710
for the fairly general case of time varied

611
00:37:15,850 --> 00:37:18,580
dynamics and time varied reward functions.

612
00:37:18,720 --> 00:37:23,140
So I'm [inaudible] function. But for purposes

613
00:37:23,230 --> 00:37:26,660
of understanding this material, you might want

614
00:37:26,780 --> 00:37:28,940
to think of just ignoring many of the

615
00:37:29,060 --> 00:37:30,570
subscripts, in terms of T.

616
00:37:30,710 --> 00:37:35,140
So for the sake of [inaudible] material,

617
00:37:35,280 --> 00:37:37,050
you might want to mentally assume

618
00:37:37,180 --> 00:37:39,250
that there are some fixed matrix, A,

619
00:37:39,390 --> 00:37:44,140
so that A is equal to A1, A2, equals A3

620
00:37:44,300 --> 00:37:47,300
and so on. Similarly, there's some matrix B.

621
00:37:47,430 --> 00:37:53,310
Okay? So write it out for the fully general,

622
00:37:53,450 --> 00:37:56,530
non-stationary case, but you might just want to

623
00:37:56,670 --> 00:37:59,110
ignore many of the time subscripts and imagine

624
00:37:59,240 --> 00:38:00,830
the stationary case for now.

625
00:38:00,990 --> 00:38:10,890
Quite a bit later, we're going to talk about an

626
00:38:11,050 --> 00:38:13,780
extension of this called differential dynamic

627
00:38:13,880 --> 00:38:15,760
programming that will actually use

628
00:38:15,880 --> 00:38:18,680
a non-stationary [inaudible] to a very powerful

629
00:38:18,820 --> 00:38:20,450
effect for a specific algorithm.

630
00:38:20,610 --> 00:38:22,470
But for most of what we're about to do,

631
00:38:22,590 --> 00:38:24,440
just pretend that MDP is stationary.

632
00:38:49,070 --> 00:38:52,180
Okay. So before I talk about models,

633
00:38:52,310 --> 00:38:54,680
let me jus say a couple of words about how you

634
00:38:54,820 --> 00:38:56,010
would go about coming up with the linear

635
00:38:56,140 --> 00:38:57,850
models. The key assumption in this model is

636
00:38:57,970 --> 00:39:00,270
that the dynamics are linear. There's also the

637
00:39:00,390 --> 00:39:02,290
assumption the reward function is quadratic,

638
00:39:02,410 --> 00:39:04,610
but let's talk about the assumption that the

639
00:39:04,750 --> 00:39:06,130
dynamics are linear.

640
00:39:06,260 --> 00:39:12,740
ST plus one equals AST plus VAT.

641
00:39:12,910 --> 00:39:15,080
Maybe time varying, maybe stationary. I'm just

642
00:39:15,220 --> 00:39:16,510
writing stationary for now.

643
00:39:16,640 --> 00:39:18,900
So how do you getmodels like this?

644
00:39:19,040 --> 00:39:21,850
We actually saw one example of this already

645
00:39:21,970 --> 00:39:23,560
in the previous lecture.

646
00:39:23,690 --> 00:39:25,160
If you have an inverted pendulum system,

647
00:39:25,280 --> 00:39:27,330
and you want to model the inverted pendulum

648
00:39:27,500 --> 00:39:30,600
using a linear model like this, maybe

649
00:39:30,730 --> 00:39:32,680
[inaudible]. I'm not going to write that down.

650
00:39:32,820 --> 00:39:35,620
One thing you could do is run your inverted

651
00:39:35,760 --> 00:39:37,590
pendulum, start it off in some state as zero,

652
00:39:37,720 --> 00:39:41,940
take some action, A0, have it get to some state,

653
00:39:42,070 --> 00:39:44,670
S1. Take action A1 and so on, get to some

654
00:39:44,800 --> 00:39:49,270
state ST. Our index is one to denote that this is

655
00:39:49,390 --> 00:39:50,740
my first trial.

656
00:39:50,880 --> 00:39:52,390
Then you can repeat this a bunch of times.

657
00:39:52,520 --> 00:39:55,530
You can repeat this N times. I'm just executing

658
00:39:55,680 --> 00:40:00,310
actions on your physical robot. It could be a

659
00:40:00,450 --> 00:40:02,890
robot, it could be a chemical plant. It could

660
00:40:03,040 --> 00:40:04,830
be whatever. Trying out different actions in

661
00:40:05,010 --> 00:40:13,820
your system and watch what states it gets to.

662
00:40:13,960 --> 00:40:18,840
So for the linear model to your data, and choose

663
00:40:19,000 --> 00:40:26,230
the parameters A and B, that minimize

664
00:40:26,380 --> 00:40:48,970
the quadratic error term. So this says how well

665
00:40:49,090 --> 00:40:52,780
does AST plus BAT predict ST plus one.

666
00:40:52,870 --> 00:40:54,800
So you minimize the quadratic penalty term.

667
00:40:54,920 --> 00:40:56,570
This would be one reasonable way to

668
00:40:56,690 --> 00:40:59,330
estimate the parameters of a linear dynamical

669
00:40:59,470 --> 00:41:02,470
system for a physical robot or a physical

670
00:41:02,590 --> 00:41:04,200
chemical part of whatever they may have.

671
00:41:04,360 --> 00:41:11,580
Another way to come up with a linear model

672
00:41:11,710 --> 00:41:21,770
consistently, if I want to control, is to take a

673
00:41:21,880 --> 00:41:25,140
nonlinear model and to linearize it. Let me

674
00:41:25,260 --> 00:41:26,950
show you what I mean by that. So you can

675
00:41:27,110 --> 00:41:39,300
linearize a nonlinear model. So let's say you

676
00:41:39,410 --> 00:41:42,760
have some nonlinear model that expresses

677
00:41:42,890 --> 00:41:49,540
ST plus one as some function of ST and AT. In

678
00:41:49,610 --> 00:41:51,530
the example in the previous lecture, I said

679
00:41:51,650 --> 00:41:56,630
for the inverted pendulum [inaudible]. By

680
00:41:56,750 --> 00:41:59,430
referring to the laws of physics. It was actually

681
00:41:59,570 --> 00:42:02,380
by downloading off the shelf software for doing

682
00:42:02,500 --> 00:42:04,510
physics simulations. So if you haven't

683
00:42:04,660 --> 00:42:06,970
seen this all before, you can go online. You

684
00:42:07,120 --> 00:42:09,970
can easily find many open-source

685
00:42:10,120 --> 00:42:12,130
packages for simulating the physics of simple

686
00:42:12,250 --> 00:42:14,090
devices like these.

687
00:42:14,200 --> 00:42:15,640
Download the software, type

688
00:42:15,740 --> 00:42:17,750
in the specifications of your robot,

689
00:42:17,880 --> 00:42:19,790
and it will simulate the physics that you use.

690
00:42:19,910 --> 00:42:21,180
There's lots of open-source software patches

691
00:42:21,300 --> 00:42:22,750
like that. You can just download them.

692
00:42:22,860 --> 00:42:25,490
But something like that, you can now build

693
00:42:25,630 --> 00:42:28,660
a physics simulator that predicts the state

694
00:42:28,760 --> 00:42:30,160
as a function of the previous state

695
00:42:30,300 --> 00:42:31,680
and the previous action.

696
00:42:31,840 --> 00:42:35,050
So you actually come up with some function

697
00:42:35,200 --> 00:42:43,680
that says that the state [inaudible] next time.

698
00:42:43,810 --> 00:42:46,180
The [inaudible] vector will be some function of

699
00:42:46,320 --> 00:42:54,060
the current state and the current action,

700
00:42:54,190 --> 00:42:57,010
where the action in this case is just

701
00:42:57,180 --> 00:42:58,190
a real number that says

702
00:42:58,280 --> 00:42:59,950
how hard you accelerated to the left or right.

703
00:43:00,100 --> 00:43:04,480
Then you can take this nonlinear model.

704
00:43:04,620 --> 00:43:06,370
I actually wrote down a sample of

705
00:43:06,460 --> 00:43:07,840
a model in the last lecture,

706
00:43:07,960 --> 00:43:09,460
but in general, F would be some nonlinear

707
00:43:09,590 --> 00:43:11,420
function. [Inaudible] of a linear function.

708
00:43:11,530 --> 00:43:15,010
So what I mean by linearize is the following.

709
00:43:15,120 --> 00:43:17,180
So here's just a cartoon.

710
00:43:17,330 --> 00:43:18,730
I'll write down the math in a second.

711
00:43:18,880 --> 00:43:22,600
Let's say the horizontal acces is the input state,

712
00:43:22,730 --> 00:43:26,760
ST, and the output state, ST plus one, as I

713
00:43:26,890 --> 00:43:35,460
said. Here's the function at F. So the next state,

714
00:43:35,600 --> 00:43:37,180
ST plus one, will be some function of the

715
00:43:37,310 --> 00:43:39,770
previous state, ST and the action AT. So to

716
00:43:39,900 --> 00:43:43,920
linearize this model, what you would do is

717
00:43:44,040 --> 00:43:47,200
you would choose a point. We'll call this bar T.

718
00:43:47,300 --> 00:43:51,540
Then you would take the derivative of

719
00:43:51,670 --> 00:43:56,590
this function. For the [inaudible] straight line to

720
00:43:56,710 --> 00:43:58,550
that function.

721
00:43:58,690 --> 00:44:02,430
So this allows you to express the next state,

722
00:44:02,570 --> 00:44:05,600
ST plus one. You can approximate the next

723
00:44:05,750 --> 00:44:09,670
state, ST plus one, as this linear function of the

724
00:44:09,800 --> 00:44:14,590
previous state, ST. So to make this

725
00:44:14,770 --> 00:44:16,480
cartoon really right, the horizontal access here

726
00:44:16,620 --> 00:44:20,030
is really a state action pair. You're

727
00:44:20,160 --> 00:44:22,660
linearizing around. So this is just a cartoon. The

728
00:44:22,760 --> 00:44:25,240
horizontal access represents the input

729
00:44:25,370 --> 00:44:26,850
state and the input action.

730
00:44:29,460 --> 00:44:46,290
So just to write this out in math, I'll write out

731
00:44:46,410 --> 00:44:48,250
the simple case first and the fully general

732
00:44:48,400 --> 00:44:52,030
one in a second. Suppose the horizontal access

733
00:44:52,180 --> 00:44:54,060
was only this state. So let's pretend

734
00:44:54,210 --> 00:44:55,960
interactions they [inaudible] now. ST plus one

735
00:44:56,090 --> 00:44:58,430
is just some function of ST, than that

736
00:44:58,560 --> 00:45:01,300
linear function I drew would be ST plus one.

737
00:45:01,440 --> 00:45:06,310
We're approximating as F prime evaluated

738
00:45:06,430 --> 00:45:10,680
at some point as bar T times ST times S bar T.

739
00:45:10,810 --> 00:45:19,510
Plus S bar T. So with this, you'd express

740
00:45:19,630 --> 00:45:24,390
ST plus one as a linear function of ST. Just note

741
00:45:24,520 --> 00:45:27,250
that S bar T is a constant.

742
00:45:27,360 --> 00:45:29,370
It's not a variable.

743
00:45:29,470 --> 00:45:34,000
Does that make sense? S bar T is a constant. F

744
00:45:34,130 --> 00:45:35,810
prime of S bar T is gradient of the function

745
00:45:35,940 --> 00:45:39,490
F at the point S bar T. This is really just the

746
00:45:39,610 --> 00:45:42,280
equation of that linear function. So you can

747
00:45:42,420 --> 00:45:44,110
then convert this to A and B matrixes.

748
00:45:51,170 --> 00:45:55,890
Jumping back one board, I'm going to point out

749
00:45:56,020 --> 00:45:58,480
one other thing. Let's say I look at this

750
00:45:58,610 --> 00:46:01,910
straight line, and I ask how well does this

751
00:46:02,030 --> 00:46:04,530
straight line approximate my function F, my

752
00:46:04,620 --> 00:46:07,020
original simulator, my original function F. Then

753
00:46:07,110 --> 00:46:09,700
you sort of notice that in this

754
00:46:09,850 --> 00:46:14,110
neighborhood, in the neighborhood of S bar,

755
00:46:14,280 --> 00:46:16,460
there's a pretty good approximation. It's

756
00:46:16,580 --> 00:46:18,950
fairly close. But then as you move further away,

757
00:46:19,120 --> 00:46:21,250
moving far off to the left here, it

758
00:46:21,400 --> 00:46:23,250
becomes a pretty terrible approximation.

759
00:46:23,420 --> 00:46:30,610
So when you linearize a nonlinear model to

760
00:46:30,720 --> 00:46:34,500
apply LQR, one of the parameters you have

761
00:46:34,620 --> 00:46:36,510
to choose would be the point around which to

762
00:46:36,620 --> 00:46:39,470
linearize your nonlinear model. So if you

763
00:46:39,660 --> 00:46:43,220
expect your inverted pendulum system to spend

764
00:46:43,350 --> 00:46:47,370
most of its time in the vicinity of this

765
00:46:47,470 --> 00:46:51,490
state, then it'd be reasonable to linearize around

766
00:46:51,610 --> 00:46:54,480
this state because that means that the

767
00:46:54,650 --> 00:46:56,390
linear approximation would be a good

768
00:46:56,530 --> 00:46:59,460
approximation, usually, for the states that you

769
00:46:59,580 --> 00:47:01,590
expect [inaudible] to spend most of this time.

770
00:47:01,740 --> 00:47:04,820
If conversely, you expect the system to spend

771
00:47:04,950 --> 00:47:07,230
most of its time at states far to the left, then

772
00:47:07,370 --> 00:47:09,270
this would be a terrible location to linearize.

773
00:47:09,420 --> 00:47:12,660
So one rule of thumb is to choose the

774
00:47:12,810 --> 00:47:14,650
position to linearize according to where you

775
00:47:14,810 --> 00:47:16,450
expect the system to spend most of its time

776
00:47:16,570 --> 00:47:19,570
so that the linear approximation will tend to be

777
00:47:19,680 --> 00:47:24,140
an accurate approximation in the vicinity

778
00:47:24,260 --> 00:47:28,040
of the states [inaudible]. Just to be fair, it is

779
00:47:28,160 --> 00:47:32,830
about choosing the point, S bar, A bar, that

780
00:47:32,930 --> 00:47:35,890
we'll use to come up with a linear function that

781
00:47:36,510 --> 00:47:38,900
we'll pretend it's a good approximation to

782
00:47:39,010 --> 00:47:41,540
my original nonlinear function, F.

783
00:47:55,630 --> 00:47:58,730
So for an example like the inverted pendulum

784
00:47:58,870 --> 00:48:01,350
problem, this problem, if you expect to do

785
00:48:01,490 --> 00:48:04,470
pretty well in this problem, then you would

786
00:48:04,610 --> 00:48:11,330
expect the state to often be near the zero state.

787
00:48:11,510 --> 00:48:14,860
If S equals zero corresponds to X being the

788
00:48:15,030 --> 00:48:18,390
center of the railway track that the inverted

789
00:48:18,540 --> 00:48:20,850
pendulum lives on. You expect to do fairly well.

790
00:48:21,020 --> 00:48:23,830
You expect the pole to mostly be upright

791
00:48:24,010 --> 00:48:26,290
[inaudible] upright at zero degrees or 90

792
00:48:26,420 --> 00:48:29,050
degrees, I guess. So you choose whatever state

793
00:48:29,240 --> 00:48:31,470
corresponds to having the pole upright.

794
00:48:31,640 --> 00:48:33,930
The zero velocity [inaudible], near zero

795
00:48:34,030 --> 00:48:37,130
velocity in the middle of the track.

796
00:48:37,270 --> 00:48:39,680
So you usually choose that as a state

797
00:48:39,820 --> 00:48:43,600
to linearize your inverted pendulum dynamics

798
00:48:43,720 --> 00:48:47,000
around. That's a region where you might want

799
00:48:47,090 --> 00:48:49,350
your approximation to be good.

800
00:48:55,320 --> 00:48:57,970
So I wrote this down. To come back to this

801
00:48:58,110 --> 00:49:00,310
formula, I wrote this down for the special

802
00:49:00,450 --> 00:49:02,570
case of a one D state variable. If there are no

803
00:49:02,690 --> 00:49:06,560
actions. The general formula for the

804
00:49:06,670 --> 00:49:09,940
linearization approximation is ST plus one were

805
00:49:10,060 --> 00:49:15,460
approximate as F of S bar T. A bar T plus

806
00:49:54,250 --> 00:49:56,710
Okay? Where these upside down triangles are

807
00:49:56,840 --> 00:50:00,810
an unusual symbol for taking the derivative

808
00:50:00,940 --> 00:50:03,390
of F with respect to [inaudible] vector value,

809
00:50:03,520 --> 00:50:07,730
second argument. So by choosing an

810
00:50:07,850 --> 00:50:10,460
appropriate state as bar T, A bar T to linearize

811
00:50:10,550 --> 00:50:15,820
around, you'd now express ST plus one as

812
00:50:15,940 --> 00:50:20,310
a linear function of the current state and the

813
00:50:20,450 --> 00:50:25,260
current action, AT. Again, these things, S bar

814
00:50:25,390 --> 00:50:28,680
T, is a constant that you choose ahead of time.

815
00:50:28,800 --> 00:50:30,480
Same for A bar T.

816
00:50:30,620 --> 00:50:37,710
Lastly, having linearized this thing, you can

817
00:50:37,830 --> 00:50:42,680
then convert it to matrixes like that.

818
00:50:42,810 --> 00:50:50,780
So the ST plus one is now a linear function of

819
00:50:50,900 --> 00:50:57,610
ST and AT. Questions about this?

820
00:51:09,560 --> 00:51:14,290
So just one tiny detail, and it's really not

821
00:51:14,410 --> 00:51:16,630
a huge deal, is that this thing below is

822
00:51:16,750 --> 00:51:18,320
technically an [inaudible] function. There might

823
00:51:18,440 --> 00:51:20,060
actually be an extra constant there, but

824
00:51:20,200 --> 00:51:24,730
this is not a difficult generalization of a linear

825
00:51:24,860 --> 00:51:28,360
dynamical system definition. One way to

826
00:51:28,470 --> 00:51:30,440
deal with that constant is actually to do

827
00:51:30,560 --> 00:51:32,250
something like take your

828
00:51:32,380 --> 00:51:33,480
definition for this state,

829
00:51:33,610 --> 00:51:35,080
let's say XX dot theta theta dot.

830
00:51:35,210 --> 00:51:37,870
You can then augment your state vector

831
00:51:38,000 --> 00:51:40,060
to have an extra interceptor,

832
00:51:40,210 --> 00:51:42,800
one. With the interceptor one and working out

833
00:51:42,880 --> 00:51:45,500
the A matrix, you can then take care of

834
00:51:45,610 --> 00:51:47,960
the extra constant, C, as well.

835
00:51:48,090 --> 00:51:50,660
So you can deal with this thing being

836
00:51:50,790 --> 00:51:52,590
Technically it's an affine function because of

837
00:51:52,700 --> 00:51:54,640
this extra offset, rather than a linear function.

838
00:51:54,750 --> 00:51:56,420
But this is just a little bit of bookkeeping

839
00:51:56,550 --> 00:51:57,940
[inaudible] for yourself

840
00:51:58,070 --> 00:51:59,390
and shouldn't be a huge deal.

841
00:52:18,170 --> 00:52:19,030
So to summarize,

842
00:52:19,170 --> 00:52:20,270
you see I have this up,

843
00:52:20,380 --> 00:52:22,200
you can learn a model,

844
00:52:22,330 --> 00:52:24,000
you can take a nonlinear model.

845
00:52:24,140 --> 00:52:25,510
Your nonlinear model can be a physics

846
00:52:25,640 --> 00:52:26,830
model or a nonlinear model you learned

847
00:52:26,920 --> 00:52:31,590
and linearize it. Now I'll post an LQR problem

848
00:52:31,710 --> 00:52:35,840
in which we have specification of the

849
00:52:35,950 --> 00:52:37,950
MDP in which the states are in RN, the actions

850
00:52:38,070 --> 00:52:42,910
are in RD, and the state has zero

851
00:52:43,040 --> 00:52:45,300
probabilities given by the [inaudible] linear

852
00:52:45,400 --> 00:52:50,070
equation. SD plus one equals ATST plus

853
00:52:50,180 --> 00:52:54,650
BTAT. Our rewards are going to be these

854
00:52:54,770 --> 00:52:56,050
quadratic functions.

855
00:52:56,170 --> 00:52:59,810
So the specification of the MDP means

856
00:52:59,920 --> 00:53:02,070
that we know the A matrixes, the B matrixes,

857
00:53:02,150 --> 00:53:04,330
the U matrixes and the V matrixes.

858
00:53:04,450 --> 00:53:07,450
Our goal is to come up with

859
00:53:07,570 --> 00:53:11,600
a policy to maximize our finite horizon

860
00:53:11,720 --> 00:53:20,620
sum of rewards. So our goal is

861
00:53:20,710 --> 00:53:24,350
to come up with a policy, first, to maximize

862
00:53:24,450 --> 00:53:27,940
the expected value of this finite

863
00:53:28,060 --> 00:53:30,330
horizon sum of rewards.

864
00:53:47,160 --> 00:53:51,040
Okay. So our approach to solving this problem

865
00:53:51,160 --> 00:53:57,250
will be exactly that finite horizon dynamic

866
00:53:57,370 --> 00:53:59,140
programming algorithm that we worked out a

867
00:53:59,300 --> 00:54:02,490
little earlier in this lecture. In particular, my

868
00:54:02,630 --> 00:54:06,750
strategy for finding the optimal policy will be

869
00:54:06,850 --> 00:54:14,270
to first find V star of T, the capital T, and

870
00:54:14,380 --> 00:54:17,330
then I'll apply by a recursion to find V star of T

871
00:54:17,470 --> 00:54:19,700
minus one, V star of T minus two and so on.

872
00:54:19,840 --> 00:54:25,280
In the dynamic programming algorithm we

873
00:54:25,390 --> 00:54:28,360
worked out, V star subscript T of the state ST,

874
00:54:28,500 --> 00:54:32,800
this is the maximum [inaudible] actions you

875
00:54:32,920 --> 00:54:39,340
might take at that time of R of STAT. Again,

876
00:54:39,450 --> 00:54:41,870
just for the sake of understanding this material,

877
00:54:41,980 --> 00:54:45,550
you can probably pretend the rewards and

878
00:54:45,650 --> 00:54:46,930
the dynamics are actually stationary.

879
00:54:47,050 --> 00:54:48,590
I'll write out all these superscripts all the time

880
00:54:48,710 --> 00:54:50,670
[inaudible] if you're reading

881
00:54:50,790 --> 00:54:52,780
this for the first time.

882
00:54:52,920 --> 00:54:58,120
The reward is equal to max of AT of minus

883
00:55:12,480 --> 00:55:13,600
right? I hope this isn't confusing.

884
00:55:13,700 --> 00:55:14,920
The superscript Ts denote transposes.

885
00:55:15,040 --> 00:55:16,840
The lowercase Ts denote the time index

886
00:55:16,960 --> 00:55:20,810
capital T. So that's just a definition of

887
00:55:20,920 --> 00:55:22,680
my next quadratic awards.

888
00:55:22,810 --> 00:55:26,320
So this is clearly maximized as minus

889
00:55:26,440 --> 00:55:38,830
ST transpose UTST because that last term is

890
00:55:38,900 --> 00:55:43,060
this is greater than or equal to zero.

891
00:55:43,180 --> 00:55:45,220
That gives me my assumption

892
00:55:45,320 --> 00:55:46,770
that VT is [inaudible] semi-definite.

893
00:55:46,890 --> 00:55:48,200
So the best action to take in

894
00:55:48,310 --> 00:55:51,520
the last time step is just the action zero.

895
00:55:51,620 --> 00:55:59,300
So pi star subscript T of ST is equal to the

896
00:55:59,420 --> 00:56:04,920
[inaudible] of actions of that same thing.

897
00:56:05,050 --> 00:56:13,420
It's just zero. It's by choosing the zero action,

898
00:56:13,560 --> 00:56:17,060
AT transpose VTAT becomes zero, and that's

899
00:56:17,200 --> 00:56:20,230
how this reward is maximized.

900
00:56:44,770 --> 00:56:52,190
Any questions, or is something illegible?

901
00:56:52,350 --> 00:57:01,580
Okay. So now let's do the dynamic

902
00:57:01,690 --> 00:57:09,910
programming step where my goal is given VT

903
00:57:10,020 --> 00:57:13,060
plus one, I want to compute VT.

904
00:57:13,180 --> 00:57:15,170
Given V star T plus one,

905
00:57:15,320 --> 00:57:16,970
I want to compute V star of T.

906
00:57:17,110 --> 00:57:18,760
So this is the dynamic programming step.

907
00:57:18,880 --> 00:57:26,880
So the DP steps I wrote down previously

908
00:57:26,980 --> 00:57:29,620
was this. So for the finite state case,

909
00:57:29,730 --> 00:57:31,120
I wrote down the following.

910
00:58:32,350 --> 00:58:35,410
So this is exactly the equation I wrote down

911
00:58:35,500 --> 00:58:38,890
previously, and this is what I wrote down for

912
00:58:39,020 --> 00:58:40,750
finite states, where you have these discreet state

913
00:58:40,900 --> 00:58:42,640
transition probabilities, and we can sum

914
00:58:42,760 --> 00:58:46,760
over this discreet set of states. Now we're going

915
00:58:46,910 --> 00:58:48,330
to continue as an infinite state again, so

916
00:58:48,450 --> 00:58:50,460
this sum over state should actually become an

917
00:58:50,610 --> 00:58:52,710
integral. I'm going to actually skip the

918
00:58:52,850 --> 00:58:54,680
integral step. We'll just go ahead and write this

919
00:58:54,820 --> 00:58:57,200
last term here as an expectation. So this is

920
00:58:57,370 --> 00:59:05,210
going to be max over actions AT plus and

921
00:59:05,350 --> 00:59:07,250
then this becomes and expectation over the

922
00:59:07,350 --> 00:59:09,090
random mixed state, ST plus one, [inaudible]

923
00:59:09,240 --> 00:59:12,150
from state transition probabilities given by

924
00:59:12,260 --> 00:59:21,400
P of STAT of V star T plus one, ST plus one. So

925
00:59:21,510 --> 00:59:24,180
this is the same equation written down

926
00:59:24,280 --> 00:59:26,370
as an expectation.

927
00:59:26,480 --> 00:59:30,070
So what I need to do is given a representation

928
00:59:30,210 --> 00:59:33,380
of V star T plus one, I need to find V star

929
00:59:33,450 --> 00:59:38,620
of T. So it turns out that LQR has the following

930
00:59:38,740 --> 00:59:40,980
useful property. It turns out that each of

931
00:59:41,110 --> 00:59:44,020
these value functions can be represented as a

932
00:59:44,110 --> 00:59:48,840
quadratic function. So concretely, let's

933
00:59:48,980 --> 00:59:58,820
suppose that V star T plus one suppose that

934
00:59:58,920 --> 01:00:00,510
this can be expressed as a quadratic

935
01:00:00,640 --> 01:00:12,980
function, written like so, where the matrix phi T

936
01:00:13,110 --> 01:00:18,010
plus one is an N by N matrix, and psi T

937
01:00:18,120 --> 01:00:23,850
plus one is just a real number.

938
01:00:23,960 --> 01:00:25,480
So in other words, suppose V star T plus one is

939
01:00:25,590 --> 01:00:33,160
just a quadratic function of the state ST

940
01:00:33,240 --> 01:00:40,940
plus one. We can then show that when you do

941
01:00:41,070 --> 01:00:43,450
one dynamic programming step when

942
01:00:43,560 --> 01:00:47,880
you plug this definition of V star T plus one

943
01:00:47,990 --> 01:00:50,240
into your dynamic programming step in the

944
01:00:50,360 --> 01:00:52,360
equation I had just now, you can show that you

945
01:00:52,480 --> 01:00:58,020
would get that V star T as well, will also

946
01:00:58,160 --> 01:01:01,800
be a quadratic function of the same form.

947
01:01:01,940 --> 01:01:13,270
[Inaudible] here, right? The sum-appropriate

948
01:01:13,380 --> 01:01:16,160
matrix, phi T and sum appropriate real number,

949
01:01:16,260 --> 01:01:20,400
psi of T.

950
01:01:20,520 --> 01:01:24,930
So what you can do is stall off the recursion

951
01:01:25,040 --> 01:01:32,490
with well, does that make sense? So what

952
01:01:32,620 --> 01:01:37,260
you can do is stall off the recursion as follows.

953
01:01:37,410 --> 01:01:39,630
So previously, we worked out that V star

954
01:01:39,770 --> 01:01:41,740
capital T, we said that this is

955
01:01:41,860 --> 01:01:45,300
minus ST transpose UTST.

956
01:01:45,420 --> 01:01:51,700
So we have that phi of capital T

957
01:01:51,820 --> 01:01:57,390
is equal to minus UT, and psi of capital T is

958
01:01:57,510 --> 01:02:02,760
equal to zero. Now V star T of ST is equal to

959
01:02:02,910 --> 01:02:08,850
ST transpose phi of T, ST plus psi of T.

960
01:02:08,990 --> 01:02:10,630
So you can start out the recursion this way with

961
01:02:10,740 --> 01:02:12,580
phi of T equals minus UT

962
01:02:12,690 --> 01:02:14,470
and psi of T equals zero.

963
01:02:14,590 --> 01:02:18,360
Then work out what the recursion is.

964
01:02:18,500 --> 01:02:24,540
I won't actually do the full derivation.

965
01:02:24,680 --> 01:02:27,740
This may be algebra, and you've actually

966
01:02:27,860 --> 01:02:32,960
done this sort of Gaussian expectation math

967
01:02:33,120 --> 01:02:34,740
a lot in your homework by now.

968
01:02:34,850 --> 01:02:41,440
So I won't do the full derivation.

969
01:02:41,560 --> 01:02:46,620
I'll just outline the one-ish G step.

970
01:02:46,740 --> 01:02:49,010
So in dynamic programming step,

971
01:02:49,130 --> 01:02:54,440
V star ST is equal to max over actions

972
01:02:54,560 --> 01:02:56,750
AT of the median reward.

973
01:02:56,870 --> 01:03:07,740
So this was R of SA from my equation in the

974
01:03:07,870 --> 01:03:10,030
dynamic programming step. Then plus an

975
01:03:10,150 --> 01:03:14,470
expected value over the random mixed state, ST

976
01:03:14,590 --> 01:03:18,380
plus one, drawn from the Gaussian

977
01:03:18,510 --> 01:03:25,030
distribution would mean ATST plus BTAT and

978
01:03:25,090 --> 01:03:30,160
covariant sigma W. So what this is, this

979
01:03:30,290 --> 01:03:33,740
is really my specification for P of STAT. This is

980
01:03:33,890 --> 01:03:36,290
my state transition distribution in the

981
01:03:36,430 --> 01:03:39,230
LQR setting. This is my state transition

982
01:03:39,360 --> 01:03:42,240
distribution [inaudible] take action AT in the

983
01:03:42,360 --> 01:03:47,230
state ST. Then my next state is distributed

984
01:03:47,340 --> 01:03:49,640
Gaussian would mean ATST plus BTAT and

985
01:03:49,750 --> 01:03:55,810
covariant sigma W. Then of the this state.

986
01:04:10,620 --> 01:04:17,650
This, of course, is just V star T plus one of ST

987
01:04:17,770 --> 01:04:24,540
plus one. I hope this makes sense. This is

988
01:04:24,650 --> 01:04:27,200
just taking that equation I had previously in the

989
01:04:27,330 --> 01:04:29,640
dynamic programming step. So the V star

990
01:04:29,780 --> 01:04:32,640
of T, ST equals max over actions of the

991
01:04:32,760 --> 01:04:37,090
immediate rewards plus an expected value over

992
01:04:37,240 --> 01:04:40,540
the mixed state of V star of the mixed state with

993
01:04:40,670 --> 01:04:43,880
the clock advanced by one. So I've just

994
01:04:44,280 --> 01:04:45,800
plugged in all the definitions as a reward of the

995
01:04:45,920 --> 01:04:47,990
state [inaudible] distribution and of the

996
01:04:48,090 --> 01:04:50,080
value function.

997
01:04:50,210 --> 01:04:56,900
Actually, could you raise your hand

998
01:04:56,990 --> 01:05:03,020
if this makes sense? Cool.

999
01:05:03,190 --> 01:05:05,810
So if you write this out and you expand

1000
01:05:05,920 --> 01:05:07,680
the expectation I know you've done

1001
01:05:07,760 --> 01:05:08,810
this many times, so I won't do it

1002
01:05:08,940 --> 01:05:11,530
this whole thing on the right-hand side

1003
01:05:11,640 --> 01:05:12,660
simplifies to a big quadratic

1004
01:05:12,770 --> 01:05:14,390
function of the action,

1005
01:05:14,500 --> 01:05:22,990
AT. So this whole thing simplifies to a big

1006
01:05:23,100 --> 01:05:38,320
quadratic function of the action AT. We want

1007
01:05:38,450 --> 01:05:40,960
to maximize this with respect to the actions AT.

1008
01:05:41,080 --> 01:05:43,810
So to maximize a big quadratic function,

1009
01:05:43,910 --> 01:05:46,320
you just take the derivatives of the functions

1010
01:05:46,460 --> 01:05:48,360
with respect to the action AT, set the

1011
01:05:48,470 --> 01:05:50,430
derivative equal to zero, and then you've

1012
01:05:50,530 --> 01:05:52,250
maximized the right-hand side, with respect to

1013
01:05:52,390 --> 01:05:54,650
the action, AT.

1014
01:05:54,760 --> 01:05:58,080
It turns out I'm just going to write this

1015
01:05:58,200 --> 01:06:00,070
expression down for completeness. You can

1016
01:06:00,210 --> 01:06:02,670
derive it yourself at any time. It turns out if you

1017
01:06:02,770 --> 01:06:05,310
actually maximize that thing on the right hand

1018
01:06:05,410 --> 01:06:07,500
side as a function of the actions, AT, you find

1019
01:06:07,600 --> 01:06:10,190
that [inaudible] AT is going to be that

1020
01:06:24,280 --> 01:06:31,170
times ST. Don't worry about this expression.

1021
01:06:31,290 --> 01:06:33,290
You can get it from [inaudible]

1022
01:06:33,420 --> 01:06:34,740
and derive it yourself.

1023
01:06:34,860 --> 01:06:36,470
But the key thing to note is that the optimal

1024
01:06:36,620 --> 01:06:38,980
action, AT, is going to be some big matrix.

1025
01:06:39,090 --> 01:06:46,670
We're going to call this thing LT times ST. In

1026
01:06:46,770 --> 01:06:50,980
other words, the optimal action to take in

1027
01:06:51,120 --> 01:06:53,860
this given state is going to be some linear

1028
01:06:53,970 --> 01:06:59,620
function of the state, ST. So having done

1029
01:06:59,750 --> 01:07:01,350
dynamic programming, you remember also

1030
01:07:01,470 --> 01:07:03,610
when we worked out the dynamic

1031
01:07:03,710 --> 01:07:05,360
programming algorithm for finite horizon

1032
01:07:05,480 --> 01:07:09,070
MDPs, we said that the way you compute the

1033
01:07:09,220 --> 01:07:13,520
optimal policy, pi star of T of ST. This is always

1034
01:07:13,610 --> 01:07:16,070
the [inaudible] of the same thing.

1035
01:07:16,160 --> 01:07:20,430
[Inaudible] of actions AT of the same thing.

1036
01:07:20,530 --> 01:07:26,590
STAT plus your expected value of

1037
01:07:26,730 --> 01:07:33,890
[inaudible] PSTAT, P-star, T plus one, ST plus

1038
01:07:34,000 --> 01:07:37,330
one. This thing on the right-hand side is

1039
01:07:37,450 --> 01:07:39,710
always the same thing as the thing we

1040
01:07:39,840 --> 01:07:42,230
maximized [inaudible].

1041
01:07:42,350 --> 01:07:44,530
So what this means is that

1042
01:07:44,640 --> 01:07:46,740
when I said this a value of A to the maximize of

1043
01:07:46,860 --> 01:07:48,610
this. So what this means is that the

1044
01:07:48,730 --> 01:07:50,610
optimal action to take from the state of ST is

1045
01:07:50,710 --> 01:07:56,420
actually equal to LT times ST.

1046
01:07:56,530 --> 01:08:06,220
What was shown is that when you're

1047
01:08:06,360 --> 01:08:07,770
in some state, ST,

1048
01:08:07,900 --> 01:08:10,070
the optimal action for that state is going to be

1049
01:08:10,180 --> 01:08:13,690
some matrix, LT, which can compute, times the

1050
01:08:13,810 --> 01:08:19,690
state, ST. In other words, the optimal action

1051
01:08:19,820 --> 01:08:21,830
is actually a linear function of the state.

1052
01:08:21,970 --> 01:08:24,690
I'm just going to point out, this is not a function

1053
01:08:24,840 --> 01:08:27,820
of approximation here, right. What we did not

1054
01:08:27,910 --> 01:08:32,990
do, we did not say, let's find the optimal linear

1055
01:08:33,100 --> 01:08:35,080
policy. We didn't say, let's look at the optimal

1056
01:08:35,200 --> 01:08:36,940
policy, and then we'll fit this straight line to

1057
01:08:37,040 --> 01:08:38,200
the optimal policy.

1058
01:08:38,320 --> 01:08:39,820
This is not about approximating

1059
01:08:39,950 --> 01:08:42,270
the optimal policy with a straight line.

1060
01:08:42,370 --> 01:08:44,560
This derivation is saying that the optimal policy

1061
01:08:44,720 --> 01:08:47,410
is a straight line. The optimal action is a

1062
01:08:47,510 --> 01:08:57,570
linear function of the current state. Moreover,

1063
01:08:57,680 --> 01:09:00,570
when you've worked out this is a value for

1064
01:09:00,700 --> 01:09:04,430
AT that maximizes this thing on the right-hand

1065
01:09:04,580 --> 01:09:06,740
side. So you take this and plug it back in

1066
01:09:06,860 --> 01:09:09,060
to do the dynamic programming recursion.

1067
01:09:09,170 --> 01:09:20,490
What you find is that so you take AT and

1068
01:09:20,620 --> 01:09:23,200
plug it back in to do the maximization. It will

1069
01:09:23,300 --> 01:09:29,450
actually get you this formula, so V star TST.

1070
01:09:29,640 --> 01:09:36,750
So you find that it will indeed be a quadratic

1071
01:09:36,880 --> 01:09:39,460
function like this of the following form

1072
01:09:39,580 --> 01:09:43,290
where and I just write out the equations for

1073
01:09:43,430 --> 01:09:46,060
the sake of completeness. Don't worry too

1074
01:09:46,170 --> 01:09:47,850
much about their forms.

1075
01:09:48,010 --> 01:09:57,760
You can derive this yourself.

1076
01:10:47,060 --> 01:10:51,570
So just to summarize, don't worry too much

1077
01:10:51,670 --> 01:10:53,000
about the forms of these equations.

1078
01:10:53,100 --> 01:10:56,140
What we've done is written down the recursion

1079
01:10:56,280 --> 01:11:00,280
to the expressor phi T and psi T as a function of

1080
01:11:00,370 --> 01:11:03,070
phi T plus one and psi T plus one.

1081
01:11:03,190 --> 01:11:06,810
So this allows you to compute

1082
01:11:06,920 --> 01:11:08,620
the optimal value function

1083
01:11:08,730 --> 01:11:12,270
for when the clock is at time lowercase T,

1084
01:11:12,400 --> 01:11:13,990
as a function of the optimal value function

1085
01:11:14,120 --> 01:11:16,190
for when the clock is at time T plus one.

1086
01:11:16,340 --> 01:11:29,780
So to summarize, GSELQG

1087
01:11:29,890 --> 01:11:32,080
here's a finite horizon of

1088
01:11:32,190 --> 01:11:35,590
actually, just to give this equation

1089
01:11:35,690 --> 01:11:37,000
a name as well. This recursion, in terms of

1090
01:11:37,120 --> 01:11:40,670
the phi Ts, this is called the discrete time

1091
01:11:40,750 --> 01:11:54,810
Bacardi equation. [Inaudible] recursion that

1092
01:11:54,930 --> 01:12:00,160
gives you phi T in terms of phi T plus one.

1093
01:12:00,270 --> 01:12:14,030
So to summarize, our algorithm for finding the

1094
01:12:14,140 --> 01:12:16,210
exact solution to finite horizon LQR

1095
01:12:16,320 --> 01:12:24,730
problems is as follows. We initialize phi T to be

1096
01:12:24,830 --> 01:12:34,410
equal to minus UT and psi T to be equal

1097
01:12:34,540 --> 01:12:47,340
to zero. Then recursively, calculate phi T and

1098
01:12:47,480 --> 01:12:53,860
psi T as a function of phi T plus one and psi

1099
01:12:53,960 --> 01:13:02,010
T plus one with the discrete time actually,

1100
01:13:02,140 --> 01:13:06,860
excuse me. So recursively calculate phi T

1101
01:13:06,950 --> 01:13:10,370
and psi T as a function of phi T plus one and psi

1102
01:13:10,510 --> 01:13:11,710
T plus one, as I showed, using the

1103
01:13:11,810 --> 01:13:16,210
discrete time Bacardi equation. So you do this

1104
01:13:16,330 --> 01:13:20,160
for T equals T minus one, T minus two

1105
01:13:20,290 --> 01:13:24,140
and so on, down to time zero.

1106
01:13:24,260 --> 01:13:33,700
Then you compute LT as a function of

1107
01:13:33,860 --> 01:13:40,220
actually, is it phi T or phi T plus one? Phi T plus

1108
01:13:40,290 --> 01:13:43,720
one, I think. As a function of phi T plus one and

1109
01:13:43,860 --> 01:13:46,710
psi T plus one. This is actually a function

1110
01:13:46,820 --> 01:13:48,430
of only phi T plus one. You don't really need psi

1111
01:13:48,540 --> 01:13:54,150
T plus one. Now you have your optimal policy.

1112
01:13:54,260 --> 01:14:03,650
So having computed the LTs, you now have the

1113
01:14:03,780 --> 01:14:07,230
optimal action to take in the state ST,

1114
01:14:07,360 --> 01:14:10,660
just given by this linear equation.

1115
01:14:10,780 --> 01:14:29,700
How much time do I have left? Okay. Let me

1116
01:14:35,330 --> 01:14:37,150
just say one last thing about this before I

1117
01:14:37,270 --> 01:14:46,060
close. Maybe I'll do it next week. I think I'll do

1118
01:14:46,160 --> 01:14:49,090
it next session instead. So it actually turns

1119
01:14:49,220 --> 01:14:51,400
out there's one cool property about this that's

1120
01:14:51,520 --> 01:14:53,170
kind of that is kind of subtle, but you'll find

1121
01:14:53,240 --> 01:14:55,560
it out in the next lecture. Are there question

1122
01:14:55,660 --> 01:14:58,140
about this before we close for today, then?

1123
01:15:04,210 --> 01:15:08,980
So the very cool thing about the solution of

1124
01:15:09,060 --> 01:15:12,140
discrete time LQR problems finite horizon

1125
01:15:12,250 --> 01:15:15,320
LQR problems is that this is a problem in an

1126
01:15:15,420 --> 01:15:17,260
infinite state, with a continuous state. But

1127
01:15:17,380 --> 01:15:20,330
nonetheless, under the assumptions we made,

1128
01:15:20,440 --> 01:15:22,330
you can prove that the value function is a

1129
01:15:22,450 --> 01:15:25,480
quadratic function of the state. Therefore, just

1130
01:15:25,580 --> 01:15:27,790
by computing these matrixes phi T and the

1131
01:15:27,900 --> 01:15:30,540
real numbers psi T, you can actually exactly

1132
01:15:30,700 --> 01:15:33,130
represent the value function, even for these

1133
01:15:33,250 --> 01:15:35,720
infinitely large state spaces, even for

1134
01:15:35,830 --> 01:15:37,460
continuous state spaces.

1135
01:15:37,580 --> 01:15:39,600
So the computation of these algorithms scales

1136
01:15:39,670 --> 01:15:43,540
only like the cube, scales only as a polynomial

1137
01:15:43,630 --> 01:15:46,080
in terms of the number of state variables

1138
01:15:46,210 --> 01:15:49,080
whereas in [inaudible] dimensionality

1139
01:15:49,190 --> 01:15:50,860
problems, with [inaudible], we had algorithms

1140
01:15:50,990 --> 01:15:52,860
of a scale exponentially dimensional problem.

1141
01:15:52,980 --> 01:15:57,370
Whereas LQR scales only are like the cube of

1142
01:15:57,510 --> 01:15:59,010
the dimension of the problem.

1143
01:15:59,140 --> 01:16:01,790
So this easily applies to problems

1144
01:16:01,880 --> 01:16:03,380
with even very large state spaces.

1145
01:16:03,540 --> 01:16:05,450
So we actually often apply variations of this

1146
01:16:05,560 --> 01:16:07,700
algorithm to some subset, to some particular

1147
01:16:07,810 --> 01:16:09,770
subset for the things we do on our helicopter,

1148
01:16:09,910 --> 01:16:11,650
which has high dimensional state spaces,

1149
01:16:11,760 --> 01:16:14,460
with twelve or higher dimensions. This has

1150
01:16:14,560 --> 01:16:18,120
worked very well for that. So it turns out

1151
01:16:18,250 --> 01:16:19,780
there are even more things you can do with this,

1152
01:16:19,890 --> 01:16:22,240
and I'll continue with that in the next

