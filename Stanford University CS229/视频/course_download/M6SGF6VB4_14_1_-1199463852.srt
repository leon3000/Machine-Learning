1
00:00:23,610 --> 00:00:26,930
好的  欢迎回来

2
00:00:27,680 --> 00:00:30,350
我今天要继续

3
00:00:30,580 --> 00:00:33,700
讲完因子分析

4
00:00:33,920 --> 00:00:34,840
具体地

5
00:00:35,050 --> 00:00:37,720
我要详细地讲解

6
00:00:37,930 --> 00:00:39,980
因子分析中EM步骤的

7
00:00:40,190 --> 00:00:41,410
推导过程

8
00:00:41,600 --> 00:00:43,680
因为其中的

9
00:00:43,830 --> 00:00:44,650
一些步骤

10
00:00:44,830 --> 00:00:45,630
比较有技巧

11
00:00:45,860 --> 00:00:48,700
而人们在推导类似于

12
00:00:48,870 --> 00:00:49,410
因子分析EM步骤

13
00:00:49,620 --> 00:00:51,850
的过程中

14
00:00:52,050 --> 00:00:53,140
经常会犯一些特定的错误

15
00:00:53,310 --> 00:00:54,310
所以我希望你们

16
00:00:54,500 --> 00:00:55,530
能够正确地理解这些推导步骤

17
00:00:55,720 --> 00:00:56,610
以使得你们处理

18
00:00:56,840 --> 00:00:57,500
类似问题时可以避免错误

19
00:00:58,320 --> 00:01:01,850
这节课的后半部分

20
00:01:02,020 --> 00:01:03,860
我会讲主成分分析

21
00:01:04,090 --> 00:01:06,510
这是一个非常强大的

22
00:01:06,750 --> 00:01:09,380
降低维度的算法

23
00:01:09,610 --> 00:01:10,730
我们稍后会看到这个名词的含义

24
00:01:11,000 --> 00:01:13,560
回顾一下之前的内容

25
00:01:15,460 --> 00:01:16,790
上一讲

26
00:01:17,140 --> 00:01:19,070
我描述了高斯分布

27
00:01:19,260 --> 00:01:20,260
的一些性质

28
00:01:20,990 --> 00:01:24,450
一个性质是

29
00:01:24,680 --> 00:01:27,040
如果你有一个随机取值的向量x

30
00:01:27,270 --> 00:01:29,370
可以将其分成两个部分

31
00:01:29,570 --> 00:01:30,240
x_1 和x_2

32
00:01:30,760 --> 00:01:34,930
如果x~N(μ  Σ)

33
00:01:35,010 --> 00:01:36,170
那么μ也可以被

34
00:01:36,250 --> 00:01:42,490
写作这样的一个分块向量

35
00:01:43,130 --> 00:01:53,380
而Σ也可以被

36
00:01:53,670 --> 00:01:54,890
写作这样的一个分块矩阵

37
00:01:55,110 --> 00:01:56,820
这里我用

38
00:01:57,010 --> 00:01:59,220
四个子块来表示Σ

39
00:01:59,710 --> 00:02:03,340
如果你得到这样的

40
00:02:03,520 --> 00:02:04,800
一个x的分布

41
00:02:05,010 --> 00:02:07,770
你可以求出x_1的边际分布

42
00:02:09,920 --> 00:02:15,080
上节课我们求出的结果是

43
00:02:15,430 --> 00:02:17,070
x_1的边际分布是一个高斯分布

44
00:02:17,330 --> 00:02:21,180
均值是μ_1

45
00:02:21,500 --> 00:02:23,140
协方差是Σ_11

46
00:02:23,490 --> 00:02:25,140
Σ_11是协方差矩阵Σ

47
00:02:25,450 --> 00:02:27,260
右上角的子块

48
00:02:27,620 --> 00:02:29,020
这个结果并不出人意料

49
00:02:29,370 --> 00:02:31,140
我还写出了

50
00:02:31,420 --> 00:02:33,140
求条件分布的公式

51
00:02:35,680 --> 00:02:39,840
也就是P(x_1 |x_2)

52
00:02:40,770 --> 00:02:42,410
上次我写的结果是

53
00:02:42,700 --> 00:02:45,040
x_1 |x_2

54
00:02:45,460 --> 00:02:47,260
也是一个高斯分布

55
00:02:47,690 --> 00:02:53,350
参数分别

56
00:02:53,640 --> 00:02:55,220
是μ_(1|2) 和Σ_(1|2)

57
00:02:55,640 --> 00:02:59,940
其中μ_(1|2)

58
00:03:00,330 --> 00:03:22,740
等于这个式子 


59
00:03:23,230 --> 00:03:26,630
所以利用这些公式

60
00:03:26,950 --> 00:03:28,840
对于一对

61
00:03:29,140 --> 00:03:30,840
联合高斯随机变量

62
00:03:31,190 --> 00:03:32,560
x_1 和x_2都是向量

63
00:03:33,010 --> 00:03:34,120
我们可以计算出边际分布

64
00:03:34,480 --> 00:03:36,030
和条件分布

65
00:03:36,410 --> 00:03:38,550
比如说:P(x_1 )和P(x_1 |x_2)

66
00:03:38,920 --> 00:03:42,420
当我推导

67
00:03:42,730 --> 00:03:43,310
E-step时  


68
00:03:43,770 --> 00:03:45,760
我会用到这个

69
00:03:46,060 --> 00:03:47,710
边际分布的公式

70
00:03:48,110 --> 00:03:49,470
当我一会儿来推导

71
00:03:49,770 --> 00:03:52,320
因子分析

72
00:03:52,600 --> 00:03:53,360
EM算法的E-step时

73
00:03:53,780 --> 00:03:55,910
我会用到这

74
00:03:56,230 --> 00:03:56,980
两个公式

75
00:03:57,360 --> 00:04:02,710
我们继续总结

76
00:04:03,050 --> 00:04:04,060
之前的内容

77
00:04:04,450 --> 00:04:11,100
在因子分析中我们的模型--


78
00:04:11,480 --> 00:04:15,360
这是一个无监督学习问题

79
00:04:15,680 --> 00:04:19,630
所以我们有一个

80
00:04:20,030 --> 00:04:24,790
无标记的训练集合

81
00:04:25,050 --> 00:04:25,580
像往常一样

82
00:04:25,910 --> 00:04:32,150
每个x^((i) )都是一个n维向量

83
00:04:32,490 --> 00:04:35,270
我们希望对P(x)进行建模

84
00:04:35,620 --> 00:04:37,480
我们的模型假设

85
00:04:37,730 --> 00:04:38,600
存在一个隐含随机变量z

86
00:04:39,080 --> 00:04:41,810
服从高斯分布

87
00:04:42,090 --> 00:04:43,190
均值是0

88
00:04:43,660 --> 00:04:46,940
协方差是单位矩阵

89
00:04:47,300 --> 00:04:49,040
z是一个d维向量

90
00:04:49,420 --> 00:04:51,000
比n维低

91
00:04:51,710 --> 00:04:59,480
我们想象:

92
00:04:59,790 --> 00:05:03,200
x=μ+Λz+ε

93
00:05:03,500 --> 00:05:05,570
ε是一个高斯分布

94
00:05:05,880 --> 00:05:07,560
均值是0

95
00:05:08,090 --> 00:05:10,510
协方差矩阵是Λ

96
00:05:11,390 --> 00:05:15,330
这个模型的参数包括

97
00:05:15,750 --> 00:05:19,210
n维向量μ

98
00:05:19,590 --> 00:05:22,800
n*d的矩阵Λ

99
00:05:23,140 --> 00:05:24,140
n*n的

100
00:05:24,420 --> 00:05:26,550
对角矩阵Ψ

101
00:05:26,860 --> 00:05:29,610
这是一个

102
00:05:29,850 --> 00:05:37,050
对角矩阵

103
00:05:37,420 --> 00:05:41,990
我记得上次画的

104
00:05:42,260 --> 00:05:43,390
因子分析的图是--

105
00:05:43,850 --> 00:05:49,470
这是一个典型的取样结果

106
00:05:49,840 --> 00:05:51,000
数据点表示为z^((i) )

107
00:05:51,460 --> 00:05:54,430
这个例子中d=1

108
00:05:54,910 --> 00:05:56,480
n=2

109
00:05:56,970 --> 00:06:00,050
这个例子中的z是一维的

110
00:06:00,360 --> 00:06:00,920
d=1

111
00:06:01,390 --> 00:06:08,010
将这些数据映射到

112
00:06:08,360 --> 00:06:10,370
μ+Λz

113
00:06:10,620 --> 00:06:16,840
可能会得到这样的一些点

114
00:06:17,140 --> 00:06:19,770
之后你可以想象在每个点周围

115
00:06:20,040 --> 00:06:22,880
都有一个高斯曲面

116
00:06:23,190 --> 00:06:25,940
之后我们在区域内进行取样

117
00:06:26,290 --> 00:06:37,260
可能会得到这样一些x点  


118
00:06:37,650 --> 00:06:39,200
这些数据是一个

119
00:06:39,480 --> 00:06:40,890
典型模型的采样结果

120
00:06:53,230 --> 00:06:56,970
应该怎样得到这个模型的参数?

121
00:06:58,050 --> 00:07:08,750
z和x 的联合分布

122
00:07:09,510 --> 00:07:14,850
是一个高斯分布

123
00:07:15,160 --> 00:07:17,340
参数是μ_xz 和

124
00:07:17,710 --> 00:07:19,460
协方差矩阵Σ

125
00:07:19,860 --> 00:07:23,780
如果将这些参数的内容写出来

126
00:07:24,150 --> 00:07:26,320
μ_xz由0向量

127
00:07:26,690 --> 00:07:29,440
和μ组成

128
00:07:29,860 --> 00:07:35,750
而Σ是这样的一个分块矩阵

129
00:07:36,150 --> 00:07:41,430
这个结果是上节课推出的

130
00:07:45,820 --> 00:07:48,080
所以我们要求的该模型下的P(x)

131
00:07:48,300 --> 00:07:49,040
在这个模型下

132
00:07:49,510 --> 00:07:53,340
x服从这样的

133
00:07:53,670 --> 00:08:00,680
高斯分布

134
00:08:01,000 --> 00:08:02,540
这两个参数

135
00:08:02,910 --> 00:08:04,420
分别对应着

136
00:08:04,730 --> 00:08:06,500
均值向量的第二部分

137
00:08:06,890 --> 00:08:11,930
和协方差矩阵

138
00:08:12,210 --> 00:08:13,370
的右下角

139
00:08:13,720 --> 00:08:17,190
这实际上就是

140
00:08:17,550 --> 00:08:19,440
高斯边际分布的公式

141
00:08:19,780 --> 00:08:21,010
只不过我这里计算的是

142
00:08:21,350 --> 00:08:23,690
第二部分的边际分布

143
00:08:24,010 --> 00:08:25,540
而不是第一部分

144
00:08:26,390 --> 00:08:28,280
这是模型下的

145
00:08:28,570 --> 00:08:29,380
边际分布

146
00:08:29,790 --> 00:08:34,180
所以如果你想学习--什么问题?

147
00:08:34,630 --> 00:08:39,010
S:

148
00:08:39,420 --> 00:08:40,440
I:让我看看

149
00:08:40,980 --> 00:08:42,340
是的

150
00:08:42,820 --> 00:08:44,410
这个式子--

151
00:08:44,760 --> 00:08:46,850
实际上我

152
00:08:47,200 --> 00:08:48,750
指定的是x|z的

153
00:08:49,090 --> 00:08:50,240
条件分布

154
00:08:50,620 --> 00:08:52,830
所以x|z应该服从

155
00:08:53,190 --> 00:08:56,090
这样的高斯分布 


156
00:08:56,490 --> 00:09:03,150
这个式子实际上

157
00:09:03,480 --> 00:09:05,620
定义了这个条件分布

158
00:09:12,110 --> 00:09:19,730
既然这是x的

159
00:09:20,060 --> 00:09:20,900
边际分布

160
00:09:21,340 --> 00:09:23,710
那么给定一个由m个

161
00:09:24,030 --> 00:09:25,030
无标记样本构成的训练集合

162
00:09:25,390 --> 00:09:27,420
我可以写出训练集合

163
00:09:27,670 --> 00:09:28,570
的对数似然性

164
00:09:28,910 --> 00:09:31,040
所以我的训练集合

165
00:09:31,510 --> 00:09:32,780
的对数似然性--

166
00:09:33,130 --> 00:09:34,440
还是写似然性吧

167
00:09:34,750 --> 00:09:38,710
给定训练集合

168
00:09:39,070 --> 00:09:40,450
参数的

169
00:09:40,800 --> 00:09:42,270
似然性应该

170
00:09:42,740 --> 00:09:44,310
等于这个式子

171
00:09:52,350 --> 00:09:54,820
我可以将这个式子展开

172
00:09:55,120 --> 00:09:57,000
因为我知道x^((i) )

173
00:09:57,370 --> 00:09:59,270
服从这样的

174
00:09:59,630 --> 00:10:00,900
高斯分布

175
00:10:01,370 --> 00:10:03,580
所以你可以

176
00:10:03,910 --> 00:10:09,190
将公式代入

177
00:10:09,620 --> 00:10:17,230
得到这个结果

178
00:10:30,250 --> 00:10:38,480
这个公式是

179
00:10:38,840 --> 00:10:41,280
这个高斯分布

180
00:10:41,630 --> 00:10:44,890
对应的概率密度函数

181
00:10:45,360 --> 00:10:49,440
这样就得到了给定

182
00:10:49,850 --> 00:10:50,800
训练集合时参数的似然性

183
00:10:51,240 --> 00:10:53,110
你可以尝试最大化

184
00:10:53,450 --> 00:10:55,620
这个似然性公式

185
00:10:55,990 --> 00:10:57,370
并得到参数的

186
00:10:57,700 --> 00:10:58,980
极大似然估计

187
00:11:00,600 --> 00:11:02,630
但是如果你尝试这样做了  


188
00:11:03,270 --> 00:11:05,610
如果你尝试对似然性取对数

189
00:11:05,980 --> 00:11:07,160
对参数求导

190
00:11:07,480 --> 00:11:08,270
令导数为0

191
00:11:08,650 --> 00:11:10,530
你会发现你无法

192
00:11:11,380 --> 00:11:13,530
获得解析形式的解

193
00:11:15,220 --> 00:11:17,290
你无法在解析意义上

194
00:11:17,550 --> 00:11:18,790
求解这个极大似然估计问题

195
00:11:19,190 --> 00:11:20,540
如果你相对于

196
00:11:20,810 --> 00:11:21,820
参数求导

197
00:11:22,580 --> 00:11:24,080
并令导数为0

198
00:11:24,570 --> 00:11:27,020
之后尝试解出参数Λ

199
00:11:27,390 --> 00:11:28,610
μ和Ψ的值

200
00:11:29,160 --> 00:11:31,120
你会发现根本求不出来

201
00:11:31,500 --> 00:11:37,980
所以我们需要用EM算法

202
00:11:38,290 --> 00:11:39,460
估计出因子分析

203
00:11:39,880 --> 00:11:41,170
模型的参数

204
00:11:48,280 --> 00:11:51,550
S:为什么对数似然性中的式子是

205
00:11:51,920 --> 00:11:54,740
P(x)而不是P(x|z)

206
00:11:56,140 --> 00:11:58,040
或者P(x  z)?

207
00:11:58,460 --> 00:11:59,770
I:你的问题是为什么

208
00:12:00,160 --> 00:12:01,720
在求似然性的时候

209
00:12:02,020 --> 00:12:03,050
用的是P(x)

210
00:12:03,330 --> 00:12:04,450
而不是P(x|z)或者P(x  z)

211
00:12:05,360 --> 00:12:08,170
让我看看 


212
00:12:09,720 --> 00:12:11,720
类比一下混合高斯

213
00:12:12,050 --> 00:12:13,670
分布模型

214
00:12:14,130 --> 00:12:17,710
给定一个由

215
00:12:18,040 --> 00:12:24,660
一系列无标记样本构成的训练集合

216
00:12:25,840 --> 00:12:28,380
出于习惯或者一些其他的原因

217
00:12:28,910 --> 00:12:32,640
我们定义联合

218
00:12:33,080 --> 00:12:33,870
概率P(x  z)

219
00:12:34,160 --> 00:12:36,260
可能会更方便

220
00:12:36,720 --> 00:12:42,020
但是我希望最大化的是--

221
00:12:42,420 --> 00:12:44,340
以参数为函数--

222
00:12:44,680 --> 00:12:46,450
这里我用θ简略地

223
00:12:46,720 --> 00:12:47,590
表示所有参数

224
00:12:48,010 --> 00:12:52,740
我希望使我观察到

225
00:12:52,960 --> 00:12:54,230
这些数据的概率最大化

226
00:12:54,590 --> 00:12:59,060
它应该等于这个式子

227
00:12:59,150 --> 00:13:01,950
这实际上是

228
00:13:02,300 --> 00:13:04,630
对z求积分的结果

229
00:13:12,230 --> 00:13:15,840
我真正观察到的只有x

230
00:13:16,200 --> 00:13:18,240
而z仅仅是

231
00:13:18,550 --> 00:13:20,000
隐含的随机变量

232
00:13:20,420 --> 00:13:24,840
所以我希望最大化的

233
00:13:25,220 --> 00:13:26,820
是这个式子

234
00:13:27,120 --> 00:13:28,370
而这里我隐含地相对于z求积分

235
00:13:29,150 --> 00:13:30,760
明白吗?

236
00:13:34,230 --> 00:13:36,160
关于因子分析模型

237
00:13:36,720 --> 00:13:37,640
还有问题吗?

238
00:13:43,410 --> 00:13:56,730
好的 EM算法是这样的

239
00:14:13,740 --> 00:14:15,590
在E-step

240
00:14:15,970 --> 00:14:19,640
我们希望计算出

241
00:14:19,950 --> 00:14:21,840
P(z^((i) ) |x^((i) )  θ)

242
00:14:22,130 --> 00:14:23,350
其中是当前的参数集合

243
00:14:24,090 --> 00:14:25,060
在M-step

244
00:15:00,670 --> 00:15:03,650
我们要求解这个最大化问题

245
00:15:04,640 --> 00:15:06,850
这个式子和之前EM

246
00:15:07,260 --> 00:15:09,340
算法中的版本有区别

247
00:15:09,740 --> 00:15:10,820
唯一的区别是

248
00:15:11,130 --> 00:15:12,370
这里我对z^((i) )求积分

249
00:15:13,030 --> 00:15:17,060
因为z^((i) )是一个高斯随机变量

250
00:15:17,340 --> 00:15:18,510
它是连续取值的

251
00:15:18,800 --> 00:15:19,890
所以这里我要对z^((i) )求积分

252
00:15:20,190 --> 00:15:21,200
而不是对z^((i) )求和

253
00:15:21,640 --> 00:15:23,700
如果你将这里的积分变成一个求和符号

254
00:15:24,070 --> 00:15:25,550
将这里的dz^((i) )省略

255
00:15:25,870 --> 00:15:26,880
那么公式和之前

256
00:15:27,200 --> 00:15:28,400
混合高斯模型

257
00:15:28,620 --> 00:15:29,560
的公式是一样的

258
00:15:29,840 --> 00:15:32,260
事实证明

259
00:15:32,490 --> 00:15:35,100
为了完成

260
00:15:35,380 --> 00:15:36,460
E-step和M-step

261
00:15:36,700 --> 00:15:38,390
求解过程和之前

262
00:15:38,460 --> 00:15:39,390
你看到的

263
00:15:39,670 --> 00:15:40,800
模型相比

264
00:15:41,070 --> 00:15:41,780
有三点不同

265
00:15:42,090 --> 00:15:42,990
我现在要讲一下

266
00:15:43,330 --> 00:15:46,390
第一点

267
00:15:46,700 --> 00:15:48,830
相对简单

268
00:15:49,160 --> 00:15:52,780
现在z^((i) )是一个连续随机变量

269
00:15:53,180 --> 00:15:56,660
所以你需要一种

270
00:15:56,930 --> 00:15:58,790
方式来表示连续取值的概率密度

271
00:15:59,370 --> 00:16:00,040
也就是说需要一个

272
00:16:00,380 --> 00:16:01,420
概率密度函数表示Q_i (z^((i) ) )

273
00:16:02,410 --> 00:16:13,030
幸运的是

274
00:16:13,410 --> 00:16:14,840
这个问题中这一点并不难做到

275
00:16:16,900 --> 00:16:24,030
具体地

276
00:16:24,290 --> 00:16:24,980
z^((i) )在x^((i) )

277
00:16:26,150 --> 00:16:27,750
和当前参数下的条件分布

278
00:16:28,170 --> 00:16:30,840
这个式子中我省略了参数

279
00:16:31,160 --> 00:16:35,790
应该等于这样的高斯分布

280
00:16:36,180 --> 00:16:40,480
它应该等于这个式子--

281
00:16:40,870 --> 00:16:49,920
我是这样得到这个公式的

282
00:16:50,180 --> 00:16:50,980
如果你将这个公式

283
00:16:59,330 --> 00:17:03,450
和我之前推出的求解高斯条件分布的公式


284
00:17:03,860 --> 00:17:08,000
进行一下比较

285
00:17:08,400 --> 00:17:09,710
你会发现这些项是一一对应的

286
00:17:10,000 --> 00:17:10,960
这些项和我之前求出的

287
00:17:11,380 --> 00:17:14,010
计算高斯条件分布的

288
00:17:14,280 --> 00:17:16,720
公式中的项

289
00:17:18,130 --> 00:17:21,780
是对应的

290
00:17:22,070 --> 00:17:23,800
协方差矩阵应该

291
00:17:24,690 --> 00:17:26,040
等于这个式子

292
00:17:26,390 --> 00:17:33,580
这些项和

293
00:17:45,650 --> 00:17:49,660
我之前写的

294
00:17:50,080 --> 00:17:52,200
计算高斯条件

295
00:17:52,530 --> 00:17:56,370
分布的

296
00:17:56,670 --> 00:17:57,970
公式中

297
00:17:58,240 --> 00:17:59,240
的项是对应的

298
00:17:59,930 --> 00:18:02,220
这就是E-step

299
00:18:02,750 --> 00:18:04,790
你需要计算出Q的分布

300
00:18:05,260 --> 00:18:09,000
你需要计算Q_i (z^((i) ))

301
00:18:09,460 --> 00:18:12,240
为了计算出Q

302
00:18:12,510 --> 00:18:14,070
你需要用这个公式计算出向量

303
00:18:14,370 --> 00:18:16,080
μ_(z^((i) ) |x^((i) ) )和

304
00:18:16,740 --> 00:18:19,570
矩阵Σ_(z^((i) ) |x^((i) ) )

305
00:18:19,880 --> 00:18:22,620
这些表示的是Q的均值和协方差矩阵

306
00:18:22,890 --> 00:18:23,910
Q服从高斯分布

307
00:18:24,390 --> 00:18:25,370
这是E-step的工作

308
00:18:30,310 --> 00:18:36,180
之后是M-step对于像这样的高斯模型

309
00:18:38,910 --> 00:18:43,230
有两种方式

310
00:18:43,520 --> 00:18:45,440
可以推导出

311
00:18:45,720 --> 00:18:46,800
M-step

312
00:18:47,110 --> 00:18:51,290
关键技巧在这里

313
00:18:51,950 --> 00:18:53,620
当你计算M-step时

314
00:18:54,180 --> 00:18:56,470
你经常需要计算

315
00:18:56,860 --> 00:18:57,890
像这样的积分

316
00:18:59,620 --> 00:19:01,260
里面是z^((i) )的函数

317
00:19:01,570 --> 00:19:03,290
让我把z^((i) )写在里面

318
00:19:04,000 --> 00:19:07,670
有两种方法可以计算

319
00:19:07,870 --> 00:19:08,620
这个积分

320
00:19:08,990 --> 00:19:09,890
一种方法--

321
00:19:10,230 --> 00:19:11,670
这是一个经常犯的错误--

322
00:19:11,960 --> 00:19:13,420
不能说是错误

323
00:19:13,740 --> 00:19:16,390
但是经常会带来不必要的麻烦

324
00:19:16,770 --> 00:19:19,920
一种计算这个积分的方式是

325
00:19:20,170 --> 00:19:21,650
将这个式子写出来

326
00:19:21,910 --> 00:19:23,210
对z^((i) )求积分

327
00:19:23,500 --> 00:19:24,610
我们知道Q_i是什么

328
00:19:25,000 --> 00:19:28,270
Q_i是一个高斯分布

329
00:19:28,600 --> 00:19:32,940
它应该等于这个式子

330
00:19:33,250 --> 00:19:35,470
这是高斯密度函数

331
00:19:35,820 --> 00:19:37,790
之后要再乘以z^((i) )

332
00:20:11,910 --> 00:20:15,610
这是一种不必要的

333
00:20:15,990 --> 00:20:16,990
复杂的计算方式

334
00:20:17,240 --> 00:20:18,850
如果你想进行积分--


335
00:20:19,090 --> 00:20:22,490
这里是乘号  不是等号--

336
00:20:23,300 --> 00:20:25,610
如果你希望求解这个积分式

337
00:20:25,930 --> 00:20:27,330
它看起来非常吓人

338
00:20:27,610 --> 00:20:29,150
我不太确定应该如何进行求解

339
00:20:29,440 --> 00:20:32,610
另外一种简单的求解方法是

340
00:20:33,020 --> 00:20:37,480
你可以认为这个式子是

341
00:20:38,230 --> 00:20:39,670
z^((i) )的期望

342
00:20:40,120 --> 00:20:42,140
z^((i) )是一个随机变量

343
00:20:45,440 --> 00:20:47,110
以Q_i 为分布

344
00:20:47,880 --> 00:20:50,930
如果你这样做的话

345
00:20:51,260 --> 00:20:52,640
你会发现这个式子就是

346
00:20:52,910 --> 00:20:55,180
求z^((i) ) 在分布Q_i下的期望值

347
00:20:55,580 --> 00:20:59,970
而Q_i是一个高斯分布

348
00:21:00,180 --> 00:21:02,710
均值是这个μ向量

349
00:21:03,150 --> 00:21:05,000
协方差是这个Σ矩阵

350
00:21:05,290 --> 00:21:06,740
所以z^((i) ) 在

351
00:21:07,020 --> 00:21:08,160
Q_i下的期望值

352
00:21:08,470 --> 00:21:09,910
就是μ_(z^((i) ) |x^((i) ) )

353
00:21:10,310 --> 00:21:15,520
明白吗?

354
00:21:15,890 --> 00:21:19,010
经过仔细观察之后

355
00:21:19,430 --> 00:21:20,260
我们会发现这个积分实际上

356
00:21:20,610 --> 00:21:21,330
就是在求一个期望

357
00:21:21,620 --> 00:21:23,020
这是一个简单地多

358
00:21:23,300 --> 00:21:24,150
的计算方法

359
00:21:38,700 --> 00:21:42,740
在M-step我们会进行同样的操作

360
00:21:43,170 --> 00:21:45,120
在M-step

361
00:21:45,460 --> 00:21:47,110
我们希望令这个式子最大化

362
00:22:03,120 --> 00:22:06,240
在外面有一个

363
00:22:06,570 --> 00:22:07,860
求和式

364
00:22:08,200 --> 00:22:09,600
这个式子实际上是

365
00:22:09,960 --> 00:22:12,000
argmax里面的项

366
00:22:12,390 --> 00:22:15,560
里面实际上是相对于z^((i) )求积分

367
00:22:15,920 --> 00:22:17,060
但是我们观察到它可以


368
00:22:17,420 --> 00:22:20,980
写成里面这个

369
00:22:21,290 --> 00:22:24,330
式子的期望值

370
00:22:33,220 --> 00:22:36,100
它可以被简化成这样的形式

371
00:23:00,150 --> 00:23:08,090
这里我用到

372
00:23:08,400 --> 00:23:09,620
了这个公式:

373
00:23:09,910 --> 00:23:12,460
P(x  z)=

374
00:23:12,890 --> 00:23:15,500
P(x│z)P(z)

375
00:23:15,950 --> 00:23:17,310
将这两项合起来

376
00:23:17,580 --> 00:23:19,500
会得到

377
00:23:19,830 --> 00:23:20,850
原始的分子

378
00:23:21,180 --> 00:23:23,780
事实证明

379
00:23:24,060 --> 00:23:25,050
对于因子分析

380
00:23:25,490 --> 00:23:27,820
这两项

381
00:23:28,270 --> 00:23:29,320
中只有

382
00:23:29,710 --> 00:23:31,250
这一项依赖于参数

383
00:23:35,170 --> 00:23:37,430
P(z^((i) ))

384
00:23:37,790 --> 00:23:38,540
没有参数

385
00:23:38,780 --> 00:23:41,210
因为z^((i) )是由

386
00:23:41,490 --> 00:23:43,040
高斯分布N(0  I)生成的

387
00:23:43,500 --> 00:23:45,980
而Q_i (z)是一个固定的高斯分布

388
00:23:46,260 --> 00:23:48,010
它同样不依赖于参数

389
00:23:48,490 --> 00:23:50,390
所以在M-step

390
00:23:50,730 --> 00:23:52,700
我们只需要使第一项

391
00:23:53,020 --> 00:23:54,220
相对于所有的

392
00:23:54,990 --> 00:23:56,860
参数最大化

393
00:24:11,450 --> 00:24:13,400
我还要讲一个关键步骤

394
00:24:13,700 --> 00:24:15,760
但是为了引出它

395
00:24:16,040 --> 00:24:18,840
我需要写出

396
00:24:19,150 --> 00:24:20,080
大量的数学公式

397
00:24:20,440 --> 00:24:28,960
让我们继续

398
00:24:38,300 --> 00:24:38,990
在M-step

399
00:24:39,230 --> 00:24:46,770
我们希望使所有相对于

400
00:24:47,240 --> 00:24:49,910
z^((i) )的期望值最大化

401
00:24:50,250 --> 00:24:51,870
这里z^((i) )服从概率分布Q_i

402
00:24:52,300 --> 00:24:54,390
但是我经常会省略掉它

403
00:25:04,730 --> 00:25:13,100
这个分布P(x_i |z_i)

404
00:25:13,500 --> 00:25:15,120
是一个高斯密度函数

405
00:25:15,730 --> 00:25:20,200
因为x^((i) ) |z^((i) )


406
00:25:20,530 --> 00:25:24,480
是一个高斯分布

407
00:25:24,780 --> 00:25:29,010
均值为μ+Λz

408
00:25:29,290 --> 00:25:30,440
协方差为Ψ

409
00:25:30,750 --> 00:25:33,100
所以这里

410
00:25:33,270 --> 00:25:34,990
我需要将里面的公式

411
00:25:35,450 --> 00:25:37,610
替换成

412
00:25:37,880 --> 00:25:40,360
高斯密度函数

413
00:25:40,600 --> 00:25:41,220
这里我要将

414
00:25:41,420 --> 00:25:43,570
这些式子代入

415
00:25:43,930 --> 00:25:48,900
得到这个式子

416
00:25:49,080 --> 00:25:52,440
这里我代入和高斯密度函数

417
00:25:52,670 --> 00:25:53,240
如果你这样做的话

418
00:25:53,580 --> 00:25:58,380
你会得到期望--


419
00:26:01,550 --> 00:26:07,170
对不起 忘了说  


420
00:26:07,500 --> 00:26:11,910
为了使推导过程不至于太复杂

421
00:26:12,270 --> 00:26:14,000
我这里只相对于参数Λ

422
00:26:14,330 --> 00:26:15,800
使其最大化

423
00:26:16,380 --> 00:26:18,470
你实际上希望

424
00:26:18,860 --> 00:26:19,670
相对于参数Λ

425
00:26:20,000 --> 00:26:20,690
Ψ和μ使这个式子最大化

426
00:26:21,090 --> 00:26:22,630
但是为了使课上写的

427
00:26:22,890 --> 00:26:23,640
数学公式不至于太多

428
00:26:23,950 --> 00:26:25,200
我只相对于Λ使其最大化

429
00:26:25,490 --> 00:26:26,880
此时假定参数

430
00:26:27,220 --> 00:26:28,960
Ψ和μ是固定的

431
00:26:29,320 --> 00:26:31,400
如果你将高斯密度函数

432
00:26:31,750 --> 00:26:32,610
代入

433
00:26:32,890 --> 00:26:34,130
你会得到一些常量的期望值

434
00:26:34,560 --> 00:26:36,070
这些常量的期望值可能依赖于Ψ

435
00:26:36,330 --> 00:26:37,530
但不能依赖于Λ

436
00:26:37,920 --> 00:26:42,060
减去这个式子

437
00:26:57,180 --> 00:27:02,370
这个二次项实际上

438
00:27:02,950 --> 00:27:05,510
来自于高斯密度函数中的指数部分

439
00:27:05,860 --> 00:27:07,300
当我对指数取对数时

440
00:27:07,780 --> 00:27:10,130
你会得到这个二次项

441
00:27:14,520 --> 00:27:20,450
如果你对上面的式子

442
00:27:24,660 --> 00:27:26,430
相对于

443
00:27:33,400 --> 00:27:36,070
矩阵Λ求导数

444
00:27:36,530 --> 00:27:41,200
并且令导数为0

445
00:27:42,080 --> 00:27:45,350
我们希望相对于

446
00:27:45,620 --> 00:27:46,920
参数Λ使这个式子最大化

447
00:27:47,220 --> 00:27:48,650
所以如果你相对于Λ

448
00:27:48,880 --> 00:27:49,590
对这个式子求导--

449
00:27:50,310 --> 00:27:51,050
对不起

450
00:27:52,360 --> 00:27:53,110
这里是减号--

451
00:27:54,650 --> 00:27:58,810
并且令导数为0

452
00:27:59,150 --> 00:28:00,710
因为你需要求

453
00:28:00,910 --> 00:28:01,720
这个式子的最大值

454
00:28:02,000 --> 00:28:04,420
如果你这样做了

455
00:28:04,730 --> 00:28:05,330
并且进行了简化

456
00:28:05,700 --> 00:28:07,470
你会得到这个式子

457
00:28:53,990 --> 00:28:56,670
在M-step

458
00:28:57,020 --> 00:29:02,260
这是你用来

459
00:29:02,720 --> 00:29:05,130
更新参数Λ的值

460
00:29:05,760 --> 00:29:11,990
期望是相对于z^((i) )的

461
00:29:12,210 --> 00:29:18,010
z^((i) ) 服从概率分布Q_i

462
00:29:18,410 --> 00:29:23,560
推导的最后一步是

463
00:29:23,880 --> 00:29:25,520
我们希望求出

464
00:29:26,060 --> 00:29:28,400
这两个期望

465
00:29:29,480 --> 00:29:35,650
第一项E[〖z^((i) )〗^T ]

466
00:29:36,060 --> 00:29:43,100
应该等于μ_(z^((i) ) |x^((i) ))^T

467
00:29:43,520 --> 00:29:46,610
因为这就是Q_i的均值

468
00:29:49,530 --> 00:29:53,270
为了求出另外一项

469
00:29:53,920 --> 00:29:56,760
让我提醒你们一下

470
00:29:56,970 --> 00:30:03,100
如果一个随机变量z

471
00:30:03,380 --> 00:30:04,240
服从高斯分布:

472
00:30:04,440 --> 00:30:05,480
N(μ  Σ)

473
00:30:05,950 --> 00:30:09,820
协方差矩阵等于

474
00:30:10,120 --> 00:30:13,960
Ezz^T-(Ez) (Ez)^T

475
00:30:19,750 --> 00:30:22,460
这是协方差的一种定义形式

476
00:30:23,010 --> 00:30:25,800
这意味着

477
00:30:26,100 --> 00:30:35,580
Ezz^T=Σ+(Ez) (Ez)^T

478
00:30:37,080 --> 00:30:45,670
所以第二项等

479
00:30:46,030 --> 00:30:54,930
于这个式子

480
00:31:03,830 --> 00:31:04,470
明白吗?

481
00:31:04,870 --> 00:31:08,340
这是计算

482
00:31:08,610 --> 00:31:10,120
E[〖z^((i) )〗^T ]和

483
00:31:10,480 --> 00:31:11,770
E[z^((i) ) 〖z^((i) )〗^T]的方法

484
00:31:12,080 --> 00:31:13,630
之后将其代入

485
00:31:14,130 --> 00:31:16,940
之后你就得到了

486
00:31:17,270 --> 00:31:19,080
M-step对参数Λ的更新公式

487
00:31:20,990 --> 00:31:23,580
关于推导过程

488
00:31:23,890 --> 00:31:25,420
我要讲的最后一点是  实际上

489
00:31:25,750 --> 00:31:26,950
可能是由于EMsuanfa


490
00:31:27,310 --> 00:31:28,190
的名字的缘故

491
00:31:28,390 --> 00:31:29,260
期望最大化

492
00:31:29,550 --> 00:31:32,710
在EM算法的

493
00:31:33,030 --> 00:31:34,690
E-step中很多人都会犯一个常见的错误

494
00:31:35,210 --> 00:31:38,110
有些人会去求

495
00:31:38,340 --> 00:31:39,080
随机变量z的期望

496
00:31:40,030 --> 00:31:41,710
之后在M-step会将其代入到

497
00:31:42,040 --> 00:31:43,310
所有出现过的地方

498
00:31:43,830 --> 00:31:44,550
所以实际上

499
00:31:44,990 --> 00:31:46,800
在推导因子分析的

500
00:31:47,110 --> 00:31:48,430
EM步骤的过程中  一个经常会犯的错误是

501
00:31:48,760 --> 00:31:49,640
有人可能会说

502
00:31:50,010 --> 00:31:51,020
" 我这里看到了zz^T

503
00:31:51,720 --> 00:31:54,340
所以可以直接将Q分布下的

504
00:31:54,660 --> 00:31:55,420
期望代入进来"

505
00:31:55,820 --> 00:32:04,120
所以代入了

506
00:32:04,580 --> 00:32:06,340
μ_(z^((i) ) |x^((i) ) )


507
00:32:06,970 --> 00:32:08,330
μ_(z^((i) ) |x^((i) ))^T

508
00:32:08,760 --> 00:32:13,700
这个推导过程是错误的

509
00:32:14,080 --> 00:32:17,240
因为少了另外一项

510
00:32:17,560 --> 00:32:18,770
Σ_(z^((i) ) |x^((i) ) )

511
00:32:19,240 --> 00:32:21,870
一个对于EM算法的常见误解是

512
00:32:22,150 --> 00:32:23,100
在E-step

513
00:32:23,490 --> 00:32:24,580
你计算出隐含

514
00:32:24,830 --> 00:32:25,650
随机变量的期望值

515
00:32:25,930 --> 00:32:26,650
而在M-step

516
00:32:26,910 --> 00:32:27,800
你将计算得到的期望值代入

517
00:32:28,090 --> 00:32:30,090
实际上在某些算法中

518
00:32:30,310 --> 00:32:31,450
这样做是可以的

519
00:32:31,800 --> 00:32:33,710
在混合高斯模型和

520
00:32:33,960 --> 00:32:34,690
混合贝叶斯模型中

521
00:32:34,990 --> 00:32:36,270
这样做都会得到正确的答案

522
00:32:36,590 --> 00:32:37,280
但是通常情况下

523
00:32:37,640 --> 00:32:38,900
EM算法会更为复杂

524
00:32:39,200 --> 00:32:40,390
它不是简单地求得

525
00:32:40,810 --> 00:32:41,730
随机变量的期望值

526
00:32:42,070 --> 00:32:44,010
之后假设它们就是

527
00:32:44,360 --> 00:32:45,160
实际观察到的值

528
00:32:45,500 --> 00:32:48,200
所以我们推导这个

529
00:32:48,450 --> 00:32:49,150
步骤以说明这一点

530
00:32:49,610 --> 00:32:55,120
我们总结一下推导过程中的

531
00:32:55,370 --> 00:32:57,630
三个关键点

532
00:32:57,960 --> 00:32:59,210
1  对于E-step

533
00:32:59,490 --> 00:33:00,450
我们用的是一个连续的

534
00:33:00,760 --> 00:33:01,660
高斯随机变量

535
00:33:01,950 --> 00:33:03,270
所以为了计算E-step

536
00:33:03,600 --> 00:33:05,760
我们需要计算分布Q_i

537
00:33:06,100 --> 00:33:07,590
的均值和协方差

538
00:33:08,050 --> 00:33:11,110
2  在M-step

539
00:33:11,440 --> 00:33:14,010
如果你将这些积分

540
00:33:14,240 --> 00:33:17,720
视为期望

541
00:33:18,050 --> 00:33:19,810
将会极大简化接下来的数学推导

542
00:33:20,040 --> 00:33:21,870
最后一点

543
00:33:22,190 --> 00:33:23,650
在M-step

544
00:33:23,900 --> 00:33:27,610
推导过程是通过求解特定的

545
00:33:27,820 --> 00:33:28,950
最大化问题完成的

546
00:33:29,300 --> 00:33:31,550
不一定要在

547
00:33:31,760 --> 00:33:33,090
每个地方代入期望值

548
00:33:35,780 --> 00:33:37,620
让我看看

549
00:33:38,060 --> 00:33:40,110
我感觉我进行了如此多的数学推导

550
00:33:40,330 --> 00:33:41,720
写了如此多的数学公式

551
00:33:42,070 --> 00:33:44,230
即便如此

552
00:33:44,550 --> 00:33:45,740
我仍然省略了很多步骤

553
00:33:46,150 --> 00:33:47,460
你可以回去从讲义上

554
00:33:47,880 --> 00:33:51,380
看到这些省略的步骤

555
00:33:51,680 --> 00:33:52,980
例如

556
00:33:53,280 --> 00:33:54,410
应该怎样相对于矩阵Λ求导

557
00:33:54,700 --> 00:33:56,490
应该如何计算其它参数

558
00:33:56,700 --> 00:33:57,470
的更新公式

559
00:33:57,720 --> 00:33:58,520
例如  μ和Ψ

560
00:33:58,790 --> 00:33:59,860
我们只讲了Λ的更新公式

561
00:34:00,170 --> 00:34:06,460
这就是因子分析算法

562
00:34:08,040 --> 00:34:10,170
Justin?

563
00:34:10,670 --> 00:34:12,910
S:我想知道

564
00:34:13,200 --> 00:34:14,130
在黑板右下角

565
00:34:14,480 --> 00:34:17,180
你说第二项中

566
00:34:17,460 --> 00:34:19,240
没有任何参数

567
00:34:19,450 --> 00:34:20,660
第一项包含了全部参数

568
00:34:20,980 --> 00:34:22,210
可是我觉得似乎

569
00:34:23,060 --> 00:34:25,850
Q_i中包含很多参数

570
00:34:28,990 --> 00:34:30,120
I:好的

571
00:34:30,580 --> 00:34:31,210
让我看看

572
00:34:31,580 --> 00:34:33,980
你的问题是

573
00:34:34,240 --> 00:34:35,150
Q_i中是否包含参数?

574
00:34:35,540 --> 00:34:41,800
实际上  


575
00:34:42,360 --> 00:34:45,320
在EM算法中

576
00:34:45,620 --> 00:34:47,370
有些时候P(z^((i) ))可能会包含参数

577
00:34:48,200 --> 00:34:50,800
但是Q_i (z^((i) ))不会包含任何参数

578
00:34:51,460 --> 00:34:54,120
在因子分析的某些情况下

579
00:34:54,450 --> 00:34:55,800
P(z^((i) ))不包含参数

580
00:34:56,230 --> 00:34:57,600
在其他的一些例子中

581
00:34:58,030 --> 00:34:59,480
例如对于混合高斯模型

582
00:34:59,810 --> 00:35:02,050
z^((i) )是一个多项式随机变量

583
00:35:02,340 --> 00:35:04,860
所以这个例子中  P(z^((i) ))包含参数

584
00:35:05,180 --> 00:35:07,200
但是事实证明  Q_i (z^((i) ))

585
00:35:07,440 --> 00:35:08,310
不包含任何参数

586
00:35:09,190 --> 00:35:12,980
具体地  Q_i (z^((i) ))


587
00:35:13,300 --> 00:35:16,420
是一个高斯分布  


588
00:35:16,730 --> 00:35:20,600
均值是μ_(z^((i) ) |x^((i) ) )

589
00:35:20,870 --> 00:35:26,840
协方差矩阵是Σ_(z^((i) ) |x^((i) ) )

590
00:35:36,150 --> 00:35:36,750
不可否认的是

591
00:35:37,270 --> 00:35:37,970
μ和Σ确实依赖于

592
00:35:38,290 --> 00:35:39,230
之前迭代中

593
00:35:39,530 --> 00:35:41,400
生成的参数的值

594
00:35:41,720 --> 00:35:44,210
但是我们认为Q

595
00:35:44,540 --> 00:35:46,740
仅仅是用之前

596
00:35:47,000 --> 00:35:48,070
迭代生成

597
00:35:48,510 --> 00:35:52,780
的参数值进行计算

598
00:35:53,140 --> 00:35:56,280
这是算法的E-step

599
00:35:56,370 --> 00:35:59,840
一旦我计算出了Q_i (z)


600
00:35:59,840 --> 00:36:02,870
那么它就是一个

601
00:36:02,870 --> 00:36:06,170
固定的分布了.


602
00:36:06,170 --> 00:36:08,240
在之后的M-step

603
00:36:08,240 --> 00:36:12,230
我之后会固定参数μ和Σ的值.

604
00:36:12,230 --> 00:36:14,720
S:我想我之前有些搞混了.


605
00:36:14,720 --> 00:36:20,000
我只看到里面


606
00:36:20,000 --> 00:36:21,470
包含了参数的值


607
00:36:21,470 --> 00:36:22,840
但是实际上它们


608
00:36:22,840 --> 00:36:23,690
只是上一次迭代的值.

609
00:36:24,810 --> 00:36:25,970
我在写Q_i (z^((i) ))的时候

610
00:36:26,390 --> 00:36:28,330
里面的参数都来

611
00:36:28,670 --> 00:36:29,990
自于上一次迭代

612
00:36:30,290 --> 00:36:31,520
我希望计算

613
00:36:31,750 --> 00:36:32,500
新的参数值

614
00:36:33,110 --> 00:36:36,400
还有问题吗?

615
00:36:47,900 --> 00:36:50,700
这可能是整门课中

616
00:36:50,930 --> 00:36:52,550
数学推导最多的一部分

617
00:36:58,210 --> 00:37:00,280
让我们来看一个不同的算法

618
00:37:02,620 --> 00:37:05,180
我在那块黑板上?

619
00:37:37,730 --> 00:37:40,940
我要讲的是一个

620
00:37:41,330 --> 00:37:42,710
称为主成分分析

621
00:37:42,990 --> 00:37:44,130
的算法

622
00:37:52,580 --> 00:37:56,000
简写为PCA

623
00:37:56,450 --> 00:37:59,010
PCA的思想是这样的

624
00:37:59,540 --> 00:38:03,880
PCA算法的目的和因子分析非常类似

625
00:38:04,240 --> 00:38:06,550
但是相比于因子分析

626
00:38:06,900 --> 00:38:09,520
PCA更为直接

627
00:38:09,960 --> 00:38:14,390
这个算法


628
00:38:14,870 --> 00:38:18,630
仍然属于无监督学习

629
00:38:18,960 --> 00:38:21,710
给定一个包含了m

630
00:38:22,150 --> 00:38:28,100
个样本的训练集合

631
00:38:28,430 --> 00:38:29,390
每个样本x^((i) )都是一个n维向量

632
00:38:29,760 --> 00:38:34,270
我希望得到

633
00:38:34,590 --> 00:38:39,200
一个维度更低的数据集合

634
00:38:39,570 --> 00:38:44,170
k严格小于n

635
00:38:44,480 --> 00:38:47,750
通常情况下会远远小于n

636
00:38:48,280 --> 00:38:53,580
我会举几个例子说明

637
00:38:53,880 --> 00:38:54,920
我们为什么要这样做

638
00:38:55,370 --> 00:38:58,960
想象一下给定一个

639
00:38:59,270 --> 00:39:01,090
包含了很多

640
00:39:01,360 --> 00:39:04,540
未知测量数据的

641
00:39:04,850 --> 00:39:06,170
训练集合  


642
00:39:06,510 --> 00:39:07,680
可能关于人的一些测量数据

643
00:39:08,130 --> 00:39:10,870
收集数据的人

644
00:39:11,260 --> 00:39:14,000
经常会用厘米和

645
00:39:14,290 --> 00:39:15,390
英尺两种形式

646
00:39:15,730 --> 00:39:17,230
保存人的身高数据

647
00:39:17,620 --> 00:39:19,420
因为一些

648
00:39:19,730 --> 00:39:21,000
舍入问题

649
00:39:21,310 --> 00:39:23,140
这两个数据并不会

650
00:39:23,480 --> 00:39:24,280
完全匹配

651
00:39:24,660 --> 00:39:27,590
如果将其视为二维数据

652
00:39:30,150 --> 00:39:33,170
这些点会离某条直线非常近

653
00:39:33,470 --> 00:39:35,430
但是因为舍入问题

654
00:39:35,840 --> 00:39:36,950
它们并不会完全落在直线上

655
00:39:37,270 --> 00:39:39,380
但是会离直线

656
00:39:39,680 --> 00:39:40,650
非常近

657
00:39:41,080 --> 00:39:44,020
我们的数据集合看起来是这样的

658
00:39:44,370 --> 00:39:45,960
你真正关心的应该

659
00:39:46,310 --> 00:39:47,490
是这条轴

660
00:39:47,880 --> 00:39:53,150
这是你真正感兴趣的变量

661
00:39:53,510 --> 00:39:57,460
它是距离一个人的真实身高

662
00:39:57,760 --> 00:39:58,550
最接近的一个量

663
00:39:58,970 --> 00:40:04,540
这个方向的坐标轴只是噪声

664
00:40:05,270 --> 00:40:08,680
如果你能够将数据

665
00:40:09,020 --> 00:40:10,580
的维度从二维降到一维

666
00:40:11,400 --> 00:40:13,370
那么你就可以避免

667
00:40:13,660 --> 00:40:15,010
数据中的噪声

668
00:40:15,520 --> 00:40:17,710
通常情况下

669
00:40:18,050 --> 00:40:19,600
你并不知道这里是厘米

670
00:40:19,820 --> 00:40:20,560
这里是英尺

671
00:40:20,810 --> 00:40:21,470
你的数据可能

672
00:40:21,740 --> 00:40:22,420
有一百个属性

673
00:40:22,720 --> 00:40:24,530
但是你并不知道那个是厘米

674
00:40:24,830 --> 00:40:25,800
哪个是英寸

675
00:40:26,180 --> 00:40:32,060
我还能想到另一个例子

676
00:40:32,370 --> 00:40:35,140
可能你们中有些人

677
00:40:35,470 --> 00:40:36,560
知道我和我的学生研究

678
00:40:37,470 --> 00:40:39,110
直升机的自动飞行

679
00:40:39,500 --> 00:40:41,150
想象一下

680
00:40:41,520 --> 00:40:44,100
假设你们需要对飞行员

681
00:40:44,330 --> 00:40:46,690
进行一些问卷调查和实际的飞行测试

682
00:40:47,040 --> 00:40:50,600
可能你的一个坐标轴

683
00:40:50,900 --> 00:40:53,660
表示飞行员的技术

684
00:40:56,740 --> 00:40:59,700
也就是飞行员驾驶技术有多娴熟

685
00:41:00,300 --> 00:41:05,650
另外一个坐标轴可能

686
00:41:05,970 --> 00:41:07,420
表示他们多喜欢飞行

687
00:41:07,880 --> 00:41:10,890
这里可能

688
00:41:11,180 --> 00:41:13,300
存在着一维数据

689
00:41:13,840 --> 00:41:17,320
这是我们感兴趣的变量

690
00:41:17,760 --> 00:41:19,600
我们称之为飞行员的资质

691
00:41:22,860 --> 00:41:24,910
这个变量表示

692
00:41:25,250 --> 00:41:26,430
了飞行员的技术和兴趣

693
00:41:26,850 --> 00:41:28,940
如果能够将这组数据

694
00:41:29,260 --> 00:41:30,700
从二维降到一维

695
00:41:31,020 --> 00:41:33,100
那么也许你会得到

696
00:41:33,370 --> 00:41:34,590
一个更好的称之为

697
00:41:34,960 --> 00:41:36,120
"松散飞行员资质"的衡量标准

698
00:41:36,450 --> 00:41:37,560
也许这个属性才是你真正想要的

699
00:41:38,050 --> 00:41:41,610
让我们先来看算法

700
00:41:41,960 --> 00:41:43,720
稍后我会再讲一些

701
00:41:44,050 --> 00:41:46,730
PCA的应用

702
00:41:53,110 --> 00:41:55,680
算法是这样的

703
00:42:10,770 --> 00:42:16,380
在运行PCA之前

704
00:42:16,750 --> 00:42:18,560
你需要按照下面的方法对数据进行预处理

705
00:42:18,940 --> 00:42:20,670
我将这些步骤写出来

706
00:43:26,040 --> 00:43:28,400
我看到你们都在写

707
00:43:29,340 --> 00:43:32,840
现在不用

708
00:43:33,170 --> 00:43:33,920
再写了

709
00:43:34,280 --> 00:43:36,270
首先计算训练样本的均值

710
00:43:36,560 --> 00:43:37,410
之后所有样本减去均值

711
00:43:37,770 --> 00:43:39,330
这样训练样本

712
00:43:39,610 --> 00:43:40,410
的均值就是0

713
00:43:40,750 --> 00:43:41,590
另外一部

714
00:43:41,930 --> 00:43:43,960
我首先计算出均值归0

715
00:43:44,370 --> 00:43:45,990
后每个特征的方差

716
00:43:46,330 --> 00:43:48,880
之后每个特征

717
00:43:49,150 --> 00:43:49,890
都要除以标准差

718
00:43:50,220 --> 00:43:52,080
这样所有的特征的方差

719
00:43:52,440 --> 00:43:53,320
都是相等的

720
00:43:53,650 --> 00:43:54,790
这些是PCA的标准的

721
00:43:55,150 --> 00:43:56,340
预处理步骤

722
00:43:56,770 --> 00:44:00,140
第二步通常用于

723
00:44:00,440 --> 00:44:02,280
不同的特征

724
00:44:02,570 --> 00:44:03,700
具有不同量级的情形

725
00:44:04,120 --> 00:44:07,610
如果数据中包含了人的测量

726
00:44:07,970 --> 00:44:11,050
一个特征可能是身高

727
00:44:11,370 --> 00:44:12,610
另一个特征可能是体重

728
00:44:12,940 --> 00:44:13,930
另一个特征可能是强壮程度

729
00:44:14,220 --> 00:44:17,030
另一个可能是年龄

730
00:44:17,360 --> 00:44:19,020
所有这些量的

731
00:44:19,290 --> 00:44:19,980
数量级都是不同的

732
00:44:20,280 --> 00:44:22,880
所以需要对方差进行标准化

733
00:44:23,240 --> 00:44:24,160
有的时候

734
00:44:24,510 --> 00:44:26,870
如果所有的x_i都是相同的量  


735
00:44:27,220 --> 00:44:31,420
例如  如果你在处理图片

736
00:44:31,740 --> 00:44:33,430
所有的x_i^((j) )

737
00:44:33,840 --> 00:44:36,200
都表示像素的值

738
00:44:36,510 --> 00:44:38,050
由于每个像素都只能取0-255之间的值

739
00:44:38,360 --> 00:44:39,930
所以所有的特征数量级都是相同的

740
00:44:40,280 --> 00:44:41,680
所以你可以省略这一步

741
00:44:43,480 --> 00:44:45,550
预处理之后

742
00:44:45,900 --> 00:44:47,550
让我们讨论一下

743
00:44:47,860 --> 00:44:52,140
应该怎样找到数据

744
00:44:52,450 --> 00:44:53,480
随之变化的主轴

745
00:44:53,790 --> 00:44:55,560
我们怎样发现

746
00:44:55,920 --> 00:44:56,850
主轴呢?

747
00:45:15,910 --> 00:45:20,350
首先让我讲一个特殊的例子

748
00:45:21,030 --> 00:45:22,830
之后我们会正式提出算法

749
00:45:30,910 --> 00:45:32,880
这是我的训练集合

750
00:45:33,190 --> 00:45:34,240
由5个样本构成

751
00:45:34,590 --> 00:45:36,360
均值大概是0

752
00:45:37,490 --> 00:45:41,550
x_1 和x_2方向上的方差

753
00:45:41,850 --> 00:45:42,800
是相同的

754
00:45:43,560 --> 00:45:44,390
这里是x_1 轴

755
00:45:44,680 --> 00:45:45,360
这里是x_2 轴

756
00:45:45,620 --> 00:45:48,630
所以主轴

757
00:45:48,920 --> 00:45:49,830
的方向大概是

758
00:45:50,100 --> 00:45:52,190
正45度角

759
00:45:52,550 --> 00:45:55,240
所以我希望我的

760
00:45:55,560 --> 00:45:57,340
算法能够

761
00:45:57,760 --> 00:45:58,960
求出这个方向u

762
00:45:59,250 --> 00:46:02,570
它是最好的数据投影的方向

763
00:46:02,920 --> 00:46:06,530
是数据随之变化的

764
00:46:06,900 --> 00:46:07,590
真正的主轴

765
00:46:08,000 --> 00:46:10,570
让我们看看应该怎样正规化

766
00:46:10,950 --> 00:46:14,730
假设我找到了

767
00:46:15,010 --> 00:46:16,280
这样的一条轴  


768
00:46:16,990 --> 00:46:18,220
这是我希望用来

769
00:46:18,530 --> 00:46:19,470
进行数据投影的轴

770
00:46:19,800 --> 00:46:21,410
我希望用它来

771
00:46:21,680 --> 00:46:23,010
捕捉数据的变化

772
00:46:23,470 --> 00:46:25,070
之后我令所有的数据点

773
00:46:25,370 --> 00:46:26,310
都对这条线进行投影

774
00:46:26,670 --> 00:46:31,790
从而得到这些点

775
00:46:35,840 --> 00:46:37,840
你会注意到  这些点

776
00:46:38,180 --> 00:46:39,480
也就是我的训练集合

777
00:46:39,760 --> 00:46:40,560
到这条轴的投影

778
00:46:40,980 --> 00:46:42,660
它们的方差很大

779
00:46:43,090 --> 00:46:45,220
相反地

780
00:46:45,530 --> 00:46:49,000
如果我选择了一个不同的方向  


781
00:46:49,410 --> 00:46:53,750
比如说我们选择了

782
00:46:54,030 --> 00:46:55,490
最差的一个方向用来投影数据

783
00:46:56,170 --> 00:46:59,090
如果我将所有的数据点

784
00:47:07,870 --> 00:47:10,440
都投影到这条轴上

785
00:47:10,720 --> 00:47:13,420
那么我会发现所有数据点

786
00:47:13,760 --> 00:47:14,670
到这条紫色线上的投影点的方差

787
00:47:15,050 --> 00:47:16,060
比之前小的多

788
00:47:16,430 --> 00:47:17,720
这次所有的点彼此之间

789
00:47:18,020 --> 00:47:18,790
离得很近

790
00:47:19,460 --> 00:47:23,800
我们正式地总结

791
00:47:24,160 --> 00:47:27,880
一下这个现象

792
00:47:28,230 --> 00:47:30,190
我希望找到一个方向向量u

793
00:47:30,550 --> 00:47:33,250
使得当我将数据投影到

794
00:47:33,610 --> 00:47:35,500
这个方向上时

795
00:47:35,950 --> 00:47:39,480
这些数据之间离得越远越好

796
00:47:39,920 --> 00:47:40,860
换句话说

797
00:47:41,200 --> 00:47:42,570
我希望找到一个方向

798
00:47:42,900 --> 00:47:43,900
使得所有数据在该方向上的投影

799
00:47:44,290 --> 00:47:47,850
尽可能地分散

800
00:47:48,340 --> 00:47:51,800
使方差尽可能的大

801
00:47:57,120 --> 00:47:59,350
让我看看

802
00:48:14,270 --> 00:48:16,810
我希望找到方向u

803
00:48:17,910 --> 00:48:20,460
回忆一下

804
00:48:22,120 --> 00:48:24,300
如果向量u的模等于1

805
00:48:24,800 --> 00:48:32,590
那么向量x^((i) )


806
00:48:38,420 --> 00:48:40,080
投影的长度应该等于

807
00:48:40,410 --> 00:48:49,400
〖x^((i) )〗^T u

808
00:48:50,040 --> 00:48:54,240
如果将x投影到一个

809
00:48:54,580 --> 00:48:55,360
单位向量上

810
00:48:55,680 --> 00:48:56,910
投影的长度应该

811
00:48:57,210 --> 00:48:58,350
等于〖x^((i) )〗^T u

812
00:48:58,720 --> 00:49:01,880
形式化地定义一下PCA问题

813
00:49:02,200 --> 00:49:05,860
我要选择


814
00:49:06,250 --> 00:49:11,740
向量u

815
00:49:12,020 --> 00:49:13,190
向量u的

816
00:49:13,490 --> 00:49:14,400
模等于1  


817
00:49:14,710 --> 00:49:16,150
使得这个式子最大化

818
00:49:16,450 --> 00:49:20,630
具体地  


819
00:49:25,160 --> 00:49:27,630
我希望投影的点

820
00:49:27,900 --> 00:49:31,380
距离原点

821
00:49:31,720 --> 00:49:33,950
尽可能的远

822
00:49:34,250 --> 00:49:35,380
我希望x到u的

823
00:49:35,730 --> 00:49:37,890
投影的方差

824
00:49:38,190 --> 00:49:39,220
最大化

825
00:49:39,580 --> 00:49:41,990
为了简化之后的数学处理

826
00:49:42,280 --> 00:49:43,540
我在前面加一个1/m

827
00:49:43,910 --> 00:49:47,360
右边的部分

828
00:49:47,730 --> 00:49:51,580
等于这个式子

829
00:49:51,980 --> 00:50:00,230
化简之后

830
00:50:05,610 --> 00:50:07,320
得到了

831
00:50:07,610 --> 00:50:10,310
这个式子

832
00:50:44,460 --> 00:50:51,160
所以我希望使这个式子最大化

833
00:50:51,530 --> 00:50:54,610
同时满足约束:

834
00:50:54,980 --> 00:50:57,080
||u||=1

835
00:50:57,910 --> 00:51:02,030
你们可能已经有人看出来了

836
00:51:02,370 --> 00:51:04,100
这意味着u是

837
00:51:04,460 --> 00:51:06,970
中间这个矩阵的主特征向量

838
00:51:07,480 --> 00:51:09,080
让我把它

839
00:51:09,420 --> 00:51:10,530
写下来

840
00:51:10,860 --> 00:51:13,810
这意味着u是

841
00:51:14,170 --> 00:51:21,250
这个矩阵的主特征向量

842
00:51:23,050 --> 00:51:24,570
我将这个矩阵称为Σ

843
00:51:25,050 --> 00:51:26,490
它实际上是一个协方差矩阵

844
00:51:33,270 --> 00:51:34,120
等于这个式子

845
00:51:34,940 --> 00:51:37,520
让我看看

846
00:51:37,880 --> 00:51:40,000
你们有多少人熟悉

847
00:51:40,340 --> 00:51:41,250
特征向量?

848
00:51:41,690 --> 00:51:42,830
很好  很多人

849
00:51:43,150 --> 00:51:43,940
几乎所有人

850
00:51:44,290 --> 00:51:45,080
很好

851
00:51:47,220 --> 00:51:50,100
我接下来讲的你们

852
00:51:50,450 --> 00:51:52,190
会非常熟悉

853
00:51:52,520 --> 00:51:54,260
但是还是值得一讲

854
00:51:55,530 --> 00:52:01,380
如果你有一个矩阵A

855
00:52:02,000 --> 00:52:02,880
和一个向量u

856
00:52:03,660 --> 00:52:05,710
并且满足:Au=λu

857
00:52:06,120 --> 00:52:07,400
那么u就是矩阵A

858
00:52:07,700 --> 00:52:08,930
的一个特征向量

859
00:52:15,420 --> 00:52:17,630
而λ被称为

860
00:52:17,940 --> 00:52:19,060
特征值

861
00:52:19,470 --> 00:52:23,690
所以主特征向量就是

862
00:52:24,240 --> 00:52:25,430
对应着最大的

863
00:52:25,760 --> 00:52:27,170
特征值的特征向量

864
00:52:27,570 --> 00:52:31,650
另外一个结论你们可能已经知道了

865
00:52:31,960 --> 00:52:33,020
但是我不确定

866
00:52:33,350 --> 00:52:34,750
我要和之前讲到的

867
00:52:35,040 --> 00:52:35,890
东西联系起来

868
00:52:36,230 --> 00:52:39,700
我们的优化问题是这样的

869
00:52:40,000 --> 00:52:44,470
在满足||u||=1的情况下

870
00:52:44,800 --> 00:52:47,110
使u^T Σu最大化 


871
00:52:47,550 --> 00:52:50,060
我将这个约束写成:

872
00:52:50,290 --> 00:52:51,220
u^T u=1

873
00:52:51,660 --> 00:52:55,600
为了求解这个

874
00:52:55,930 --> 00:52:57,190
约束优化问题

875
00:52:57,980 --> 00:52:59,920
我们写出拉格朗日算子

876
00:53:07,400 --> 00:53:11,090
这个是拉格朗日乘数

877
00:53:18,020 --> 00:53:20,910
由于这是一个约束优化问题

878
00:53:21,250 --> 00:53:23,440
所以为了求解这个最优化问题

879
00:53:23,840 --> 00:53:25,920
你需要相对于

880
00:53:26,280 --> 00:53:31,230
u对L求导  得到这个式子

881
00:53:31,640 --> 00:53:35,740
之后令这个式子等于0

882
00:53:36,050 --> 00:53:37,700
这意味着Σu=λu

883
00:53:38,180 --> 00:53:40,170
所以能够求解

884
00:53:40,450 --> 00:53:42,210
这个约束优化

885
00:53:42,500 --> 00:53:43,600
问题的u一定是

886
00:53:43,940 --> 00:53:44,900
Σ的一个特征向量

887
00:53:45,180 --> 00:53:46,030
实际上

888
00:53:46,230 --> 00:53:47,080
它是主特征向量

889
00:53:50,710 --> 00:53:55,490
总结一下

890
00:53:55,860 --> 00:53:58,220
我们都做了什么

891
00:53:58,470 --> 00:53:59,760
我们已经证明了  给定一个训练集合

892
00:54:00,130 --> 00:54:01,830
如果你想找到

893
00:54:02,140 --> 00:54:03,040
数据变化的主轴

894
00:54:03,340 --> 00:54:04,750
如果你希望找到一维主轴

895
00:54:05,160 --> 00:54:06,810
使得数据在这个方向上的变化最大

896
00:54:07,170 --> 00:54:09,270
我们需要做的是够哦

897
00:54:09,600 --> 00:54:10,890
早协方差矩阵Σ

898
00:54:11,220 --> 00:54:13,640
也就是我刚才写的那个矩阵Σ

899
00:54:14,000 --> 00:54:16,380
之后你就可以求出

900
00:54:16,680 --> 00:54:18,650
矩阵Σ的主特征向量

901
00:54:18,990 --> 00:54:20,720
这样就得到了

902
00:54:21,110 --> 00:54:23,390
投影数据的最优的

903
00:54:23,640 --> 00:54:24,390
一维子空间

904
00:54:31,820 --> 00:54:32,950
更为一般地

905
00:54:47,040 --> 00:54:48,300
如果你希望选择k

906
00:55:13,350 --> 00:55:16,780
维子空间

907
00:55:17,100 --> 00:55:17,990
用来投影数据

908
00:55:18,420 --> 00:55:20,710
你需要选择Σ的前k个特征向量:

909
00:55:21,040 --> 00:55:23,430
u_1 到u_k

910
00:55:24,260 --> 00:55:25,770
前k个的意思是

911
00:55:26,080 --> 00:55:27,570
你需要选择特征值最大的

912
00:55:28,020 --> 00:55:30,290
k个特征向量

913
00:55:31,030 --> 00:55:34,170
我只证明了一维子

914
00:55:34,560 --> 00:55:36,810
空间的情况

915
00:55:37,170 --> 00:55:38,350
但是这个结论是普遍成立的

916
00:55:38,710 --> 00:55:40,110
现在

917
00:55:40,590 --> 00:55:45,360
这k个特征向量

918
00:55:45,770 --> 00:55:47,980
提供了一种表示数据的

919
00:55:48,340 --> 00:55:49,170
新的方法

920
00:55:49,640 --> 00:55:56,730
S:

921
00:55:57,520 --> 00:55:58,240
I:让我看看

922
00:55:58,500 --> 00:55:59,250
习惯上

923
00:55:59,550 --> 00:56:00,810
PCA会选择正交的轴

924
00:56:01,320 --> 00:56:05,350
我认为是这样的

925
00:56:05,680 --> 00:56:07,170
这里还有一个例子

926
00:56:07,530 --> 00:56:09,020
想象一下你们有一个三维的训练集合

927
00:56:09,370 --> 00:56:13,240
想象一下你们的

928
00:56:13,610 --> 00:56:14,470
三维数据集合


929
00:56:14,860 --> 00:56:17,220
--在黑板上很难画出3D效果来

930
00:56:17,500 --> 00:56:18,650
试试这样

931
00:56:18,990 --> 00:56:21,470
想象一下我的

932
00:56:21,830 --> 00:56:23,740
x_1 和x_2轴坐落在黑板的平面上

933
00:56:24,150 --> 00:56:26,150
x_3轴垂直地

934
00:56:26,480 --> 00:56:27,450
指出黑板

935
00:56:28,170 --> 00:56:29,610
想象一下你的数据集合中

936
00:56:30,000 --> 00:56:34,600
大多数点都坐落在

937
00:56:35,000 --> 00:56:35,830
黑板平面上

938
00:56:36,130 --> 00:56:37,300
但是边缘可能会有一些例外的点

939
00:56:37,570 --> 00:56:38,170
想象一下

940
00:56:38,410 --> 00:56:39,340
x_3轴垂直地

941
00:56:39,710 --> 00:56:40,640
指出黑板平面

942
00:56:41,000 --> 00:56:42,840
所有的数据点大概

943
00:56:43,160 --> 00:56:43,920
都坐落在黑板上

944
00:56:44,220 --> 00:56:46,040
但是区域的边缘

945
00:56:46,340 --> 00:56:48,150
可能会有一点模糊

946
00:56:48,700 --> 00:56:49,810
因为可能会有一些点

947
00:56:50,110 --> 00:56:51,080
在离黑板几毫米之外的地方

948
00:56:51,460 --> 00:56:52,650
如果你在这组数据上运行PCA

949
00:56:52,950 --> 00:56:57,410
你会发现u_1和u_2

950
00:56:57,860 --> 00:56:59,540
会是一组

951
00:56:59,900 --> 00:57:01,630
坐落在黑板平面上的轴向量

952
00:57:02,030 --> 00:57:06,650
而u_3会是一个

953
00:57:06,980 --> 00:57:07,930
和黑板平面垂直

954
00:57:08,250 --> 00:57:09,140
指出平面的向量

955
00:57:09,730 --> 00:57:13,390
所以如果我将

956
00:57:13,700 --> 00:57:14,930
这些数据降成两维

957
00:57:15,250 --> 00:57:16,610
那么u_1 和u_2

958
00:57:17,670 --> 00:57:19,640
将会是我们用来在

959
00:57:19,920 --> 00:57:21,290
低维空间中表示数据的

960
00:57:21,570 --> 00:57:22,470
基础坐标轴

961
00:57:24,540 --> 00:57:27,290
让我们具体看一下这是什么意思

962
00:57:27,610 --> 00:57:36,830
之前我们说过

963
00:57:37,140 --> 00:57:39,290
x^((i) )的维度很高  是一个n维向量

964
00:57:39,670 --> 00:57:42,690
现在如果我希望将

965
00:57:43,270 --> 00:57:44,250
x^((i) )在以u_1

966
00:57:44,340 --> 00:57:45,930
到u_k为基础坐标轴的空间中表示出来

967
00:58:03,270 --> 00:58:06,270
对每一个原始的样本x^((i) )

968
00:58:06,800 --> 00:58:09,400
我希望用

969
00:58:09,710 --> 00:58:12,510
另外一个向量代替它

970
00:58:12,900 --> 00:58:14,080
称之为y^((i) )

971
00:58:14,750 --> 00:58:16,000
应该这样计算

972
00:58:16,350 --> 00:58:21,180
这样

973
00:58:29,120 --> 00:58:37,090
y^((i) )就是一个

974
00:58:38,030 --> 00:58:39,810
k维向量  


975
00:58:40,190 --> 00:58:42,160
你选择的k值小于n

976
00:58:42,500 --> 00:58:45,150
这样你就得到了

977
00:58:45,470 --> 00:58:47,000
数据的低维表示

978
00:58:47,350 --> 00:58:49,510
这可以认为是原始数据的

979
00:58:49,810 --> 00:58:50,650
一种近似的表示

980
00:58:51,320 --> 00:58:52,980
但是你只是用

981
00:58:53,270 --> 00:58:54,490
了k个维度

982
00:58:54,850 --> 00:59:01,670
而不是n个维度  让我看看

983
00:59:17,640 --> 00:59:19,050
I:是否可能不使用特征向量 而只使用

984
00:59:19,330 --> 00:59:20,220
平凡特征向量?

985
00:59:31,470 --> 00:59:32,000
I:让我看看

986
00:59:32,230 --> 00:59:33,810
你说的平凡特征向量是什么意思?

987
00:59:42,090 --> 00:59:42,700
I:好的

988
00:59:42,950 --> 00:59:43,660
我知道了

989
00:59:43,980 --> 00:59:46,010
让我看看

990
00:59:46,270 --> 00:59:48,020
有一些矩阵  


991
00:59:48,920 --> 00:59:50,650
我记得它们是"degenerate"的  


992
00:59:50,970 --> 00:59:52,470
也就是说它们没有完整的一套特征向量

993
00:59:52,900 --> 00:59:53,930
对不起

994
00:59:54,290 --> 00:59:55,690
我记得这样的矩阵称为缺秩矩阵

995
00:59:56,010 --> 00:59:56,650
是这么叫吗

996
00:59:56,970 --> 00:59:57,700
Ziko?是的

997
00:59:58,060 --> 00:59:58,830
有些矩阵是缺秩的

998
00:59:59,120 --> 01:00:00,660
它们的特征向量集合是不完整的

999
01:00:01,070 --> 01:00:02,800
例如  对于一个n*n的矩阵

1000
01:00:03,100 --> 01:00:04,990
它们的特征向量不足n个

1001
01:00:05,320 --> 01:00:06,770
但是我们这里这种情况是

1002
01:00:07,140 --> 01:00:09,360
不可能出现的

1003
01:00:09,710 --> 01:00:10,620
因为协方差矩阵是对称的

1004
01:00:10,940 --> 01:00:13,930
对称矩阵不可能缺秩

1005
01:00:14,330 --> 01:00:15,610
所以矩阵Σ一定有

1006
01:00:15,910 --> 01:00:16,710
一套完整的特征向量

1007
01:00:17,260 --> 01:00:23,610
还有另外一个问题

1008
01:00:23,840 --> 01:00:25,070
就是可能出现重复的特征向量

1009
01:00:25,390 --> 01:00:27,190
例如

1010
01:00:27,560 --> 01:00:30,530
在这个例子中

1011
01:00:30,900 --> 01:00:34,150
如果我的协方差矩阵看起来是这样的

1012
01:00:39,440 --> 01:00:42,370
那么在选取前两个特征向量

1013
01:00:42,690 --> 01:00:43,560
的时候会出现歧义

1014
01:00:43,970 --> 01:00:47,100
这意味着你可以这样

1015
01:00:47,410 --> 01:00:49,570
选择u_1 和u_2

1016
01:00:50,020 --> 01:00:53,270
或者进行旋转之后这样选取

1017
01:00:53,710 --> 01:00:56,690
u_1 和u_2

1018
01:00:57,360 --> 01:00:59,600
或者这样选取

1019
01:01:00,180 --> 01:01:01,180
u_1 和u_2

1020
01:01:01,560 --> 01:01:02,360
等等

1021
01:01:02,600 --> 01:01:04,490
当你在应用PCA的时候

1022
01:01:04,910 --> 01:01:06,690
要记住一点

1023
01:01:07,010 --> 01:01:08,980
有的时候特征向量

1024
01:01:09,340 --> 01:01:10,860
是可以在其所在的子空间中自由旋转的

1025
01:01:11,360 --> 01:01:13,380
这种情况在

1026
01:01:13,790 --> 01:01:15,240
存在重复的

1027
01:01:15,630 --> 01:01:20,340
或者接近的特征值时就会出现

1028
01:01:20,680 --> 01:01:22,270
一种理解向量u的方式是

1029
01:01:22,600 --> 01:01:23,450
将其视为一组表示数据的基础向量坐标轴

1030
01:01:23,700 --> 01:01:25,630
但是有些时候这些基础向量

1031
01:01:25,900 --> 01:01:26,630
可以随意旋转

1032
01:01:26,960 --> 01:01:28,530
所以很多时候一次

1033
01:01:28,840 --> 01:01:30,370
只考虑一个特征向量是没用的

1034
01:01:30,730 --> 01:01:31,610
有人会说

1035
01:01:31,880 --> 01:01:34,200
这是我第一个特征向量

1036
01:01:34,600 --> 01:01:35,860
用来捕获人的身高

1037
01:01:36,240 --> 01:01:37,280
这是我的第二个特征向量

1038
01:01:37,630 --> 01:01:38,780
用来捕获技能等等

1039
01:01:39,110 --> 01:01:40,600
在运行PCA的时候

1040
01:01:40,980 --> 01:01:42,090
这样做是非常危险的

1041
01:01:43,150 --> 01:01:45,490
唯一有意义的是

1042
01:01:45,860 --> 01:01:46,750
由特征向量确定的子空间

1043
01:01:47,130 --> 01:01:48,930
一次只考虑一个

1044
01:01:49,370 --> 01:01:50,710
特征向量是很危险的

1045
01:01:52,480 --> 01:01:53,330
因为空间经常可以自由旋转

1046
01:01:53,760 --> 01:01:57,230
细微的数值变化

1047
01:01:57,570 --> 01:01:58,470
可能引起特征向量的巨大变化

1048
01:01:58,710 --> 01:02:00,340
但是由前k个

1049
01:02:00,670 --> 01:02:02,220
特征向量确定的子空间大致是相同的

1050
01:02:31,750 --> 01:02:33,500
实际上对于PCA

1051
01:02:33,930 --> 01:02:35,680
有很多不同的解释

1052
01:02:36,130 --> 01:02:37,580
我给出一种解释

1053
01:02:37,920 --> 01:02:38,670
但是不给出证明

1054
01:02:39,100 --> 01:02:46,530
这个例子是这样的 


1055
01:02:46,890 --> 01:02:48,390
给定这样的一个训练集合

1056
01:02:49,090 --> 01:02:51,610
另外一种视角是--


1057
01:02:52,100 --> 01:02:53,570
让我选择一个方向

1058
01:02:53,880 --> 01:02:54,720
这不是主成分

1059
01:02:55,170 --> 01:02:56,640
我选择一个方向

1060
01:02:57,350 --> 01:02:58,550
在其上对数据做投影

1061
01:03:01,760 --> 01:03:02,530
这显然不是PCA

1062
01:03:02,840 --> 01:03:03,690
会选取的方向

1063
01:03:04,200 --> 01:03:05,100
但是你可以认为PCA

1064
01:03:05,420 --> 01:03:06,060
是在做这样的事情

1065
01:03:06,470 --> 01:03:08,000
选择一个子空间

1066
01:03:08,290 --> 01:03:10,380
并在其上对数据进行投影

1067
01:03:10,690 --> 01:03:13,010
并使得所有投影点到

1068
01:03:13,360 --> 01:03:15,150
原始点的距离的平方和最小化

1069
01:03:15,600 --> 01:03:16,320
换句话说

1070
01:03:16,700 --> 01:03:18,590
另外一种理解PCA的方式是

1071
01:03:18,940 --> 01:03:21,570
使得我们的数据点

1072
01:03:21,920 --> 01:03:26,270
和投影点之间的

1073
01:03:26,650 --> 01:03:29,070
距离的

1074
01:03:29,370 --> 01:03:30,250
平方和最小化

1075
01:03:30,690 --> 01:03:34,670
实际上

1076
01:03:34,960 --> 01:03:36,290
有9到10种PCA的

1077
01:03:36,640 --> 01:03:37,190
解释方法

1078
01:03:37,600 --> 01:03:38,900
这是另外一种

1079
01:03:39,240 --> 01:03:41,410
有很多方法可以推出PCA

1080
01:03:41,710 --> 01:03:44,760
你们在下个problem set

1081
01:03:45,140 --> 01:03:46,320
中会接触到更多不同的观点

1082
01:03:56,730 --> 01:03:58,080
我接下来要讲几个

1083
01:03:58,530 --> 01:04:00,070
PCA的应用

1084
01:04:16,290 --> 01:04:19,200
PCA主要应用在几个领域

1085
01:04:19,570 --> 01:04:21,020
其中一个是可视化领域

1086
01:04:24,700 --> 01:04:29,130
通常情况下数据集合的维度很高

1087
01:04:29,550 --> 01:04:30,950
当有人给你一个50维对的

1088
01:04:31,270 --> 01:04:31,910
数据集合时

1089
01:04:32,260 --> 01:04:34,780
你很难观察并理解

1090
01:04:35,080 --> 01:04:36,360
50维的数据

1091
01:04:36,690 --> 01:04:37,710
因为你不能将

1092
01:04:38,000 --> 01:04:38,760
它们画出来

1093
01:04:39,220 --> 01:04:41,260
通常情况下

1094
01:04:41,650 --> 01:04:42,380
如果你希望将一个

1095
01:04:42,670 --> 01:04:43,600
高维数据集合

1096
01:04:43,940 --> 01:04:45,790
显示出来

1097
01:04:46,170 --> 01:04:47,460
你需要将数据投影

1098
01:04:47,890 --> 01:04:49,080
到2维空间或者3维空间中

1099
01:04:49,430 --> 01:04:50,820
这样你可以画出一个数据的三维视图

1100
01:04:51,180 --> 01:04:53,290
这样你可以令高维数据可视化

1101
01:04:53,640 --> 01:04:55,320
从而能够更好地理解数据中的结构

1102
01:04:55,730 --> 01:04:59,160
我最近接触的一个具体的例子

1103
01:04:59,490 --> 01:05:01,580
来自于Stanford大学

1104
01:05:01,920 --> 01:05:04,820
Krishna Shenoy的实验室

1105
01:05:05,310 --> 01:05:08,990
他从猴脑的50个

1106
01:05:09,320 --> 01:05:09,920
不同的部分读取信号

1107
01:05:10,300 --> 01:05:12,370
我不记得具体的数字是否是50

1108
01:05:12,690 --> 01:05:13,940
大概有

1109
01:05:14,700 --> 01:05:15,460
几十个部分

1110
01:05:15,820 --> 01:05:17,320
所以特征向量

1111
01:05:17,620 --> 01:05:18,400
有50个维度

1112
01:05:18,820 --> 01:05:20,840
对应着猴脑

1113
01:05:21,250 --> 01:05:22,780
不同部分的

1114
01:05:23,170 --> 01:05:24,500
电波活动

1115
01:05:24,850 --> 01:05:25,810
实际上有50个神经元

1116
01:05:26,080 --> 01:05:29,430
这几十个

1117
01:05:29,760 --> 01:05:30,480
猴脑的神经元

1118
01:05:30,800 --> 01:05:32,320
对应着几十个维度的时间序列

1119
01:05:32,670 --> 01:05:34,050
很难将这些

1120
01:05:34,430 --> 01:05:35,370
高维数据可视化

1121
01:05:35,770 --> 01:05:39,020
他使用PCA将

1122
01:05:39,350 --> 01:05:41,340
这50维的数据表示

1123
01:05:41,640 --> 01:05:42,450
成了三维数据

1124
01:05:42,800 --> 01:05:44,690
这样你就可以将

1125
01:05:44,990 --> 01:05:47,160
这些数据在三维空间中表示出来

1126
01:05:47,480 --> 01:05:48,740
所以你可以看到

1127
01:05:49,070 --> 01:05:50,350
猴子在不同的时间内都在想些什么

1128
01:05:53,280 --> 01:05:55,950
另一个PCA

1129
01:05:56,320 --> 01:05:58,020
的常见应用是压缩

1130
01:05:59,660 --> 01:06:01,260
如果你有很高为的数据

1131
01:06:01,650 --> 01:06:03,430
而且你希望用一系列数字进行存储

1132
01:06:04,190 --> 01:06:05,740
PCA可以很好地完成这个工作

1133
01:06:06,140 --> 01:06:08,400
事实证明

1134
01:06:08,700 --> 01:06:11,820
有时在机器学习中

1135
01:06:12,650 --> 01:06:14,060
有的时候输入数据的

1136
01:06:14,380 --> 01:06:15,430
维度非常高

1137
01:06:15,850 --> 01:06:17,500
因为一些计算上的原因

1138
01:06:17,880 --> 01:06:19,210
你不希望处理

1139
01:06:19,560 --> 01:06:20,450
这些高维数据

1140
01:06:24,890 --> 01:06:27,560
所以  


1141
01:06:27,890 --> 01:06:29,350
通常情况下

1142
01:06:29,770 --> 01:06:31,240
可以用PCA处理高维数据

1143
01:06:31,590 --> 01:06:34,790
将它们表示在 低维的子空间中

1144
01:06:35,060 --> 01:06:36,100
也就是我们

1145
01:06:36,350 --> 01:06:37,100
之前写的y^((i) )

1146
01:06:37,380 --> 01:06:38,500
所以这样你就可以处理

1147
01:06:38,720 --> 01:06:39,540
维数低得多的数据了

1148
01:06:39,870 --> 01:06:42,350
实际上

1149
01:06:42,620 --> 01:06:43,970
在实际应用

1150
01:06:44,200 --> 01:06:45,130
中你会发现

1151
01:06:45,370 --> 01:06:46,560
当你的数据

1152
01:06:46,900 --> 01:06:49,390
维度很高时  


1153
01:06:49,870 --> 01:06:51,700
通常情况下

1154
01:06:52,070 --> 01:06:54,260
这些高维数据

1155
01:06:54,570 --> 01:06:56,540
会坐落在

1156
01:06:56,850 --> 01:06:58,560
维度低得多的子空间中

1157
01:06:58,950 --> 01:07:01,430
所以你经常可以机打的

1158
01:07:01,730 --> 01:07:02,710
降低数据的维度

1159
01:07:03,070 --> 01:07:06,020
与此同时并没有

1160
01:07:06,280 --> 01:07:08,560
丢失太多的信息

1161
01:07:10,320 --> 01:07:13,120
让我看看

1162
01:07:13,500 --> 01:07:14,560
如果你的学习算法

1163
01:07:15,060 --> 01:07:15,970
在处理高维数据的时候

1164
01:07:16,230 --> 01:07:18,150
会花很多时间

1165
01:07:18,540 --> 01:07:19,820
那么你经常可以使用

1166
01:07:20,160 --> 01:07:21,040
PCA将这些数据压缩成低维的形式

1167
01:07:21,380 --> 01:07:22,430
从而可以让

1168
01:07:22,720 --> 01:07:23,520
你的学习算法运行地更快

1169
01:07:23,850 --> 01:07:25,870
你经常可以在

1170
01:07:26,210 --> 01:07:27,220
学习算法中这样做

1171
01:07:27,630 --> 01:07:28,530
基本不会有什么性能上的损失

1172
01:07:29,070 --> 01:07:31,620
PCA在学习算法中

1173
01:07:33,020 --> 01:07:33,990
的另外一个应用是--


1174
01:07:34,400 --> 01:07:36,280
你记得当我们讲

1175
01:07:36,590 --> 01:07:37,440
学习理论的时候

1176
01:07:37,790 --> 01:07:38,710
我们说过

1177
01:07:39,080 --> 01:07:40,170
你的特征越多

1178
01:07:40,420 --> 01:07:42,330
假设类就越复杂

1179
01:07:42,690 --> 01:07:44,200
我们当时讲的是线性分类的例子

1180
01:07:44,490 --> 01:07:45,340
如果你有更多的特征

1181
01:07:45,590 --> 01:07:46,260
如果你有很多特征

1182
01:07:46,670 --> 01:07:48,580
那么你就更容易过拟合

1183
01:07:48,940 --> 01:07:51,040
所以你可以用

1184
01:07:51,340 --> 01:07:52,770
PCA降低

1185
01:07:53,060 --> 01:07:53,900
数据的维度

1186
01:07:54,160 --> 01:07:55,940
从而减少特征的数量

1187
01:07:56,230 --> 01:07:57,220
从而减少

1188
01:07:57,500 --> 01:07:58,320
过拟合的可能性

1189
01:07:58,720 --> 01:08:01,620
PCA在学习算法中的这种应用

1190
01:08:02,000 --> 01:08:03,330
有的时候是有效的

1191
01:08:03,700 --> 01:08:04,340
它经常有效

1192
01:08:04,620 --> 01:08:06,190
我稍后会给出

1193
01:08:06,580 --> 01:08:07,560
几个有效的例子

1194
01:08:07,890 --> 01:08:10,280
但是PCA的这类应用

1195
01:08:10,630 --> 01:08:12,950
在工业界

1196
01:08:13,270 --> 01:08:14,450
有点被滥用了

1197
01:08:14,850 --> 01:08:17,520
我们稍后

1198
01:08:17,870 --> 01:08:18,690
会讲

1199
01:08:19,190 --> 01:08:21,850
到这一点

1200
01:08:22,260 --> 01:08:25,180
还有其它的

1201
01:08:25,440 --> 01:08:26,160
一些应用

1202
01:08:26,410 --> 01:08:28,830
一个应用是异常检测

1203
01:08:34,290 --> 01:08:36,550
它的主要做法是

1204
01:08:36,960 --> 01:08:40,000
假设给你一个数据集合

1205
01:08:40,770 --> 01:08:41,960
你之后可以运行PCA

1206
01:08:42,310 --> 01:08:44,520
找到数据大致位于

1207
01:08:44,820 --> 01:08:45,650
那个子空间中

1208
01:08:46,240 --> 01:08:49,120
之后如果你想发现

1209
01:08:49,470 --> 01:08:50,370
未来样本中的异常值

1210
01:08:50,780 --> 01:08:52,140
你需要观察当前样本

1211
01:08:52,430 --> 01:08:54,490
是否距离子空间非常远

1212
01:08:54,880 --> 01:08:56,710
这并不是一个很好的

1213
01:08:57,010 --> 01:08:57,780
异常检测算法

1214
01:08:58,200 --> 01:08:59,800
它的做法是--


1215
01:09:00,190 --> 01:09:03,040
如果给你一个数据集

1216
01:09:03,480 --> 01:09:06,290
你可能它们坐落

1217
01:09:06,600 --> 01:09:07,490
在一个低维的子空间中

1218
01:09:07,720 --> 01:09:09,110
如果你找到了一个

1219
01:09:09,370 --> 01:09:10,110
距离子空间非常远的点

1220
01:09:10,490 --> 01:09:11,060
你就可以认为

1221
01:09:11,350 --> 01:09:11,980
它是一个异常点

1222
01:09:12,320 --> 01:09:14,580
这不是最好的

1223
01:09:14,870 --> 01:09:15,750
异常检测算法

1224
01:09:16,200 --> 01:09:17,100
但是有的时候这个算法已经足够了

1225
01:09:17,390 --> 01:09:21,280
我要讲的

1226
01:09:21,580 --> 01:09:24,450
最后一个应用

1227
01:09:25,150 --> 01:09:30,480
是匹配或者进行

1228
01:09:30,830 --> 01:09:31,700
更好的距离计算

1229
01:09:37,360 --> 01:09:39,440
先让我解释一下这是什么意思

1230
01:09:39,890 --> 01:09:41,250
最后这个应用我要将一些细节

1231
01:09:57,030 --> 01:09:59,200
它是这个意思

1232
01:09:59,780 --> 01:10:03,320
假设你们想进行人脸识别

1233
01:10:03,780 --> 01:10:13,050
假设你想进行人脸识别

1234
01:10:13,960 --> 01:10:16,800
输入图片的大小是100*100

1235
01:10:19,080 --> 01:10:23,630
一张人脸的图片

1236
01:10:23,960 --> 01:10:24,660
看起来是这样的

1237
01:10:24,920 --> 01:10:26,810
是一个像素矩阵

1238
01:10:27,200 --> 01:10:29,500
每个像素都有不同的灰度值

1239
01:10:29,850 --> 01:10:31,210
根据不同的

1240
01:10:31,630 --> 01:10:32,960
灰度值

1241
01:10:33,270 --> 01:10:34,990
你得到不同人的脸部照片

1242
01:10:48,360 --> 01:10:53,350
假设输入图片是100*100的大小

1243
01:10:53,720 --> 01:10:54,870
你将每张人脸的照片

1244
01:10:55,310 --> 01:10:59,870
表示成一个10000维的向量

1245
01:11:00,490 --> 01:11:02,720
维度非常高

1246
01:11:08,700 --> 01:11:11,110
我们可以用图表示一下

1247
01:11:11,590 --> 01:11:12,680
你可以认为这里是

1248
01:11:13,020 --> 01:11:16,280
10000维空间中的点

1249
01:11:17,140 --> 01:11:18,030
如果你有许多张

1250
01:11:18,390 --> 01:11:19,490
不同的脸部照片

1251
01:11:19,910 --> 01:11:23,170
每张照片就对应着一个

1252
01:11:23,460 --> 01:11:25,620
10000维空间中的点

1253
01:11:28,450 --> 01:11:30,980
这张图中

1254
01:11:31,460 --> 01:11:32,420
我希望你们认识到  


1255
01:11:32,770 --> 01:11:34,970
大多数数据都存在于

1256
01:11:35,300 --> 01:11:36,170
一个相对低维的子空间中

1257
01:11:36,510 --> 01:11:38,430
因为在这个

1258
01:11:38,730 --> 01:11:40,400
10000维的空间中

1259
01:11:40,820 --> 01:11:42,520
不是所有的点

1260
01:11:42,910 --> 01:11:44,190
都对应着

1261
01:11:44,580 --> 01:11:46,500
有效的人脸图片

1262
01:11:46,890 --> 01:11:49,630
有很多维度的值

1263
01:11:50,020 --> 01:11:51,560
都对应着无意义的随机噪声

1264
01:11:51,910 --> 01:11:53,290
例如:一些不属于人脸的东西

1265
01:11:53,620 --> 01:11:56,910
可能的脸部图像

1266
01:11:57,230 --> 01:11:59,610
位于低维的子空间中

1267
01:12:00,030 --> 01:12:01,610
我们需要

1268
01:12:19,990 --> 01:12:21,390
找到数据

1269
01:12:22,730 --> 01:12:25,530
所在的低维子

1270
01:12:26,510 --> 01:12:27,360
空间

1271
01:12:27,640 --> 01:12:30,240
使用PCA的话一般

1272
01:12:30,570 --> 01:12:32,020
50维是比较常见的

1273
01:12:34,920 --> 01:12:36,400
所以你可以认为


1274
01:12:37,300 --> 01:12:39,870
有一些轴能够真正

1275
01:12:40,260 --> 01:12:42,110
衡量脸部的形状

1276
01:12:42,670 --> 01:12:46,170
但是另外的轴

1277
01:12:46,850 --> 01:12:48,760
我们不感兴趣

1278
01:12:49,130 --> 01:12:50,650
它们只是随机噪声

1279
01:12:51,180 --> 01:12:56,750
在脸部识别应用中

1280
01:12:57,160 --> 01:12:59,060
我可能会给你一张人脸的图片

1281
01:12:59,470 --> 01:13:01,010
然后问你那一张脸

1282
01:13:01,350 --> 01:13:02,180
看起来和这张图片最像

1283
01:13:02,490 --> 01:13:03,300
我会给你一个人的照片

1284
01:13:03,630 --> 01:13:04,560
然后问你  你能找到同一个人

1285
01:13:04,890 --> 01:13:05,770
的其他照片吗?

1286
01:13:06,160 --> 01:13:11,050
实现这个功能的关键步骤

1287
01:13:11,370 --> 01:13:12,130
是给你两张脸部照片

1288
01:13:12,440 --> 01:13:13,580
看看这两张脸有多相似

1289
01:13:20,860 --> 01:13:22,580
所以这里是我们是这样使用PCA的

1290
01:13:23,210 --> 01:13:29,510
这里有一张脸  这里也有一张脸

1291
01:13:29,970 --> 01:13:32,430
我衡量这两张脸

1292
01:13:32,760 --> 01:13:33,590
之间差异的时

1293
01:13:34,000 --> 01:13:34,860
并不会直接使用欧几里得

1294
01:13:35,150 --> 01:13:36,350
距离作为相似度评判标准

1295
01:13:36,710 --> 01:13:39,470
我会将这两个点

1296
01:13:39,840 --> 01:13:41,640
分别投影到50维的


1297
01:13:42,070 --> 01:13:44,330
子空间中

1298
01:13:44,620 --> 01:13:45,470
之后求出

1299
01:13:45,860 --> 01:13:48,380
这两个点

1300
01:13:48,680 --> 01:13:50,040
在子空间中的距离

1301
01:13:50,830 --> 01:13:52,540
我这样做的时候

1302
01:13:52,950 --> 01:13:54,630
这两张在原空间

1303
01:13:54,940 --> 01:13:56,160
中看起来

1304
01:13:56,530 --> 01:13:57,420
非常远的脸

1305
01:13:57,740 --> 01:13:59,350
当我将它们投影到子空间中之后

1306
01:13:59,740 --> 01:14:02,000
它们看起来非常近

1307
01:14:02,640 --> 01:14:04,460
因此更为相似

1308
01:14:04,750 --> 01:14:05,620
我一会儿要在笔记本

1309
01:14:05,940 --> 01:14:10,470
上给你们展示

1310
01:14:10,800 --> 01:14:15,310
对于10000维的训练样本

1311
01:14:15,680 --> 01:14:18,210
我会将它们用灰度图片表示出来

1312
01:14:18,650 --> 01:14:19,980
这样我就得到了

1313
01:14:20,350 --> 01:14:21,070
人脸的照片

1314
01:14:21,860 --> 01:14:24,160
我还要给你们展示

1315
01:14:24,540 --> 01:14:26,140
特征向量

1316
01:14:26,650 --> 01:14:28,300
特征向量存在于

1317
01:14:28,700 --> 01:14:30,650
10000维的子空间中

1318
01:14:31,140 --> 01:14:35,990
我会将它们

1319
01:14:36,350 --> 01:14:40,770
画成特征脸

1320
01:14:41,310 --> 01:14:44,220
PCA的工作是用

1321
01:14:44,520 --> 01:14:47,320
这些特征脸图片的

1322
01:14:47,660 --> 01:14:49,550
线性组合来逼近这些x^((i) )

1323
01:14:49,900 --> 01:14:52,060
特征脸相当于

1324
01:14:52,370 --> 01:14:53,460
这些基础坐标轴

1325
01:14:53,700 --> 01:14:56,550
u_1 和u_2

1326
01:14:56,850 --> 01:14:59,020
它们会扩展出

1327
01:14:59,330 --> 01:15:01,350
真正的人脸图片所在的子平面

1328
01:17:22,330 --> 01:17:25,200
这是一个包含了

1329
01:17:25,570 --> 01:17:28,150
若干张脸部图片的训练集合

1330
01:17:28,480 --> 01:17:29,570
真正的训练集合的大小

1331
01:17:29,860 --> 01:17:30,950
比这里展示的要大得多

1332
01:17:31,330 --> 01:17:32,860
当你运行PCA之后

1333
01:17:33,190 --> 01:17:41,390
这里是一些u_i

1334
01:17:41,760 --> 01:17:46,410
记得我刚才说过

1335
01:17:46,680 --> 01:17:47,390
我们可以像画出原始图片

1336
01:17:47,700 --> 01:17:48,730
那样画出这些特征向量u_i

1337
01:17:49,000 --> 01:17:49,790
所以你可以将这些特征向量

1338
01:17:50,070 --> 01:17:51,990
画成灰度图像

1339
01:17:52,290 --> 01:17:54,600
我们可以用这些特征向量的

1340
01:17:54,900 --> 01:17:56,470
特征组合来逼近原始的图像

1341
01:17:56,820 --> 01:17:58,520
我之前说过  只看一个单独的特征向量

1342
01:17:58,830 --> 01:17:59,970
是很危险的

1343
01:18:00,370 --> 01:18:02,000
你们不应该这样做

1344
01:18:02,260 --> 01:18:03,790
但是让我们稍微尝试一下

1345
01:18:04,120 --> 01:18:05,230
如果你看第一个特征向量

1346
01:18:05,590 --> 01:18:07,470
你会发现这个特征的含义大概是

1347
01:18:07,770 --> 01:18:08,890
脸部是否

1348
01:18:09,210 --> 01:18:10,110
从左到右逐渐被照亮

1349
01:18:10,400 --> 01:18:12,160
所以取决于

1350
01:18:12,480 --> 01:18:13,220
这张图的权重

1351
01:18:13,540 --> 01:18:14,830
我们大概可以捕捉到光线的变化

1352
01:18:15,430 --> 01:18:18,220
第二个特征比较难以辨认

1353
01:18:18,500 --> 01:18:19,740
它似乎捕捉的是

1354
01:18:19,970 --> 01:18:20,960
整个脸部的明亮度的变化

1355
01:18:21,240 --> 01:18:23,330
第三个特征向量

1356
01:18:23,600 --> 01:18:25,960
大致捕捉的

1357
01:18:26,360 --> 01:18:27,640
是脸部的阴影

1358
01:18:27,960 --> 01:18:29,430
或者胡须等区域

1359
01:18:29,840 --> 01:18:31,000
虽然单独查看一个

1360
01:18:31,280 --> 01:18:31,850
特征向量是危险的

1361
01:18:32,080 --> 01:18:34,610
但是从中我们可以得到一些信息

1362
01:18:34,890 --> 01:18:36,320
这里是一个

1363
01:18:36,640 --> 01:18:39,560
特征脸的应用

1364
01:18:39,950 --> 01:18:41,860
这项工作来自

1365
01:18:42,120 --> 01:18:44,580
Alex Pentland MIT的实验室

1366
01:18:44,870 --> 01:18:46,310
左上角的

1367
01:18:47,040 --> 01:18:49,240
这张图是特征脸

1368
01:18:49,640 --> 01:18:51,530
算法的输入

1369
01:18:52,100 --> 01:18:55,140
算法之后会访问数据库

1370
01:18:55,480 --> 01:18:56,450
从中找到和输入最为相似的脸

1371
01:18:56,890 --> 01:18:58,700
第二张图片是

1372
01:18:59,770 --> 01:19:01,250
算法认为和输入

1373
01:19:01,550 --> 01:19:02,340
最接近的脸

1374
01:19:02,700 --> 01:19:04,640
下一张是第二像的

1375
01:19:04,900 --> 01:19:06,450
这一章是第三像的

1376
01:19:06,830 --> 01:19:07,530
等等

1377
01:19:07,890 --> 01:19:10,630
使用特征脸

1378
01:19:10,900 --> 01:19:12,230
通过衡量不同的

1379
01:19:12,660 --> 01:19:13,500
脸在低维子空间中的相似度

1380
01:19:13,790 --> 01:19:15,240
我们可以识别出

1381
01:19:15,560 --> 01:19:17,090
同一个人的照片

1382
01:19:17,410 --> 01:19:20,260
即使这个人的照片已经被删除

1383
01:19:20,630 --> 01:19:22,200
下一行显示的是

1384
01:19:22,560 --> 01:19:23,880
接下来的最接近的脸

1385
01:19:24,100 --> 01:19:29,310
这是第四像的脸

1386
01:19:29,670 --> 01:19:30,360
等等

1387
01:19:30,680 --> 01:19:33,670
这是特征脸的一个常见的应用

1388
01:19:35,230 --> 01:19:36,730
最后我要说的是

1389
01:19:37,050 --> 01:19:38,290
当有人询问我

1390
01:19:39,060 --> 01:19:40,190
关于机器学习的问题时

1391
01:19:40,480 --> 01:19:41,530
我经常会推荐

1392
01:19:41,890 --> 01:19:42,660
他们尝试一下PCA

1393
01:19:42,960 --> 01:19:44,070
因为PCA有很多用途

1394
01:19:44,460 --> 01:19:45,550
例如可以用来进行压缩

1395
01:19:45,880 --> 01:19:46,940
可视化  等等

1396
01:19:47,210 --> 01:19:48,370
但是在工业界

1397
01:19:48,700 --> 01:19:51,520
我经常看到PCA被更多地使用

1398
01:19:51,790 --> 01:19:52,690
很多时候你会看到

1399
01:19:53,060 --> 01:19:53,990
有人在本不该使用

1400
01:19:54,410 --> 01:19:55,290
PCA的时候使用它

1401
01:19:55,630 --> 01:19:56,860
所以我的建议是

1402
01:19:57,220 --> 01:19:59,680
在使用PCA之前

1403
01:20:00,060 --> 01:20:01,970
想一想原始数据x^((i) )

1404
01:20:02,340 --> 01:20:03,520
是否

1405
01:20:03,830 --> 01:20:05,230
不经过压缩就可以使用

1406
01:20:05,630 --> 01:20:06,720
我经常看到

1407
01:20:07,110 --> 01:20:08,380
有人在不该压缩数据

1408
01:20:08,720 --> 01:20:09,560
的时候压缩数据

1409
01:20:09,920 --> 01:20:11,190
我经常

1410
01:20:11,570 --> 01:20:13,250
建议人们

1411
01:20:13,570 --> 01:20:14,830
将PCA应用在不同的问题上

1412
01:20:15,140 --> 01:20:16,080
它通常是有效的

1413
01:20:16,430 --> 01:20:17,350
很抱歉拖堂了

1414
01:20:17,680 --> 01:20:18,870
今天就到这里吧

