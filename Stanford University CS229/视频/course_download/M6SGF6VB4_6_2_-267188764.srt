1
00:00:23,290 --> 00:00:26,040
What I want to do today

2
00:00:26,160 --> 00:00:29,620
is continue our discussion of Na.ve Bayes,

3
00:00:29,730 --> 00:00:31,390
which is the learning algorithm that

4
00:00:31,550 --> 00:00:34,390
I started to discuss in the previous lecture

5
00:00:34,520 --> 00:00:37,410
and talk about a couple of

6
00:00:37,520 --> 00:00:39,140
different event models in Na.ve Bayes,

7
00:00:39,270 --> 00:00:41,620
and then I'll take a brief digression

8
00:00:41,730 --> 00:00:43,060
to talk about neural networks,

9
00:00:43,170 --> 00:00:44,820
which is something that

10
00:00:44,930 --> 00:00:46,380
I actually won't spend a lot of time on,

11
00:00:46,500 --> 00:00:48,880
and then I want to start to talk about

12
00:00:48,990 --> 00:00:50,010
support vector machines,

13
00:00:50,130 --> 00:00:51,430
and support vector machines

14
00:00:51,520 --> 00:00:53,490
is the learning algorithms,

15
00:00:53,590 --> 00:00:55,320
the supervised learning algorithm that

16
00:00:55,440 --> 00:00:59,030
many people consider the most effective,

17
00:00:59,150 --> 00:01:01,990
off-the-shelf supervised learning algorithm.

18
00:01:02,110 --> 00:01:03,520
That point of view is debatable,

19
00:01:03,610 --> 00:01:04,980
but there are many people that

20
00:01:05,090 --> 00:01:06,000
hold that point of view,

21
00:01:06,150 --> 00:01:07,870
and we'll start discussing that today,

22
00:01:07,970 --> 00:01:09,560
and this will actually take us

23
00:01:09,670 --> 00:01:11,180
a few lectures to complete.

24
00:01:11,270 --> 00:01:14,680
So let's talk about Na.ve Bayes.

25
00:01:14,790 --> 00:01:17,770
To recap from the previous lecture,

26
00:01:17,880 --> 00:01:25,750
I started off describing spam classification

27
00:01:25,850 --> 00:01:27,880
as the most [inaudible] example for Na.ve Bayes

28
00:01:27,990 --> 00:01:30,370
in which we would create feature vectors like these,

29
00:01:30,540 --> 00:01:34,930
right, that correspond to words in a dictionary.

30
00:01:35,030 --> 00:01:40,340
And so, you know, based on

31
00:01:40,460 --> 00:01:42,010
what words appear in a piece of email

32
00:01:42,120 --> 00:01:44,300
were represented as a feature vector with

33
00:01:44,480 --> 00:01:47,130
ones and zeros in the corresponding places,

34
00:01:47,260 --> 00:01:52,060
and Na.ve Bayes was a generative learning algorithm,

35
00:01:52,150 --> 00:02:01,330
and by that I mean it's an algorithm

36
00:02:01,440 --> 00:02:03,220
in which we model PFX given Y,

37
00:02:03,320 --> 00:02:07,400
and for Na.ve Bayes, specifically,

38
00:02:07,520 --> 00:02:11,740
we modeled it as product from I equals one to N,

39
00:02:11,840 --> 00:02:14,700
PFXI given Y,

40
00:02:14,700 --> 00:02:18,660
and also we model PFY,

41
00:02:18,780 --> 00:02:20,900
and then we use Bayes Rule, right,

42
00:02:21,020 --> 00:02:22,010
to combine these two together,

43
00:02:22,120 --> 00:02:24,570
and so our predictions,

44
00:02:24,670 --> 00:02:26,430
when you give it a new piece of email

45
00:02:26,540 --> 00:02:27,990
you want to tell if it's spam or not spam,

46
00:02:28,790 --> 00:02:31,770
you predict RFX over Y, PFY given X,

47
00:02:31,900 --> 00:02:35,900
which by Bayes Rule is RFX over Y,

48
00:02:36,020 --> 00:02:40,540
PFX given Y, times BY, okay?

49
00:02:40,660 --> 00:02:42,520
So this is Na.ve Bayes,

50
00:02:42,650 --> 00:02:45,250
and just to draw attention to two things,

51
00:02:45,370 --> 00:02:47,070
one is that in this model,

52
00:02:47,190 --> 00:02:50,790
each of our features were zero, one,

53
00:02:50,900 --> 00:02:53,060
so indicating whether different words appear,

54
00:02:53,150 --> 00:02:57,960
and the length or the feature vector was, sort of,

55
00:02:58,020 --> 00:02:59,210
the length N of the feature vector

56
00:02:59,320 --> 00:03:01,940
was the number of words in the dictionary.

57
00:03:02,060 --> 00:03:10,580
So it might be on this version

58
00:03:10,710 --> 00:03:13,710
on the order of 50,000 words, say.

59
00:03:13,820 --> 00:03:19,030
What I want to do now

60
00:03:19,130 --> 00:03:21,050
is describe two variations on this algorithm.

61
00:03:21,160 --> 00:03:23,350
The first one is the simpler one,

62
00:03:23,620 --> 00:03:25,890
which it's just a generalization to

63
00:03:26,020 --> 00:03:34,490
if XI takes on more values. So, you know,

64
00:03:34,610 --> 00:03:36,200
one thing that's commonly done

65
00:03:36,310 --> 00:03:37,920
is to apply Na.ve Bayes to problems

66
00:03:38,040 --> 00:03:40,620
where some of these features, XI,

67
00:03:40,710 --> 00:03:42,780
takes on K values rather than just two values,

68
00:03:42,890 --> 00:03:49,120
and in that case, you actually build, sort of,

69
00:03:49,200 --> 00:03:51,380
a very similar model where PFX given Y

70
00:03:51,480 --> 00:03:56,630
is really the same thing, right,

71
00:03:56,740 --> 00:03:58,400
where now these are going to

72
00:03:58,510 --> 00:04:03,420
be multinomial probabilities rather than Bernoulli's

73
00:04:03,540 --> 00:04:05,690
because the XI's can, maybe,

74
00:04:05,770 --> 00:04:07,000
take on up to K values.

75
00:04:07,110 --> 00:04:09,420
It turns out, the situation where

76
00:04:09,520 --> 00:04:12,150
one situation where this arises very commonly is

77
00:04:12,260 --> 00:04:14,360
if you have a feature

78
00:04:14,460 --> 00:04:16,080
that's actually continuous valued,

79
00:04:16,180 --> 00:04:18,500
and you choose to dispertise it,

80
00:04:18,590 --> 00:04:19,360
and you choose to

81
00:04:19,470 --> 00:04:21,310
take a continuous value feature and dispertise it

82
00:04:21,420 --> 00:04:24,160
into a finite set of K values,

83
00:04:24,270 --> 00:04:27,230
and so it's a perfect example

84
00:04:27,320 --> 00:04:33,690
if you remember our very first supervised learning

85
00:04:33,780 --> 00:04:36,420
problem of predicting the price of houses.

86
00:04:36,550 --> 00:04:38,900
If you have the classification

87
00:04:39,020 --> 00:04:40,220
problem on these houses,

88
00:04:40,340 --> 00:04:42,010
so based on features of a house,

89
00:04:42,120 --> 00:04:43,290
and you want to predict whether or not

90
00:04:43,400 --> 00:04:45,150
the house will be sold in the next six months, say.

91
00:04:45,260 --> 00:04:46,260
That's a classification problem,

92
00:04:46,370 --> 00:04:47,960
and once you use Na.ve Bayes,

93
00:04:48,070 --> 00:04:50,500
then given a continuous value feature

94
00:04:50,600 --> 00:04:55,000
like the living area, you know,

95
00:04:55,100 --> 00:04:56,670
one pretty common thing to do

96
00:04:56,770 --> 00:04:59,040
would be take the continuous value living area

97
00:04:59,140 --> 00:05:03,140
and just dispertise it into a few

98
00:05:03,260 --> 00:05:14,400
discreet buckets, and so depending on whether

99
00:05:14,500 --> 00:05:15,780
the living area of the house

100
00:05:15,860 --> 00:05:17,150
is less than 500 square feet

101
00:05:17,260 --> 00:05:19,860
or between 1,000 and 1500 square feet, and so on,

102
00:05:19,950 --> 00:05:21,820
or whether it's greater than 2,000 square feet,

103
00:05:21,940 --> 00:05:24,300
you choose the value of the corresponding feature,

104
00:05:24,430 --> 00:05:28,390
XI, to be one, two, three, or four, okay?

105
00:05:28,480 --> 00:05:32,050
So that was the first variation or generalization

106
00:05:32,160 --> 00:05:34,100
of Na.ve Bayes I wanted to talk about.

107
00:05:34,220 --> 00:05:36,940
I should just check;

108
00:05:37,030 --> 00:05:49,710
are there questions about this? Okay. Cool.

109
00:05:49,820 --> 00:05:52,110
And so it turns out that in practice,

110
00:05:52,220 --> 00:05:54,940
it's fairly common to use about ten buckets

111
00:05:55,060 --> 00:05:57,780
to dispertise a continuous value feature.

112
00:05:57,890 --> 00:05:59,760
I drew four here only to save on writing.

113
00:05:59,860 --> 00:06:05,450
The second and, sort of, final variation that

114
00:06:05,550 --> 00:06:06,820
I want to talk about for Na.ve Bayes

115
00:06:06,930 --> 00:06:08,430
is a variation that's specific

116
00:06:08,580 --> 00:06:12,600
to classifying text documents, or,

117
00:06:12,690 --> 00:06:14,600
more generally, for classifying sequences.

118
00:06:14,720 --> 00:06:16,760
So the text document, like a piece of email,

119
00:06:16,850 --> 00:06:18,780
you can think of as a sequence of words

120
00:06:18,870 --> 00:06:21,420
and you can apply this, sort of,

121
00:06:21,530 --> 00:06:22,780
model I'm about to describe

122
00:06:22,830 --> 00:06:24,420
to classifying other sequences as well,

123
00:06:24,550 --> 00:06:26,260
but let me just focus on text,

124
00:06:26,380 --> 00:06:30,410
and here's the idea.

125
00:06:30,520 --> 00:06:34,530
So the Na.ve Bayes algorithm

126
00:06:34,620 --> 00:06:36,350
as I've described it so far, right,

127
00:06:36,470 --> 00:06:38,720
given a piece of email, we were representing it

128
00:06:38,820 --> 00:06:41,700
using this binary vector value representation,

129
00:06:41,820 --> 00:06:43,790
and one of the things that this loses,

130
00:06:43,900 --> 00:06:45,820
for instance, is the number of times that

131
00:06:45,900 --> 00:06:48,140
different words appear, all right?

132
00:06:48,220 --> 00:06:51,100
So, for example, if some word appears a lot of times,

133
00:06:51,230 --> 00:06:52,420
and you see the word, you know,

134
00:06:52,520 --> 00:06:53,530
"buy" a lot of times.

135
00:06:53,620 --> 00:06:55,020
You see the word "Viagra";

136
00:06:55,150 --> 00:06:56,780
it seems to be a common email example.

137
00:06:56,900 --> 00:06:59,050
You see the word Viagra a ton of times in the email,

138
00:06:59,160 --> 00:07:01,710
it is more likely to be spam than it appears,

139
00:07:01,820 --> 00:07:02,860
I guess, only once

140
00:07:02,860 --> 00:07:03,460
because even once,

141
00:07:03,560 --> 00:07:04,980
I guess, is enough.

142
00:07:05,080 --> 00:07:10,180
So let me just try a different,

143
00:07:10,250 --> 00:07:12,340
what's called an event model for Na.ve Bayes

144
00:07:12,420 --> 00:07:14,280
that will take into account the number of times

145
00:07:14,390 --> 00:07:15,960
a word appears in the email,

146
00:07:16,070 --> 00:07:18,720
and to give this previous model a name

147
00:07:18,840 --> 00:07:21,340
as well this particular model

148
00:07:21,430 --> 00:07:24,240
for text classification is called

149
00:07:24,320 --> 00:07:33,050
the Multivariate Bernoulli Event Model.

150
00:07:33,160 --> 00:07:34,580
It's not a great name.

151
00:07:34,700 --> 00:07:36,310
Don't worry about what the name means.

152
00:07:36,390 --> 00:07:40,460
It refers to the fact that

153
00:07:40,570 --> 00:07:42,380
there are multiple Bernoulli random variables,

154
00:07:42,500 --> 00:07:43,350
but it's really

155
00:07:43,420 --> 00:07:44,850
don't worry about what the name means.

156
00:07:44,940 --> 00:07:47,180
In contrast, what I want to do now

157
00:07:47,300 --> 00:07:49,020
is describe a different representation

158
00:07:49,140 --> 00:07:51,100
for email in terms of the feature vector,

159
00:07:51,220 --> 00:08:00,150
and this is called the Multinomial Event Model, and,

160
00:08:00,280 --> 00:08:02,210
again, there is a rationale behind the name,

161
00:08:02,340 --> 00:08:03,370
but it's slightly cryptic,

162
00:08:03,460 --> 00:08:04,570
so don't worry about why it's called

163
00:08:04,670 --> 00:08:06,000
the Multinomial Event Model;

164
00:08:06,090 --> 00:08:07,020
it's just called that.

165
00:08:07,180 --> 00:08:08,650
And here's what we're gonna do,

166
00:08:08,740 --> 00:08:10,340
given a piece of email,

167
00:08:10,450 --> 00:08:12,020
I'm going to represent my email

168
00:08:12,190 --> 00:08:13,300
as a feature vector,

169
00:08:13,410 --> 00:08:17,620
and so my ith training example,

170
00:08:17,740 --> 00:08:24,920
XI will be a feature vector, XI sub group one,

171
00:08:24,980 --> 00:08:37,140
XI sub group two, XI subscript NI where NI is equal to

172
00:08:37,260 --> 00:08:43,670
the number of words in this email, right?

173
00:08:43,780 --> 00:08:45,080
So if one of my training examples

174
00:08:45,190 --> 00:08:46,800
is an email with 300 words in it,

175
00:08:46,910 --> 00:08:50,850
then I represent this email via a feature vector

176
00:08:50,930 --> 00:08:53,290
with 300 elements,

177
00:08:53,400 --> 00:09:02,570
and each of these elements of the feature vector

178
00:09:02,680 --> 00:09:05,040
lets see. Let me just write this as X subscript J.

179
00:09:05,160 --> 00:09:12,420
These will be an index into my dictionary, okay?

180
00:09:12,550 --> 00:09:15,410
And so if my dictionary has 50,000 words,

181
00:09:15,540 --> 00:09:19,020
then each position in my feature vector will be

182
00:09:19,140 --> 00:09:22,710
a variable that takes on one of 50,000 possible values

183
00:09:22,830 --> 00:09:27,510
corresponding to what word appeared

184
00:09:27,620 --> 00:09:31,060
in the J position of my email, okay?

185
00:09:31,180 --> 00:09:33,390
So, in other words, I'm gonna take

186
00:09:33,470 --> 00:09:34,420
all the words in my email

187
00:09:34,540 --> 00:09:36,530
and you have a feature vector that

188
00:09:36,620 --> 00:09:39,260
just says which word in my dictionary

189
00:09:39,370 --> 00:09:42,210
was each word in the email, okay?

190
00:09:42,320 --> 00:09:45,470
So a different definition for NI now,

191
00:09:45,580 --> 00:09:47,430
NI now varies and is different for

192
00:09:47,530 --> 00:09:48,570
every training example,

193
00:09:48,700 --> 00:09:51,920
and this XJ is now indexed into the dictionary.

194
00:09:52,040 --> 00:09:54,530
You know, the components of the feature vector

195
00:09:54,610 --> 00:09:56,630
are no longer binary random variables;

196
00:09:56,740 --> 00:09:58,960
they're these indices in the dictionary that

197
00:09:59,060 --> 00:10:00,400
take on a much larger set of values.

198
00:10:00,500 --> 00:10:09,560
And so our generative model for this will be that

199
00:10:09,660 --> 00:10:23,700
the joint distribution over X and Y will be that,

200
00:10:23,790 --> 00:10:26,420
where again N is now the length of the email,

201
00:10:26,530 --> 00:10:29,280
all right? So the way to think about

202
00:10:29,390 --> 00:10:31,360
this formula is you imagine that

203
00:10:31,580 --> 00:10:34,950
there was some probably distribution over emails.

204
00:10:35,060 --> 00:10:36,430
There's some random distribution that

205
00:10:36,530 --> 00:10:37,550
generates the emails,

206
00:10:37,660 --> 00:10:39,680
and that process proceeds as follows:

207
00:10:39,730 --> 00:10:43,130
First, Y is chosen, first the class label.

208
00:10:43,260 --> 00:10:44,750
Is someone gonna send you spam email or

209
00:10:44,870 --> 00:10:47,350
not spam emails is chosen for us.

210
00:10:47,460 --> 00:10:50,570
So first Y, the random variable Y,

211
00:10:50,690 --> 00:10:51,580
the class label of spam

212
00:10:51,580 --> 00:10:52,780
or not spam is generated,

213
00:10:52,890 --> 00:10:56,450
and then having decided whether

214
00:10:56,540 --> 00:10:58,200
they sent you spam or not spam,

215
00:10:58,320 --> 00:11:02,380
someone iterates over all 300 positions of the email,

216
00:11:02,490 --> 00:11:04,930
or 300 words that are going to

217
00:11:05,020 --> 00:11:06,380
compose them as email,

218
00:11:06,480 --> 00:11:09,500
and would generate words from some distribution

219
00:11:09,590 --> 00:11:11,190
that depends on whether they chose

220
00:11:11,300 --> 00:11:12,520
to send you spam or not spam.

221
00:11:12,630 --> 00:11:13,600
So if they sent you spam,

222
00:11:13,700 --> 00:11:14,790
they'll send you words

223
00:11:14,880 --> 00:11:15,900
they'll tend to generate words like,

224
00:11:16,010 --> 00:11:17,030
you know, buy, and Viagra,

225
00:11:17,140 --> 00:11:19,250
and whatever at discounts, sale, whatever.

226
00:11:19,340 --> 00:11:21,530
And if somebody chose to send you not spam,

227
00:11:21,640 --> 00:11:22,900
then they'll send you, sort of,

228
00:11:22,990 --> 00:11:24,940
the more normal words you get in an email, okay?

229
00:11:25,030 --> 00:11:28,660
So, sort of, just careful, right?

230
00:11:28,740 --> 00:11:30,750
XI here has a very different definition

231
00:11:30,830 --> 00:11:32,240
from the previous event model,

232
00:11:32,270 --> 00:11:34,020
and N has a very different definition from

233
00:11:34,140 --> 00:11:35,180
the previous event model.

234
00:11:35,260 --> 00:11:41,190
And so the parameters of this model are

235
00:11:41,220 --> 00:11:47,550
let's see. Phi subscript K given Y equals one,

236
00:11:47,660 --> 00:11:55,700
which is the probability that, you know,

237
00:11:55,810 --> 00:11:58,060
conditioned on someone deciding

238
00:11:58,120 --> 00:11:59,210
to spend you spam,

239
00:11:59,320 --> 00:12:00,510
what's the probability that

240
00:12:00,610 --> 00:12:01,730
the next word they choose

241
00:12:01,860 --> 00:12:04,080
to email you in the spam email

242
00:12:04,190 --> 00:12:10,340
is going to be word K, and similarly, you know,

243
00:12:10,470 --> 00:12:13,260
sort of, same thing

244
00:12:13,360 --> 00:12:15,740
well, I'll just write it out, I guess

245
00:12:15,860 --> 00:12:25,040
and Phi Y and just same as before, okay?

246
00:12:25,140 --> 00:12:27,100
So these are the parameters of the model,

247
00:12:27,220 --> 00:12:32,320
and given a training set,

248
00:12:32,420 --> 00:12:33,490
you can work out the

249
00:12:33,600 --> 00:12:35,260
maximum likelihood estimates of the parameters.

250
00:12:35,380 --> 00:12:44,290
So the maximum likelihood estimate of

251
00:12:44,390 --> 00:12:48,820
the parameters will be equal to

252
00:12:48,930 --> 00:12:50,360
and now I'm gonna write one of these,

253
00:12:50,460 --> 00:12:52,410
you know, big indicator function things again.

254
00:12:52,520 --> 00:12:58,020
It'll be a sum over your training sets indicator

255
00:12:58,140 --> 00:13:07,800
whether that was spam times the sum

256
00:13:07,910 --> 00:13:09,260
over all the words in that email

257
00:13:09,340 --> 00:13:11,560
where N subscript I is the number of words

258
00:13:11,700 --> 00:13:14,050
in email I in your training set,

259
00:13:14,170 --> 00:13:38,780
times indicator XIJ, SK times that I, okay?

260
00:13:39,540 --> 00:13:42,420
So the numerator says sum over all your emails

261
00:13:42,510 --> 00:13:44,550
and take into account all the emails that

262
00:13:44,680 --> 00:13:45,990
had class label one,

263
00:13:46,080 --> 00:13:47,430
take into account only of the emails that

264
00:13:47,540 --> 00:13:49,630
were spam because if Y equals zero,

265
00:13:49,730 --> 00:13:51,320
then this is zero, and this would go away,

266
00:13:51,400 --> 00:13:55,140
and then times sum over all the words

267
00:13:55,230 --> 00:13:56,180
in your spam email,

268
00:13:56,270 --> 00:13:57,860
and it counts up the number of times

269
00:13:57,970 --> 00:14:00,410
you observed the word K in your spam emails.

270
00:14:00,520 --> 00:14:03,170
So, in other words, the numerator is look at

271
00:14:03,280 --> 00:14:04,570
all the spam emails in your training set

272
00:14:04,680 --> 00:14:06,190
and count up the total number of times

273
00:14:06,310 --> 00:14:07,870
the word K appeared in this email.

274
00:14:07,990 --> 00:14:14,260
The denominator then

275
00:14:14,340 --> 00:14:18,000
is sum over I into our training set of whenever

276
00:14:18,110 --> 00:14:20,490
one of your examples is spam, you know,

277
00:14:20,600 --> 00:14:22,670
sum up the length of that spam email,

278
00:14:22,750 --> 00:14:26,310
and so the denominator is the total length of

279
00:14:26,440 --> 00:14:28,330
all of the spam emails in your training set.

280
00:14:28,430 --> 00:14:35,050
And so the ratio is just out of all your spam emails,

281
00:14:35,150 --> 00:14:36,790
what is the fraction of words in all

282
00:14:36,870 --> 00:14:38,810
your spam emails that were word K,

283
00:14:38,910 --> 00:14:40,830
and that's your estimate for the probability of

284
00:14:40,930 --> 00:14:44,200
the next piece of spam mail generating

285
00:14:44,310 --> 00:14:45,220
the word K in any

286
00:14:45,220 --> 00:14:46,420
given position, okay?

287
00:14:46,530 --> 00:14:54,590
At the end of the previous lecture,

288
00:14:54,670 --> 00:14:56,100
I talked about LaPlace smoothing,

289
00:14:56,210 --> 00:14:57,850
and so when you do that as well,

290
00:14:57,960 --> 00:15:00,790
you add one to the numerator and K

291
00:15:00,910 --> 00:15:02,840
to the denominator,

292
00:15:02,840 --> 00:15:03,940
and this is the LaPlace

293
00:15:04,020 --> 00:15:09,120
smoothed estimate of this parameter, okay?

294
00:15:09,190 --> 00:15:10,810
And similarly, you do the same thing for

295
00:15:10,920 --> 00:15:14,650
and you can work out the estimates

296
00:15:14,760 --> 00:15:16,200
for the other parameters yourself, okay?

297
00:15:16,310 --> 00:15:19,050
So it's very similar. Yeah?

298
00:15:19,160 --> 00:15:22,220
Student:I'm sorry. On the right on the top,

299
00:15:22,290 --> 00:15:24,600
I was just wondering what the X of I is,

300
00:15:24,700 --> 00:15:26,260
and what the N of

301
00:15:26,350 --> 00:15:27,280
Instructor (Andrew Ng):Right.

302
00:15:27,390 --> 00:15:30,730
So in this second event model, the definition for XI

303
00:15:30,840 --> 00:15:33,180
and the definition for N are different, right?

304
00:15:33,280 --> 00:15:37,340
So here well, this is for one example XY. So here,

305
00:15:37,450 --> 00:15:46,710
N is the number of words in a given email, right?

306
00:15:46,830 --> 00:15:48,750
And if it's the I email subscripting

307
00:15:48,840 --> 00:15:50,320
then this N subscript I,

308
00:15:50,430 --> 00:15:52,460
and so N will be different for

309
00:15:52,560 --> 00:15:53,780
different training examples,

310
00:15:53,890 --> 00:16:00,930
and here XI will be, you know,

311
00:16:01,060 --> 00:16:03,400
these values from 1 to 50,000,

312
00:16:03,510 --> 00:16:08,620
and XI is essentially the identity of

313
00:16:08,740 --> 00:16:15,220
the Ith word in a given piece of email, okay?

314
00:16:15,320 --> 00:16:17,100
So that's why this is grouping,

315
00:16:17,220 --> 00:16:17,990
or this is a

316
00:16:18,100 --> 00:16:19,370
product over all the different words of

317
00:16:19,460 --> 00:16:23,100
your email of their probability the Ith word

318
00:16:23,210 --> 00:16:26,000
in your email, conditioned on Y. Yeah?

319
00:16:26,130 --> 00:16:28,900
Student:[Off mic].

320
00:16:28,990 --> 00:16:31,160
Instructor (Andrew Ng):Oh, no, actually,

321
00:16:31,260 --> 00:16:32,390
you know what, I apologize.

322
00:16:32,490 --> 00:16:35,450
I just realized that overload the notation, right,

323
00:16:35,560 --> 00:16:37,820
and I shouldn't have used K here.

324
00:16:37,940 --> 00:16:40,410
Let me use a different alphabet

325
00:16:40,510 --> 00:16:41,560
and see if that makes sense;

326
00:16:41,670 --> 00:16:44,430
does that make sense?

327
00:16:44,520 --> 00:16:46,320
Oh, you know what, I'm sorry.

328
00:16:46,410 --> 00:16:47,660
You're absolutely right.

329
00:16:47,750 --> 00:16:49,550
Thank you. All right.

330
00:16:49,630 --> 00:16:52,120
So in LaPlace smoothing, that shouldn't be K.

331
00:16:52,230 --> 00:16:56,640
This should be, you know, 50,000,

332
00:16:56,740 --> 00:16:59,710
if you have 50,000 words in your dictionary.

333
00:16:59,820 --> 00:17:01,390
Yeah, thanks. Great.

334
00:17:01,490 --> 00:17:03,970
I stole notation from the previous lecture

335
00:17:04,070 --> 00:17:06,260
and didn't translate it properly.

336
00:17:06,370 --> 00:17:08,160
So LaPlace smoothing, right,

337
00:17:08,270 --> 00:17:10,410
this is the number of possible values that

338
00:17:10,490 --> 00:17:14,710
the random variable XI can take on. Cool.

339
00:17:16,010 --> 00:17:18,350
Raise your hand if this makes sense?

340
00:17:18,450 --> 00:17:22,220
Okay. Some of you,

341
00:17:22,280 --> 00:17:27,200
are there more questions about this? Yeah.

342
00:17:27,310 --> 00:17:33,280
Student:On LaPlace smoothing,

343
00:17:33,400 --> 00:17:36,560
the denominator and the plus A

344
00:17:36,670 --> 00:17:39,300
is the number of values that Y could take?

345
00:17:39,430 --> 00:17:44,310
Instructor (Andrew Ng):Yeah, let's see.

346
00:17:44,430 --> 00:17:52,590
So LaPlace smoothing is a method to give you,

347
00:17:52,690 --> 00:17:54,260
sort of, hopefully, better estimates of

348
00:17:54,360 --> 00:17:56,900
their probability distribution over a multinomial,

349
00:17:57,020 --> 00:18:04,280
and so was I using X to Y in the previous lecture?

350
00:18:04,360 --> 00:18:06,040
So in trying to estimate the probability

351
00:18:06,130 --> 00:18:07,250
over a multinomial

352
00:18:07,360 --> 00:18:08,830
I think X and Y are different.

353
00:18:08,920 --> 00:18:11,860
I think was it X or Y?

354
00:18:11,950 --> 00:18:13,520
I think it was X, actually. Well

355
00:18:13,640 --> 00:18:14,800
oh, I see, right, right.

356
00:18:14,890 --> 00:18:16,790
I think I was using a different definition for

357
00:18:16,870 --> 00:18:19,360
the random variable Y because suppose you have

358
00:18:19,440 --> 00:18:20,680
a multinomial random variable,

359
00:18:20,780 --> 00:18:24,640
X which takes on

360
00:18:24,750 --> 00:18:29,230
let's use a different alphabet.

361
00:18:29,290 --> 00:18:31,340
Suppose you have a multinomial random

362
00:18:31,400 --> 00:18:33,530
variable X which takes on L different values,

363
00:18:33,640 --> 00:18:37,330
then the maximum likelihood estimate for

364
00:18:37,440 --> 00:18:42,000
the probability of X, PFX equals K,

365
00:18:42,120 --> 00:18:44,020
will be equal to, right,

366
00:18:44,110 --> 00:18:45,440
the number of observations.

367
00:18:45,520 --> 00:18:55,220
The maximum likelihood estimate for

368
00:18:55,320 --> 00:18:57,050
the probability of X being equal to K

369
00:18:57,160 --> 00:19:02,000
will be the number of observations of X

370
00:19:02,110 --> 00:19:06,080
equals K divided by the total number

371
00:19:06,180 --> 00:19:11,260
of observations of X, okay?

372
00:19:11,370 --> 00:19:13,020
So that's the maximum likelihood estimate.

373
00:19:13,120 --> 00:19:16,960
And to add LaPlace smoothing to this,

374
00:19:17,070 --> 00:19:19,140
you, sort of, add one to the numerator,

375
00:19:19,250 --> 00:19:22,200
and you add L to the denominator

376
00:19:22,310 --> 00:19:24,750
where L was the number of possible values that

377
00:19:25,000 --> 00:19:27,840
X can take on. So, in this case,

378
00:19:27,950 --> 00:19:31,170
this is a probability that X equals K,

379
00:19:31,300 --> 00:19:34,800
and X can take on 50,000 values if 50,000

380
00:19:34,920 --> 00:19:36,190
is the length of your dictionary;

381
00:19:36,300 --> 00:19:37,150
it may be something else,

382
00:19:37,250 --> 00:19:38,960
but that's why I add 50,000 to the denominator.

383
00:19:39,070 --> 00:19:41,140
Are there other questions? Yeah.

384
00:19:41,250 --> 00:19:42,850
Student:Is there a specific definition

385
00:19:42,940 --> 00:19:45,060
for a maximum likelihood estimation of a parameter?

386
00:19:45,150 --> 00:19:47,810
We've talked about it a couple times,

387
00:19:47,890 --> 00:19:49,120
and all the examples make sense,

388
00:19:49,230 --> 00:19:50,370
but I don't know what the, like,

389
00:19:50,440 --> 00:19:51,730
general formula for it is.

390
00:19:51,830 --> 00:19:52,710
Instructor (Andrew Ng):I see. Yeah, right.

391
00:19:52,800 --> 00:19:54,020
So the definition of maximum likelihood

392
00:19:54,110 --> 00:19:56,630
so the question is what's the definition for

393
00:19:56,730 --> 00:19:57,800
maximum likelihood estimate?

394
00:19:57,910 --> 00:20:03,630
So actually in today's lecture and the previous lecture

395
00:20:03,730 --> 00:20:05,630
when I talk about Gaussian Discriminant Analysis

396
00:20:05,740 --> 00:20:08,020
I was, sort of, throwing out the maximum likelihood

397
00:20:08,110 --> 00:20:10,340
estimates on the board without proving them.

398
00:20:10,420 --> 00:20:12,930
The way to actually work this out

399
00:20:13,040 --> 00:20:21,480
is to actually write down the likelihood.

400
00:20:21,590 --> 00:20:31,240
So the way to figure out all of

401
00:20:31,350 --> 00:20:33,030
these maximum likelihood estimates is to

402
00:20:33,150 --> 00:20:36,680
write down the likelihood of the parameters,

403
00:20:36,800 --> 00:20:45,690
phi K given Y being zero, phi Y, right?

404
00:20:45,780 --> 00:20:48,250
And so given a training set, the likelihood,

405
00:20:48,350 --> 00:20:49,780
I guess, I should be writing log likelihood

406
00:20:49,860 --> 00:20:53,710
will be the log of the product of I equals one to N,

407
00:20:53,800 --> 00:20:57,870
PFXI, YI, you know,

408
00:20:57,980 --> 00:21:05,740
parameterized by these things, okay?

409
00:21:05,830 --> 00:21:09,750
Where PFXI, YI, right,

410
00:21:09,850 --> 00:21:25,390
is given by NI, PFX, YJ given YI.

411
00:21:25,500 --> 00:21:27,800
They are parameterized by

412
00:21:27,920 --> 00:21:29,510
well, I'll just drop the parameters

413
00:21:29,600 --> 00:21:34,020
to write this more simply oh, I just put it in

414
00:21:34,140 --> 00:21:48,600
times PFYI, okay?

415
00:21:48,710 --> 00:21:50,700
So this is my log likelihood,

416
00:21:50,810 --> 00:21:53,510
and so the way you get the maximum likelihood

417
00:21:53,620 --> 00:21:55,300
estimate of the parameters is you

418
00:21:55,390 --> 00:21:57,400
so if given a fixed training set,

419
00:21:57,510 --> 00:21:59,050
given a set of fixed IYI's,

420
00:21:59,150 --> 00:22:03,010
you maximize this in terms of these parameters,

421
00:22:03,110 --> 00:22:05,520
and then you get the maximum likelihood estimates

422
00:22:05,630 --> 00:22:06,760
that I've been writing out.

423
00:22:06,850 --> 00:22:09,300
So in a previous section of today's lecture

424
00:22:09,400 --> 00:22:11,220
I wrote out some maximum likelihood estimates

425
00:22:11,330 --> 00:22:13,470
for the Gaussian Discriminant Analysis model,

426
00:22:13,560 --> 00:22:16,250
and for Na.ve Bayes, and then this

427
00:22:16,380 --> 00:22:19,390
I didn't prove them, but you get to, sort of,

428
00:22:19,490 --> 00:22:21,720
play with that yourself in the homework problem

429
00:22:21,790 --> 00:22:25,130
as well and for one of these models,

430
00:22:25,220 --> 00:22:26,720
and you'll be able to verify that

431
00:22:26,800 --> 00:22:27,960
when you maximize the likelihood

432
00:22:28,080 --> 00:22:29,930
and maximize the log likelihood that

433
00:22:30,040 --> 00:22:31,740
hopefully you do get the same formulas

434
00:22:31,840 --> 00:22:34,390
as what I was drawing up on the board,

435
00:22:34,480 --> 00:22:36,100
but a way is to find the way these are derived

436
00:22:36,190 --> 00:22:40,680
is by maximizing this, okay? Cool. All right.

437
00:22:40,780 --> 00:22:46,340
So that wraps up what I wanted to say about

438
00:22:46,430 --> 00:22:52,360
oh, so that, more or less,

439
00:22:52,470 --> 00:22:54,670
wraps up what I wanted to say about Na.ve Bayes,

440
00:22:54,740 --> 00:22:57,400
and it turns out that for text classification,

441
00:22:57,520 --> 00:23:00,400
the Na.ve Bayes algorithm

442
00:23:00,510 --> 00:23:02,050
with this second event model,

443
00:23:02,150 --> 00:23:05,290
the last Na.ve Bayes model I presented with

444
00:23:05,380 --> 00:23:06,580
the multinomial event model,

445
00:23:06,670 --> 00:23:09,260
it turns out that almost always does better

446
00:23:09,370 --> 00:23:13,320
than the first Na.ve Bayes model I talked about

447
00:23:13,430 --> 00:23:15,890
when you're applying it to the specific case

448
00:23:15,970 --> 00:23:17,840
to the specific of text classification,

449
00:23:17,950 --> 00:23:23,120
and one of the reasons is hypothesized for this

450
00:23:23,230 --> 00:23:25,020
is that this second model,

451
00:23:25,110 --> 00:23:26,380
the multinomial event model,

452
00:23:26,500 --> 00:23:28,320
takes into account the number of

453
00:23:28,470 --> 00:23:33,190
times a word appears in a document,

454
00:23:33,270 --> 00:23:34,620
whereas the former model doesn't.

455
00:23:34,720 --> 00:23:38,580
I should say that in truth that actually turns out

456
00:23:38,690 --> 00:23:40,130
not to be completely understood

457
00:23:40,230 --> 00:23:41,490
why the latter model does better than

458
00:23:41,590 --> 00:23:43,860
the former one for text classification, and, sort of,

459
00:23:43,980 --> 00:23:47,530
researchers are still debating about it a little bit,

460
00:23:47,600 --> 00:23:49,340
but if you ever have a text classification problem,

461
00:23:49,450 --> 00:23:51,630
you know, Na.ve Bayes Classify is probably not,

462
00:23:51,750 --> 00:23:53,930
by far, the best learning algorithm out there,

463
00:23:54,040 --> 00:23:57,110
but it is relatively straightforward to implement,

464
00:23:57,200 --> 00:23:59,160
and it's a very good algorithm to try

465
00:23:59,270 --> 00:24:00,270
if you have a text

466
00:24:00,270 --> 00:24:02,040
classification problem, okay?

467
00:24:02,160 --> 00:24:05,200
Still a question? Yeah.

468
00:24:05,300 --> 00:24:06,680
Student:So the second model

469
00:24:06,760 --> 00:24:07,790
is still positioning a variant, right?

470
00:24:07,900 --> 00:24:09,310
It doesn't actually care where the words are.

471
00:24:09,410 --> 00:24:10,830
Instructor (Andrew Ng):Yes, all right.

472
00:24:10,950 --> 00:24:12,810
Student:And, I mean, X variable,

473
00:24:12,920 --> 00:24:14,720
if my model like you had exclamation in,

474
00:24:14,810 --> 00:24:16,730
does that usually do better if you have enough data?

475
00:24:16,840 --> 00:24:18,630
Instructor (Andrew Ng):Yeah, so the question is,

476
00:24:18,720 --> 00:24:19,870
sort of, the second model, right?

477
00:24:19,960 --> 00:24:22,390
The second model, the multinomial event model

478
00:24:22,490 --> 00:24:23,870
actually doesn't care about the ordering of the words.

479
00:24:23,960 --> 00:24:25,240
You can shuffle all the words in the email,

480
00:24:25,350 --> 00:24:26,540
and it does exactly the same thing.

481
00:24:26,630 --> 00:24:28,890
So in natural language processing,

482
00:24:28,970 --> 00:24:30,430
there's actually another name;

483
00:24:30,520 --> 00:24:31,670
it's called a Unigram Model

484
00:24:31,730 --> 00:24:32,790
in natural language processing,

485
00:24:32,890 --> 00:24:37,010
and there's some other models like, sort of, say,

486
00:24:37,090 --> 00:24:38,390
higher order markup models that

487
00:24:38,460 --> 00:24:40,150
take into account some of the ordering of the words.

488
00:24:40,240 --> 00:24:42,210
It turns out that for text classification,

489
00:24:42,320 --> 00:24:48,150
the models like the bigram models or trigram models,

490
00:24:48,240 --> 00:24:50,010
I believe they do only very

491
00:24:50,020 --> 00:24:52,380
slightly better, if at all,

492
00:24:52,470 --> 00:24:55,310
but that's when you're

493
00:24:55,410 --> 00:24:56,380
applying them to

494
00:24:56,380 --> 00:24:57,790
text classification, okay?

495
00:24:57,840 --> 00:25:04,590
All right. So the next thing I want to talk about

496
00:25:04,670 --> 00:25:08,360
is to start again to discussion of non-linear classifiers.

497
00:25:08,480 --> 00:25:12,570
So it turns out

498
00:25:12,670 --> 00:25:28,440
well, and so the very first classification algorithm

499
00:25:28,530 --> 00:25:30,390
we talked about was logistic regression,

500
00:25:30,480 --> 00:25:33,960
which had the forming form for hypothesis,

501
00:25:34,050 --> 00:25:39,770
and you can think of this as predicting one

502
00:25:40,330 --> 00:25:43,630
when this estimator probability is greater

503
00:25:43,740 --> 00:25:46,390
or equal to 0.5 and predicting zero, right,

504
00:25:46,500 --> 00:25:49,290
when this is less than 0.5,

505
00:25:49,380 --> 00:26:02,480
and given a training set, right?

506
00:26:02,590 --> 00:26:04,870
Logistic regression will maybe do grade

507
00:26:04,980 --> 00:26:07,080
and descends or something or use Newton's method

508
00:26:07,130 --> 00:26:09,980
to find a straight line that reasonably separates

509
00:26:10,070 --> 00:26:11,530
the positive and negative classes.

510
00:26:11,650 --> 00:26:13,970
But sometimes a data set just can't

511
00:26:14,040 --> 00:26:15,380
be separated by a straight line,

512
00:26:15,480 --> 00:26:17,200
so is there an algorithm that

513
00:26:17,300 --> 00:26:19,430
will let you start to learn these sorts of

514
00:26:19,520 --> 00:26:20,780
non-linear division boundaries?

515
00:26:20,880 --> 00:26:27,100
And so how do you go about

516
00:26:27,190 --> 00:26:28,770
getting a non-linear classifier?

517
00:26:28,870 --> 00:26:32,260
And, by the way, one cool result is that

518
00:26:32,340 --> 00:26:33,880
remember when I said

519
00:26:33,980 --> 00:26:35,580
when we talked about generative learning algorithms,

520
00:26:35,660 --> 00:26:39,840
I said that if you assume Y given X

521
00:26:39,930 --> 00:26:45,620
is exponential family, right, with parameter A,

522
00:26:45,740 --> 00:26:49,620
and if you build a generative learning algorithm

523
00:26:49,710 --> 00:26:53,070
using this, right, plus one, if this is A to one.

524
00:26:53,160 --> 00:26:56,930
This is exponential family

525
00:26:57,020 --> 00:27:01,980
with natural parameter A to zero, right.

526
00:27:02,090 --> 00:27:03,260
I think when we talked about

527
00:27:03,380 --> 00:27:04,470
Gaussian Discriminant Analysis,

528
00:27:04,580 --> 00:27:05,880
I said that if this holds true,

529
00:27:05,950 --> 00:27:08,360
then you end up with a logistic posterior.

530
00:27:08,450 --> 00:27:10,560
It actually turns out that a Na.ve Bayes

531
00:27:10,670 --> 00:27:13,280
model actually falls into this as well.

532
00:27:13,380 --> 00:27:15,070
So the Na.ve Bayes model actually falls into

533
00:27:15,170 --> 00:27:17,940
this exponential family as well, and, therefore,

534
00:27:18,050 --> 00:27:20,540
under the Na.ve Bayes model,

535
00:27:20,650 --> 00:27:21,820
you're actually

536
00:27:21,920 --> 00:27:24,830
using this other linear classifier as well, okay?

537
00:27:24,940 --> 00:27:29,800
So the question is how can you start to

538
00:27:29,880 --> 00:27:31,070
get non-linear classifiers?

539
00:27:31,150 --> 00:27:34,820
And I'm going to talk about one method today

540
00:27:34,920 --> 00:27:43,340
which is and we started to talk about it very briefly

541
00:27:43,430 --> 00:27:47,480
which is taking a simpler algorithm

542
00:27:47,570 --> 00:27:52,120
like logistic regression and using it to build up

543
00:27:52,220 --> 00:27:55,910
to more complex non-linear classifiers, okay?

544
00:27:56,030 --> 00:27:59,610
So to motivate this discussion,

545
00:27:59,700 --> 00:28:03,040
I'm going to use the little picture let's see.

546
00:28:03,140 --> 00:28:06,900
So suppose you have features X1, X2, and X3,

547
00:28:07,010 --> 00:28:08,460
and so by convention,

548
00:28:08,540 --> 00:28:11,450
I'm gonna follow our earlier convention that

549
00:28:11,550 --> 00:28:15,180
X0 is set to one, and so I'm gonna use a little diagram

550
00:28:15,290 --> 00:28:21,300
like this to denote our logistic regression unit, okay?

551
00:28:21,420 --> 00:28:26,280
So think of a little picture like that, you know,

552
00:28:26,400 --> 00:28:29,100
this little circle as denoting a computation note that

553
00:28:29,180 --> 00:28:31,160
takes this input, you know, several features and then

554
00:28:31,260 --> 00:28:33,190
it outputs another number

555
00:28:33,280 --> 00:28:35,190
that's X subscript theta of X,

556
00:28:35,280 --> 00:28:37,470
given by a sigmoid function,

557
00:28:37,570 --> 00:28:39,550
and so this little computational unit

558
00:28:39,660 --> 00:28:43,740
well, will have parameters theta. Now,

559
00:28:43,860 --> 00:28:46,260
in order to get non-linear division boundaries,

560
00:28:46,360 --> 00:28:49,120
all we need to do well, at least one thing to do

561
00:28:49,210 --> 00:28:55,320
is just come up with a way

562
00:28:55,430 --> 00:28:57,390
to represent hypotheses that

563
00:28:57,500 --> 00:28:59,520
can output non-linear division boundaries,

564
00:28:59,640 --> 00:29:03,200
right, and so this is

565
00:29:03,310 --> 00:29:09,470
when you put a bunch of those little pictures that

566
00:29:09,800 --> 00:29:12,560
I drew on the previous board,

567
00:29:12,660 --> 00:29:14,810
you can then get what's called

568
00:29:14,920 --> 00:29:21,570
a Neural Network in which you think of

569
00:29:21,660 --> 00:29:24,140
having my features here and then

570
00:29:24,230 --> 00:29:32,120
I would feed them to say a few of

571
00:29:32,200 --> 00:29:35,710
these little sigmoidal units,

572
00:29:35,790 --> 00:29:41,310
and these together will feed into yet

573
00:29:41,410 --> 00:29:42,950
another sigmoidal unit, say,

574
00:29:43,060 --> 00:29:47,890
which will output my final output H

575
00:29:47,960 --> 00:29:49,720
subscript theta of X, okay?

576
00:29:49,830 --> 00:29:53,030
And just to give these things names,

577
00:29:53,090 --> 00:29:55,890
let me call the values output by

578
00:29:55,980 --> 00:29:57,970
these three intermediate sigmoidal units;

579
00:29:58,060 --> 00:30:00,070
let me call them A1, A2, A3.

580
00:30:00,180 --> 00:30:03,590
And let me just be completely concrete about

581
00:30:03,670 --> 00:30:05,270
what this formula represents, right?

582
00:30:05,380 --> 00:30:09,370
So each of these units in the middle will

583
00:30:09,440 --> 00:30:11,200
have their own associated set of parameters,

584
00:30:11,260 --> 00:30:13,950
and so the value A1 will be

585
00:30:13,980 --> 00:30:17,080
computed as G of X transpose,

586
00:30:17,130 --> 00:30:19,430
and then some set of parameters,

587
00:30:19,460 --> 00:30:22,120
which I'll write as theta one, and similarly,

588
00:30:22,220 --> 00:30:28,130
A2 will be computed as G of X transpose theta two,

589
00:30:28,210 --> 00:30:35,000
and A3 will be G of X transpose, theta three,

590
00:30:35,110 --> 00:30:38,580
where G is the sigmoid function, all right?

591
00:30:38,610 --> 00:30:46,310
So G of Z, and then, finally, our hypothesis will

592
00:30:46,330 --> 00:30:57,880
output G of A transpose theta four, right?

593
00:30:57,970 --> 00:30:59,580
Where, you know,

594
00:30:59,680 --> 00:31:04,910
this A vector is a vector of A1, A2, A3.

595
00:31:05,010 --> 00:31:08,860
We can append another one to it at first

596
00:31:08,970 --> 00:31:10,570
if you want, okay?

597
00:31:10,680 --> 00:31:15,970
Let me just draw up here this

598
00:31:16,080 --> 00:31:17,710
I'm sorry about the cluttered board.

599
00:31:17,830 --> 00:31:20,640
And so H subscript theta of X,

600
00:31:20,750 --> 00:31:26,070
this is a function of all the parameters theta

601
00:31:26,210 --> 00:31:30,170
one through theta four,

602
00:31:30,280 --> 00:31:35,830
and so one way to learn parameters for this model

603
00:31:35,940 --> 00:31:40,190
is to write down the cost function, say,

604
00:31:40,290 --> 00:31:42,640
J of theta equals one-half sum

605
00:31:42,720 --> 00:31:45,030
from Y equals one to M,

606
00:31:45,150 --> 00:31:52,480
YI minus H subscript theta of XI squared, say.

607
00:31:54,780 --> 00:31:57,300
Okay, so that's our familiar quadratic cost function,

608
00:31:57,410 --> 00:32:01,570
and so one way to learn the parameters of

609
00:32:01,680 --> 00:32:03,600
an algorithm like this is to just use

610
00:32:03,690 --> 00:32:06,450
gradient interscent to minimize J of theta

611
00:32:06,550 --> 00:32:08,950
as a function of theta, okay?

612
00:32:09,050 --> 00:32:12,860
See, in the phi gradient descent

613
00:32:12,920 --> 00:32:14,490
to minimize this square area,

614
00:32:14,590 --> 00:32:15,960
which stated differently means you use

615
00:32:16,070 --> 00:32:18,260
gradient descent to make the predictions of

616
00:32:18,350 --> 00:32:20,390
your neural network as close as possible to

617
00:32:20,510 --> 00:32:22,040
what you observed as the labels

618
00:32:22,130 --> 00:32:25,660
in your training set, okay?

619
00:32:25,750 --> 00:32:31,940
So it turns out green descent

620
00:32:31,990 --> 00:32:34,350
on this neural network is a specific name,

621
00:32:34,410 --> 00:32:36,720
the algorithm that implements grand descent

622
00:32:36,820 --> 00:32:38,070
is called back propagation,

623
00:32:38,150 --> 00:32:39,560
and so if you ever hear that all that means is

624
00:32:39,670 --> 00:32:41,110
it just means gradient interscent

625
00:32:41,200 --> 00:32:43,720
on a cost function like this or a variation of this

626
00:32:43,840 --> 00:32:46,100
on the neural network that looks like that,

627
00:32:46,190 --> 00:32:52,520
and well, this algorithm actually has

628
00:32:52,610 --> 00:32:53,780
some advantages and disadvantages,

629
00:32:53,900 --> 00:32:57,730
but let me actually show you. So, let's see.

630
00:32:57,840 --> 00:32:59,610
One of the interesting things about

631
00:32:59,700 --> 00:33:01,740
the neural network is that you can look at

632
00:33:01,850 --> 00:33:03,890
what these intermediate notes are computing, right?

633
00:33:03,970 --> 00:33:06,880
So this neural network has what's called

634
00:33:06,990 --> 00:33:10,750
a hidden layer before you then have the output layer,

635
00:33:10,840 --> 00:33:13,440
and, more generally, you can actually

636
00:33:13,520 --> 00:33:16,790
have inputs feed into these computation units,

637
00:33:16,880 --> 00:33:18,650
feed into more layers of computation units,

638
00:33:18,720 --> 00:33:20,350
to even more layers, to more layers,

639
00:33:20,450 --> 00:33:22,150
and then finally you have an output layer at the end

640
00:33:22,260 --> 00:33:25,370
And one cool thing you can do is look at

641
00:33:25,480 --> 00:33:26,950
all of these intermediate units,

642
00:33:27,040 --> 00:33:29,280
look at these units and what's called

643
00:33:29,360 --> 00:33:31,310
a hidden layer of the neural network.

644
00:33:31,400 --> 00:33:33,180
Don't worry about why it's called that.

645
00:33:33,280 --> 00:33:34,680
Look at computations of the hidden unit

646
00:33:34,760 --> 00:33:36,580
and ask what is the hidden unit

647
00:33:36,660 --> 00:33:40,210
computing the neural network? So to, maybe,

648
00:33:40,300 --> 00:33:41,840
get a better sense of neural networks

649
00:33:41,930 --> 00:33:43,340
might be doing, let me show you a video

650
00:33:43,430 --> 00:33:44,650
I'm gonna switch to the laptop

651
00:33:44,750 --> 00:33:47,630
this is made by a friend,

652
00:33:47,720 --> 00:33:50,930
Yann LeCun who's currently a professor

653
00:33:51,030 --> 00:33:52,060
at New York University.

654
00:33:52,160 --> 00:33:54,400
Can I show a video on the laptop?

655
00:33:54,480 --> 00:33:59,770
So let me show you a video from Yann LeCun

656
00:33:59,850 --> 00:34:03,280
on a neural network that he developed for

657
00:34:03,390 --> 00:34:05,640
Hammerton Digit Recognition.

658
00:34:05,760 --> 00:34:08,350
There was one other thing he did in this neural network

659
00:34:08,460 --> 00:34:10,260
that I'm not gonna talk about called

660
00:34:10,370 --> 00:34:12,660
a Convolutional Neural Network that

661
00:34:12,760 --> 00:34:23,290
well, his system is called LeNet, and let's see.

662
00:34:23,370 --> 00:34:25,130
Would you put on the laptop display?

663
00:34:25,220 --> 00:34:38,730
Hum, actually maybe if

664
00:34:38,830 --> 00:34:40,730
or you can just put on the screen on the side;

665
00:34:40,840 --> 00:34:43,810
that would work too if the big screen isn't working.

666
00:34:43,920 --> 00:35:14,810
Let's see. I'm just trying to think, okay,

667
00:35:14,910 --> 00:35:16,190
how do I keep you guys entertained

668
00:35:16,290 --> 00:35:17,770
while we're waiting for the video to come on?

669
00:35:17,860 --> 00:35:21,300
Well, let me say a few more things

670
00:35:21,390 --> 00:35:22,180
about neural network.

671
00:35:22,280 --> 00:35:26,950
So it turns out that when you write

672
00:35:27,060 --> 00:35:29,240
a quadratic cost function like I wrote down

673
00:35:29,350 --> 00:35:30,680
on the chalkboard just now,

674
00:35:30,780 --> 00:35:33,720
it turns out that unlike logistic regression,

675
00:35:33,830 --> 00:35:35,530
that will almost always

676
00:35:35,630 --> 00:35:37,610
respond to non-convex optimization problem,

677
00:35:37,720 --> 00:35:42,160
and so whereas for logistic regression

678
00:35:42,290 --> 00:35:43,760
if you run gradient descent or Newton's method

679
00:35:43,860 --> 00:35:45,620
or whatever, you converse the global optimer.

680
00:35:45,740 --> 00:35:47,770
This is not true for neural networks.

681
00:35:47,870 --> 00:35:50,770
In general, there are lots of local optimer and,

682
00:35:50,880 --> 00:35:52,810
sort of, much harder optimization problem.

683
00:35:52,930 --> 00:35:57,810
So neural networks, if you're, sort of,

684
00:35:57,920 --> 00:35:59,110
familiar with them, and you're good at making

685
00:35:59,220 --> 00:36:00,900
design choices like what learning rate to use,

686
00:36:01,080 --> 00:36:03,370
and how many hidden units to use, and so on,

687
00:36:03,490 --> 00:36:06,440
you can, sort of, get them to be fairly effective,

688
00:36:06,530 --> 00:36:10,910
and there's, sort of, often ongoing debates about,

689
00:36:11,050 --> 00:36:12,280
you know, is this learning algorithm better,

690
00:36:12,400 --> 00:36:13,440
or is that learning algorithm better?

691
00:36:13,550 --> 00:36:14,990
The vast majority of

692
00:36:15,080 --> 00:36:16,230
machine learning researchers today

693
00:36:16,340 --> 00:36:18,760
seem to perceive support vector machines,

694
00:36:18,870 --> 00:36:19,980
which is what I'll talk about later,

695
00:36:20,080 --> 00:36:21,870
to be a much more effective off-theshelf

696
00:36:21,980 --> 00:36:23,420
learning algorithm than neural networks.

697
00:36:23,540 --> 00:36:25,730
This point of view is contested a bit,

698
00:36:25,840 --> 00:36:29,250
but so neural networks are not something that

699
00:36:29,360 --> 00:36:30,730
I personally use a lot right now

700
00:36:30,850 --> 00:36:33,840
because there's a hard optimization problem

701
00:36:33,940 --> 00:36:35,370
and you should do so often verge,

702
00:36:35,480 --> 00:36:37,970
and it actually, sort of works.

703
00:36:38,090 --> 00:36:39,180
It, sort of,

704
00:36:39,180 --> 00:36:40,370
works reasonably well.

705
00:36:40,490 --> 00:36:43,580
It's just because this is fairly complicated,

706
00:36:43,690 --> 00:36:45,080
there's not an algorithm that

707
00:36:45,170 --> 00:36:48,490
I use commonly or that my friends use all time.

708
00:36:48,600 --> 00:36:52,120
Oh, cool. So but let me just go

709
00:36:52,230 --> 00:36:54,000
and show you an example of neural network,

710
00:36:54,120 --> 00:36:57,210
which was for many years, you know,

711
00:36:57,320 --> 00:36:58,980
the most effective learning algorithm

712
00:36:59,090 --> 00:37:00,800
before support vector machines were invented.

713
00:37:00,920 --> 00:37:04,840
So here's Yann LeCun's video, and

714
00:37:04,910 --> 00:37:08,800
well, there's actually audio on this too, the soundboard.

715
00:37:08,880 --> 00:37:10,320
So I'll just tell you what's happening.

716
00:37:10,410 --> 00:37:14,030
What you're seeing is a trained neural network,

717
00:37:14,130 --> 00:37:16,230
and this display where my mouse pointer

718
00:37:16,320 --> 00:37:17,270
is pointing, right,

719
00:37:17,380 --> 00:37:22,680
this big three there is the input to the neural network.

720
00:37:22,780 --> 00:37:24,530
So you're showing the neural network this image,

721
00:37:24,640 --> 00:37:26,490
and it's trying to recognize what is this.

722
00:37:26,590 --> 00:37:29,190
The final answer output by the neural network

723
00:37:29,290 --> 00:37:30,310
is this number up here,

724
00:37:30,390 --> 00:37:32,260
right below where it says LeNet-5,

725
00:37:32,350 --> 00:37:36,020
and the neural network correctly recognizes

726
00:37:36,110 --> 00:37:37,150
this image as a three,

727
00:37:37,280 --> 00:37:39,660
and if you look to the left of this image,

728
00:37:39,780 --> 00:37:42,950
what's interesting about this is the display

729
00:37:43,050 --> 00:37:45,750
on the left portion of this is actually showing

730
00:37:45,870 --> 00:37:48,630
the intermediate computations of the neural network.

731
00:37:48,740 --> 00:37:50,360
In other words, it's showing you what are

732
00:37:50,460 --> 00:37:53,270
the hidden layers of the neural network computing.

733
00:37:53,390 --> 00:37:55,540
And so, for example, if you look at this one,

734
00:37:55,630 --> 00:37:57,220
the third image down from the top,

735
00:37:57,340 --> 00:37:59,450
this seems to be computing, you know,

736
00:37:59,560 --> 00:38:01,930
certain edges into digits, right?

737
00:38:02,020 --> 00:38:04,000
We're just computing digits on the right-hand side

738
00:38:04,110 --> 00:38:07,080
of the bottom or something of

739
00:38:07,190 --> 00:38:11,220
the input display of the input image, okay?

740
00:38:11,330 --> 00:38:13,140
So let me just play this video,

741
00:38:13,250 --> 00:38:18,120
and you can see some of the inputs and

742
00:38:18,230 --> 00:38:19,410
outputs of the neural network,

743
00:38:19,520 --> 00:38:22,710
and those are very different fonts.

744
00:38:22,830 --> 00:39:16,480
There's this robustness to noise. All right.

745
00:39:16,590 --> 00:39:45,840
Multiple digits, that's, kind of, cool. All right.

746
00:39:45,980 --> 00:39:55,170
So, just for fun,

747
00:39:55,270 --> 00:39:59,210
let me show you one more video, which was

748
00:39:59,330 --> 00:40:05,000
let's see. This is another video from the various CV's,

749
00:40:05,100 --> 00:40:06,400
the machine that changed the world,

750
00:40:06,510 --> 00:40:09,920
which was produced by WGBH Television in corporation

751
00:40:10,040 --> 00:40:12,210
with British Foreclass Incorporation,

752
00:40:12,320 --> 00:40:15,260
and it was aired on PBS a few years ago, I think.

753
00:40:15,370 --> 00:40:18,120
I want to show you a video describing

754
00:40:18,230 --> 00:40:20,420
the NETtalk Neural Network,

755
00:40:20,510 --> 00:40:23,110
which was developed by Terry Sejnowski;

756
00:40:23,200 --> 00:40:24,170
he's a researcher.

757
00:40:24,250 --> 00:40:27,900
And so NETtalk was actually one of the major

758
00:40:28,990 --> 00:40:30,970
milestones in the history of neural network,

759
00:40:31,100 --> 00:40:32,930
and this specific application

760
00:40:33,040 --> 00:40:36,120
is getting the neural network to read text.

761
00:40:36,230 --> 00:40:39,150
So, in other words, can you show a piece of

762
00:40:39,260 --> 00:40:43,300
English to a computer and have the computer read,

763
00:40:43,430 --> 00:40:45,550
sort of, verbally produce sounds that could

764
00:40:45,650 --> 00:40:48,970
respond to the reading of the text.

765
00:40:49,080 --> 00:40:52,530
And it turns out that in the history of AI

766
00:40:52,640 --> 00:40:53,820
and the history of machine learning,

767
00:40:53,930 --> 00:40:58,000
this video created a lot of excitement about

768
00:40:58,120 --> 00:40:59,890
neural networks and about machine learning.

769
00:40:59,990 --> 00:41:04,050
Part of the reason was that Terry Sejnowski had

770
00:41:04,170 --> 00:41:07,710
the foresight to choose to use, in his video,

771
00:41:07,830 --> 00:41:11,650
a child-like voice talking about visiting

772
00:41:11,760 --> 00:41:12,910
your grandmother's house and so on.

773
00:41:12,990 --> 00:41:14,450
You'll see it in a second,

774
00:41:14,510 --> 00:41:18,920
and so this really created the perception of

775
00:41:19,300 --> 00:41:21,380
created the impression of the neural network

776
00:41:21,490 --> 00:41:23,250
being like a young child learning how to speak,

777
00:41:23,360 --> 00:41:25,190
and talking about going to your grandmothers,

778
00:41:25,290 --> 00:41:28,050
and so on. So this actually helped generate

779
00:41:28,140 --> 00:41:29,930
a lot of excitement within academia

780
00:41:30,030 --> 00:41:32,240
and outside academia on neural networks, sort of,

781
00:41:32,320 --> 00:41:33,720
early in the history of neural networks.

782
00:41:33,820 --> 00:41:35,140
I'm just gonna show you the video.

783
00:41:35,250 --> 00:41:36,960
[Begin Video] You're going to hear first

784
00:41:37,050 --> 00:41:39,010
what the network sounds like

785
00:41:39,090 --> 00:41:40,420
at the very beginning of the training,

786
00:41:40,500 --> 00:41:41,930
and it won't sound like words,

787
00:41:42,040 --> 00:41:44,670
but it'll sound like attempts that

788
00:41:44,770 --> 00:41:46,100
will get better and better with time.

789
00:41:46,220 --> 00:41:55,270
[Computer's voice] The network takes the letters,

790
00:41:55,370 --> 00:41:58,390
say the phrase, "grandmother's house,"

791
00:41:58,480 --> 00:42:01,850
and makes a random attempt at pronouncing it.

792
00:42:07,360 --> 00:42:08,260
[Computer's voice] Grandmother's house.

793
00:42:08,360 --> 00:42:10,870
The phonetic difference between the guess

794
00:42:10,980 --> 00:42:13,200
and the right pronunciation is sent back

795
00:42:13,310 --> 00:42:14,730
through the network. [Computer's voice]

796
00:42:14,830 --> 00:42:19,710
Grandmother's house.

797
00:42:19,820 --> 00:42:22,300
By adjusting the connection strengths

798
00:42:22,410 --> 00:42:24,990
after each attempt, the net slowly improves.

799
00:42:25,100 --> 00:42:28,920
And, finally, after letting it train overnight,

800
00:42:29,010 --> 00:42:32,000
the next morning it sounds like this:

801
00:42:32,110 --> 00:42:34,140
Grandmother's house,

802
00:42:34,280 --> 00:42:37,100
I'd like to go to my grandmother's house.

803
00:42:37,200 --> 00:42:41,690
Well, because she gives us candy. Well, and we

804
00:42:41,800 --> 00:42:44,250
NETtalk understands nothing about the language.

805
00:42:44,360 --> 00:42:47,370
It is simply associating letters with sounds.

806
00:42:47,470 --> 00:42:50,240
[End Video] All right.

807
00:42:50,360 --> 00:42:54,640
So at the time this was done, I mean,

808
00:42:54,730 --> 00:42:56,410
this is an amazing piece of work.

809
00:42:56,510 --> 00:43:00,190
I should say today there are other text

810
00:43:00,280 --> 00:43:01,860
to speech systems that work better than

811
00:43:01,970 --> 00:43:03,040
what you just saw,

812
00:43:03,160 --> 00:43:06,390
and you'll also appreciate getting candy from

813
00:43:06,500 --> 00:43:08,970
your grandmother's house is a little bit less

814
00:43:09,080 --> 00:43:10,880
impressive than talking about the Dow Jones

815
00:43:10,970 --> 00:43:13,410
falling 15 points, and profit taking, whatever.

816
00:43:13,520 --> 00:43:17,280
So but I wanted to show that just

817
00:43:17,370 --> 00:43:19,320
because that was another cool,

818
00:43:19,410 --> 00:43:21,730
major landmark in the history of neural networks.

819
00:43:21,840 --> 00:43:30,010
Okay. So let's switch back to the chalkboard,

820
00:43:30,110 --> 00:43:39,990
and what I want to do next is tell you about

821
00:43:40,100 --> 00:43:43,690
Support Vector Machines, okay? That, sort of,

822
00:43:43,800 --> 00:43:45,620
wraps up our discussion on neural networks.

823
00:43:45,710 --> 00:44:10,440
So I started off talking about neural networks

824
00:44:10,520 --> 00:44:14,170
by motivating it as a way to get us

825
00:44:14,300 --> 00:44:16,320
to output non-linear classifiers, right?

826
00:44:16,440 --> 00:44:17,510
I don't really approve of it.

827
00:44:17,620 --> 00:44:19,600
It turns out that you'd be able to come up with

828
00:44:19,700 --> 00:44:21,860
non-linear division boundaries using a neural network

829
00:44:21,960 --> 00:44:24,960
like what I drew on the chalkboard earlier.

830
00:44:25,050 --> 00:44:28,540
Support Vector Machines will be

831
00:44:28,640 --> 00:44:29,890
another learning algorithm that

832
00:44:30,000 --> 00:44:30,960
will give us a way

833
00:44:31,040 --> 00:44:32,710
to come up with non-linear classifiers.

834
00:44:32,820 --> 00:44:33,970
There's a very effective,

835
00:44:34,070 --> 00:44:35,550
off-the-shelf learning algorithm,

836
00:44:35,660 --> 00:44:37,940
but it turns out that in the discussion

837
00:44:38,040 --> 00:44:40,990
I'm gonna in the progression and

838
00:44:41,080 --> 00:44:42,130
development I'm gonna pursue,

839
00:44:42,240 --> 00:44:44,550
I'm actually going to start off by describing yet

840
00:44:44,690 --> 00:44:46,360
another class of linear classifiers with linear

841
00:44:46,530 --> 00:44:50,890
division boundaries, and only be later, sort of,

842
00:44:51,000 --> 00:44:53,840
in probably the next lecture or the one after that,

843
00:44:53,940 --> 00:44:55,940
that we'll then take the support

844
00:44:56,040 --> 00:44:57,640
vector machine idea and, sort of,

845
00:44:57,750 --> 00:45:00,170
do some clever things to it to make it work very well

846
00:45:00,270 --> 00:45:02,230
to generate non-linear division boundaries

847
00:45:02,320 --> 00:45:03,200
as well, okay?

848
00:45:03,310 --> 00:45:04,620
But we'll actually start by talking

849
00:45:04,730 --> 00:45:06,110
about linear classifiers a little bit more.

850
00:45:06,240 --> 00:45:13,000
And to do that, I want to convey

851
00:45:13,120 --> 00:45:15,600
two intuitions about classification.

852
00:45:15,720 --> 00:45:21,530
One is you think about logistic regression;

853
00:45:21,670 --> 00:45:24,350
we have this logistic function that was

854
00:45:24,460 --> 00:45:26,350
outputting the probability that Y equals one,

855
00:45:26,880 --> 00:45:32,110
and it crosses this line at zero.

856
00:45:32,220 --> 00:45:34,400
So when you run logistic regression,

857
00:45:34,520 --> 00:45:39,020
I want you to think of it as an algorithm that

858
00:45:39,130 --> 00:45:43,660
computes theta transpose X, and then

859
00:45:43,720 --> 00:45:52,200
it predicts one, right, if and only if,

860
00:45:52,270 --> 00:45:54,190
theta transpose X is greater than zero, right?

861
00:45:54,290 --> 00:45:55,690
IFF stands for if and only if.

862
00:45:55,810 --> 00:45:58,420
It means the same thing as a double implication,

863
00:45:58,520 --> 00:46:05,350
and it predicts zero, if and only if,

864
00:46:05,440 --> 00:46:07,860
theta transpose X is less than zero, okay?

865
00:46:07,990 --> 00:46:16,490
So if it's the case that theta transpose X

866
00:46:16,600 --> 00:46:17,880
is much greater than zero,

867
00:46:18,450 --> 00:46:20,470
the double greater than sign means these

868
00:46:20,560 --> 00:46:22,360
are much greater than, all right.

869
00:46:22,480 --> 00:46:24,540
So if theta transpose X is much greater than zero,

870
00:46:24,630 --> 00:46:27,780
then, you know, think of that

871
00:46:27,850 --> 00:46:39,100
as a very confident prediction that

872
00:46:39,210 --> 00:46:40,290
Y is equal to one, right?

873
00:46:40,400 --> 00:46:43,080
If theta transpose X is much greater than zero,

874
00:46:43,180 --> 00:46:44,460
then we're gonna predict one

875
00:46:44,560 --> 00:46:46,380
then moreover we're very confident it's one,

876
00:46:46,480 --> 00:46:47,600
and the picture for that

877
00:46:47,670 --> 00:46:49,920
is if theta transpose X is way out here,

878
00:46:50,020 --> 00:46:53,020
then we're estimating that the probability of

879
00:46:53,110 --> 00:46:55,510
Y being equal to one on the sigmoid function,

880
00:46:55,600 --> 00:46:57,050
it will be very close to one.

881
00:46:57,130 --> 00:47:01,240
And, in the same way, if theta transpose X

882
00:47:01,330 --> 00:47:04,000
is much less than zero,

883
00:47:04,080 --> 00:47:13,820
then we're very confident that Y is equal to zero.

884
00:47:13,910 --> 00:47:20,610
So wouldn't it be nice

885
00:47:20,710 --> 00:47:23,050
so when we fit logistic regression of some of

886
00:47:23,150 --> 00:47:24,350
the classifiers is your training set,

887
00:47:24,450 --> 00:47:30,490
then so wouldn't it be nice if, right,

888
00:47:30,590 --> 00:47:35,660
for all I such that Y is equal to one.

889
00:47:35,740 --> 00:47:42,310
We have theta transpose XI

890
00:47:42,360 --> 00:47:43,950
is much greater than zero,

891
00:47:44,070 --> 00:47:50,050
and for all I such that Y is equal to zero,

892
00:47:50,170 --> 00:48:00,750
we have theta transpose XI is much less than zero,

893
00:48:00,860 --> 00:48:02,240
okay? So wouldn't it be nice

894
00:48:02,240 --> 00:48:03,230
if this is true?

895
00:48:03,330 --> 00:48:07,940
That, essentially, if our training set,

896
00:48:08,030 --> 00:48:10,310
we can find parameters theta so that

897
00:48:10,400 --> 00:48:13,020
our learning algorithm not only makes

898
00:48:13,110 --> 00:48:14,840
correct classifications on all the examples

899
00:48:14,930 --> 00:48:17,550
in a training set, but further it's, sort of,

900
00:48:17,620 --> 00:48:18,470
is very confident about

901
00:48:18,560 --> 00:48:19,850
all of those correct classifications.

902
00:48:19,940 --> 00:48:24,240
This is the first intuition that I want you to have,

903
00:48:24,330 --> 00:48:27,560
and we'll come back to this first intuition in a second

904
00:48:27,630 --> 00:48:33,310
when we talk about functional margins, okay?

905
00:48:33,420 --> 00:48:34,660
We'll define this later.

906
00:48:34,760 --> 00:48:45,050
The second intuition that I want to convey,

907
00:48:45,140 --> 00:48:59,330
and it turns out for the rest of today's lecture

908
00:48:59,390 --> 00:49:01,190
I'm going to assume that a training set

909
00:49:01,310 --> 00:49:03,150
is linearly separable, okay?

910
00:49:03,270 --> 00:49:07,390
So by that I mean for the rest of today's lecture,

911
00:49:07,470 --> 00:49:08,510
I'm going to assume that

912
00:49:08,600 --> 00:49:10,470
there is indeed a straight line that can

913
00:49:10,590 --> 00:49:11,720
separate your training set,

914
00:49:11,820 --> 00:49:14,420
and we'll remove this assumption later,

915
00:49:14,540 --> 00:49:16,210
but just to develop the algorithm,

916
00:49:16,330 --> 00:49:18,790
let's take away the linearly separable trainingset.

917
00:49:18,890 --> 00:49:21,430
And so there's a sense that

918
00:49:21,540 --> 00:49:22,790
out of all the straight lines that

919
00:49:22,920 --> 00:49:24,200
separate the training set, you know,

920
00:49:24,310 --> 00:49:25,230
maybe that straight

921
00:49:25,320 --> 00:49:26,530
line isn't such a good one,

922
00:49:26,640 --> 00:49:30,990
and that one actually isn't such a great one either,

923
00:49:31,150 --> 00:49:36,070
but maybe that line in the middle is a much better

924
00:49:36,170 --> 00:49:38,040
linear separator than the others, right?

925
00:49:38,150 --> 00:49:42,100
And one reason that when you and I look at it

926
00:49:42,220 --> 00:49:46,740
this one seems best is because this line is

927
00:49:46,840 --> 00:49:48,110
just further from the data, all right?

928
00:49:48,190 --> 00:49:51,170
That is separates the data with a greater distance

929
00:49:51,250 --> 00:49:52,450
between your positive and your negative

930
00:49:52,520 --> 00:49:55,000
examples and division boundary, okay?

931
00:49:55,100 --> 00:49:57,260
And this second intuition,

932
00:49:57,350 --> 00:49:58,710
we'll come back to this shortly,

933
00:49:58,820 --> 00:50:02,450
about this final line that I drew being, maybe,

934
00:50:02,560 --> 00:50:07,750
the best line this notion of distance from

935
00:50:07,860 --> 00:50:09,340
the training examples.

936
00:50:09,440 --> 00:50:10,740
This is the second intuition I want to convey,

937
00:50:10,800 --> 00:50:14,160
and we'll formalize it later when we talk about

938
00:50:14,250 --> 00:50:20,090
geometric margins of our classifiers, okay?

939
00:50:20,200 --> 00:50:43,680
So in order to describe support vector machine,

940
00:50:43,790 --> 00:50:46,080
unfortunately, I'm gonna have to

941
00:50:46,190 --> 00:50:50,250
pull a notation change, and, sort of,

942
00:50:50,330 --> 00:50:52,130
unfortunately, it, sort of,

943
00:50:52,240 --> 00:50:54,000
was impossible to do logistic regression,

944
00:50:54,130 --> 00:50:55,140
and support vector machines,

945
00:50:55,250 --> 00:50:57,170
and all the other algorithms

946
00:50:57,290 --> 00:50:59,490
using one completely consistent notation,

947
00:50:59,600 --> 00:51:03,190
and so I'm actually gonna change notations

948
00:51:03,300 --> 00:51:05,360
slightly for linear classifiers,

949
00:51:05,470 --> 00:51:07,100
and that will actually make it much easier for us

950
00:51:07,190 --> 00:51:09,610
that'll make it much easier later today

951
00:51:09,720 --> 00:51:11,170
and in next week's lectures

952
00:51:11,300 --> 00:51:13,140
to actually talk about support vector machine.

953
00:51:13,280 --> 00:51:15,990
But the notation that I'm gonna use

954
00:51:16,750 --> 00:51:21,410
for the rest of today and for most of next week

955
00:51:21,470 --> 00:51:24,000
will be that my B equals Y,

956
00:51:24,110 --> 00:51:27,220
and instead of be zero, one,

957
00:51:27,330 --> 00:51:28,780
they'll be minus one and plus one,

958
00:51:28,880 --> 00:51:35,680
and a development of a support vector machine

959
00:51:35,790 --> 00:51:44,720
we will have H, have a hypothesis output values

960
00:51:44,840 --> 00:51:50,870
to the either plus one or minus one,

961
00:51:50,960 --> 00:51:59,370
and so we'll let G of Z be equal to one if Z

962
00:51:59,490 --> 00:52:01,380
is greater or equal to zero,

963
00:52:01,480 --> 00:52:03,710
and minus one otherwise, right?

964
00:52:03,810 --> 00:52:05,290
So just rather than zero and one,

965
00:52:05,400 --> 00:52:06,810
we change everything to plus one and minus one.

966
00:52:06,940 --> 00:52:12,290
And, finally, whereas previously I wrote G

967
00:52:12,410 --> 00:52:17,100
subscript theta of X equals G of theta

968
00:52:17,210 --> 00:52:19,960
transpose X and we had the convention that

969
00:52:20,070 --> 00:52:22,590
X zero is equal to one, right?

970
00:52:22,690 --> 00:52:27,090
And so X is an RN plus one.

971
00:52:27,200 --> 00:52:35,200
I'm gonna drop this convention of letting X zero

972
00:52:35,300 --> 00:52:37,810
equals a one, and letting X be an RN plus one,

973
00:52:37,910 --> 00:52:40,030
and instead I'm going to parameterize my linear

974
00:52:40,160 --> 00:52:42,190
classifier as H subscript W,

975
00:52:42,280 --> 00:52:50,290
B of X equals G of W transpose X plus B, okay?

976
00:52:50,400 --> 00:52:55,490
And so B just now plays the role of theta zero,

977
00:52:55,600 --> 00:52:58,440
and W now plays the role of

978
00:52:58,530 --> 00:52:59,960
the rest of the parameters,

979
00:53:00,040 --> 00:53:02,760
theta one through theta N, okay?

980
00:53:02,860 --> 00:53:08,680
So just by separating out the interceptor B

981
00:53:08,800 --> 00:53:10,050
rather than lumping it together,

982
00:53:10,140 --> 00:53:11,230
it'll make it easier for us

983
00:53:11,340 --> 00:53:24,890
to develop support vector machines. So yes.

984
00:53:25,010 --> 00:53:28,000
Student:[Off mic].

985
00:53:28,080 --> 00:53:29,360
Instructor (Andrew Ng):Oh, yes. Right, yes.

986
00:53:29,480 --> 00:53:35,820
So W is right. So W is a vector in RN,

987
00:53:36,090 --> 00:53:39,500
and X is now a vector in RN rather than N plus one,

988
00:53:39,600 --> 00:53:53,600
and a lowercase b is a real number. Okay.

989
00:53:53,720 --> 00:53:57,830
Now, let's formalize the notion of

990
00:53:57,920 --> 00:53:59,520
functional margin and germesh margin.

991
00:53:59,630 --> 00:54:02,990
Let me make a definition.

992
00:54:03,100 --> 00:54:06,190
I'm going to say that the functional margin

993
00:54:06,310 --> 00:54:22,640
of the hyper plane WB with respect

994
00:54:22,740 --> 00:54:27,630
to a specific training example, XIYI is

995
00:54:27,740 --> 00:54:30,560
WRT stands for with respect to

996
00:54:30,680 --> 00:54:34,200
the function margin of a hyper plane WB

997
00:54:34,320 --> 00:54:37,000
with respect to a certain training example,

998
00:54:37,120 --> 00:54:40,910
XIYI has been defined as Gamma Hat I equals

999
00:54:41,020 --> 00:54:50,360
YI times W transpose XI plus B, okay?

1000
00:54:50,550 --> 00:54:52,870
And so a set of parameters,

1001
00:54:52,950 --> 00:54:53,780
W, B defines a classifier

1002
00:54:53,870 --> 00:55:00,360
it, sort of, defines a linear separating boundary,

1003
00:55:00,460 --> 00:55:02,180
and so when I say hyper plane,

1004
00:55:02,280 --> 00:55:05,020
I just mean the decision boundary

1005
00:55:05,130 --> 00:55:09,170
that's defined by the parameters W, B.

1006
00:55:09,280 --> 00:55:13,360
You know what, if you're confused

1007
00:55:13,470 --> 00:55:15,250
by the hyper plane term, just ignore it.

1008
00:55:15,360 --> 00:55:18,390
The hyper plane of a classifier with parameters W, B

1009
00:55:18,510 --> 00:55:21,400
with respect to a training example

1010
00:55:21,510 --> 00:55:23,390
is given by this formula, okay?

1011
00:55:23,500 --> 00:55:29,300
And interpretation of this is that if YI is equal to one,

1012
00:55:29,400 --> 00:55:31,640
then for each to have a large functional margin,

1013
00:55:31,760 --> 00:55:38,080
you want W transpose XI plus B to be large, right?

1014
00:55:38,160 --> 00:55:42,120
And if YI is equal minus one,

1015
00:55:42,220 --> 00:55:45,980
then in order for the functional margin to be large

1016
00:55:46,090 --> 00:55:47,710
we, sort of, want the functional margins to large,

1017
00:55:47,810 --> 00:55:49,650
but in order for the function margins to be large,

1018
00:55:49,790 --> 00:55:51,490
if YI is equal to minus one,

1019
00:55:51,600 --> 00:55:53,960
then the only way for this to be big is

1020
00:55:54,070 --> 00:55:59,870
if W transpose XI plus B is much less than zero, okay?

1021
00:55:59,980 --> 00:56:05,350
So this captures the intuition that

1022
00:56:05,470 --> 00:56:07,390
we had earlier about functional margins

1023
00:56:07,490 --> 00:56:11,780
the intuition we had earlier that if YI is equal to one,

1024
00:56:11,880 --> 00:56:12,850
we want this to be big,

1025
00:56:12,950 --> 00:56:13,950
and if YI is equal to minus one,

1026
00:56:14,060 --> 00:56:17,670
we want this to be small, and this, sort of,

1027
00:56:17,780 --> 00:56:19,830
practice of two cases into one statement that

1028
00:56:19,940 --> 00:56:21,700
we'd like the functional margin to be large.

1029
00:56:21,810 --> 00:56:27,450
And notice this is also that so long as

1030
00:56:27,470 --> 00:56:31,860
YI times W transpose XY plus B,

1031
00:56:31,960 --> 00:56:33,360
so long as this is greater than zero,

1032
00:56:33,450 --> 00:56:37,820
that means we classified it correctly, okay?

1033
00:56:37,930 --> 00:57:15,580
And one more definition,

1034
00:57:15,690 --> 00:57:18,960
I'm going to say that the functional margin

1035
00:57:19,080 --> 00:57:21,530
of a hyper plane with respect to

1036
00:57:21,610 --> 00:57:33,050
an entire training set is going to define gamma hat

1037
00:57:33,150 --> 00:57:36,220
to be equal to min over

1038
00:57:36,330 --> 00:57:39,080
all your training examples of gamma hat, I, right?

1039
00:57:39,200 --> 00:57:41,930
So if you have a training set,

1040
00:57:42,070 --> 00:57:43,440
if you have just more than one training example,

1041
00:57:43,560 --> 00:57:46,130
I'm going to define the functional margin

1042
00:57:46,250 --> 00:57:48,500
with respect to the entire training set

1043
00:57:48,610 --> 00:57:51,170
as the worst case of all of

1044
00:57:51,290 --> 00:57:53,320
your functional margins of the entire training set.

1045
00:57:53,420 --> 00:57:56,110
And so for now we should think of

1046
00:57:56,210 --> 00:58:00,080
the first function like an intuition of saying that

1047
00:58:00,170 --> 00:58:02,850
we would like the function margin to be large,

1048
00:58:02,980 --> 00:58:05,290
and for our purposes, for now,

1049
00:58:05,410 --> 00:58:08,000
let's just say we would like the worst-case

1050
00:58:08,120 --> 00:58:10,750
functional margin to be large, okay?

1051
00:58:10,840 --> 00:58:12,750
And we'll change this a little bit later as well.

1052
00:58:12,860 --> 00:58:17,970
Now, it turns out that there's one little problem

1053
00:58:18,060 --> 00:58:22,200
with this intuition that will, sort of, edge us later,

1054
00:58:22,310 --> 00:58:24,610
which it actually turns out to be very easy

1055
00:58:24,720 --> 00:58:26,570
to make the functional margin large, all right?

1056
00:58:26,680 --> 00:58:27,800
So, for example,

1057
00:58:27,910 --> 00:58:30,820
so as I have a classifiable parameters W and B.

1058
00:58:30,910 --> 00:58:34,480
If I take W and multiply it by two

1059
00:58:34,590 --> 00:58:36,500
and take B and multiply it by two,

1060
00:58:36,610 --> 00:58:39,040
then if you refer to the definition

1061
00:58:39,140 --> 00:58:41,210
of the functional margin, I guess that was what?

1062
00:58:41,330 --> 00:58:44,380
Gamma I, gamma hat I equals

1063
00:58:44,490 --> 00:58:50,420
YI times W times transpose B.

1064
00:58:50,520 --> 00:58:53,710
If I double W and B,

1065
00:58:53,810 --> 00:58:57,620
then I can easily double my functional margin.

1066
00:58:57,710 --> 00:59:00,870
So this goal of making the functional margin large,

1067
00:59:00,990 --> 00:59:02,950
in and of itself, isn't so useful

1068
00:59:03,050 --> 00:59:04,870
because it's easy to make the functional margin

1069
00:59:04,950 --> 00:59:07,480
arbitrarily large just by scaling other parameters.

1070
00:59:07,590 --> 00:59:10,200
And so maybe one thing we need to do later

1071
00:59:10,310 --> 00:59:14,480
is add a normalization condition.

1072
00:59:14,540 --> 00:59:17,640
For example, maybe we want to

1073
00:59:17,750 --> 00:59:19,780
add a normalization condition that de-norm,

1074
00:59:19,900 --> 00:59:23,860
the alter-norm of the parameter W is equal to one,

1075
00:59:23,960 --> 00:59:28,210
and we'll come back to this in a second. All right.

1076
00:59:28,340 --> 00:59:35,280
And then so Okay. Now, let's talk about

1077
00:59:35,390 --> 00:59:41,020
see how much time we have, 15 minutes.

1078
00:59:41,110 --> 00:59:53,650
Well, see, I'm trying to decide how much

1079
00:59:53,770 --> 01:00:04,630
to try to do in the last 15 minutes. Okay.

1080
01:00:04,740 --> 01:00:11,270
So let's talk about the geometric margin, and so

1081
01:00:11,400 --> 01:00:15,750
the geometric margin of a training example [inaudible],

1082
01:00:15,860 --> 01:00:29,470
right? So the division boundary of my classifier

1083
01:00:29,580 --> 01:00:33,820
is going to be given by the plane W

1084
01:00:33,940 --> 01:00:37,380
transpose X plus B is equal to zero, okay?

1085
01:00:37,880 --> 01:00:40,680
Right, and these are the X1, X2 axis, say,

1086
01:00:40,790 --> 01:00:43,220
and we're going to draw relatively

1087
01:00:43,310 --> 01:00:44,540
few training examples here.

1088
01:00:44,660 --> 01:00:48,620
Let's say I'm drawing deliberately

1089
01:00:48,720 --> 01:00:50,120
few training examples so that

1090
01:00:50,230 --> 01:00:52,220
I can add things to this, okay?

1091
01:00:52,330 --> 01:00:56,670
And so assuming we classified an example correctly,

1092
01:00:56,780 --> 01:00:58,960
I'm going to define the geometric margin

1093
01:00:59,070 --> 01:01:04,550
as just a geometric distance between

1094
01:01:04,660 --> 01:01:06,520
a point between the training example

1095
01:01:06,610 --> 01:01:07,630
yeah,

1096
01:01:07,720 --> 01:01:10,990
between the training example XI, YI

1097
01:01:11,100 --> 01:01:15,470
and the distance given by this separating line,

1098
01:01:15,590 --> 01:01:18,430
given by this separating hyper plane, okay?

1099
01:01:18,530 --> 01:01:19,630
That's what I'm going to

1100
01:01:19,710 --> 01:01:21,650
define the geometric margin to be.

1101
01:01:21,770 --> 01:01:27,680
And so I'm gonna do some algebra fairly quickly.

1102
01:01:27,790 --> 01:01:29,800
In case it doesn't make sense, and read through

1103
01:01:30,020 --> 01:01:32,120
the lecture notes more carefully for details.

1104
01:01:32,230 --> 01:01:36,970
Sort of, by standard geometry, the normal,

1105
01:01:37,060 --> 01:01:40,190
or in other words, the vector that's 90 degrees

1106
01:01:40,300 --> 01:01:42,460
to the separating hyper plane is going to

1107
01:01:42,680 --> 01:01:46,240
be given by W divided by the norm of W;

1108
01:01:46,340 --> 01:01:48,680
that's just how planes and high dimensions work.

1109
01:01:48,790 --> 01:01:52,100
If this stuff some of this you have to use,

1110
01:01:52,210 --> 01:01:54,230
take a look t the lecture notes on the website.

1111
01:01:54,330 --> 01:02:02,400
And so let's say this distance is gamma I, okay?

1112
01:02:02,480 --> 01:02:04,500
And so I'm going to use the convention that

1113
01:02:04,590 --> 01:02:06,120
I'll put a hat on top

1114
01:02:06,210 --> 01:02:07,750
where I'm referring to functional margins,

1115
01:02:07,850 --> 01:02:10,530
and no hat on top for geometric margins.

1116
01:02:10,640 --> 01:02:12,720
So let's say geometric margin,

1117
01:02:12,800 --> 01:02:14,600
as this example, is gamma I.

1118
01:02:14,720 --> 01:02:26,910
That means that this point here, right, is going to

1119
01:02:27,000 --> 01:02:38,520
be XI minus gamma I times W over normal W, okay?

1120
01:02:38,630 --> 01:02:43,430
Because W over normal W is the unit vector,

1121
01:02:43,550 --> 01:02:46,110
is the length one vector that is normal

1122
01:02:46,200 --> 01:02:47,800
to the separating hyper plane,

1123
01:02:47,900 --> 01:02:51,360
and so when we subtract gamma I times the unit

1124
01:02:51,440 --> 01:02:53,430
vector from this point, XI,

1125
01:02:53,530 --> 01:02:54,950
or at this point here is XI.

1126
01:02:55,080 --> 01:02:57,400
So XI minus, you know,

1127
01:02:57,520 --> 01:03:00,530
this little vector here is going to be this point that

1128
01:03:00,630 --> 01:03:03,340
I've drawn as a heavy circle, okay?

1129
01:03:03,450 --> 01:03:06,960
So this heavy point here is XI minus this vector,

1130
01:03:07,040 --> 01:03:10,340
and this vector is gamma I time W

1131
01:03:10,470 --> 01:03:12,810
over norm of W, okay?

1132
01:03:12,900 --> 01:03:18,580
And so because this heavy point

1133
01:03:18,680 --> 01:03:21,370
is on the separating hyper plane, right,

1134
01:03:21,480 --> 01:03:29,040
this point must satisfy W transpose times that

1135
01:03:29,160 --> 01:03:36,010
point equals zero, right?

1136
01:03:36,120 --> 01:03:39,610
Because all points X on the separating

1137
01:03:39,690 --> 01:03:41,560
hyper plane satisfy the equation W transpose X

1138
01:03:41,680 --> 01:03:43,940
plus B equals zero, and so this point is

1139
01:03:44,050 --> 01:03:45,320
on the separating hyper plane, therefore,

1140
01:03:45,430 --> 01:03:48,000
it must satisfy W transpose this point

1141
01:03:48,110 --> 01:03:56,640
oh, excuse me. Plus B is equal to zero, okay?

1142
01:03:56,730 --> 01:03:59,170
Raise your hand if this makes sense so far?

1143
01:03:59,260 --> 01:04:03,050
Oh, okay. Cool, most of you, but, again,

1144
01:04:03,130 --> 01:04:05,710
I'm, sort of, being slightly fast in this geometry.

1145
01:04:05,820 --> 01:04:07,440
So if you're not quite sure

1146
01:04:07,520 --> 01:04:08,740
why this is a normal vector,

1147
01:04:08,820 --> 01:04:09,430
or how I subtracted this,

1148
01:04:09,430 --> 01:04:10,040
or whatever,

1149
01:04:10,150 --> 01:04:12,100
take a look at the details in the lecture notes.

1150
01:04:12,200 --> 01:04:25,160
And so what I'm going to do

1151
01:04:25,240 --> 01:04:26,600
is I'll just take this equation,

1152
01:04:26,680 --> 01:04:28,920
and I'll solve for gamma, right?

1153
01:04:29,010 --> 01:04:30,160
So this equation I just wrote down,

1154
01:04:30,270 --> 01:04:33,050
solve this equation for gamma or gamma I,

1155
01:04:33,150 --> 01:04:34,850
and you find that

1156
01:04:34,980 --> 01:04:46,440
you saw that previous equation from gamma I

1157
01:04:46,530 --> 01:04:50,710
well, why don't I just do it?

1158
01:04:50,800 --> 01:04:54,800
You have W transpose XI plus B equals

1159
01:04:54,890 --> 01:05:01,160
gamma I times W transpose W over norm of W;

1160
01:05:01,300 --> 01:05:06,470
that's just equal to gamma times the norm of W

1161
01:05:06,560 --> 01:05:11,000
because W transpose W is the norm of W squared,

1162
01:05:11,080 --> 01:05:18,370
and, therefore, gamma is just

1163
01:05:18,450 --> 01:05:32,880
well, transpose X equals, okay?

1164
01:05:33,010 --> 01:05:34,720
And, in other words,

1165
01:05:34,830 --> 01:05:36,970
this little calculation just showed us that

1166
01:05:37,090 --> 01:05:41,540
if you have a training example XI,

1167
01:05:41,670 --> 01:05:44,360
then the distance between XI

1168
01:05:44,450 --> 01:05:46,800
and the separating hyper plane defined

1169
01:05:46,910 --> 01:05:48,020
by the parameters W and B

1170
01:05:48,130 --> 01:05:53,100
can be computed by this formula, okay?

1171
01:05:53,220 --> 01:06:04,190
So the last thing I want to do

1172
01:06:04,190 --> 01:06:05,190
is actually take into account the sign of the –

1173
01:06:08,040 --> 01:06:11,080
the correct classification of the training example.

1174
01:06:11,470 --> 01:06:12,830
So I've been assuming that

1175
01:06:12,920 --> 01:06:15,500
we've been classifying an example correctly.

1176
01:06:15,610 --> 01:06:22,510
So, more generally, to find the geometric margin

1177
01:06:22,630 --> 01:06:31,060
of a training example to be gamma I equals

1178
01:06:31,160 --> 01:06:44,170
YI times that thing on top, okay?

1179
01:06:44,300 --> 01:06:46,370
And so this is very similar

1180
01:06:46,370 --> 01:06:49,070
to the functional margin,

1181
01:06:49,200 --> 01:06:51,800
except for the normalization by the norm of W,

1182
01:06:51,890 --> 01:06:54,900
and so as before, you know,

1183
01:06:55,010 --> 01:06:56,870
this says that so long as

1184
01:06:56,990 --> 01:06:59,680
we would like the geometric margin to be large,

1185
01:06:59,760 --> 01:07:01,590
and all that means is that so long as

1186
01:07:01,710 --> 01:07:03,350
we're classifying the example correctly,

1187
01:07:03,470 --> 01:07:06,260
we would ideally hope of the example to be

1188
01:07:06,360 --> 01:07:08,700
as far as possible from the separating hyper plane,

1189
01:07:08,800 --> 01:07:09,760
so long as it's on the right side of

1190
01:07:09,870 --> 01:07:10,880
the separating hyper plane,

1191
01:07:10,980 --> 01:07:13,360
and that's what YI multiplied into this does.

1192
01:07:21,310 --> 01:07:24,440
And so a couple of easy facts,

1193
01:07:24,520 --> 01:07:30,130
one is if the norm of W is equal to one,

1194
01:07:30,190 --> 01:07:34,980
then the functional margin

1195
01:07:35,100 --> 01:07:37,130
is equal to the geometric margin,

1196
01:07:37,240 --> 01:07:39,630
and you see that quite easily, and,

1197
01:07:39,750 --> 01:07:44,170
more generally, the geometric margin

1198
01:07:44,280 --> 01:07:49,140
is just equal to the functional margin divided

1199
01:07:49,280 --> 01:08:11,900
by the norm of W, okay? Let's see, okay.

1200
01:08:12,010 --> 01:08:23,190
And so one final definition is so far I've defined

1201
01:08:23,260 --> 01:08:24,790
the geometric margin with respect to

1202
01:08:24,900 --> 01:08:28,270
a single training example, and so as before,

1203
01:08:28,390 --> 01:08:30,480
I'll define the geometric margin with respect to

1204
01:08:30,580 --> 01:08:33,590
an entire training set as gamma equals

1205
01:08:33,710 --> 01:08:36,820
min over I of gamma I,

1206
01:08:36,820 --> 01:08:40,170
all right?

1207
01:08:40,290 --> 01:08:49,300
And so the maximum margin classifier,

1208
01:08:49,380 --> 01:08:51,530
which is a precursor to the support vector machine,

1209
01:08:51,640 --> 01:09:01,360
is the learning algorithm that

1210
01:09:01,470 --> 01:09:05,610
chooses the parameters W and B so as to

1211
01:09:05,690 --> 01:09:07,800
maximize the geometric margin,

1212
01:09:07,910 --> 01:09:10,040
and so I just write that down.

1213
01:09:10,150 --> 01:09:13,250
The maximum margin classified poses

1214
01:09:13,340 --> 01:09:15,080
the following optimization problem.

1215
01:09:15,190 --> 01:09:20,550
It says choose gamma, W, and B so as to

1216
01:09:20,660 --> 01:09:21,950
maximize the geometric margin,

1217
01:09:22,050 --> 01:09:26,700
subject to that YI times

1218
01:09:26,800 --> 01:09:37,820
well, this is just one way to write it, subject to

1219
01:09:37,930 --> 01:09:43,540
actually, do I write it like that? Yeah, fine.

1220
01:09:43,630 --> 01:09:46,470
There are several ways to write this, and one of the things

1221
01:09:46,520 --> 01:09:47,930
we'll do next time is actually

1222
01:09:48,000 --> 01:09:50,080
I'm trying to figure out

1223
01:09:50,130 --> 01:09:51,350
if I can do this in five minutes.

1224
01:09:51,400 --> 01:09:53,330
I'm guessing this could be difficult.

1225
01:09:53,410 --> 01:09:56,600
Well, so this maximizing your classifier

1226
01:09:56,680 --> 01:09:58,340
is the maximization problem

1227
01:09:58,450 --> 01:10:03,360
over parameter gamma W and B, and for now,

1228
01:10:03,470 --> 01:10:06,260
it turns out that the geometric margin doesn't change

1229
01:10:06,350 --> 01:10:08,420
depending on the norm of W, right?

1230
01:10:08,470 --> 01:10:10,840
Because in the definition of the geometric margin,

1231
01:10:10,920 --> 01:10:13,740
notice that we're dividing by the norm of W anyway.

1232
01:10:13,830 --> 01:10:15,820
So you can actually set the norm of

1233
01:10:15,920 --> 01:10:17,160
W to be anything you want,

1234
01:10:17,280 --> 01:10:19,200
and you can multiply W and B by any constant;

1235
01:10:19,290 --> 01:10:21,680
it doesn't change the geometric margin.

1236
01:10:21,780 --> 01:10:23,860
This will actually be important,

1237
01:10:23,960 --> 01:10:25,640
and we'll come back to this later.

1238
01:10:25,740 --> 01:10:28,540
Notice that you can take the parameters WB,

1239
01:10:28,660 --> 01:10:30,740
and you can impose

1240
01:10:30,840 --> 01:10:32,340
any normalization constant to it,

1241
01:10:32,460 --> 01:10:34,160
or you can change W and B by

1242
01:10:34,270 --> 01:10:36,730
any scaling factor and replace them

1243
01:10:36,810 --> 01:10:38,290
by ten W and ten B whatever,

1244
01:10:38,390 --> 01:10:42,140
and it does not change the geometric margin, okay?

1245
01:10:42,250 --> 01:10:45,540
And so in this first formulation,

1246
01:10:45,660 --> 01:10:46,930
I'm just gonna impose a constraint

1247
01:10:46,980 --> 01:10:48,230
and say that the norm of W was one,

1248
01:10:48,370 --> 01:10:50,540
and so the function of the geometric margins

1249
01:10:50,630 --> 01:10:51,540
will be the same,

1250
01:10:51,660 --> 01:10:53,220
and then we'll say maximize

1251
01:10:53,300 --> 01:10:54,730
the geometric margins subject to

1252
01:10:54,830 --> 01:10:56,950
you maximize gamma subject to that

1253
01:10:57,060 --> 01:10:58,720
every training example must

1254
01:10:58,810 --> 01:11:01,190
have geometric margin at least gamma,

1255
01:11:01,300 --> 01:11:03,640
and this is a geometric margin because

1256
01:11:03,740 --> 01:11:05,810
when the norm of W is equal to one,

1257
01:11:05,900 --> 01:11:06,890
then the functional of

1258
01:11:06,980 --> 01:11:09,460
the geometric margin are identical, okay?

1259
01:11:09,580 --> 01:11:12,800
So this is the maximum margin classifier,

1260
01:11:12,900 --> 01:11:15,200
and it turns out that if you do this, it'll run,

1261
01:11:15,320 --> 01:11:17,240
you know, maybe about as well as a

1262
01:11:17,350 --> 01:11:18,550
maybe slight

1263
01:11:18,660 --> 01:11:20,560
maybe comparable to logistic regression,

1264
01:11:20,660 --> 01:11:23,690
but it turns out that

1265
01:11:23,800 --> 01:11:25,890
as we develop this algorithm further,

1266
01:11:25,980 --> 01:11:28,470
there will be a clever way to allow us

1267
01:11:28,580 --> 01:11:30,580
to change this algorithm to let it work

1268
01:11:30,700 --> 01:11:32,260
in infinite dimensional feature spaces

1269
01:11:32,370 --> 01:11:33,900
and come up with

1270
01:11:34,000 --> 01:11:35,570
very efficient non-linear classifiers.

1271
01:11:35,690 --> 01:11:39,700
So there's a ways to go before we turn this into

1272
01:11:39,830 --> 01:11:42,600
a support vector machine, but this is the first step.

1273
01:11:42,700 --> 01:11:47,510
So are there questions about this? Yeah.

1274
01:11:47,620 --> 01:12:02,140
Student:[Off mic].

1275
01:12:02,220 --> 01:12:04,180
Instructor (Andrew Ng):For now,

1276
01:12:04,230 --> 01:12:05,550
let's just say you're given a fixed training set,

1277
01:12:05,650 --> 01:12:06,920
and you can't yeah, for now,

1278
01:12:07,020 --> 01:12:08,650
let's just say you're given a fixed training set,

1279
01:12:08,700 --> 01:12:11,310
and the scaling of the training set is not

1280
01:12:11,380 --> 01:12:12,600
something you get to play with, right?

1281
01:12:12,740 --> 01:12:15,220
So everything I've said is for a fixed training set,

1282
01:12:15,310 --> 01:12:18,680
so that you can't change the X's,

1283
01:12:18,780 --> 01:12:20,070
and you can't change the Y's.

1284
01:12:20,180 --> 01:12:22,020
Are there other questions?

1285
01:12:22,140 --> 01:12:32,580
Okay. So all right. Next week we will take this,

1286
01:12:32,690 --> 01:12:34,070
and we'll talk about authorization algorithms,

1287
01:12:34,170 --> 01:12:37,150
and work our way towards turning this into

1288
01:12:37,300 --> 01:12:38,670
one of the most effective

1289
01:12:38,780 --> 01:12:40,700
off-theshelf learning algorithms,

1290
01:12:40,810 --> 01:12:43,380
and just a final reminder again,

1291
01:12:43,500 --> 01:12:45,370
this next discussion session

1292
01:12:45,490 --> 01:12:47,620
will be on Matlab and Octaves.

1293
01:12:47,730 --> 01:12:48,640
So show up for that

1294
01:12:48,740 --> 01:12:50,270
if you want to see a tutorial. Okay.

1295
01:12:50,380 --> 01:12:51,990
See you guys in the next class.

