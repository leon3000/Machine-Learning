1
00:00:18,690 --> 00:00:24,480
机器学习--第16讲

2
00:00:24,560 --> 00:00:25,570
好的  欢迎回来

3
00:00:25,650 --> 00:00:28,380
我今天开始学习新的章节

4
00:00:28,460 --> 00:00:30,120
关于机器学习的一个新的讨论

5
00:00:30,200 --> 00:00:32,280
特别是  我想谈谈一个

6
00:00:32,350 --> 00:00:33,750
机器学习问题的不同类型

7
00:00:33,830 --> 00:00:35,180
称为强化学习

8
00:00:35,260 --> 00:00:37,710
所以这是马尔可夫决策过程

9
00:00:37,780 --> 00:00:40,060
价值函数  值迭代

10
00:00:40,140 --> 00:00:42,100
和政策迭代

11
00:00:42,170 --> 00:00:45,070
其中最后两个项目

12
00:00:45,160 --> 00:00:47,730
都是解决强化学习问题的算法

13
00:00:47,830 --> 00:00:50,350
正如你可以看到

14
00:00:50,440 --> 00:00:51,640
我们今天在不同的房间录音

15
00:00:51,730 --> 00:00:52,900
这个背景有点不同

16
00:00:52,990 --> 00:00:56,440
所以在这个环境下

17
00:00:56,540 --> 00:00:59,560
我们在这个类中的四个主要议题

18
00:00:59,640 --> 00:01:02,590
第一是监督学习  在监督学习中

19
00:01:02,670 --> 00:01:05,640
我们有训练集

20
00:01:05,720 --> 00:01:08,230
我们给出

21
00:01:08,310 --> 00:01:10,810
所有训练例子的某种"正确的"答案

22
00:01:10,880 --> 00:01:12,320
然后  然后是一点学习算法

23
00:01:12,420 --> 00:01:13,920
用来复制更多正确的答案

24
00:01:14,030 --> 00:01:17,350
然后是学习理论

25
00:01:17,430 --> 00:01:19,060
然后我们谈到非监督学习

26
00:01:19,150 --> 00:01:21,340
在非监督学习中

27
00:01:21,420 --> 00:01:24,680
我们有一堆未标记的数据  只是x的

28
00:01:24,790 --> 00:01:26,580
它是的学习算法的工作

29
00:01:26,680 --> 00:01:28,390
用来发现数据中的所谓的结构

30
00:01:28,470 --> 00:01:30,460
和像聚类分析  K – means

31
00:01:30,580 --> 00:01:32,150
各种的PCA的混合

32
00:01:32,260 --> 00:01:33,890
ICA  等多种算法

33
00:01:33,990 --> 00:01:36,630
今天  我想谈一个不同类的学习算法

34
00:01:36,720 --> 00:01:39,300
它是某种介于在监督和无人监督之间

35
00:01:39,380 --> 00:01:40,990
所以会有一类问题

36
00:01:41,070 --> 00:01:43,280
有一个水平的监督

37
00:01:43,380 --> 00:01:46,180
它比我们看到的监督学习中的监督少得多

38
00:01:46,290 --> 00:01:48,630
这是一个形式主义问题

39
00:01:48,730 --> 00:01:51,210
所谓的强化学习

40
00:01:51,320 --> 00:01:53,130
所以接下来是幻灯片

41
00:01:53,240 --> 00:01:55,310
让我展示给你

42
00:01:55,410 --> 00:01:59,630
作为一个移动的例子

43
00:01:59,730 --> 00:02:01,380
这里有一个几分像强化学习的例子

44
00:02:01,490 --> 00:02:07,410
这是一张图片--其中一些知识

45
00:02:07,490 --> 00:02:08,990
我在演讲1中谈到过

46
00:02:09,070 --> 00:02:10,580
但这里有一个图片--

47
00:02:10,670 --> 00:02:14,550
在斯坦福大学我们有一个自主的直升机

48
00:02:14,660 --> 00:02:16,710
所以  你会如何写一个程序

49
00:02:16,820 --> 00:02:18,870
使直升机可以像这样自主的飞翔?

50
00:02:18,980 --> 00:02:21,280
我会给你看一个有趣的视频

51
00:02:21,370 --> 00:02:24,490
我认为  这实际上是在第一场演讲中

52
00:02:24,590 --> 00:02:27,650
我在课堂上播放的视频

53
00:02:27,730 --> 00:02:30,540
这是拍摄于斯坦福大学足球场的视频

54
00:02:30,630 --> 00:02:32,320
使用机器学习算法飞行的直升机

55
00:02:32,460 --> 00:02:34,870
所以让我们来播放视频

56
00:02:34,950 --> 00:02:42,990
你可以放大视频

57
00:02:43,070 --> 00:02:48,040
并看到天空中的树木

58
00:02:48,150 --> 00:02:51,540
因此  依据自主直升机的飞行

59
00:02:51,640 --> 00:02:53,370
这是由我和我的以一些学生写的

60
00:02:53,470 --> 00:02:54,990
在自主直升机飞行中

61
00:02:55,090 --> 00:02:57,600
这是最困难的特技飞行演习之一

62
00:02:57,690 --> 00:03:00,290
这实际上是很难写一个程序

63
00:03:00,390 --> 00:03:03,310
使直升机做到这一点

64
00:03:03,410 --> 00:03:04,940
这是使用所谓的

65
00:03:05,040 --> 00:03:06,400
强化学习算法所写的

66
00:03:06,500 --> 00:03:10,470
只是为了使这更具体  好的

67
00:03:10,560 --> 00:03:13,590
在直升机飞行的学习问题是每秒10次

68
00:03:13,670 --> 00:03:16,260
直升机上的传感器为您提供了

69
00:03:16,360 --> 00:03:18,950
直升机的方向的位置非常准确的估计

70
00:03:19,070 --> 00:03:21,310
所以在所有时间点

71
00:03:21,400 --> 00:03:22,830
你非常准确知道直升机在哪个地方

72
00:03:22,940 --> 00:03:25,910
并且你的工作  是使用此输入

73
00:03:25,990 --> 00:03:29,190
位置定位  并且输出的

74
00:03:29,280 --> 00:03:33,560
对应的一组数据

75
00:03:33,670 --> 00:03:35,460
相对应的使用移动控制杆来控制直升机

76
00:03:35,560 --> 00:03:38,360
使它飞翔  从右侧向上

77
00:03:38,460 --> 00:03:40,260
从上飞到下

78
00:03:40,350 --> 00:03:41,680
各种你想要的演习

79
00:03:41,770 --> 00:03:44,230
这和监督学习不同

80
00:03:44,330 --> 00:03:46,840
因为通常我们根本不知道

81
00:03:46,940 --> 00:03:50,700
"正确"的控制动作是什么

82
00:03:50,780 --> 00:03:53,590
更具体地说  如果这架直升机

83
00:03:53,680 --> 00:03:55,550
位于某个位置的方向

84
00:03:55,640 --> 00:03:57,500
它实际上很难说  这架直升机

85
00:03:57,580 --> 00:03:59,070
什么时候这样处理

86
00:03:59,150 --> 00:04:01,930
你应该把控制杆移动到这些精确的位置

87
00:04:02,000 --> 00:04:03,520
因此很难对这个问题

88
00:04:03,620 --> 00:04:05,390
使用监督学习算法

89
00:04:05,480 --> 00:04:08,950
因为我们不能提出一个训练集

90
00:04:09,070 --> 00:04:11,020
使它的位置作为输入

91
00:04:11,070 --> 00:04:12,040
并且输出是所有"正确"的控制行动

92
00:04:12,150 --> 00:04:13,440
这真的是很难提出一个这样的训练集

93
00:04:13,530 --> 00:04:16,780
取而代之的强化学习

94
00:04:16,880 --> 00:04:19,020
我们将提供给学习算法不同类型的反馈

95
00:04:19,110 --> 00:04:20,490
基本上所谓的奖励信号

96
00:04:20,580 --> 00:04:23,370
它会告诉直升机

97
00:04:23,440 --> 00:04:27,330
什么时候它的表现很好

98
00:04:27,420 --> 00:04:29,190
什么时候表现不佳

99
00:04:29,280 --> 00:04:30,790
所以我们将要做的是

100
00:04:30,870 --> 00:04:34,230
我们提出所谓的奖励信号

101
00:04:34,320 --> 00:04:36,440
我等下会使其标准化

102
00:04:36,520 --> 00:04:38,090
这是一个直升机该如何做的估量

103
00:04:38,180 --> 00:04:39,570
然后将是学习算法的工作

104
00:04:39,660 --> 00:04:40,940
这个奖励函数作为输入  并尝试飞得好

105
00:04:41,040 --> 00:04:44,670
强化学习的另一个很好的例子就是

106
00:04:44,770 --> 00:04:46,610
考虑得到一个玩游戏的程序

107
00:04:46,710 --> 00:04:49,150
下棋  一盘棋

108
00:04:49,230 --> 00:04:53,870
在游戏中的任何阶段

109
00:04:53,980 --> 00:04:57,180
我们不知道的"最佳"的举动是什么

110
00:04:57,300 --> 00:05:01,070
所以很难以构成一个下棋的监督学习问题

111
00:05:01,180 --> 00:05:03,590
因为我们不可以说

112
00:05:03,700 --> 00:05:05,920
X公司是在董事会的立场

113
00:05:06,020 --> 00:05:07,710
和Y公司是最佳举动

114
00:05:07,820 --> 00:05:09,550
因为我们不知道如何创造

115
00:05:09,640 --> 00:05:10,920
最佳象棋动作训练的例子

116
00:05:11,040 --> 00:05:15,350
但我们确实知道的是

117
00:05:15,460 --> 00:05:16,910
如果你让一台电脑下国际象棋游戏

118
00:05:16,980 --> 00:05:18,540
我们知道当它赢得了比赛

119
00:05:18,640 --> 00:05:21,560
当其失去了比赛  所以我们要做的是

120
00:05:21,680 --> 00:05:25,020
我们给它一个奖励信号的

121
00:05:25,110 --> 00:05:26,540
所以赢得一盘棋  就给它一个积极的奖励

122
00:05:26,630 --> 00:05:27,810
它输了时  就给它一个惩罚

123
00:05:27,900 --> 00:05:29,180
并希望随着时间的推移

124
00:05:29,380 --> 00:05:30,720
它本身要学会赢越来越多的游戏

125
00:05:30,830 --> 00:05:34,820
那么我想你把强化学习

126
00:05:34,910 --> 00:05:36,600
认为是训练的狗

127
00:05:36,700 --> 00:05:39,080
每当你的狗做的好

128
00:05:39,180 --> 00:05:40,940
你告诉它  "好狗"

129
00:05:41,040 --> 00:05:42,330
每次坏事  你告诉它  "坏狗"

130
00:05:42,430 --> 00:05:44,320
并随着时间的推移

131
00:05:44,440 --> 00:05:46,810
你的狗学会做越来越多的好事情

132
00:05:47,780 --> 00:05:48,900
因此  以同样的方式

133
00:05:48,970 --> 00:05:49,980
当我们尝试飞行的直升机

134
00:05:50,060 --> 00:05:51,380
每次直升机飞得好  你说  "好直升机  "

135
00:05:51,470 --> 00:05:53,040
每一次崩溃  你说  "坏直升机"

136
00:05:53,130 --> 00:05:56,200
然后随着时间的推移

137
00:05:56,300 --> 00:05:57,800
它会越来越频繁学会做正确的事情

138
00:05:57,920 --> 00:06:03,020
究其原因--强化学习

139
00:06:03,100 --> 00:06:05,650
比监督学习更难的原因之一是因为

140
00:06:05,720 --> 00:06:07,840
这不是一个一次性的决策问题

141
00:06:07,940 --> 00:06:10,310
所以  在监督学习中

142
00:06:10,420 --> 00:06:12,950
如果你有一个分类

143
00:06:13,050 --> 00:06:14,540
预测是否有人患有癌症

144
00:06:14,620 --> 00:06:16,420
你做一个预测

145
00:06:16,510 --> 00:06:18,130
然后就大功告成了  对吗?

146
00:06:18,220 --> 00:06:20,020
你的病人要么患有癌症或没有患癌症

147
00:06:20,110 --> 00:06:22,080
你无论是对还是错  他们生或死  等等

148
00:06:22,180 --> 00:06:23,420
你做出决定  然后就大功告成了

149
00:06:23,540 --> 00:06:25,340
在强化学习中  你必须随着

150
00:06:25,430 --> 00:06:27,830
时间的推移一直采取行动

151
00:06:27,920 --> 00:06:30,900
因此它被称为渐进的决策过程

152
00:06:30,990 --> 00:06:33,220
具体地说  假设一个程序移到第60步

153
00:06:33,330 --> 00:06:37,030
输了一盘棋 那么它实际上

154
00:06:37,110 --> 00:06:38,760
在输了这盘棋之前

155
00:06:38,850 --> 00:06:43,420
它移动了60步

156
00:06:43,500 --> 00:06:45,400
使得它很难学习这个算法

157
00:06:45,500 --> 00:06:48,050
这就是所谓的信贷分配问题

158
00:06:48,160 --> 00:07:00,390
而要非正式的表述它

159
00:07:00,490 --> 00:07:02,900
这是什么  如果程序在60个移动中

160
00:07:03,000 --> 00:07:05,780
失去了一盘棋

161
00:07:05,870 --> 00:07:08,020
你其实不是很确定它所有的移动

162
00:07:08,100 --> 00:07:10,240
哪些是正确的移动  哪些是坏的举动

163
00:07:10,320 --> 00:07:12,660
并且也许这是因为

164
00:07:12,780 --> 00:07:14,690
你已经在第23步就失蹄了

165
00:07:14,800 --> 00:07:16,540
然后其他的一切可能是完美的

166
00:07:16,650 --> 00:07:18,310
但因为你在第23步

167
00:07:18,420 --> 00:07:19,550
就犯了一个错误

168
00:07:19,650 --> 00:07:20,950
最终导致你在第60步失去了比赛

169
00:07:21,050 --> 00:07:23,410
因此  非常粗略地定义分配问题

170
00:07:23,490 --> 00:07:25,090
是你是否得到积极或消极的奖励

171
00:07:25,180 --> 00:07:26,380
因此找出你是做对了还是做错了

172
00:07:26,470 --> 00:07:29,870
所产生的奖励

173
00:07:29,950 --> 00:07:31,840
所以你可以做更多的正确的事

174
00:07:31,940 --> 00:07:34,250
少做错事

175
00:07:34,340 --> 00:07:35,560
这是使强化学习

176
00:07:35,650 --> 00:07:36,700
困难的原因之一

177
00:07:38,780 --> 00:07:42,290
以同样的方式  如果直升机坠毁

178
00:07:42,380 --> 00:07:45,860
你可能不知道 以同样的方式

179
00:07:45,940 --> 00:07:48,580
如果直升机坠毁

180
00:07:48,660 --> 00:07:50,940
这可能是你好多分钟之前  导致直升机坠毁

181
00:07:51,020 --> 00:07:54,000
事实上  如果你撞坏了一辆汽车

182
00:07:54,070 --> 00:07:55,180
你会希望不在一场车祸中

183
00:07:55,260 --> 00:07:56,440
但有人撞坏的汽车时

184
00:07:56,560 --> 00:07:58,360
通常的事情  他们通常做的

185
00:07:58,440 --> 00:08:00,370
正确的事情是  踩住的刹车

186
00:08:00,460 --> 00:08:01,860
并让车缓慢下来  以免产生碰撞

187
00:08:01,950 --> 00:08:04,560
通常踩着刹车

188
00:08:04,660 --> 00:08:05,780
不会导致碰撞

189
00:08:05,890 --> 00:08:08,870
相反  它使得碰撞的伤害稍微减少

190
00:08:08,960 --> 00:08:12,880
但  强化算法  你看到这个模式

191
00:08:12,980 --> 00:08:14,650
你踩刹车的时候  你碰撞了

192
00:08:14,740 --> 00:08:17,100
这不是你撞车的原因

193
00:08:17,180 --> 00:08:18,260
并且很难搞清楚

194
00:08:18,360 --> 00:08:19,900
它是否是踩刹车导致你的撞车

195
00:08:19,990 --> 00:08:21,760
而是在你踩刹车之前的一些动作

196
00:08:21,880 --> 00:08:29,260
因此  让我去定义

197
00:08:29,360 --> 00:08:31,360
使强化学习的问题更加正规化

198
00:08:31,460 --> 00:08:34,740
作为引语  我要说的算法

199
00:08:34,830 --> 00:08:37,050
被应用于广泛的问题

200
00:08:37,160 --> 00:08:40,180
但因为机器人学很容易在演讲中表现

201
00:08:40,280 --> 00:08:41,720
我有很多--贯穿这个演讲

202
00:08:41,830 --> 00:08:43,340
我用一堆机器人作为例子

203
00:08:43,430 --> 00:08:46,010
但后来  我们将谈论这些思想的应用

204
00:08:46,120 --> 00:08:47,820
使问题应用更加广泛

205
00:08:47,900 --> 00:08:49,850
但我面临的基本问题

206
00:08:49,940 --> 00:08:53,510
是渐进的决策过程

207
00:08:53,620 --> 00:08:56,430
我们需要作出许多决定

208
00:08:56,540 --> 00:08:57,890
并且你的决定也许

209
00:08:57,990 --> 00:08:59,700
会有长远的后果

210
00:08:59,800 --> 00:09:03,720
因此  让我们标准化

211
00:09:03,800 --> 00:09:08,260
这个强化学习问题

212
00:09:08,350 --> 00:09:10,690
强化学习问题模型世界上

213
00:09:10,780 --> 00:09:13,410
使用所谓的MDP或马氏决策

214
00:09:13,520 --> 00:09:28,490
过程的形式体系 而让我们来看看

215
00:09:28,600 --> 00:09:31,170
MDP是五元组--我没有足够的空间

216
00:09:31,270 --> 00:09:35,310
以及 由五个集合组成

217
00:09:46,470 --> 00:09:50,400
因此  让我说这些是什么

218
00:09:50,490 --> 00:09:53,540
其实  请把屏幕提高点?

219
00:09:53,620 --> 00:09:55,270
今天我不需要笔记本电脑了

220
00:09:55,360 --> 00:10:04,030
更多的空间 是的  开始  好的 谢谢

221
00:10:04,140 --> 00:10:09,310
因此  一个MDP  组成五元组

222
00:10:09,400 --> 00:10:12,720
其中第一个元素  S是一个状态集

223
00:10:12,830 --> 00:10:16,170
对于直升机的例子

224
00:10:16,270 --> 00:10:18,110
状态集可能是直升机的位置和方向

225
00:10:18,230 --> 00:10:24,130
A是一个动作集

226
00:10:24,220 --> 00:10:27,310
所以  对于直升机的例子

227
00:10:27,390 --> 00:10:30,720
这将是所有可能的位置的集合

228
00:10:30,800 --> 00:10:32,770
我们可以把我们的控制杆放进去

229
00:10:32,860 --> 00:10:47,850
P  S  A是状态转换分布

230
00:10:47,950 --> 00:10:51,090
因此  每个状态和每个动作

231
00:10:51,190 --> 00:10:53,430
这是高概率分布

232
00:10:53,510 --> 00:10:59,320
所以S`的总和  P prime等于1

233
00:10:59,390 --> 00:11:00,590
PSA S`被创建为0

234
00:11:00,690 --> 00:11:07,110
并且状态转换分配是

235
00:11:07,190 --> 00:11:09,020
状态转移概率如下工作

236
00:11:09,110 --> 00:11:11,800
p给我提供  什么状态

237
00:11:11,880 --> 00:11:14,780
我将过渡到的状态的概率分布

238
00:11:14,890 --> 00:11:18,130
如果我在状态S时采取行动

239
00:11:18,230 --> 00:11:22,230
这是S`状态的概率分布

240
00:11:22,330 --> 00:11:25,800
然后我在状态S的时候

241
00:11:25,890 --> 00:11:27,900
采取的行动

242
00:11:28,000 --> 00:11:30,160
现在  我将快速把它读一遍

243
00:11:32,140 --> 00:11:34,530
Gamma是一个数字  所谓的贴现因子

244
00:11:34,610 --> 00:11:39,180
不要担心这个 我等下会讲它是什么

245
00:11:39,250 --> 00:11:42,180
通常有严格的额定数字  严格小于1

246
00:11:42,260 --> 00:11:44,670
并在额定等于0

247
00:11:44,790 --> 00:11:50,200
R是我们的奖励函数

248
00:11:50,290 --> 00:11:57,470
所以奖励函数与状态集对应

249
00:11:57,580 --> 00:12:00,790
为实数

250
00:12:00,870 --> 00:12:05,140
并且可能为正数或负数

251
00:12:05,220 --> 00:12:09,530
这是实数集

252
00:12:09,620 --> 00:12:15,090
所以要使这些元素具体点

253
00:12:15,190 --> 00:12:18,590
让我举一个MDP的具体的例子

254
00:12:18,680 --> 00:12:21,780
而不是谈论像

255
00:12:21,860 --> 00:12:22,840
直升机那样复杂的东西

256
00:12:22,920 --> 00:12:24,650
我会使用一个较小的MDP

257
00:12:24,750 --> 00:12:28,260
作为今天的讲座的例子

258
00:12:28,360 --> 00:12:29,620
我们会在以后的演讲中

259
00:12:29,720 --> 00:12:32,410
看一些

260
00:12:32,490 --> 00:12:34,180
更复杂的MDP

261
00:12:34,280 --> 00:12:37,590
这就是一个从一本教科书改编的例子

262
00:12:37,680 --> 00:12:39,570
由Stuart Russell和Peter Norvig编写

263
00:12:39,660 --> 00:12:41,410
叫做《人工智能:一种现代方法(第二版)》

264
00:12:41,490 --> 00:12:44,640
这是一个小MDP的模型

265
00:12:44,710 --> 00:12:47,620
机器人导航任务

266
00:12:47,690 --> 00:12:50,170
如果你能想象你有一个机器人

267
00:12:50,250 --> 00:12:54,170
生活在世界网格中

268
00:12:54,250 --> 00:12:55,990
阴暗的单元是一个障碍

269
00:12:56,080 --> 00:12:57,450
所以机器人不能在这个单元中经过

270
00:12:57,530 --> 00:13:06,510
所以  让我们来看看

271
00:13:06,600 --> 00:13:08,990
我想让机器人  到达右上角北的单元

272
00:13:09,040 --> 00:13:11,890
比方说  所以我要用+1奖励来关联那个单元

273
00:13:11,990 --> 00:13:14,400
我想让它避免该网格单元

274
00:13:14,510 --> 00:13:17,520
所以我要使用-1奖励

275
00:13:17,620 --> 00:13:18,820
去关联那个网格单元

276
00:13:18,920 --> 00:13:23,240
所以  我们实际上要遍历MDP的那5个元素

277
00:13:23,310 --> 00:13:25,610
要看看对于这个问题  他们是什么

278
00:13:25,670 --> 00:13:28,460
所以机器人可以在这11个位置中的

279
00:13:28,540 --> 00:13:33,430
任何一个中  所以我有11个状态的MDP

280
00:13:33,490 --> 00:13:36,270
并且它是一组大写字母S

281
00:13:36,340 --> 00:13:39,270
对应的11个可能的到达的位置

282
00:13:39,350 --> 00:13:45,310
比方说  我的机器人在此集合中

283
00:13:45,400 --> 00:13:48,670
高度简化为一个逻辑的例子

284
00:13:48,750 --> 00:13:50,730
可以尝试在每个方向移动方向盘

285
00:13:50,800 --> 00:13:52,080
所以在这MDP中

286
00:13:52,170 --> 00:13:55,270
我将有四个动作  相

287
00:13:55,320 --> 00:13:56,660
应在北  南  东  西指南针方向移动

288
00:13:56,740 --> 00:14:01,840
让我们来看看 比方说

289
00:14:01,910 --> 00:14:03,530
我的机器人的动力比较嘈杂

290
00:14:03,590 --> 00:14:05,560
如果之前从事过机器人相关的工作

291
00:14:05,630 --> 00:14:08,400
你懂的  如果你命令一个机器人去北方

292
00:14:08,490 --> 00:14:13,510
由于车轮打滑或你的行为的核心设计

293
00:14:13,590 --> 00:14:15,450
你的机器人可能会走偏方向

294
00:14:15,530 --> 00:14:17,320
所以  你命令你的机器人向前推进一米

295
00:14:17,400 --> 00:14:19,560
它通常会移动介于

296
00:14:19,640 --> 00:14:21,310
大概95厘米

297
00:14:21,390 --> 00:14:23,760
到105厘米之间

298
00:14:23,860 --> 00:14:26,440
因此  在这个高度简化的网格世界中

299
00:14:26,530 --> 00:14:30,460
我要去建立我的机器人随机动态模型如下

300
00:14:30,550 --> 00:14:32,090
我会说  如果你指挥机器人北上

301
00:14:32,190 --> 00:14:35,680
实际上是一个有10％的机会

302
00:14:35,760 --> 00:14:39,530
使它不小心向左侧偏离

303
00:14:39,630 --> 00:14:41,420
或者10%的几率向右侧偏离

304
00:14:41,520 --> 00:14:44,360
只有0.8的机会

305
00:14:44,440 --> 00:14:46,280
它会向你指挥的方向移动

306
00:14:46,370 --> 00:14:48,350
这是一个粗略的模型

307
00:14:48,440 --> 00:14:50,420
机器人模型上的车轮滑行

308
00:14:50,520 --> 00:14:53,320
如果机器人撞到墙上

309
00:14:53,440 --> 00:14:55,720
那么它只是停留在原地  什么也没有发生

310
00:14:55,830 --> 00:15:01,600
让我们看看 具体来说

311
00:15:01,680 --> 00:15:04,140
我们会写下来使用的状态转移概率

312
00:15:04,230 --> 00:15:07,730
因此  例如  让我们使用这个状态

313
00:15:07,830 --> 00:15:09,920
让我把它叫做 (3  1)状态

314
00:15:10,020 --> 00:15:13,700
比方说  你指挥机器人北上

315
00:15:13,800 --> 00:15:15,970
要指定这些嘈杂的动力的机器人

316
00:15:16,070 --> 00:15:18,590
你会写下如下的机器人状态转移概率

317
00:15:18,670 --> 00:15:20,250
你说  如果你在状态"3 1"

318
00:15:20,360 --> 00:15:22,780
并且你采取行动北上

319
00:15:22,880 --> 00:15:25,590
你到达"3 2"的机会是0.8

320
00:15:25,670 --> 00:15:30,120
如果你在状态"3 1"  你采取行动北上

321
00:15:30,190 --> 00:15:32,990
你到达(4  1)的概率是0.1

322
00:15:33,080 --> 00:15:38,380
依此类推

323
00:15:38,470 --> 00:15:54,840
依此类推  OK?

324
00:15:54,920 --> 00:15:57,080
最后一行是  如果你在状态(3  1)

325
00:15:57,160 --> 00:16:01,410
并且你采取行动北上

326
00:16:01,490 --> 00:16:04,270
你到达状态(3  3)的机会是0

327
00:16:04,360 --> 00:16:05,310
这是一次性的设置的转换概率

328
00:16:05,400 --> 00:16:07,090
状态(3  3)等于零

329
00:16:07,160 --> 00:16:09,980
因此  这些都是我的MDP

330
00:16:10,070 --> 00:16:12,730
的状态转移概率

331
00:16:12,840 --> 00:16:18,980
让我们来看看 最后两个五元组的元素

332
00:16:19,070 --> 00:16:21,560
是伽马和奖励功能

333
00:16:21,660 --> 00:16:24,180
我们先不管伽马

334
00:16:24,300 --> 00:16:26,550
但我的奖励功能如下

335
00:16:26,670 --> 00:16:30,630
所以我真的希望机器人获得的第四个

336
00:16:30,690 --> 00:16:32,590
我使用(4  3)

337
00:16:32,700 --> 00:16:35,500
这是它的索引的状态

338
00:16:35,600 --> 00:16:36,560
通过使用我写在网格的侧的数字

339
00:16:36,650 --> 00:16:39,040
所以我到达状态(4  3)的奖励是+1

340
00:16:39,140 --> 00:16:43,250
并且我到达状态(4  2)的奖励是-1

341
00:16:43,320 --> 00:16:53,410
作为常见的做法是

342
00:16:53,500 --> 00:17:06,240
让我们来看看

343
00:17:06,340 --> 00:17:09,640
在导航任务中  这是相当普遍的做法

344
00:17:09,740 --> 00:17:12,530
对所有其他状态  终端状态

345
00:17:12,620 --> 00:17:14,760
我要关联一个小的负面奖励

346
00:17:14,850 --> 00:17:17,340
你可以认为这作为一个小的负报酬:

347
00:17:17,400 --> 00:17:20,910
它的电池消耗或它

348
00:17:21,090 --> 00:17:22,590
每移动一步的燃料消费量

349
00:17:22,670 --> 00:17:27,060
对于这样一个小的负面奖励

350
00:17:27,150 --> 00:17:28,500
使机器人可以随意跑动

351
00:17:28,580 --> 00:17:31,650
促使系统计算解决方案

352
00:17:31,760 --> 00:17:33,630
不要浪费时间

353
00:17:33,710 --> 00:17:36,890
并且使其尽快达到目标

354
00:17:36,990 --> 00:17:38,230
因为它收取燃油消耗费用

355
00:17:38,330 --> 00:17:48,380
OK 所以  好了  让我说说

356
00:17:48,470 --> 00:17:51,520
实际上一个其他的复杂情况

357
00:17:51,590 --> 00:17:54,190
我不需要太担心它

358
00:17:54,270 --> 00:17:56,000
在这个具体的例子中

359
00:17:56,090 --> 00:17:57,920
除非你要假设  当机器人

360
00:17:58,030 --> 00:18:01,660
到达的+1或-1奖励时

361
00:18:01,750 --> 00:18:04,220
那么过程的结束  所以你到达+1

362
00:18:04,310 --> 00:18:06,030
就是这样 处理过程结束 在那之后

363
00:18:06,130 --> 00:18:08,980
没有更多的正的或者负的奖励  对不对?

364
00:18:09,070 --> 00:18:10,720
这里有着各种方法来建立模型

365
00:18:10,810 --> 00:18:15,070
方式之一是  你可能想象

366
00:18:15,170 --> 00:18:16,700
有实际上有12个状态

367
00:18:16,800 --> 00:18:18,580
这被称为零成本吸收状态

368
00:18:18,670 --> 00:18:21,070
使每当你得到+1或-1

369
00:18:21,150 --> 00:18:24,670
然后你过渡到这12个状态之一

370
00:18:24,740 --> 00:18:26,830
并且你永远留在这12个状态

371
00:18:26,940 --> 00:18:29,570
没有更多的回报 我刚才提到

372
00:18:29,660 --> 00:18:31,490
当你到达+1或-1  思考将要完成的问题

373
00:18:31,580 --> 00:18:35,870
我之所以这样做

374
00:18:35,970 --> 00:18:38,270
是因为它使一些数字变得更利于运算

375
00:18:38,370 --> 00:18:42,050
更容易理解 这是一个状态

376
00:18:42,130 --> 00:18:44,760
你去哪里  有时你会听到

377
00:18:44,870 --> 00:18:47,580
零成本吸收状态

378
00:18:47,660 --> 00:18:49,390
这是另一种状态

379
00:18:49,460 --> 00:18:51,010
所以  当你进入该状态

380
00:18:51,110 --> 00:18:52,670
没有更多的回报 你永远留在该状态

381
00:18:59,190 --> 00:19:00,470
好的 因此  让我们来看看

382
00:19:00,580 --> 00:19:04,580
MDP是如何工作的  它的工作原理如下:

383
00:19:04,670 --> 00:19:10,090
在时间0  你的机器人开始在状态0出发

384
00:19:10,200 --> 00:19:14,310
取决于你在哪个地方

385
00:19:14,390 --> 00:19:19,370
你可以选择一个动作A0决定

386
00:19:19,450 --> 00:19:22,980
去北  南  东或西 根据你的选择

387
00:19:23,060 --> 00:19:26,640
你会得到一个状态S1

388
00:19:26,710 --> 00:19:28,470
这是通过状态0和你选择的动作

389
00:19:28,560 --> 00:19:32,230
随机提取过渡分布指数

390
00:19:32,310 --> 00:19:33,630
因此  你的下一个状态取决于

391
00:19:33,720 --> 00:19:36,580
好了  它取决于你先前的

392
00:19:36,650 --> 00:19:39,990
状态和你采取的行动上的概率

393
00:19:40,090 --> 00:19:41,740
在你到达状态S1之后

394
00:19:41,830 --> 00:19:47,430
你选择一个新的行动A1

395
00:19:47,510 --> 00:19:51,260
然后作为它的结果

396
00:19:51,360 --> 00:19:57,190
你得到一些新的状态S2

397
00:19:57,260 --> 00:20:01,430
随机状态转换分布  等等 OK?

398
00:20:20,430 --> 00:20:22,960
你的机器人处理了一段时间之后

399
00:20:23,050 --> 00:20:26,470
将走访了一些状态序列S0

400
00:20:26,550 --> 00:20:34,170
S1  S2  依此类推  并评估

401
00:20:34,260 --> 00:20:39,270
我们做的好不好  我们将

402
00:20:39,370 --> 00:20:47,310
使用奖励功能  我们将它

403
00:20:47,390 --> 00:20:49,850
应用到状态序列  并且计算机器人

404
00:20:49,950 --> 00:20:52,300
获得它访问的奖励序列的总和

405
00:20:52,370 --> 00:20:55,880
状态S0是你的行动  你会得到状态S1

406
00:20:55,960 --> 00:20:58,270
采取行动  你就会得到S2等

407
00:20:58,370 --> 00:21:00,190
所以你保持奖励功能序列中的每个状态

408
00:21:00,290 --> 00:21:02,570
这就是你获得的奖励的总和

409
00:21:02,650 --> 00:21:04,910
让我教给你们多一点

410
00:21:05,000 --> 00:21:07,540
你可以用伽玛乘以它  伽玛平方

411
00:21:07,650 --> 00:21:09,960
下一轮将会乘以伽玛的立方

412
00:21:10,060 --> 00:21:14,030
这就是所谓的

413
00:21:14,120 --> 00:21:20,510
我把它叫做机器人访问的

414
00:21:20,620 --> 00:21:27,220
状态序列S0  S1  S2……的总回报

415
00:21:27,310 --> 00:21:32,620
因此  让我也说说Gamma是什么

416
00:21:32,690 --> 00:21:33,590
质量Gamma是一个数字

417
00:21:33,680 --> 00:21:37,170
通常小于1

418
00:21:37,260 --> 00:21:38,860
它通常是一个像0.99的数字

419
00:21:38,960 --> 00:21:43,650
因此  伽玛效果表示

420
00:21:43,770 --> 00:21:47,660
你在时间1获得的奖励

421
00:21:47,740 --> 00:21:49,740
略小于你在时间0得到的奖励

422
00:21:49,820 --> 00:21:52,380
然后你在时间2获得的奖励

423
00:21:52,490 --> 00:21:54,460
略小于你在之前的时间

424
00:21:54,560 --> 00:21:58,810
集得到的奖励

425
00:21:58,900 --> 00:22:07,260
让我们来看看 所以  如果这是一个

426
00:22:07,360 --> 00:22:11,310
经济应用程序  那么你就像

427
00:22:11,400 --> 00:22:13,170
使用高斯算法做股票的市场交易

428
00:22:13,260 --> 00:22:15,330
这是一个经济应用程序

429
00:22:15,410 --> 00:22:20,160
那么你的回报是赢得和失去美元

430
00:22:20,250 --> 00:22:22,290
然后对于这种因子

431
00:22:22,380 --> 00:22:25,260
γ可以自然地解释为美元货币的时间价值

432
00:22:25,350 --> 00:22:28,170
因为美元今天的价值略低于

433
00:22:28,240 --> 00:22:29,420
对不起  今天的美元价值

434
00:22:29,500 --> 00:22:31,070
比明天的美元价值略多

435
00:22:31,180 --> 00:22:34,050
因为美元在银行可以赚一点点的利息

436
00:22:34,130 --> 00:22:37,040
与此相反  明天支付一美元

437
00:22:37,130 --> 00:22:39,730
比今天支付一美元更好

438
00:22:39,820 --> 00:22:43,440
所以换句话说

439
00:22:43,540 --> 00:22:46,160
该压缩伽玛效果趋于加重WINS

440
00:22:46,270 --> 00:22:50,530
或者说在未来的WINS或亏损

441
00:22:50,630 --> 00:22:53,060
比现在的WINS或亏损要少

442
00:22:53,160 --> 00:22:55,710
在遥远的未来  趋于重量WINS

443
00:22:55,810 --> 00:22:58,350
和亏损损失少于现在的WINS和亏损

444
00:22:58,450 --> 00:23:05,010
强化学习算法的周期

445
00:23:05,110 --> 00:23:07,980
是随着时间的推移而选择行动

446
00:23:08,070 --> 00:23:16,550
选择行动A0  A1等  尽量让这个

447
00:23:16,630 --> 00:23:23,150
总收益的预期值最大化

448
00:23:23,250 --> 00:23:39,260
而且更具体地讲

449
00:23:39,360 --> 00:23:44,370
我们会尽量使我们的强化学习算法

450
00:23:44,450 --> 00:23:46,900
计算出一个政策

451
00:23:46,990 --> 00:23:54,780
我用小写p表示它

452
00:23:54,880 --> 00:24:01,610
所有的政策  政策的定义

453
00:24:01,690 --> 00:24:04,150
是行动状态的函数映射

454
00:24:04,250 --> 00:24:06,860
所以它是一个政策

455
00:24:06,950 --> 00:24:10,700
告诉我们每一个状态

456
00:24:10,800 --> 00:24:16,170
建议我们在该状态采取什么行动

457
00:24:16,260 --> 00:24:26,770
所以具体地说  这里是一个政策例子

458
00:24:26,860 --> 00:24:29,280
这实际上是MDP的最佳政策

459
00:24:29,370 --> 00:24:34,790
我等下会告诉你

460
00:24:34,900 --> 00:24:51,720
我是如何计算它的 这是政策的一个例子

461
00:24:51,820 --> 00:24:54,260
一个政策只是一个状态到行动的映射

462
00:24:54,350 --> 00:24:56,790
所以我们的政策告诉我

463
00:24:56,890 --> 00:24:58,910
当你在这种状态下的时候  你应该采取

464
00:24:59,000 --> 00:25:03,520
左侧的行动等 我抽出的这个特殊的政策

465
00:25:03,600 --> 00:25:06,360
在这个意义上  正好是你执行

466
00:25:06,440 --> 00:25:08,980
这一政策之后的政策

467
00:25:09,080 --> 00:25:11,220
这将最大限度地提高你的

468
00:25:11,300 --> 00:25:13,810
总收益的预期价值 这将最大限度

469
00:25:13,910 --> 00:25:16,050
地提高您预期的计数器的奖励的总和

470
00:25:16,150 --> 00:25:20,470
学生:这个政策可以是多重的吗

471
00:25:20,570 --> 00:25:22,780
呀  那么政策可以是多个状态么

472
00:25:22,890 --> 00:25:25,680
它可以--是一个函数

473
00:25:25,770 --> 00:25:26,710
但不仅有当前状态

474
00:25:26,780 --> 00:25:28,120
而是我之前的状态么

475
00:25:28,210 --> 00:25:30,570
因此  答案是肯定的

476
00:25:30,650 --> 00:25:33,160
有时  人们称他们为策略

477
00:25:33,250 --> 00:25:35,490
而不是政策  但通常你会

478
00:25:35,570 --> 00:25:39,710
使用政策 实际上对于MDP

479
00:25:39,810 --> 00:25:43,450
你允许的政策取决于我以前的状态

480
00:25:43,530 --> 00:25:45,490
不会让你做更好的处理

481
00:25:45,550 --> 00:25:47,750
至少在我们谈论的有限的范围内

482
00:25:47,850 --> 00:25:50,180
所以  换句话说  存在一个政策

483
00:25:50,250 --> 00:25:52,250
基于目前的状态

484
00:25:52,330 --> 00:25:54,810
使我的预期总收益最大化

485
00:25:54,890 --> 00:25:56,780
该声明对于一些更丰富的模型不会成功

486
00:25:56,890 --> 00:25:58,820
我们一会儿谈  但现在

487
00:25:58,920 --> 00:26:00,820
所有我们需要做的是--这就够了

488
00:26:00,900 --> 00:26:01,690
只是看目前的状态和动作

489
00:26:01,780 --> 00:26:06,940
有时他们使用的项

490
00:26:07,020 --> 00:26:08,950
"可执行的政策"

491
00:26:09,030 --> 00:26:10,690
意味着  我会根据政策采取行动

492
00:26:10,770 --> 00:26:12,990
所以我要执行的政策p

493
00:26:13,070 --> 00:26:16,350
但这意味着我要

494
00:26:16,460 --> 00:26:21,990
些状态s时  鉴于目前的状态

495
00:26:22,090 --> 00:26:23,500
我会采取行动  使政策p输出

496
00:26:23,620 --> 00:26:29,780
好的 结果MDP很擅长的一件事情之一是

497
00:26:29,860 --> 00:26:34,050
好的  让我们来看看我们的状态

498
00:26:34,130 --> 00:26:38,330
说在此状态下的"最佳政策"是向左走

499
00:26:38,430 --> 00:26:41,970
实际上  有--这可能不是很明显

500
00:26:42,050 --> 00:26:43,810
为什么  你向左走的动作

501
00:26:43,910 --> 00:26:46,760
需要较长的路径呢?

502
00:26:46,850 --> 00:26:48,610
另一种方法将北上

503
00:26:48,690 --> 00:26:50,480
试图找到一个更短的路径到达+1状态

504
00:26:50,560 --> 00:26:53,520
但在这种状态下  当你在这里

505
00:26:53,610 --> 00:26:58,840
我猜想  (3  2)的状态

506
00:26:58,910 --> 00:27:01,080
在该状态时  在那里  当你往北走

507
00:27:01,150 --> 00:27:02,950
有一个0.1的机会

508
00:27:03,030 --> 00:27:06,070
你不小心偏离到-1状态

509
00:27:06,160 --> 00:27:07,800
所以会有微妙的折中方案

510
00:27:07,890 --> 00:27:10,210
采取更长  更安全的路线是否更好呢?

511
00:27:10,320 --> 00:27:13,480
但贴现因子往往劝阻那样做

512
00:27:13,580 --> 00:27:15,800
每步0.02的费用往往会阻止它 还

513
00:27:15,880 --> 00:27:17,570
是更愿意采取较短  风险较高的路线呢

514
00:27:17,680 --> 00:27:21,850
因此  这是对于我来说

515
00:27:21,930 --> 00:27:25,930
不是显而易见的  直到我计算它

516
00:27:26,010 --> 00:27:26,960
但只要看看行动

517
00:27:27,030 --> 00:27:28,160
这是MDP机器非常擅长的一件事情

518
00:27:28,250 --> 00:27:30,000
处理微妙的折中方案  以使它们最佳化

519
00:27:30,090 --> 00:27:36,790
所以我想下一步要做的是

520
00:27:36,900 --> 00:27:38,870
多整理几个定义

521
00:27:38,970 --> 00:27:41,730
这将导致我们的第一种算法

522
00:27:41,810 --> 00:27:43,710
用于计算最优的政策和MDP

523
00:27:43,780 --> 00:27:46,300
所以在MDP上寻找最佳方式

524
00:27:46,390 --> 00:27:48,390
在我行动之前  让我们来检查

525
00:27:58,710 --> 00:27:59,660
有关MDP体系的问题 好吧  酷

526
00:27:59,760 --> 00:28:04,050
现在让我们谈谈我们

527
00:28:04,150 --> 00:28:06,580
如何去计算这样的最优政策

528
00:28:06,700 --> 00:28:09,200
到达那一步  我需要定义一些事情

529
00:28:09,300 --> 00:28:21,420
因此  先预览一下我将要

530
00:28:21,520 --> 00:28:24,310
去采取的下一步行动

531
00:28:24,390 --> 00:28:28,050
我定义了一个称为VΠ的东西

532
00:28:28,160 --> 00:28:32,640
然后我要去定义V *  然后我要去定义p

533
00:28:32,720 --> 00:28:35,600
它将是我定义的结果

534
00:28:35,690 --> 00:28:37,230
即P *是最佳的政策

535
00:28:37,340 --> 00:28:39,720
所以我说我定义了这些东西

536
00:28:39,810 --> 00:28:42,420
记住定义是什么

537
00:28:42,530 --> 00:28:44,960
定义的结果是什么

538
00:28:45,080 --> 00:28:48,430
特别的  我不会定义P *为最佳的政策

539
00:28:48,540 --> 00:28:50,920
但我会通过不同的方程定义P

540
00:28:51,010 --> 00:28:53,030
并且这将是我的定义的结果

541
00:28:53,120 --> 00:28:54,620
P *是最优的政策

542
00:28:54,710 --> 00:29:00,500
第一个我要定义的是Vπ

543
00:29:00,590 --> 00:29:06,910
所以对于任何给定的政策p

544
00:29:07,020 --> 00:29:15,020
任何政策p  我要定义值函数VΠ

545
00:29:15,120 --> 00:29:18,580
有时我把它称为p的值函数

546
00:29:18,690 --> 00:29:20,710
所以我想找到Vπ的值函数

547
00:29:20,820 --> 00:29:25,240
从已知数字的映射状态函数

548
00:29:25,320 --> 00:29:31,650
这样VΠ(S)的预期收益

549
00:29:31,760 --> 00:29:41,950
是预期总收益

550
00:29:42,040 --> 00:29:53,560
如果你在状态s开始执行p

551
00:29:53,680 --> 00:29:58,430
所以换句话说  VΠ(S)等于

552
00:29:58,490 --> 00:30:01,230
这里的这个预期值

553
00:30:01,340 --> 00:30:07,070
这个计数奖励的总和  总回报

554
00:30:07,170 --> 00:30:12,120
你执行政策p

555
00:30:12,220 --> 00:30:14,510
并且序列中的第一个状态是零  是状态s

556
00:30:14,630 --> 00:30:17,650
我说这是稍微粗糙的概率符号

557
00:30:17,770 --> 00:30:20,560
所以p不是真的在变量附近

558
00:30:20,640 --> 00:30:22,050
所以也许我不应该以p为条件

559
00:30:22,140 --> 00:30:26,530
这是中等标准的符号

560
00:30:26,620 --> 00:30:28,250
所以我们使用稳定的

561
00:30:28,340 --> 00:30:29,450
粗糙的政策符号

562
00:30:32,040 --> 00:31:11,980
因此  作为一个具体的例子

563
00:31:12,060 --> 00:31:14,850
这里是一个政策

564
00:31:14,930 --> 00:31:16,210
这是不是一个伟大的政策

565
00:31:16,300 --> 00:31:17,870
这仅仅是一些政策p

566
00:31:17,950 --> 00:31:19,810
它实际上是一个非常糟糕的政策

567
00:31:19,890 --> 00:31:21,090
对许多状态来说  似乎是走向-1

568
00:31:24,910 --> 00:31:27,530
而不是在+1 所以价值函数是

569
00:31:27,610 --> 00:31:29,160
从已知的数字状态的映射

570
00:31:29,240 --> 00:31:33,380
因此它用一个数字关联每个状态

571
00:31:33,450 --> 00:31:37,800
并且在这种情况下  这是Vπ

572
00:31:58,930 --> 00:32:01,730
因此  那就是这个政策的价值函数

573
00:32:01,830 --> 00:32:05,690
你会注意到  例如  对于底下

574
00:32:05,770 --> 00:32:09,990
两行所有的状态  我猜

575
00:32:10,060 --> 00:32:12,590
这是一个非常糟糕的政策

576
00:32:12,680 --> 00:32:14,970
具有较高的机会带你到-1的状态

577
00:32:15,080 --> 00:32:19,850
因此  底下两行所有的状态的值都是负的

578
00:32:19,940 --> 00:32:22,770
因此  在这种期望中

579
00:32:22,890 --> 00:32:25,470
你的总收益将为负数

580
00:32:25,560 --> 00:32:28,050
如果执行这个相当糟糕政策

581
00:32:28,140 --> 00:32:30,190
你在最后一行的任何状态

582
00:32:30,270 --> 00:32:32,480
如果你在最上面一行开始

583
00:32:32,580 --> 00:32:34,980
总收益将是正的 这不是一种可怕的

584
00:32:35,090 --> 00:32:36,890
坏的政策  如果它停留在最上面的行

585
00:32:37,000 --> 00:32:43,730
因此  任何给定政策

586
00:32:43,830 --> 00:32:53,180
你可以写下了这一政策的价值函数

587
00:32:53,280 --> 00:32:54,770
如果你们有些人还在写

588
00:32:54,870 --> 00:32:56,590
我将保持它一会儿  我来擦另外两块黑板

589
00:32:56,700 --> 00:33:19,400
好了 所以以下的情况下

590
00:33:19,520 --> 00:33:26,030
Vπ(S)等于--好  R的预期值

591
00:33:26,140 --> 00:33:30,210
如果S是零

592
00:33:30,310 --> 00:33:33,830
这是只是初始状态S时

593
00:33:33,950 --> 00:33:37,200
立即获得的奖励

594
00:33:37,310 --> 00:33:41,370
让我这样写--我会写γ

595
00:33:41,490 --> 00:33:45,610
然后R S1+γ  R S2+  点  点  点

596
00:33:45,710 --> 00:33:59,030
p的条件  OK?

597
00:34:07,600 --> 00:34:09,670
所以去掉这些括号

598
00:34:09,780 --> 00:34:13,100
这里的第一个项  有时也被

599
00:34:13,200 --> 00:34:15,070
称为即时奖励

600
00:34:15,170 --> 00:34:17,580
这是你从零状态开始立即得到的报酬

601
00:34:17,680 --> 00:34:20,230
然后这里的第二个项

602
00:34:20,320 --> 00:34:22,280
这些有时被称为未来的奖励

603
00:34:22,380 --> 00:34:24,790
这是你在未来的一个时间步长得到的回报

604
00:34:24,880 --> 00:34:29,340
我想要你注意的是

605
00:34:29,470 --> 00:34:32,280
这个项是什么 这个项实际上

606
00:34:32,390 --> 00:34:41,290
是状态S1的值函数

607
00:34:41,390 --> 00:34:43,170
因为这个括号中的项

608
00:34:43,250 --> 00:34:44,730
这的确是--假如我是在状态S1开始

609
00:34:44,810 --> 00:34:47,980
这是我会得到的计数器回报的总和

610
00:34:48,080 --> 00:34:50,250
如果我在状态S1开始

611
00:34:50,330 --> 00:34:52,290
那么  我在S1开始的立即回报

612
00:34:52,360 --> 00:34:54,710
将是R(S1)  然后加上γ

613
00:34:54,810 --> 00:34:56,800
乘以未来的额外未来奖励

614
00:34:56,920 --> 00:35:04,850
所以结果你可以在项中写VRp递归

615
00:35:04,930 --> 00:35:13,260
假定VRp等于立即奖励+伽玛乘以

616
00:35:13,350 --> 00:35:17,090
其实  让我写的

617
00:35:17,180 --> 00:35:24,370
这纯粹是作为符号的映射

618
00:35:24,480 --> 00:35:27,250
S0映射到S

619
00:35:27,360 --> 00:35:29,750
S1映射到S`

620
00:35:29,850 --> 00:35:31,390
让它们都相互对应

621
00:35:31,500 --> 00:35:35,790
因此  p到状态0的价值函数是即时奖励

622
00:35:35,880 --> 00:35:38,110
加上这个因素伽玛乘以

623
00:35:38,220 --> 00:35:43,860
现在你有S1的Vπ 这里S`的Vπ

624
00:35:43,950 --> 00:35:47,370
但S`是一个随机变量

625
00:35:47,490 --> 00:35:50,870
因为你到后一个时间集时

626
00:35:50,980 --> 00:35:53,890
下一个状态是随机的

627
00:35:53,990 --> 00:35:57,150
所以特别的  采用期望

628
00:35:57,260 --> 00:36:04,920
这是你到达那个状态S`的

629
00:36:05,010 --> 00:36:19,750
概率的总和 为了明确这个符号

630
00:36:19,860 --> 00:36:22,260
好的  这是p prime的p下标(S  A)

631
00:36:22,350 --> 00:36:25,410
是你当在状态s时

632
00:36:25,510 --> 00:36:27,210
你采取行动a

633
00:36:27,310 --> 00:36:28,540
到达状态S`的机会

634
00:36:28,660 --> 00:36:31,730
在这种情况下  我们执行政策p

635
00:36:31,850 --> 00:36:38,210
所以这是P(S)  p(s)  因为我们

636
00:36:38,290 --> 00:36:41,040
要在状态s采取的行动是行动p分之一

637
00:36:41,140 --> 00:36:46,710
因此  这是--换句话说  这个PS

638
00:36:46,810 --> 00:36:51,750
p(s)  你会转移到这个分配夸大S`

639
00:36:51,850 --> 00:36:54,130
一个时间步长  当你在状态s时

640
00:36:54,200 --> 00:36:58,470
采取行动P(S)

641
00:36:58,570 --> 00:37:04,990
所以为了给它一个名称

642
00:37:05,080 --> 00:37:12,520
这个方程被称为贝尔曼方程

643
00:37:12,620 --> 00:37:15,320
是当我们解决MDP问题时

644
00:37:15,440 --> 00:37:25,370
将要经常用到的主要方程之一

645
00:37:25,450 --> 00:37:30,180
如果明白了这个方程  请举手

646
00:37:30,730 --> 00:37:33,090
你们当中有些人没有举手 你有问题吗?

647
00:37:33,190 --> 00:37:45,610
因此  让我们再重复一遍

648
00:37:45,690 --> 00:37:47,400
其实  对于那些没有举手同学

649
00:37:47,480 --> 00:37:50,920
那些符号是不明白的呢?

650
00:37:51,000 --> 00:37:53,070
你现在后悔没举手  不是吗?

651
00:37:53,150 --> 00:37:57,680
让我们试着再说一遍

652
00:37:57,780 --> 00:38:03,550
也许等下它会更加清晰 那么它是什么呢?

653
00:38:03,670 --> 00:38:07,120
因此  这个等式是有点像

654
00:38:07,200 --> 00:38:12,720
我当前的状态值等于R(S)+γ

655
00:38:12,800 --> 00:38:15,980
并根据我得到的下一个状态

656
00:38:16,060 --> 00:38:21,420
我从状态S`得到的

657
00:38:21,520 --> 00:38:26,660
预期总回报是Vπ(S`)

658
00:38:26,750 --> 00:38:28,670
然而prime是后一个时间步长的状态

659
00:38:28,760 --> 00:38:31,720
所以我要去采取一些行动

660
00:38:31,780 --> 00:38:33,700
我也得到一些状态的prime

661
00:38:33,800 --> 00:38:36,310
这个等式是我的预期总收益

662
00:38:36,390 --> 00:38:37,360
用于在状态s

663
00:38:37,460 --> 00:38:38,670
执行政策p

664
00:38:38,770 --> 00:38:43,520
不过S`是随机的  因为我得到的

665
00:38:43,600 --> 00:38:47,660
下一个状态是随机的  好的

666
00:38:47,740 --> 00:38:49,660
我们将使用下一块黑板

667
00:38:49,760 --> 00:38:53,690
我得到一些具体的状态S`的机会

668
00:38:53,770 --> 00:38:58,100
是通过我们有一个P下标(S  A)S`

669
00:38:58,170 --> 00:38:59,270
因为这些都是【无声】

670
00:38:59,370 --> 00:39:04,660
并且概率--我选择的行动是由P( s)

671
00:39:04,740 --> 00:39:07,770
给出的  因为我在当前的状态s执行行动a

672
00:39:07,880 --> 00:39:11,670
当你插入这个

673
00:39:11,760 --> 00:39:15,530
你会得到p下标s R(S)

674
00:39:15,630 --> 00:39:17,960
作为prime

675
00:39:18,040 --> 00:39:19,810
我只是给出了一步过渡的状态分布

676
00:39:19,900 --> 00:39:22,260
因此  只需要贝尔曼方程

677
00:39:22,350 --> 00:39:30,320
所以既然贝尔曼方程为你提供了

678
00:39:30,400 --> 00:39:36,350
一种解决政策价值函数的封闭形式的方式

679
00:39:36,450 --> 00:39:39,030
再次  问题是  假设我给一个固定的政策

680
00:39:39,120 --> 00:39:41,350
我该如何解出Vπ?

681
00:39:41,450 --> 00:39:46,890
我该如何解决

682
00:39:46,980 --> 00:39:49,410
给予固定的政策  我怎么解这个等式?

683
00:39:49,560 --> 00:39:52,290
事实证明  贝尔曼方程为你

684
00:39:52,370 --> 00:39:54,880
提供了一个这样做的方式

685
00:39:54,980 --> 00:39:59,160
所以回到这个黑板  它原来是

686
00:39:59,260 --> 00:40:06,290
回来之前的黑板  周围

687
00:40:06,390 --> 00:40:07,890
你可以将摄像机移动到指向这个黑板吗?

688
00:40:07,980 --> 00:40:11,270
好的  酷 回到这个黑板

689
00:40:11,360 --> 00:40:18,840
我们处理这个问题的方程

690
00:40:18,940 --> 00:40:20,720
这个方程  好的  让我们假设

691
00:40:20,810 --> 00:40:22,360
有一个固定的政策p

692
00:40:22,450 --> 00:40:27,070
我想解决政策p的值函数

693
00:40:27,160 --> 00:40:29,720
那么这个等式只是在价值函数上

694
00:40:29,820 --> 00:40:31,900
施加了线性约束集 所以  尤其是说

695
00:40:31,980 --> 00:40:34,800
对于一个给定的状态的值

696
00:40:34,890 --> 00:40:37,860
等于某个常数  然后一些其他值的线性函数

697
00:40:37,960 --> 00:40:41,990
所以你可以在MDP中

698
00:40:42,090 --> 00:40:45,120
为每一个状态写下方程

699
00:40:45,210 --> 00:40:48,330
这对价值功能加以线性约束

700
00:40:48,430 --> 00:40:50,960
然后事实证明

701
00:40:51,060 --> 00:40:54,080
通过求解线性系统方程

702
00:40:54,190 --> 00:40:55,370
然后你可以解出价值功能Vπ(s)

703
00:40:55,470 --> 00:40:58,740
有一个高层次的描述

704
00:40:58,830 --> 00:41:00,430
现在让我具体化它

705
00:41:00,540 --> 00:41:14,070
那么特别地  让我使用(3  1)状态

706
00:41:14,170 --> 00:41:18,680
我们使用那个状态作为例子

707
00:41:18,790 --> 00:41:21,550
因此  贝尔曼方程告诉我说

708
00:41:21,650 --> 00:41:25,970
对于状态(3  1)的P的状态值

709
00:41:26,050 --> 00:41:28,590
哦  比如说  我有具体的政策  p是(3  1)

710
00:41:28,680 --> 00:41:33,150
让我们说  它需要一个朝北的行动

711
00:41:33,230 --> 00:41:35,620
这不是最终的行动

712
00:41:35,740 --> 00:41:39,060
对于这项政策  贝尔曼方程告诉我

713
00:41:39,190 --> 00:41:44,750
(3  1)的Vπ等于状态(3  1)的R

714
00:41:44,860 --> 00:41:50,800
然后加上γ*0.8  使我到达状态(3  2)

715
00:41:50,900 --> 00:41:58,870
相当于0.1到达状态(4  1)

716
00:41:58,960 --> 00:42:11,770
它将乘以0.1

717
00:42:11,860 --> 00:42:16,330
达到状态(2  1)

718
00:42:16,430 --> 00:42:18,550
我所做的就是  我写下(3  1)状态的

719
00:42:18,650 --> 00:42:20,470
贝尔曼方程 我希望你知道

720
00:42:20,560 --> 00:42:23,400
它的意思是什么 它在我的低MDP中

721
00:42:23,500 --> 00:42:27,160
我索引的状态1  2  3  4

722
00:42:27,270 --> 00:42:31,250
所以那边的这个状态

723
00:42:31,350 --> 00:42:33,360
我画的圆圈的地方是(3  1)状态

724
00:42:33,440 --> 00:42:37,770
因此  对于MDP中11个状态中的任意一个

725
00:42:37,880 --> 00:42:42,050
我可以写下这样的公式

726
00:42:42,130 --> 00:42:44,760
这只是代表一个状态

727
00:42:44,840 --> 00:42:49,110
你会发现  如果我试图解出那个值

728
00:42:49,200 --> 00:42:54,810
因此  这些都是未知数

729
00:42:54,900 --> 00:42:57,770
那么我将有11个变量

730
00:42:57,870 --> 00:42:59,810
因为我想解出我的11个状态的价值函数

731
00:42:59,930 --> 00:43:03,280
我将有11个限制

732
00:43:03,370 --> 00:43:07,150
因为我可以写下11这种形式的方程

733
00:43:07,250 --> 00:43:08,940
每一个我的状态方程

734
00:43:09,040 --> 00:43:11,120
因此  如果你这样做

735
00:43:11,210 --> 00:43:12,650
如果你写下这每一个状态的这种公式

736
00:43:12,740 --> 00:43:16,010
然后这样处理  你会有11个

737
00:43:16,110 --> 00:43:18,950
未知数和11个变量的线性方程组

738
00:43:19,050 --> 00:43:22,130
对不起  11个约束  或者有11个未知数的

739
00:43:22,220 --> 00:43:24,700
11个方程  所以你可以

740
00:43:24,830 --> 00:43:28,850
解出那个线性系统方程组

741
00:43:28,930 --> 00:43:30,430
得到一个明确Vπ的解答

742
00:43:30,550 --> 00:43:34,040
所以如果你有N个状态

743
00:43:34,150 --> 00:43:36,490
你最终与n个方程和N未知数

744
00:43:36,570 --> 00:43:38,890
然后要解出它  得到所有状态的值

745
00:43:38,980 --> 00:43:50,010
好的  酷 事实上  如果你明白了这些

746
00:43:50,090 --> 00:43:53,210
可以举手吗?酷

747
00:43:53,310 --> 00:43:59,950
好的  所以这是具体政策的价值函数

748
00:44:00,040 --> 00:44:02,850
以及如何解决它 让我再确定一件事

749
00:44:02,940 --> 00:44:14,070
因此  最优值函数定义为V *(S)

750
00:44:14,220 --> 00:44:21,290
等于在所有政策P的最大Vπ(s)

751
00:44:21,370 --> 00:44:25,800
因此  换句话说

752
00:44:25,900 --> 00:44:28,210
对于任何给定的状态s

753
00:44:28,290 --> 00:44:29,470
最优值函数是说

754
00:44:29,580 --> 00:44:32,030
假设我对于所有可能的

755
00:44:32,120 --> 00:44:34,660
政策P选择一个最优值

756
00:44:34,760 --> 00:44:39,030
什么是最好的预期

757
00:44:39,120 --> 00:44:40,810
些可以期望获得的计数回报?

758
00:44:40,890 --> 00:44:41,720
或者对于开始状态

759
00:44:41,800 --> 00:44:44,240
我的最佳预期总回报是什么

760
00:44:44,330 --> 00:44:46,010
所以采取了对于

761
00:44:46,110 --> 00:44:48,390
所有可能的政策P的最优值

762
00:44:50,610 --> 00:44:52,890
所以事实证明  对于V

763
00:44:52,970 --> 00:44:56,470
有一个贝尔曼方程的版本

764
00:44:56,560 --> 00:45:02,250
以及这也叫做V *贝尔曼方程

765
00:45:02,370 --> 00:45:04,530
而不Vπ  我将把它写下来

766
00:45:04,640 --> 00:45:09,040
这就是说  你可以从状态s得到的

767
00:45:09,130 --> 00:45:16,460
最佳回报等于--所以我在

768
00:45:16,530 --> 00:45:18,530
这里要先多写几组 在这里是多的

769
00:45:18,600 --> 00:45:20,570
让我们看看 只要在状态s出发

770
00:45:20,670 --> 00:45:25,370
你会立即得到R(s)  然后根据

771
00:45:25,470 --> 00:45:29,680
你采取的行动  你的预期

772
00:45:29,760 --> 00:45:36,720
总收益由它给出 所以

773
00:45:36,840 --> 00:45:39,950
如果我在状态s下采取行动a

774
00:45:40,030 --> 00:45:43,510
然后用S`的p下标(s; a)给出的概率

775
00:45:43,590 --> 00:45:45,080
通过这个概率  我们的状态S`

776
00:45:45,170 --> 00:45:47,560
并且当我们到了状态S`

777
00:45:47,650 --> 00:45:50,170
我会期望从那里得到总回报

778
00:45:50,250 --> 00:45:51,900
由V *( S)prime给出

779
00:45:51,990 --> 00:45:53,410
因为我现在也开始使用S`

780
00:45:53,500 --> 00:45:55,470
所以在这个等式中

781
00:45:55,570 --> 00:45:58,750
我唯一需要填写的是行动a

782
00:45:58,850 --> 00:46:01,590
以获得最佳的预期回报

783
00:46:01,690 --> 00:46:05,730
并获得的最大或最优的预期总收益

784
00:46:05,820 --> 00:46:09,030
你应该选择在这里

785
00:46:09,120 --> 00:46:11,470
是我们的行动的最优值

786
00:46:11,570 --> 00:46:16,630
选择你的行动a

787
00:46:16,710 --> 00:46:18,610
最大限度地提高你的总收益的预期值

788
00:46:18,750 --> 00:46:21,110
那么明白了没有

789
00:46:21,200 --> 00:46:23,510
这里有一个V *贝尔曼方程的版本

790
00:46:23,600 --> 00:46:26,800
而不Vπ  我将再重复一遍  它说

791
00:46:26,890 --> 00:46:28,100
我的最佳预期的总收益

792
00:46:28,200 --> 00:46:31,310
是我的即时奖励+

793
00:46:31,390 --> 00:46:33,710
然后它可以选择的最好的行动

794
00:46:33,770 --> 00:46:36,240
使我预计的未来收益最大化

795
00:46:39,840 --> 00:46:44,610
而这也引出了我的p*的定义

796
00:46:44,720 --> 00:46:51,530
这是比方说  我在状态s

797
00:46:51,640 --> 00:46:55,690
我想知道选择什么样的行动

798
00:46:56,230 --> 00:46:59,520
那么  如果我在状态s的时候

799
00:46:59,610 --> 00:47:02,440
我将在这里立即得到R(S)

800
00:47:02,540 --> 00:47:04,700
所以我选择的最佳行动是

801
00:47:04,790 --> 00:47:07,710
能够使我最大化第二个时间步长

802
00:47:07,780 --> 00:47:11,480
以及如果我的机器人在状态s

803
00:47:11,580 --> 00:47:13,290
它想知道选择什么样的行动

804
00:47:13,390 --> 00:47:15,230
我想选择的行动是

805
00:47:15,330 --> 00:47:20,790
将最大限度地发挥我的预期总收益

806
00:47:20,870 --> 00:47:24,140
因此P*(S)要定义为

807
00:47:24,230 --> 00:47:25,910
在行动a上的R(max)

808
00:47:33,230 --> 00:47:35,440
我也可以把γ放在那里

809
00:47:35,520 --> 00:47:38,070
但是γ是一个正数

810
00:47:38,170 --> 00:47:42,670
γ是几乎总是正的

811
00:47:42,770 --> 00:47:44,020
所以我把它去掉

812
00:47:44,100 --> 00:47:46,130
因为它只是一个恒定的规模  没有影响R

813
00:47:46,200 --> 00:47:52,160
因此  其定义的结果是

814
00:47:52,270 --> 00:47:55,310
p *实际上是最佳的政策

815
00:47:55,390 --> 00:47:58,000
因为p *能最大化我的期望总报酬

816
00:48:03,390 --> 00:48:05,530
酷 关于这一点有什么问题吗?

817
00:48:14,760 --> 00:48:22,590
酷 所以现在我想做的唯一的事

818
00:48:22,700 --> 00:48:25,550
就是谈论算法如何计算高起点

819
00:48:25,640 --> 00:48:28,190
如何计算最优政策

820
00:48:28,280 --> 00:48:30,310
我应该在那样做之前 写下一点东西

821
00:48:30,390 --> 00:48:33,510
但是注意

822
00:48:33,590 --> 00:48:37,740
如果我能够计算出V

823
00:48:37,820 --> 00:48:41,160
如果我能够计算出最优值函数

824
00:48:41,250 --> 00:48:43,180
那我可以把它变成这个公式

825
00:48:43,270 --> 00:48:46,700
然后我将会完成 因此  如果我能够计算V

826
00:48:46,790 --> 00:48:48,430
你就用这个p *的定义

827
00:48:48,500 --> 00:48:50,490
然后可以计算最优政策

828
00:48:50,540 --> 00:48:51,900
所以我的用于计算最优政策的策略是

829
00:48:51,980 --> 00:48:55,610
将计算V *  然后转换成这个公式

830
00:48:55,690 --> 00:48:58,170
这将给我的最优政策p

831
00:48:58,250 --> 00:49:00,570
所以我的目标  我的下一个目标

832
00:49:00,650 --> 00:49:02,760
将会计算V

833
00:49:02,890 --> 00:49:07,700
但这里的V *的定义不会导致一个

834
00:49:07,800 --> 00:49:09,930
漂亮的算法来计算它  因为让我们看看

835
00:49:10,040 --> 00:49:12,670
所以我知道对于任意给定的政策

836
00:49:12,770 --> 00:49:15,230
通过线性系统方程求解  如何计算Vπ

837
00:49:15,300 --> 00:49:19,140
但这里有指数形式的大量政策

838
00:49:19,200 --> 00:49:22,070
所以你得到11个状态和4个行为

839
00:49:22,150 --> 00:49:25,990
并且政策的数量平均是11

840
00:49:26,080 --> 00:49:28,150
这是一个大容量的可能政策

841
00:49:28,250 --> 00:49:30,770
所以我不能用尽所有政策联合

842
00:49:30,870 --> 00:49:33,350
然后采用一个

843
00:49:33,440 --> 00:49:35,150
最大的V

844
00:49:35,250 --> 00:49:38,450
所以我应该首先写下一些别的东西

845
00:49:38,550 --> 00:49:40,520
只是为了接触到那个符号

846
00:49:40,600 --> 00:49:43,510
但我要做的就是  最终想出

847
00:49:43,600 --> 00:49:45,990
一个计算V *最优值函数的算法

848
00:49:46,070 --> 00:49:48,110
然后我们就把它们插入这个函数

849
00:49:48,190 --> 00:49:49,790
它将给我们最优政策p

850
00:49:49,890 --> 00:49:55,810
所以我等下要写下这个算法

851
00:49:55,910 --> 00:49:58,390
只是为了接触那个符号

852
00:50:08,020 --> 00:50:10,660
是的  我们忽略它

853
00:50:10,750 --> 00:50:19,290
让我们只讨论算法 所以

854
00:50:19,400 --> 00:50:23,510
这是一个称为"值迭代"的算法

855
00:50:23,590 --> 00:50:26,790
它利用Bellman方程作为最佳政策

856
00:50:26,870 --> 00:51:06,130
来计算V * 所以这里的算法

857
00:51:06,240 --> 00:51:09,070
OK  这就是算法的全部

858
00:51:09,150 --> 00:51:10,670
你重复上面的步骤  我想

859
00:51:10,730 --> 00:51:12,510
你们不停地做这一步

860
00:51:12,560 --> 00:51:15,030
所以为了更具体点  让我们说

861
00:51:15,110 --> 00:51:16,550
在我的11个状态的MDP中

862
00:51:16,630 --> 00:51:19,180
第一步就是初始化V(s)等于零

863
00:51:19,260 --> 00:51:20,790
那就是说  我在计算机

864
00:51:20,870 --> 00:51:22,340
实现中创建一个数组

865
00:51:22,410 --> 00:51:25,270
创建一个有11个元素的数组

866
00:51:25,370 --> 00:51:27,210
并且把它们设置为零

867
00:51:27,290 --> 00:51:28,740
比如  我可以初始化为任何数据

868
00:51:28,810 --> 00:51:30,970
这无关紧要

869
00:51:31,050 --> 00:51:32,420
现在我打算做的是

870
00:51:32,500 --> 00:51:35,050
我要使用Bellman方程

871
00:51:35,130 --> 00:51:37,010
并且我们将使用Bellman方程的右边

872
00:51:37,110 --> 00:51:39,320
覆盖和复制它的左边

873
00:51:39,390 --> 00:51:42,570
所以我们本质上  一直试图

874
00:51:42,630 --> 00:51:46,960
让Bellman保持正确

875
00:51:47,070 --> 00:51:49,020
对于一直存储的数字V(s)

876
00:51:49,120 --> 00:51:51,020
所以这里的V(s)是在11个

877
00:51:51,100 --> 00:51:53,860
元素的数组中  并且我要反复

878
00:51:53,940 --> 00:51:56,470
计算右手边并复制它到V(s)

879
00:51:56,550 --> 00:52:03,350
而实际上  当你这样做时

880
00:52:03,440 --> 00:52:14,930
这将使V(s)收敛于V *(s)

881
00:52:14,990 --> 00:52:16,450
所以可不必惊讶  因为我们知道V

882
00:52:16,510 --> 00:52:18,670
V*(s)在Bellman的方程中

883
00:52:18,760 --> 00:52:23,730
只是告诉你  其中一些想法

884
00:52:23,830 --> 00:52:25,520
比他们问题多  所以我不会

885
00:52:25,590 --> 00:52:29,400
证明这种算法的转换

886
00:52:29,480 --> 00:52:33,040
一些实现细节  原来有两种

887
00:52:33,120 --> 00:52:36,420
方式你可以做此更新

888
00:52:36,500 --> 00:52:39,080
一个是当我说  对于每一个

889
00:52:39,180 --> 00:52:43,840
执行更新的状态s  某种程度上

890
00:52:43,920 --> 00:52:46,030
对于每个状态  你可以做的

891
00:52:46,110 --> 00:52:48,310
一个方式是  你可以计算它的右边

892
00:52:48,400 --> 00:52:50,510
然后你就能同时覆盖左边

893
00:52:50,590 --> 00:52:52,570
对于每一个状态 所以如果你这么做

894
00:52:52,640 --> 00:52:56,160
这叫做序列更新 好的  并且和序列

895
00:52:56,250 --> 00:52:59,060
【听不清】  因此可以同时更新所有的状态

896
00:52:59,160 --> 00:53:02,070
假如你这么做  有时如下所写

897
00:53:02,170 --> 00:53:05,280
如果你同步更新  就好像

898
00:53:05,340 --> 00:53:08,620
你有一些的价值函数

899
00:53:08,700 --> 00:53:10,950
你在第I次循环或者第T次迭代算法

900
00:53:11,030 --> 00:53:13,100
然后你将会计算你的

901
00:53:13,190 --> 00:53:16,320
全部价值函数中的一些

902
00:53:16,410 --> 00:53:20,600
然后你可以设置你的价值函数

903
00:53:20,680 --> 00:53:23,170
到一个新版本  因此同时

904
00:53:23,270 --> 00:53:25,820
更新s空间中值函数中的所有11个值

905
00:53:25,900 --> 00:53:28,530
所以有时写成这样的

906
00:53:28,640 --> 00:53:30,730
我这里的B被称为Bellman备份操作

907
00:53:30,820 --> 00:53:33,650
所以同步估算

908
00:53:33,720 --> 00:53:35,090
你就会采用那个值函数

909
00:53:35,180 --> 00:53:37,090
你为它应用Bellman备份操作

910
00:53:37,210 --> 00:53:38,820
然后Bellman备份操作表示

911
00:53:38,910 --> 00:53:40,710
对所有的状态计算它的右边

912
00:53:40,800 --> 00:53:43,240
你已经覆盖整个值函数

913
00:53:43,320 --> 00:53:46,870
唯一执行这些更新的方式是异步更新

914
00:53:46,980 --> 00:53:52,310
这就是你每次只能更新一个

915
00:53:52,410 --> 00:53:54,650
所以你通过固定的顺序

916
00:53:54,730 --> 00:53:57,180
遍历这些状态

917
00:53:57,300 --> 00:54:01,640
所以为状态1号更新V(s)

918
00:54:01,700 --> 00:54:03,790
然后我想为状态2号更新V(s)

919
00:54:03,870 --> 00:54:06,250
然后状态3号  等等

920
00:54:06,340 --> 00:54:12,210
当我正在为状态5号更新V(s)

921
00:54:12,310 --> 00:54:14,810
如果V(s)prime  如果最后

922
00:54:14,910 --> 00:54:16,810
我使用状态1 2 3 4在右手边的值

923
00:54:16,890 --> 00:54:19,300
那么我就用在右手边的最近更新的值

924
00:54:19,390 --> 00:54:22,170
所以当你在第五状态中

925
00:54:22,250 --> 00:54:24,830
进行更新时  你要用值 新值

926
00:54:24,910 --> 00:54:28,350
为状态1 2 3 4

927
00:54:28,420 --> 00:54:30,900
那就叫做一个异步更新

928
00:54:30,990 --> 00:54:34,450
其他版本将导致V(s)转换成是*(s)

929
00:54:34,530 --> 00:54:38,270
在同步更新中  这能使他们

930
00:54:38,360 --> 00:54:40,440
只是稍微更快 但是它们都能找到

931
00:54:40,510 --> 00:54:43,630
然后结果是

932
00:54:43,710 --> 00:54:45,770
价值迭代同步更新的分析

933
00:54:45,870 --> 00:54:49,310
也更容易分析那些事项【听不清】

934
00:54:49,390 --> 00:54:52,100
异步稍微快一点

935
00:54:52,180 --> 00:55:12,370
所以  当你在MDP中运行这个算法

936
00:55:12,450 --> 00:55:15,750
我忘了说  所有这些值是

937
00:55:15,840 --> 00:55:20,050
使用γ=0.99来计算的

938
00:55:20,140 --> 00:55:25,130
Roger Gross  他是  我想

939
00:55:25,230 --> 00:55:27,620
精通【听不清】可以帮助

940
00:55:27,710 --> 00:55:30,300
我计算这些数字 那么你可以

941
00:55:30,410 --> 00:55:32,510
计算它 你在MDP上运行

942
00:55:32,630 --> 00:55:37,460
价值关系的方式 你得到的数字V *如下:

943
00:55:37,530 --> 00:55:44,630
0.86  0.90--那种数字不会影响什么

944
00:55:44,720 --> 00:55:48,150
只需要看一看它

945
00:55:48,250 --> 00:55:51,600
确保能够直观地理解它

946
00:55:51,690 --> 00:56:03,100
然后当你把这些在公式转换

947
00:56:03,190 --> 00:56:05,780
用来计算的时候  我早些时候

948
00:56:05,860 --> 00:56:09,550
写下了它  根据V*计算p

949
00:56:09,640 --> 00:56:12,040
然后--嗯  我之前画了这个

950
00:56:12,170 --> 00:56:14,160
但这里是最优政策p

951
00:56:22,750 --> 00:56:25,460
所以  总结一下  这个过程

952
00:56:25,570 --> 00:56:28,570
是运行价值迭代用来计算V

953
00:56:28,640 --> 00:56:31,140
所以这是数字表格  然后我用

954
00:56:31,220 --> 00:56:33,530
我的P *的形式来计算最优的政策

955
00:56:33,610 --> 00:56:35,570
这是在这种情况下的政策

956
00:56:35,660 --> 00:56:38,370
现在  为了更具体些  让我们再看看

957
00:56:38,450 --> 00:56:40,940
(3  1)状态 向左走更好?

958
00:56:41,070 --> 00:56:43,280
还是向北走更好呢?因此  让我说明为什么

959
00:56:43,350 --> 00:56:47,150
我宁愿向左走  而不向北走

960
00:56:47,230 --> 00:56:53,430
在P *的形式下  如果我向西走  然后总结

961
00:56:53,500 --> 00:57:01,640
S`  P(s  a)prime  P *(sp)

962
00:57:01,740 --> 00:57:04,180
这将是--好  让我写下来

963
00:57:13,630 --> 00:57:40,260
好的  如果我北上  那么因为那个

964
00:57:40,350 --> 00:57:46,620
我很快地写下来  所以它比较凌乱

965
00:57:46,700 --> 00:57:49,750
我得到这些数字的方式是

966
00:57:49,820 --> 00:57:52,950
假设我在这种状态下  在这个(3  1)

967
00:57:53,020 --> 00:57:56,180
状态下 如果让我选择去西边

968
00:57:56,250 --> 00:58:01,710
机会是0.8  我得到0.75

969
00:58:01,810 --> 00:58:04,350
有了0.1的机会偏离  并获得了0.69

970
00:58:04,490 --> 00:58:06,900
然后在0.1的机会

971
00:58:06,990 --> 00:58:08,530
我向南走  我在墙壁反弹  并且我留在原处

972
00:58:08,620 --> 00:58:10,770
所以这就是为什么我去西部的

973
00:58:10,830 --> 00:58:14,320
预计未来收益是0.8* 0.75

974
00:58:14,400 --> 00:58:19,230
加上0.1* 0.69  加上0.1* 0.71

975
00:58:19,300 --> 00:58:21,030
最后的0.71反弹南墙

976
00:58:21,110 --> 00:58:22,910
然后我再看到我在哪里  返回给你0.740

977
00:58:22,990 --> 00:58:25,930
然后  你可以重复同样的过程  来估计

978
00:58:26,010 --> 00:58:29,410
你的预期总收益  如果你北上

979
00:58:29,490 --> 00:58:31,850
所以如果你那样做  一个0.8的机会

980
00:58:31,910 --> 00:58:34,770
你最终北上了  这样就可以得到0.69

981
00:58:34,850 --> 00:58:36,850
一个0.1的机会  你最后到达这里

982
00:58:36,930 --> 00:58:39,570
并且你有0.1的机会到达那里 这张地图

983
00:58:39,650 --> 00:58:42,120
精神上导致那个表达式  并且计算期望值

984
00:58:42,210 --> 00:58:45,780
你会得到0.676 所以你的总回报

985
00:58:45,870 --> 00:58:48,510
是比去西边更高的

986
00:58:48,590 --> 00:58:51,090
你去西方比你去北方得到的预期总收益

987
00:58:51,210 --> 00:58:52,550
更高 这就是为什么

988
00:58:52,630 --> 00:58:54,100
在这种状态下的最佳行动是往西走

989
00:59:01,830 --> 00:59:19,420
所以这就是"值迭代"

990
00:59:19,520 --> 00:59:24,040
原来有两个计算最佳的

991
00:59:24,120 --> 00:59:26,630
MDP政策的标准算法

992
00:59:26,710 --> 00:59:29,410
值迭代是其中之一 一旦你写完了

993
00:59:29,550 --> 00:59:34,350
所以值迭代是一个算法

994
00:59:34,430 --> 00:59:36,530
另外一个计算最佳的MDP政策的算法

995
00:59:36,600 --> 00:59:39,130
被称为"政策迭代"

996
00:59:39,210 --> 00:59:43,230
让我--


997
00:59:43,330 --> 00:59:45,220
我只是打算把它写下来

998
00:59:45,320 --> 00:59:53,460
在政策迭代中  我们的政策p

999
00:59:53,560 --> 00:59:57,350
随机初始化  没有关系

1000
00:59:57,460 --> 00:59:59,770
它可以使偏北政策

1001
00:59:59,870 --> 01:00:02,270
或者采取随机行动或任何政策

1002
01:00:02,360 --> 01:00:07,230
然后我们会反复做以下处理

1003
01:00:37,500 --> 01:00:39,800
好的  就是这个算法

1004
01:00:39,890 --> 01:00:44,940
因此  该算法具有两个步骤

1005
01:00:45,010 --> 01:00:46,610
在第一步中  我们解决的问题

1006
01:00:46,700 --> 01:00:48,580
我们采取的现行政策P

1007
01:00:48,680 --> 01:00:51,750
并且我们解出Bellman方程

1008
01:00:51,820 --> 01:00:53,340
获得Vπ 所以请记住

1009
01:00:53,420 --> 01:00:56,160
刚才我说过  如果你有一个

1010
01:00:56,240 --> 01:00:58,000
固定的政策p  那么

1011
01:00:58,080 --> 01:01:00,300
Bellman方程用11个未知数

1012
01:01:00,380 --> 01:01:02,080
和11个线性约束定义

1013
01:01:02,160 --> 01:01:05,290
这个线性方程组系统

1014
01:01:05,370 --> 01:01:07,490
等你解出这个线性方程组

1015
01:01:07,590 --> 01:01:09,660
你会得到您当前的政策p的值函数

1016
01:01:09,740 --> 01:01:12,130
并通过这个符号  我的意思只是

1017
01:01:12,220 --> 01:01:14,570
让V成为政策p的价值函数

1018
01:01:14,680 --> 01:01:17,560
然后第二步是更新的政策

1019
01:01:17,640 --> 01:01:19,880
换句话说  假装你的

1020
01:01:19,960 --> 01:01:21,600
目前猜测值函数V

1021
01:01:21,700 --> 01:01:23,960
是确实的最优值函数

1022
01:01:24,060 --> 01:01:28,010
你让P(S)等于最大的公式

1023
01:01:28,090 --> 01:01:29,280
以便更新你的政策p

1024
01:01:29,360 --> 01:01:33,100
所以事实证明  如果你这样做

1025
01:01:33,180 --> 01:01:38,230
那么V将收敛至V *  P会收敛到P

1026
01:01:38,310 --> 01:01:41,720
所以这是另一种方式

1027
01:01:41,800 --> 01:01:43,430
来找到MDP的最优政策

1028
01:01:43,510 --> 01:01:49,730
在权衡方面  事实证明

1029
01:01:49,820 --> 01:01:52,920
让我们来看看--政策迭代

1030
01:01:53,000 --> 01:01:57,990
计算成本的步骤是这个

1031
01:01:58,080 --> 01:02:00,120
你需要解出这个线性方程组系统

1032
01:02:00,200 --> 01:02:02,090
你有n个方程和n个未知数

1033
01:02:02,160 --> 01:02:04,720
如果你有N个状态

1034
01:02:04,790 --> 01:02:06,740
所以如果你有一个合理的

1035
01:02:06,800 --> 01:02:08,540
极少数状态的问题

1036
01:02:08,620 --> 01:02:09,890
如果你有一个比如11个状态的问题

1037
01:02:09,990 --> 01:02:12,410
你可以相当有效地求解线性系统方程

1038
01:02:12,480 --> 01:02:15,010
政策迭代往往非常好的

1039
01:02:15,090 --> 01:02:16,920
处理少量的状态问题

1040
01:02:17,020 --> 01:02:19,710
实际上你可以有效解决

1041
01:02:19,780 --> 01:02:22,470
这些线性方程组系统

1042
01:02:22,540 --> 01:02:24,480
所以  如果你有一千个状态

1043
01:02:24,560 --> 01:02:26,410
任何少于它的  可以非常

1044
01:02:26,470 --> 01:02:28,190
有效地解决一千方程系统

1045
01:02:28,250 --> 01:02:31,040
所以政策迭代往往处理地很好

1046
01:02:31,130 --> 01:02:34,250
如果你的MDP有数量庞大的状态

1047
01:02:34,330 --> 01:02:37,720
那么我们将实际上往往

1048
01:02:37,820 --> 01:02:39,390
看到数万数千或数百数千或数百万

1049
01:02:39,480 --> 01:02:42,850
数以千万计的状态的MDP

1050
01:02:42,950 --> 01:02:45,680
如果你有10万状态的问题

1051
01:02:45,780 --> 01:02:47,780
你尝试运用政策迭代

1052
01:02:47,850 --> 01:02:52,220
那么这一步需要解决的10万

1053
01:02:52,300 --> 01:02:53,960
方程的线性系统

1054
01:02:54,060 --> 01:02:55,850
这将是个昂贵的计算

1055
01:02:55,940 --> 01:02:57,740
因此对于这些  真的  真的大的MDP

1056
01:02:57,820 --> 01:02:59,290
我倾向于使用值迭代

1057
01:02:59,380 --> 01:03:05,310
让我们来看看 对此有任何疑问吗?

1058
01:03:09,250 --> 01:03:11,040
学生:所以这是一个凸函数

1059
01:03:11,120 --> 01:03:13,060
它擅长于局部的

1060
01:03:13,150 --> 01:03:14,800
优化方案

1061
01:03:14,940 --> 01:03:19,470
哎呀  没错  你说得对 这是一个

1062
01:03:19,550 --> 01:03:21,540
很好的问题:这是一个凸函数吗?

1063
01:03:21,620 --> 01:03:23,450
它可以形成一个解决V*的方式

1064
01:03:23,510 --> 01:03:26,220
作为一个凸优化问题

1065
01:03:26,300 --> 01:03:28,340
作为一个线性规划

1066
01:03:28,430 --> 01:03:30,450
举例来说  我可以打破这个解决方案

1067
01:03:30,540 --> 01:03:33,780
你写下V *  作为一种解决方案

1068
01:03:33,880 --> 01:03:36,020
使线性将是唯一可以解决的问题

1069
01:03:36,120 --> 01:03:43,130
策略迭代收敛于γT变换

1070
01:03:43,220 --> 01:03:45,160
我们不只是陷入局部最优

1071
01:03:45,230 --> 01:03:47,270
但政策迭代转换的证明

1072
01:03:47,340 --> 01:03:49,380
是和凸优化

1073
01:03:49,460 --> 01:03:51,190
有所不同的原则

1074
01:03:51,270 --> 01:03:54,640
至少我现在看到的版本

1075
01:03:54,730 --> 01:03:56,160
是的 你也许可以关联此凸优化

1076
01:03:56,230 --> 01:03:57,600
但不需要明白

1077
01:03:57,670 --> 01:03:59,680
为什么这里总是收敛

1078
01:03:59,750 --> 01:04:06,230
证明并不难  但它也较长

1079
01:04:06,300 --> 01:04:07,660
我不想在课堂上温习它 是的

1080
01:04:07,760 --> 01:04:13,340
这是一个很好的点 酷

1081
01:04:13,420 --> 01:04:14,670
事实上  对这些有什么疑问吗?

1082
01:04:25,700 --> 01:04:30,060
OK  所以我们现在有两个算法

1083
01:04:30,140 --> 01:04:33,360
解决MDP 给定  5元组

1084
01:04:33,440 --> 01:04:35,450
5个状态的集合  行动的集合

1085
01:04:35,520 --> 01:04:37,820
状态过渡性质  贴现因子的奖励函数集

1086
01:04:37,920 --> 01:04:40,550
现在你可以应用政策

1087
01:04:40,640 --> 01:04:42,740
迭代或值迭代

1088
01:04:42,840 --> 01:04:45,650
来计算MDP的最佳政策

1089
01:04:45,740 --> 01:04:49,380
最后我想谈的是  如果你

1090
01:04:49,480 --> 01:04:57,860
不知道的状态转移概率

1091
01:04:57,980 --> 01:05:00,110
有时你会不知道奖励函数R

1092
01:05:00,180 --> 01:05:01,580
但让我们把它放到一边

1093
01:05:01,660 --> 01:05:05,820
例如  让我们说

1094
01:05:05,900 --> 01:05:08,770
你试图飞直升机

1095
01:05:08,860 --> 01:05:11,460
你之前真的不知道

1096
01:05:11,550 --> 01:05:13,090
你的直升机将会过渡到什么状态

1097
01:05:13,170 --> 01:05:14,820
并在一定状态下采取行动

1098
01:05:14,890 --> 01:05:16,340
因为直升机动力很嘈杂

1099
01:05:16,420 --> 01:05:18,860
你真的往往不知道最终会到达什么状态

1100
01:05:18,940 --> 01:05:24,250
因此  要做的标准的事情

1101
01:05:24,330 --> 01:05:27,280
或一个要做的标准的事情

1102
01:05:27,370 --> 01:05:29,860
是尝试从数据来估计状态转移概率

1103
01:05:29,930 --> 01:05:32,910
让我写出这一点 事实证明

1104
01:05:32,980 --> 01:05:36,330
MDP有5元组  对吗?

1105
01:05:36,440 --> 01:05:41,550
S  A  你必须转移概率

1106
01:05:41,610 --> 01:05:43,550
γ  和R.S和A  你几乎都知道

1107
01:05:43,650 --> 01:05:45,970
状态空间是你来定义

1108
01:05:46,030 --> 01:05:48,250
最底层的状态空间是什么

1109
01:05:48,330 --> 01:05:50,890
因数你试图控制  不管怎样 动作是  再次

1110
01:05:50,980 --> 01:05:52,630
只是你的行动之一 通常情况下

1111
01:05:52,710 --> 01:05:55,530
我们几乎都知道这些 γ

1112
01:05:55,630 --> 01:05:58,490
贴现因子是你选择你要权衡

1113
01:05:58,570 --> 01:06:01,030
当前与未来的回报多少而定

1114
01:06:01,100 --> 01:06:03,680
奖励功能  你通常都知道

1115
01:06:03,760 --> 01:06:05,660
有一些特殊情况下 通常情况下

1116
01:06:05,740 --> 01:06:07,560
你提出奖励函数  所以你通常

1117
01:06:07,640 --> 01:06:08,980
都知道奖励函数是什么

1118
01:06:09,070 --> 01:06:10,920
有时候  你并不知道

1119
01:06:11,010 --> 01:06:12,520
但是我们先不管它

1120
01:06:12,610 --> 01:06:15,070
最常见的你需要学习的事情是

1121
01:06:15,170 --> 01:06:16,970
状态转移概率 因此

1122
01:06:17,040 --> 01:06:18,900
我们就谈谈如何学习它

1123
01:06:18,980 --> 01:06:21,710
所以  当你不知道的状态

1124
01:06:21,800 --> 01:06:22,980
转移概率的时候  最常见的处理

1125
01:06:23,060 --> 01:06:24,530
就是估计数据 所以我的意思是

1126
01:06:24,620 --> 01:06:28,650
想象一些机器人

1127
01:06:28,730 --> 01:06:30,710
也许一个机器人围绕走廊漫游

1128
01:06:30,810 --> 01:06:32,890
就像那个网格的例子

1129
01:06:33,010 --> 01:06:36,370
你会让机器人

1130
01:06:36,470 --> 01:06:42,690
在MDP中采取行动

1131
01:06:42,790 --> 01:06:46,000
并且你会估计你的状态

1132
01:06:46,080 --> 01:06:48,590
转移概率P下标(S  ?A)S`

1133
01:06:48,680 --> 01:06:49,880
它几乎是你所期望的

1134
01:06:49,990 --> 01:06:53,040
这将是你在状态s采取行动的次数

1135
01:06:53,120 --> 01:07:03,620
并且你到达S`

1136
01:07:03,710 --> 01:07:09,360
除以你在状态a的

1137
01:07:09,460 --> 01:07:18,300
采取行动的次数  OK?

1138
01:07:18,380 --> 01:07:21,210
所以这个估计只是

1139
01:07:21,260 --> 01:07:23,010
所有你在状态s采取的行动的次数

1140
01:07:23,100 --> 01:07:24,910
你到达状态S`的一小部分

1141
01:07:24,990 --> 01:07:28,650
这几乎正是你所希望的

1142
01:07:28,770 --> 01:07:35,850
或者  你可以--或者万一

1143
01:07:35,930 --> 01:07:39,360
你从来没有在状态s试过的行动

1144
01:07:39,470 --> 01:07:42,100
因此  如果这个结果是0比0

1145
01:07:42,210 --> 01:07:44,050
然后你可以对矢量均匀分布进行默认估计

1146
01:07:44,140 --> 01:07:45,900
这是合理的默认估计

1147
01:07:45,980 --> 01:08:02,300
因此  把它放在一起 通过这个方式

1148
01:08:02,420 --> 01:08:04,470
结果强化学习在这个类的前面部分

1149
01:08:04,570 --> 01:08:07,150
在那里我们做了监督学习

1150
01:08:07,240 --> 01:08:09,090
我谈到

1151
01:08:09,170 --> 01:08:10,560
逻辑回归算法

1152
01:08:10,670 --> 01:08:13,220
所以它是一个算法

1153
01:08:13,300 --> 01:08:16,010
并且大多数用于实现逻辑回归

1154
01:08:16,070 --> 01:08:18,160
就像一个相当标准的做逻辑回归的方式

1155
01:08:18,260 --> 01:08:20,350
是SVM或者更快的分析  等等

1156
01:08:20,430 --> 01:08:22,510
事实证明  再强化学习中

1157
01:08:22,590 --> 01:08:26,060
有更多的混合搭配  我猜想

1158
01:08:26,170 --> 01:08:28,330
所以你往往可以

1159
01:08:28,430 --> 01:08:29,990
选择使用不同的算法

1160
01:08:30,070 --> 01:08:33,310
所以在我写下的一些算法中

1161
01:08:33,390 --> 01:08:36,430
有不止一种处理的方法

1162
01:08:36,510 --> 01:08:38,430
我给出具体的例子

1163
01:08:38,530 --> 01:08:41,080
但如果你面对AI(人工智能)问题

1164
01:08:41,160 --> 01:08:43,760
有些人控制机器人

1165
01:08:43,850 --> 01:08:46,080
你想在这里使用值迭代

1166
01:08:46,160 --> 01:08:48,310
而不使用政策迭代

1167
01:08:48,400 --> 01:08:50,430
你想要做一个具体的事情

1168
01:08:50,510 --> 01:08:53,450
与我写下的略有不同

1169
01:08:53,520 --> 01:08:55,850
这实际上是相当普遍的

1170
01:08:55,950 --> 01:08:57,610
所以在强化学习中  有其他主要方式

1171
01:08:57,690 --> 01:08:59,530
适用于不同的算法

1172
01:08:59,610 --> 01:09:01,370
并且混合搭配不同的算法

1173
01:09:01,450 --> 01:09:04,010
这将再次出现在每周一次的讲座中

1174
01:09:04,110 --> 01:09:12,290
因此  把我说的东西整理在一起

1175
01:09:12,370 --> 01:09:14,260
这里将是--现在这将是一个例子

1176
01:09:14,340 --> 01:09:16,490
你怎么可能在在MDP中

1177
01:09:16,540 --> 01:09:18,860
估计状态转移概率  并找到它的政策

1178
01:09:18,930 --> 01:09:24,110
因此  你可能反复如下处理 让我们来看看

1179
01:09:24,180 --> 01:09:37,320
使用一些政策p来采取行动

1180
01:09:37,420 --> 01:09:47,290
并在MDP中获得经验

1181
01:09:47,390 --> 01:09:50,950
意思是  只要执行政策p观察状态转换

1182
01:09:51,040 --> 01:09:54,150
根据你的数据  然后更新

1183
01:09:54,230 --> 01:10:02,600
你的状态转移概率p下标(S  A)

1184
01:10:02,680 --> 01:10:05,810
在你刚到观察经验估计的基础上

1185
01:10:05,900 --> 01:10:10,410
这时  你可能解决贝尔曼方程

1186
01:10:10,500 --> 01:10:18,730
使用的价值迭代

1187
01:10:18,830 --> 01:10:21,560
我缩写成VI

1188
01:10:21,640 --> 01:10:25,250
以及通过贝尔曼方程

1189
01:10:25,340 --> 01:10:27,750
我的意思是贝尔曼方程V *  不是Vπ

1190
01:10:27,820 --> 01:10:35,440
使用值迭代  获取P *的估计

1191
01:10:35,530 --> 01:10:40,440
来解决Bellman方程

1192
01:10:40,530 --> 01:10:44,440
然后通过事件更新你的政策

1193
01:10:59,410 --> 01:11:01,170
现在你有一个新的政策

1194
01:11:01,280 --> 01:11:04,330
这样你就可以回去和执行

1195
01:11:04,430 --> 01:11:05,930
这一政策的MDP  来获得

1196
01:11:06,010 --> 01:11:08,820
更多的状态转移观察

1197
01:11:08,900 --> 01:11:11,080
获取MDP嘈杂的部分

1198
01:11:11,160 --> 01:11:12,870
使用它更新你的状态转移概率

1199
01:11:12,950 --> 01:11:15,480
使用值迭代或策略迭代

1200
01:11:15,570 --> 01:11:17,860
来解决【无声】的值函数

1201
01:11:17,940 --> 01:11:22,570
得到一个新的政策等 OK?

1202
01:11:22,660 --> 01:11:24,090
事实证明  当你这样做

1203
01:11:24,180 --> 01:11:26,250
其实我写下的价值迭代  是有一个原因的

1204
01:11:26,350 --> 01:11:28,950
事实证明  在算法的第三步  如果你使用的

1205
01:11:29,040 --> 01:11:32,560
值迭代  而不是政策迭代

1206
01:11:32,640 --> 01:11:35,780
初始化值迭代  如果你使用之前的

1207
01:11:35,870 --> 01:11:38,480
解决方案的算法  好的

1208
01:11:38,570 --> 01:11:41,330
那是一个很好的初始化条件

1209
01:11:41,420 --> 01:11:43,070
并且会更加迅速收敛

1210
01:11:43,200 --> 01:11:46,780
因为值迭代尝试为每一个状态解出V(S)

1211
01:11:46,880 --> 01:11:51,060
它试图估计V *(S)和

1212
01:11:51,160 --> 01:11:55,470
来自V(s)的*中的S

1213
01:11:55,570 --> 01:11:57,740
所以如果你遍历它

1214
01:11:57,860 --> 01:12:01,490
并且使用你之前估计的值

1215
01:12:01,570 --> 01:12:04,460
来初始化你的值迭代算法

1216
01:12:04,560 --> 01:12:07,230
通常使这个收敛速度更快

1217
01:12:07,330 --> 01:12:09,380
但再次  这是在这里再次强调

1218
01:12:09,540 --> 01:12:11,130
你也可以在这里调整策略

1219
01:12:11,230 --> 01:12:13,900
迭代的一个小部分  不管怎样

1220
01:12:13,990 --> 01:12:17,180
这是一个如何解出政策的相当典型例子

1221
01:12:17,270 --> 01:12:19,700
正确的数字  然后关键是

1222
01:12:19,790 --> 01:12:22,660
设法为问题找到一个良好的政策

1223
01:12:22,750 --> 01:12:24,820
在你不知道在前进的

1224
01:12:24,910 --> 01:12:26,490
状态转移概率的情况下

1225
01:12:26,610 --> 01:12:29,290
酷 关于这个有什么问题吗?

1226
01:12:38,030 --> 01:12:40,180
酷 所以这肯定是激动人心的

1227
01:12:40,280 --> 01:12:43,020
这是我们一个讲座中

1228
01:12:43,130 --> 01:12:45,620
讲的前两个MDP算法

1229
01:12:45,620 --> 01:12:50,530
今天我们就到这里 谢谢大家 再见

