1
00:00:23,230 --> 00:00:24,680
Instructor (Andrew Ng):Okay.

2
00:00:24,810 --> 00:00:25,730
Good morning. Welcome back.

3
00:00:27,450 --> 00:00:31,530
What I want to do today is actually

4
00:00:31,600 --> 00:00:32,690
wrap up our discussion

5
00:00:32,760 --> 00:00:34,410
on learning theory and sort of on

6
00:00:34,480 --> 00:00:37,950
and I'm gonna start by talking about

7
00:00:38,070 --> 00:00:38,970
Bayesian statistics

8
00:00:39,040 --> 00:00:39,790
and regularization,

9
00:00:39,870 --> 00:00:43,410
and then take a very brief digression

10
00:00:43,490 --> 00:00:44,720
to tell you about online learning.

11
00:00:44,790 --> 00:00:47,190
And most of today's lecture will

12
00:00:47,270 --> 00:00:49,290
actually be on various pieces of that,

13
00:00:49,360 --> 00:00:50,620
by applying machine learning

14
00:00:50,700 --> 00:00:52,280
algorithms to problems like,

15
00:00:52,350 --> 00:00:53,110
you know, like the project

16
00:00:53,180 --> 00:00:54,530
or other problems you may go work on

17
00:00:54,610 --> 00:00:55,950
after you graduate from this class.

18
00:00:56,030 --> 00:00:58,290
But let's start the talk about

19
00:00:58,360 --> 00:00:59,280
Bayesian statistics

20
00:00:59,360 --> 00:00:59,920
and regularization.

21
00:01:01,480 --> 00:01:03,950
So you remember from last week,

22
00:01:04,030 --> 00:01:05,390
we started to talk about

23
00:01:05,470 --> 00:01:06,300
learning theory and

24
00:01:06,380 --> 00:01:07,970
we learned about bias and variance.

25
00:01:08,040 --> 00:01:10,650
And I guess in the previous lecture,

26
00:01:10,730 --> 00:01:12,180
we spent most of the previous lecture

27
00:01:12,270 --> 00:01:15,380
talking about algorithms for model

28
00:01:15,460 --> 00:01:18,140
selection and for feature selection.

29
00:01:18,210 --> 00:01:19,950
We talked about cross-validation.Right?

30
00:01:20,030 --> 00:01:22,350
So most of the methods we talked about

31
00:01:22,430 --> 00:01:24,790
in the previous lecture were ways

32
00:01:24,870 --> 00:01:27,070
for you to try to simplify the model.

33
00:01:27,150 --> 00:01:27,910
So for example,

34
00:01:27,990 --> 00:01:29,350
the feature selection algorithms

35
00:01:29,420 --> 00:01:30,010
we talked about

36
00:01:30,080 --> 00:01:30,780
gives you a way

37
00:01:30,860 --> 00:01:32,480
to eliminate a number of features,

38
00:01:32,550 --> 00:01:34,560
so as to reduce the number of

39
00:01:34,640 --> 00:01:35,690
parameters you need to fit

40
00:01:35,770 --> 00:01:37,150
and thereby reduce overfitting.

41
00:01:37,240 --> 00:01:38,240
Right? You remember that?

42
00:01:38,320 --> 00:01:39,690
So feature selection algorithms

43
00:01:39,760 --> 00:01:43,260
choose a subset of the features

44
00:01:43,340 --> 00:01:45,420
so that you have less parameters and

45
00:01:45,490 --> 00:01:47,110
you may be less likely to overfit.

46
00:01:47,200 --> 00:01:47,740
Right?

47
00:01:48,040 --> 00:01:49,930
What I want to do today is to talk about

48
00:01:50,010 --> 00:01:52,200
a different way to prevent overfitting.

49
00:01:52,280 --> 00:01:54,620
And there's a method

50
00:01:54,700 --> 00:01:56,250
called regularization and there's a way

51
00:01:56,330 --> 00:01:57,950
that lets you keep all the parameters.

52
00:01:58,940 --> 00:02:00,810
So here's the idea,

53
00:02:00,890 --> 00:02:03,150
and I'm gonna illustrate this example

54
00:02:03,230 --> 00:02:05,490
with, say, linear regression.

55
00:02:13,230 --> 00:02:14,250
So you take the linear regression

56
00:02:14,330 --> 00:02:15,710
model, the very first model

57
00:02:15,790 --> 00:02:17,020
we learned about, right,

58
00:02:17,090 --> 00:02:19,170
we said that we would choose the

59
00:02:19,250 --> 00:02:24,440
parameters via maximum likelihood.

60
00:02:26,410 --> 00:02:29,650
Right?

61
00:02:29,750 --> 00:02:31,830
And that meant that, you know,

62
00:02:31,900 --> 00:02:33,750
you would choose the parameters theta

63
00:02:35,050 --> 00:02:41,760
that maximized the probability of the

64
00:02:41,840 --> 00:02:44,210
data, which is parameters theta that

65
00:02:44,290 --> 00:02:45,640
maximized the probability of

66
00:02:45,730 --> 00:02:46,990
the data we observe. Right?

67
00:02:50,400 --> 00:02:52,400
And so to give this

68
00:02:52,480 --> 00:02:53,460
sort of procedure a name,

69
00:02:53,540 --> 00:02:54,690
this is one example of

70
00:02:55,950 --> 00:02:57,820
most common frequencies procedure,

71
00:02:59,660 --> 00:03:00,870
and frequency,

72
00:03:00,950 --> 00:03:02,100
you can think of sort of

73
00:03:02,180 --> 00:03:03,820
as maybe one school of statistics.

74
00:03:03,900 --> 00:03:07,060
And the philosophical view behind

75
00:03:07,150 --> 00:03:09,420
writing this down was we envisioned

76
00:03:09,490 --> 00:03:10,960
that there was some true parameter

77
00:03:11,050 --> 00:03:12,620
theta out there that generated,

78
00:03:12,700 --> 00:03:14,040
you know, the Xs and the Ys.

79
00:03:14,130 --> 00:03:15,330
There's some true parameter theta

80
00:03:15,410 --> 00:03:17,910
that govern housing prices,

81
00:03:17,990 --> 00:03:19,590
Y is a function of X,

82
00:03:19,670 --> 00:03:21,190
and we don't know

83
00:03:21,260 --> 00:03:22,440
what the value of theta is,

84
00:03:22,530 --> 00:03:23,830
and we'd like to

85
00:03:23,900 --> 00:03:25,120
come up with some procedure

86
00:03:25,210 --> 00:03:26,750
for estimating the value of theta.

87
00:03:26,830 --> 00:03:27,510
Okay?

88
00:03:27,590 --> 00:03:29,070
And so, maximum likelihood is just

89
00:03:29,140 --> 00:03:31,520
one possible procedure for estimating

90
00:03:31,600 --> 00:03:33,690
the unknown value for theta.

91
00:03:35,160 --> 00:03:39,120
And the way you formulated this, you know

92
00:03:39,190 --> 00:03:40,960
theta was not a random variable.Right?

93
00:03:41,040 --> 00:03:41,550
That's what why said,

94
00:03:41,630 --> 00:03:43,760
so theta is just some true value out

95
00:03:43,840 --> 00:03:44,900
there. It's not random or anything,

96
00:03:44,980 --> 00:03:46,130
we just don't know what it is,

97
00:03:46,210 --> 00:03:47,120
and we have a procedure

98
00:03:47,190 --> 00:03:48,300
called maximum likelihood

99
00:03:48,370 --> 00:03:50,940
for estimating the value for theta.

100
00:03:51,020 --> 00:03:52,270
So this is one example of what's called

101
00:03:52,340 --> 00:03:53,320
a frequencies procedure.

102
00:03:53,400 --> 00:03:57,290
The alternative to the, I guess,

103
00:03:57,370 --> 00:03:58,970
the frequency school of statistics

104
00:03:59,050 --> 00:04:01,010
is the Bayesian school,

105
00:04:03,500 --> 00:04:07,380
in which we're gonna say that

106
00:04:07,460 --> 00:04:08,370
we don't know what theta is,

107
00:04:08,450 --> 00:04:12,040
and so we will put a prior

108
00:04:12,120 --> 00:04:14,860
on theta. Okay?

109
00:04:14,940 --> 00:04:16,430
So in the Bayesian school

110
00:04:16,510 --> 00:04:17,120
students would say,

111
00:04:17,200 --> 00:04:18,380
"Well don't know

112
00:04:18,450 --> 00:04:19,100
What's the value of theta is

113
00:04:19,190 --> 00:04:21,080
so let's represent our uncertainty

114
00:04:21,160 --> 00:04:22,630
over theta with a prior."

115
00:04:27,270 --> 00:04:32,640
So for example, our prior on theta

116
00:04:32,740 --> 00:04:36,920
may be a Gaussian distribution with

117
00:04:37,000 --> 00:04:38,950
mean zero and curvalence matrix

118
00:04:39,040 --> 00:04:41,740
given by tau squared I. Okay?

119
00:04:41,820 --> 00:04:46,680
And so actually, if I use S to denote

120
00:04:46,760 --> 00:04:55,580
my training set, well right,

121
00:04:55,660 --> 00:04:58,170
so theta represents my beliefs about

122
00:04:58,250 --> 00:04:58,920
what the parameters are

123
00:04:58,990 --> 00:05:00,760
in the absence of any data.

124
00:05:00,840 --> 00:05:02,210
So not having seen any data,

125
00:05:02,290 --> 00:05:04,450
theta represents, you know, what I

126
00:05:04,530 --> 00:05:06,430
think theta it probably represents

127
00:05:06,500 --> 00:05:08,300
what I think theta is most likely to be.

128
00:05:09,920 --> 00:05:11,490
And so given the training set, S,

129
00:05:11,570 --> 00:05:14,070
in the sort of Bayesian procedure,

130
00:05:14,150 --> 00:05:22,850
we would, well, calculate the

131
00:05:22,930 --> 00:05:25,350
probability, the posterior probability

132
00:05:25,430 --> 00:05:27,460
by parameters given my training sets,

133
00:05:27,530 --> 00:05:31,120
and let's write this on the next board.

134
00:05:31,200 --> 00:05:34,200
So my posterior on my parameters

135
00:05:34,290 --> 00:05:35,150
given my training set,

136
00:05:35,230 --> 00:05:36,140
by Bayes' rule,

137
00:05:36,220 --> 00:05:39,690
this will be proportional to, you know,

138
00:05:49,740 --> 00:05:51,580
this. Right?

139
00:05:51,650 --> 00:05:52,770
So by Bayes' rule.

140
00:05:55,690 --> 00:05:57,560
Let's call it posterior.

141
00:06:00,640 --> 00:06:03,360
And this distribution now represents

142
00:06:03,440 --> 00:06:04,920
my beliefs about what theta is

143
00:06:05,000 --> 00:06:06,660
after I've seen the training set.

144
00:06:07,640 --> 00:06:09,350
And when you now want to

145
00:06:09,430 --> 00:06:10,170
make a new prediction

146
00:06:10,240 --> 00:06:11,250
on the price of a new house,

147
00:06:18,950 --> 00:06:23,260
on the input X, I would say that, well,

148
00:06:23,330 --> 00:06:25,530
the distribution over the possible

149
00:06:25,600 --> 00:06:28,100
housing prices for this new house

150
00:06:28,170 --> 00:06:29,390
I'm trying to estimate the price of,

151
00:06:29,460 --> 00:06:31,300
say, given the size of the house,

152
00:06:31,370 --> 00:06:32,720
the features of the house at X,

153
00:06:32,800 --> 00:06:35,400
and the training set I had previously,

154
00:06:36,280 --> 00:06:47,830
it is going to be given by an integral

155
00:06:47,910 --> 00:06:50,660
over my parameters theta of probably of

156
00:06:50,730 --> 00:06:53,540
Y given X comma theta and times the

157
00:06:53,630 --> 00:06:55,980
posterior distribution of theta given

158
00:06:56,050 --> 00:06:58,250
the training set. Okay?

159
00:06:58,330 --> 00:07:01,730
And in particular,

160
00:07:01,810 --> 00:07:03,740
if you want your prediction to be

161
00:07:03,820 --> 00:07:10,170
the expected value of Y given the

162
00:07:10,360 --> 00:07:11,470
input X in training set,

163
00:07:11,610 --> 00:07:13,720
you would say

164
00:07:13,870 --> 00:07:24,460
integrate over Y times the posterior.

165
00:07:24,620 --> 00:07:26,110
Okay?

166
00:07:26,260 --> 00:07:27,910
You would take an expectation of Y

167
00:07:28,070 --> 00:07:28,970
with respect to your

168
00:07:29,120 --> 00:07:32,520
posterior distribution. Okay?

169
00:07:32,680 --> 00:07:36,660
And you notice that when I was

170
00:07:36,850 --> 00:07:37,600
writing this down,

171
00:07:37,760 --> 00:07:39,250
so with the Bayesian formulation,

172
00:07:39,410 --> 00:07:40,560
and now started to write up here

173
00:07:40,720 --> 00:07:42,350
Y given X comma theta

174
00:07:42,510 --> 00:07:44,970
because this formula now is the

175
00:07:45,130 --> 00:07:46,950
property of Y conditioned on the values

176
00:07:47,100 --> 00:07:48,720
of the random variables X and theta.

177
00:07:49,080 --> 00:07:50,120
So I'm no longer writing

178
00:07:50,280 --> 00:07:51,480
semicolon theta, I'm writing comma

179
00:07:51,650 --> 00:07:53,530
theta because I'm now

180
00:07:53,700 --> 00:07:57,170
treating theta as a random variable.

181
00:07:58,270 --> 00:08:01,030
So all of this is somewhat abstract

182
00:08:01,640 --> 00:08:04,760
but this is and it turns out

183
00:08:04,930 --> 00:08:05,810
actually let's check.

184
00:08:05,960 --> 00:08:07,000
Are there questions about this?

185
00:08:10,330 --> 00:08:14,490
No? Okay.

186
00:08:14,650 --> 00:08:15,960
Let's try to make this more concrete.

187
00:08:16,120 --> 00:08:16,990
It turns out that

188
00:08:17,160 --> 00:08:18,680
for many problems,

189
00:08:18,850 --> 00:08:23,070
both of these steps in the computation

190
00:08:23,220 --> 00:08:24,350
are difficult

191
00:08:24,500 --> 00:08:27,110
because if, you know, theta is an

192
00:08:27,270 --> 00:08:28,430
N plus one-dimensional vector,

193
00:08:28,580 --> 00:08:29,930
is an N plus one-dimensional

194
00:08:30,100 --> 00:08:30,760
parameter vector,

195
00:08:30,920 --> 00:08:32,180
then this is one an integral

196
00:08:32,340 --> 00:08:33,410
over an N plus one-dimensional,

197
00:08:33,560 --> 00:08:35,360
you know, over RN plus one.

198
00:08:35,520 --> 00:08:37,570
And because numerically it's very

199
00:08:37,740 --> 00:08:39,950
difficult to compute integrals over

200
00:08:40,100 --> 00:08:42,340
very high dimensional spaces,

201
00:08:42,500 --> 00:08:43,350
all right?

202
00:08:43,500 --> 00:08:45,830
So usually this integral

203
00:08:45,980 --> 00:08:47,310
actually usually it's hard to

204
00:08:47,460 --> 00:08:48,880
compute the posterior in theta and

205
00:08:49,050 --> 00:08:51,270
it's also hard to compute this integral

206
00:08:51,420 --> 00:08:52,390
if theta is very high dimensional.

207
00:08:52,550 --> 00:08:54,720
There are few exceptions

208
00:08:54,880 --> 00:08:56,580
for which this can be done

209
00:08:56,740 --> 00:08:57,410
in closed form,

210
00:08:57,560 --> 00:08:59,000
but for many learning algorithms,

211
00:08:59,170 --> 00:09:01,010
say, Bayesian logistic regression,

212
00:09:01,180 --> 00:09:02,660
this is hard to do.

213
00:09:02,830 --> 00:09:07,500
And so what's commonly done is to

214
00:09:07,680 --> 00:09:08,800
take the posterior distribution

215
00:09:08,960 --> 00:09:11,130
and instead of actually computing

216
00:09:11,290 --> 00:09:13,080
a full posterior distribution, chi of

217
00:09:13,240 --> 00:09:16,750
theta given S, we'll instead take this

218
00:09:16,930 --> 00:09:18,210
quantity on the right-hand side

219
00:09:18,380 --> 00:09:21,520
and just maximize this quantity

220
00:09:21,680 --> 00:09:22,480
on the right-hand side.

221
00:09:22,640 --> 00:09:23,450
So let me write this down.

222
00:09:23,600 --> 00:09:26,570
So commonly,

223
00:09:26,720 --> 00:09:27,810
instead of computing

224
00:09:27,970 --> 00:09:29,370
the full posterior distribution,

225
00:09:29,520 --> 00:09:42,170
we will choose the following. Okay?

226
00:09:42,320 --> 00:09:43,040
We will choose

227
00:09:43,190 --> 00:09:44,630
what's called the MAP estimate,

228
00:09:44,830 --> 00:09:45,820
or the maximum a posteriori

229
00:09:45,970 --> 00:09:47,440
estimate of theta,

230
00:09:47,600 --> 00:09:49,480
which is the most likely value of theta,

231
00:09:49,650 --> 00:09:51,380
most probable value of theta

232
00:09:51,540 --> 00:09:52,870
onto your posterior distribution.

233
00:09:53,020 --> 00:10:08,360
And that's just ont max chi of theta.

234
00:10:08,520 --> 00:10:12,530
And then when you need to

235
00:10:12,720 --> 00:10:21,740
make a prediction, you know,

236
00:10:21,890 --> 00:10:28,050
you would just predict, say, well,

237
00:10:34,180 --> 00:10:37,110
using your usual hypothesis

238
00:10:37,280 --> 00:10:40,300
and using this MAP value of theta

239
00:10:40,460 --> 00:10:44,510
in place of as the parameter vector

240
00:10:44,660 --> 00:10:45,600
you'd choose.

241
00:10:45,750 --> 00:10:46,650
Okay?

242
00:10:46,810 --> 00:10:48,260
And notice,

243
00:10:48,440 --> 00:10:49,640
the only difference between this and

244
00:10:49,850 --> 00:10:51,050
standard maximum likelihood estimation

245
00:10:51,210 --> 00:10:53,320
is that when you're choosing, you know,

246
00:10:53,470 --> 00:10:54,630
the instead of choosing

247
00:10:54,790 --> 00:10:55,890
the maximum likelihood value for theta,

248
00:10:56,060 --> 00:10:58,160
you're instead maximizing this,

249
00:10:58,320 --> 00:10:59,430
which is what you have

250
00:10:59,590 --> 00:11:00,600
for maximum likelihood estimation,

251
00:11:00,750 --> 00:11:02,420
and then times this other quantity

252
00:11:02,580 --> 00:11:04,380
which is the prior. Right?

253
00:11:04,690 --> 00:11:09,880
And let's see,

254
00:11:10,040 --> 00:11:12,290
when intuition is that

255
00:11:12,440 --> 00:11:21,660
if your prior is theta

256
00:11:21,850 --> 00:11:24,130
being Gaussian and with mean zero

257
00:11:24,280 --> 00:11:25,390
and some covariance,

258
00:11:25,540 --> 00:11:27,610
then for a distribution like this,

259
00:11:27,780 --> 00:11:29,250
most of the [inaudible] mass is

260
00:11:29,400 --> 00:11:30,710
close to zero. Right?

261
00:11:30,860 --> 00:11:31,760
So there's a Gaussian

262
00:11:31,920 --> 00:11:33,250
centered around the point zero,

263
00:11:33,410 --> 00:11:34,610
and so [inaudible] mass

264
00:11:34,770 --> 00:11:35,840
is close to zero.

265
00:11:37,730 --> 00:11:39,280
And so the prior distribution,

266
00:11:39,440 --> 00:11:41,310
instead of saying that you think mostof

267
00:11:41,470 --> 00:11:43,110
the parameters should be close to zero,

268
00:11:43,270 --> 00:11:46,490
and if you remember our discussion

269
00:11:46,660 --> 00:11:47,530
on feature selection,

270
00:11:47,680 --> 00:11:48,830
if you eliminate a feature

271
00:11:48,990 --> 00:11:50,070
from consideration

272
00:11:50,220 --> 00:11:51,890
that's the same as

273
00:11:52,050 --> 00:11:54,450
setting the source and value of theta

274
00:11:54,610 --> 00:11:55,610
to be equal to zero.

275
00:11:55,770 --> 00:11:56,910
All right?

276
00:11:57,060 --> 00:11:57,920
So if you set

277
00:11:58,070 --> 00:11:59,510
theta five to be equal to zero,

278
00:11:59,660 --> 00:12:01,040
that's the same as, you know,

279
00:12:01,190 --> 00:12:02,130
eliminating feature five

280
00:12:02,310 --> 00:12:03,570
from the your hypothesis.

281
00:12:03,730 --> 00:12:06,470
so, this is the prior that drives

282
00:12:06,470 --> 00:12:07,470
most of the parameter values to zero –

283
00:12:09,020 --> 00:12:11,050
to values close to zero.

284
00:12:11,210 --> 00:12:12,130
And you'll think of this as

285
00:12:12,280 --> 00:12:14,110
doing something analogous, if

286
00:12:14,270 --> 00:12:15,330
doing something reminiscent

287
00:12:15,480 --> 00:12:16,420
of feature selection.

288
00:12:16,580 --> 00:12:17,340
Okay?

289
00:12:17,500 --> 00:12:18,500
And it turns out that

290
00:12:18,670 --> 00:12:19,920
with this formulation, the parameters

291
00:12:20,160 --> 00:12:21,660
won't actually be exactly zero

292
00:12:21,980 --> 00:12:23,160
but many of the values

293
00:12:23,510 --> 00:12:24,450
will be close to zero.

294
00:12:24,730 --> 00:12:27,820
And I guess in pictures,

295
00:12:31,210 --> 00:12:35,110
if you remember,

296
00:12:35,270 --> 00:12:37,950
I said that if you have, say,

297
00:12:38,110 --> 00:12:39,170
five data points and you fit a

298
00:12:39,320 --> 00:12:41,150
fourth-order polynomial

299
00:12:47,630 --> 00:12:49,070
well I think that had too many bumps

300
00:12:49,220 --> 00:12:50,420
in it, but never mind.

301
00:12:50,580 --> 00:12:51,950
If you fit it a

302
00:12:52,110 --> 00:12:54,090
if you fit very high polynomial

303
00:12:54,250 --> 00:12:55,490
to a very small dataset,

304
00:12:55,660 --> 00:12:56,330
you can get

305
00:12:56,490 --> 00:12:57,440
these very large oscillations

306
00:12:57,600 --> 00:12:59,370
if you use maximum likelihood

307
00:12:59,520 --> 00:13:00,850
estimation. All right?

308
00:13:01,010 --> 00:13:02,100
In contrast,

309
00:13:02,260 --> 00:13:03,130
if you apply this

310
00:13:03,280 --> 00:13:04,290
sort of Bayesian regularization,

311
00:13:04,440 --> 00:13:06,390
you can actually fit a

312
00:13:06,550 --> 00:13:07,590
higher-order polynomial

313
00:13:07,750 --> 00:13:11,570
that still get sort of a smoother and

314
00:13:11,720 --> 00:13:13,690
smoother fit to the data as you decrease

315
00:13:13,880 --> 00:13:15,480
tau. So as you decrease tau,

316
00:13:15,650 --> 00:13:16,860
you're driving the parameters

317
00:13:17,020 --> 00:13:18,120
to be closer and closer to zero.

318
00:13:18,290 --> 00:13:19,870
And that in practice

319
00:13:20,020 --> 00:13:21,110
it's sort of hard to see,

320
00:13:21,280 --> 00:13:22,400
but you can take my word for it.

321
00:13:22,560 --> 00:13:24,080
As tau becomes smaller and smaller,

322
00:13:24,240 --> 00:13:26,430
the curves you tend to fit your data

323
00:13:26,590 --> 00:13:28,070
also become smoother and smoother,

324
00:13:28,230 --> 00:13:30,290
and so you tend less and less overfit,

325
00:13:30,440 --> 00:13:31,850
even when you're fitting

326
00:13:32,020 --> 00:13:32,970
a large number of parameters.

327
00:13:33,110 --> 00:13:34,260
Okay?

328
00:13:36,660 --> 00:13:45,370
Let's see, and one last piece of

329
00:13:45,520 --> 00:13:47,410
intuition that I would just toss out

330
00:13:47,570 --> 00:13:49,010
there. And you get to play more with

331
00:13:49,160 --> 00:13:51,280
this particular set of ideas more

332
00:13:51,440 --> 00:13:53,010
in Problem Set 3,

333
00:13:53,170 --> 00:13:54,310
which I'll post online

334
00:13:54,480 --> 00:13:56,020
later this week I guess.

335
00:13:56,180 --> 00:13:59,030
Is that whereas maximum likelihood

336
00:13:59,200 --> 00:14:12,430
tries to minimize, say, this, right?

337
00:14:12,590 --> 00:14:13,590
Whereas maximum likelihood for,

338
00:14:13,740 --> 00:14:14,730
say, linear regression

339
00:14:14,890 --> 00:14:16,430
turns out to be minimizing this,

340
00:14:16,590 --> 00:14:18,280
it turns out that if you

341
00:14:18,440 --> 00:14:20,690
add this prior term there,

342
00:14:20,880 --> 00:14:21,950
it turns out that

343
00:14:22,100 --> 00:14:24,090
the authorization objective you end up

344
00:14:24,240 --> 00:14:27,360
optimizing turns out to be that.

345
00:14:27,510 --> 00:14:29,630
Where you add an extra term that,

346
00:14:29,790 --> 00:14:31,790
you know, penalizes your parameter

347
00:14:31,950 --> 00:14:33,110
theta as being large.

348
00:14:33,270 --> 00:14:35,670
And so this ends up being an algorithm

349
00:14:35,830 --> 00:14:36,940
that's very similar to maximum

350
00:14:37,100 --> 00:14:38,510
likelihood, expect that you

351
00:14:38,670 --> 00:14:40,040
tend to keep your parameters small.

352
00:14:40,210 --> 00:14:42,030
And this has the effect.

353
00:14:42,200 --> 00:14:43,590
Again, it's kind of hard to see

354
00:14:43,740 --> 00:14:44,700
but just take my word for it.

355
00:14:44,850 --> 00:14:45,810
That strengthening the parameters

356
00:14:45,960 --> 00:14:47,320
has the effect of keeping the functions

357
00:14:47,480 --> 00:14:49,330
you fit to be smoother

358
00:14:49,500 --> 00:14:52,050
and less likely to overfit. Okay?

359
00:14:54,990 --> 00:14:58,300
Okay, hopefully this will make more

360
00:14:58,470 --> 00:15:00,060
sense when you play with these ideas

361
00:15:00,210 --> 00:15:01,010
a bit more in the next problem set.

362
00:15:01,170 --> 00:15:02,420
But let's check questions

363
00:15:02,580 --> 00:15:03,610
about all this.

364
00:15:11,000 --> 00:15:13,660
Student:The smoothing behavior is it

365
00:15:13,840 --> 00:15:16,330
because [inaudible] actually get

366
00:15:16,490 --> 00:15:18,020
different

367
00:15:18,170 --> 00:15:19,540
[inaudible]?

368
00:15:19,700 --> 00:15:20,890
Instructor (Andrew Ng):Let's see.

369
00:15:21,040 --> 00:15:22,150
Yeah. It depends on

370
00:15:22,320 --> 00:15:24,030
well most priors with

371
00:15:24,190 --> 00:15:25,460
most of the mass close to zero

372
00:15:25,620 --> 00:15:26,790
will get this effect, I guess.

373
00:15:26,960 --> 00:15:28,220
And just by convention,

374
00:15:28,380 --> 00:15:31,040
the Gaussian prior is what's most

375
00:15:31,190 --> 00:15:33,360
used the most common for models like

376
00:15:33,520 --> 00:15:35,050
logistic regression and linear

377
00:15:35,200 --> 00:15:36,710
regression, generalized in yourmodels.

378
00:15:36,870 --> 00:15:39,150
There are a few other priors

379
00:15:39,310 --> 00:15:40,020
that I sometimes use,

380
00:15:40,200 --> 00:15:41,080
like the Laplace prior,

381
00:15:41,240 --> 00:15:42,290
but all of them will tend to

382
00:15:42,450 --> 00:15:43,970
have these sorts of smoothing effects.

383
00:15:45,790 --> 00:15:50,100
All right. Cool.

384
00:15:50,250 --> 00:15:51,210
And so it turns out that

385
00:15:51,360 --> 00:15:54,030
for problems like text classification,

386
00:15:54,180 --> 00:15:56,840
text classification is like 30,000

387
00:15:57,010 --> 00:15:58,510
features or 50,000 features,

388
00:15:58,660 --> 00:16:01,130
where it seems like an algorithm

389
00:16:01,280 --> 00:16:02,460
like logistic regression would be

390
00:16:02,620 --> 00:16:04,010
very much prone to overfitting. Right?

391
00:16:04,170 --> 00:16:04,830
So imagine trying to

392
00:16:04,990 --> 00:16:05,510
build a spam classifier,

393
00:16:05,670 --> 00:16:07,770
maybe you have 100 training examples

394
00:16:07,940 --> 00:16:09,490
but you have 30,000 features

395
00:16:09,650 --> 00:16:13,130
or 50,000 features, that seems clearly

396
00:16:13,510 --> 00:16:14,980
to be prone to overfitting. Right?

397
00:16:15,140 --> 00:16:16,070
But it turns out that with

398
00:16:16,230 --> 00:16:18,860
this sort of Bayesian regularization,

399
00:16:19,010 --> 00:16:21,640
with [inaudible] Gaussian,

400
00:16:21,790 --> 00:16:23,710
logistic regression becomes

401
00:16:23,860 --> 00:16:24,950
a very effective text

402
00:16:25,110 --> 00:16:26,120
classification algorithm

403
00:16:26,280 --> 00:16:29,700
Alex?

404
00:16:29,870 --> 00:16:32,780
Student:[Inaudible]?

405
00:16:32,940 --> 00:16:33,660
Instructor (Andrew Ng):Yeah, right,

406
00:16:33,810 --> 00:16:34,920
and so pick and to pick

407
00:16:35,090 --> 00:16:36,310
either tau squared or lambda.

408
00:16:36,480 --> 00:16:37,780
I think the relation is

409
00:16:37,930 --> 00:16:39,500
lambda equals one over tau squared.

410
00:16:39,650 --> 00:16:40,470
But right, so pick either

411
00:16:40,630 --> 00:16:41,310
tau squared or lambda,

412
00:16:41,480 --> 00:16:42,810
you could use cross-validation, yeah.

413
00:16:42,980 --> 00:16:46,870
All right? Okay, cool.

414
00:16:47,030 --> 00:16:52,250
So all right, that was all

415
00:16:52,400 --> 00:16:53,070
I want to say about

416
00:16:53,240 --> 00:16:55,180
methods for preventing overfitting.

417
00:16:55,340 --> 00:16:57,480
What I want to do next is just

418
00:16:57,640 --> 00:16:59,500
spend, you know, five minutes

419
00:16:59,660 --> 00:17:01,270
talking about online learning.

420
00:17:01,430 --> 00:17:03,710
And this is sort of a digression.

421
00:17:03,860 --> 00:17:05,330
And so, you know,

422
00:17:05,490 --> 00:17:07,830
when you're designing the syllabus of a

423
00:17:08,000 --> 00:17:09,720
class, I guess, sometimes there are

424
00:17:09,910 --> 00:17:11,270
just some ideas you want to talk about

425
00:17:11,430 --> 00:17:12,790
but can't find a very good place

426
00:17:12,950 --> 00:17:13,730
to fit in anywhere.

427
00:17:13,890 --> 00:17:14,900
So this is one of those ideas

428
00:17:15,050 --> 00:17:17,220
that may seem a bit disjointed

429
00:17:17,390 --> 00:17:18,220
from the rest of the class

430
00:17:18,370 --> 00:17:18,930
but I just want to

431
00:17:19,100 --> 00:17:19,890
tell you a little bit about it.

432
00:17:22,600 --> 00:17:30,460
Okay. So here's the idea.

433
00:17:31,870 --> 00:17:35,080
So far, all the learning algorithms

434
00:17:35,240 --> 00:17:36,630
we've talked about are what's

435
00:17:36,790 --> 00:17:37,820
called batch learning algorithms,

436
00:17:37,970 --> 00:17:39,750
where you're given a training set

437
00:17:39,910 --> 00:17:40,790
and then you get to run your

438
00:17:40,940 --> 00:17:42,110
learning algorithm on the training set

439
00:17:42,260 --> 00:17:43,050
and then maybe you test it

440
00:17:43,210 --> 00:17:44,470
on some other test set.

441
00:17:44,630 --> 00:17:48,000
And there's another learning setting

442
00:17:48,160 --> 00:17:49,380
called online learning,

443
00:17:49,530 --> 00:17:50,660
in which you have to

444
00:17:50,820 --> 00:17:52,230
make predictions even while you are

445
00:17:52,410 --> 00:17:53,320
in the process of learning.

446
00:17:53,480 --> 00:17:56,230
So here's how the problem sees.

447
00:17:56,390 --> 00:17:57,370
All right?

448
00:17:57,530 --> 00:17:58,740
I'm first gonna give you X one.

449
00:17:58,900 --> 00:18:00,560
Let's say there's a classification

450
00:18:00,710 --> 00:18:03,020
problem, so I'm first gonna give you X one

451
00:18:03,170 --> 00:18:04,800
and then gonna ask you, you know,

452
00:18:04,960 --> 00:18:06,190
"Can you make a prediction on X one?

453
00:18:06,350 --> 00:18:07,490
Is the label one or zero?"

454
00:18:07,660 --> 00:18:08,950
And you've not seen any data yet.

455
00:18:09,120 --> 00:18:11,130
And so, you make a guess. Right?

456
00:18:11,290 --> 00:18:13,150
You guess we'll call

457
00:18:13,310 --> 00:18:14,210
your guess Y hat one.

458
00:18:14,360 --> 00:18:17,060
And after you've made your prediction,

459
00:18:17,250 --> 00:18:18,370
I will then reveal to you

460
00:18:18,520 --> 00:18:21,270
the true label Y one. Okay?

461
00:18:21,430 --> 00:18:22,850
And not having seen any data before,

462
00:18:23,000 --> 00:18:24,920
your odds of getting the first one right

463
00:18:25,080 --> 00:18:26,150
are only 50 percent, right,

464
00:18:26,330 --> 00:18:27,370
if you guess randomly.

465
00:18:29,440 --> 00:18:31,420
And then I show you X two.

466
00:18:31,570 --> 00:18:33,200
And then I ask you,

467
00:18:33,370 --> 00:18:34,510
"Can you make a prediction on X two?"

468
00:18:34,670 --> 00:18:36,680
And so you now maybe are gonna

469
00:18:36,850 --> 00:18:37,580
make a slightly

470
00:18:37,750 --> 00:18:38,690
more educated guess

471
00:18:38,850 --> 00:18:39,640
and call that Y hat two.

472
00:18:39,790 --> 00:18:41,350
And after you've made your guess,

473
00:18:41,520 --> 00:18:42,880
I reveal the true label to you.

474
00:18:43,030 --> 00:18:45,460
And so, then I show you X three,

475
00:18:45,620 --> 00:18:47,160
and then you make your guess,

476
00:18:47,320 --> 00:18:50,720
and learning proceeds as follows.

477
00:18:50,880 --> 00:18:54,930
So this is just a lot of machine

478
00:18:55,070 --> 00:18:56,140
learning and batch learning,

479
00:18:56,300 --> 00:18:57,800
and the model settings where you

480
00:18:57,980 --> 00:19:00,750
have to keep learning even as

481
00:19:00,910 --> 00:19:02,050
you're making predictions, okay?

482
00:19:02,220 --> 00:19:04,270
So I don't know,

483
00:19:04,420 --> 00:19:06,220
setting your website and

484
00:19:06,390 --> 00:19:07,230
you have users coming in.

485
00:19:07,380 --> 00:19:08,390
And as the first user comes in,

486
00:19:08,720 --> 00:19:09,760
you need to start making predictions

487
00:19:09,960 --> 00:19:11,010
already about what the user

488
00:19:11,170 --> 00:19:12,050
likes or dislikes.

489
00:19:12,240 --> 00:19:13,560
And there's only, you know, as you're

490
00:19:13,730 --> 00:19:15,020
making predictions you get to show

491
00:19:15,190 --> 00:19:16,580
more and more training examples.

492
00:19:16,730 --> 00:19:18,980
So in online learning

493
00:19:19,130 --> 00:19:20,240
what you care about is

494
00:19:20,380 --> 00:19:21,570
the total online error,

495
00:19:26,210 --> 00:19:30,920
which is sum from I equals one to MC if

496
00:19:31,060 --> 00:19:32,440
you get the sequence of M examples

497
00:19:32,590 --> 00:19:37,780
all together, indicator Y hat I not

498
00:19:37,950 --> 00:19:40,900
equal to Y hi. Okay?

499
00:19:41,060 --> 00:19:43,480
So the total online error is the total

500
00:19:43,640 --> 00:19:46,000
number of mistakes you make on a

501
00:19:46,170 --> 00:19:47,330
sequence of examples like this.

502
00:19:47,500 --> 00:19:53,470
And it turns out that, you know,

503
00:19:53,630 --> 00:19:55,450
many of the learning algorithms you

504
00:19:55,620 --> 00:19:56,920
have when you finish all the

505
00:19:57,080 --> 00:19:57,950
learning algorithms, you've learned

506
00:19:58,100 --> 00:19:59,120
about and can apply to this setting.

507
00:19:59,290 --> 00:20:01,370
One thing you could do is

508
00:20:01,530 --> 00:20:04,520
when you're asked to make prediction

509
00:20:04,690 --> 00:20:06,270
on Y hat three, right,

510
00:20:06,420 --> 00:20:08,410
one simple thing to do is well you've

511
00:20:08,570 --> 00:20:10,000
seen some other training examples up to

512
00:20:10,160 --> 00:20:11,820
this point so you can just take your

513
00:20:11,980 --> 00:20:13,100
learning algorithm and run it

514
00:20:13,260 --> 00:20:15,680
on the examples, you know,

515
00:20:15,840 --> 00:20:17,520
leading up to Y hat three.

516
00:20:17,680 --> 00:20:19,050
So just run the learning algorithm on

517
00:20:19,210 --> 00:20:20,120
all the examples you've seen previous

518
00:20:20,270 --> 00:20:22,470
to being asked to make a prediction

519
00:20:22,640 --> 00:20:24,120
on certain example, and then use your

520
00:20:24,280 --> 00:20:25,460
learning algorithm to make a prediction

521
00:20:25,620 --> 00:20:26,420
on the next example.

522
00:20:26,580 --> 00:20:28,830
And it turns out that there are also

523
00:20:28,990 --> 00:20:30,460
algorithms, especially the algorithms

524
00:20:30,610 --> 00:20:32,160
that we saw that you could use the

525
00:20:32,330 --> 00:20:33,540
stochastic gradient descent, that,

526
00:20:33,710 --> 00:20:34,790
you know, can be adapted

527
00:20:34,950 --> 00:20:35,610
very nicely to this.

528
00:20:35,770 --> 00:20:37,820
So as a concrete example,

529
00:20:40,650 --> 00:20:42,270
if you remember the perceptron

530
00:20:42,430 --> 00:20:45,370
algorithms, say, right,

531
00:20:45,540 --> 00:20:49,160
you would say initialize the parameter

532
00:20:49,320 --> 00:20:50,700
theta to be equal to zero.

533
00:20:50,860 --> 00:20:58,550
And then after seeing the Ith training

534
00:20:58,700 --> 00:21:01,440
example, you'd update the parameters,

535
00:21:09,670 --> 00:21:15,500
you know, using

536
00:21:15,650 --> 00:21:17,560
you've see this reel a lot of times now,

537
00:21:17,730 --> 00:21:18,630
right, using the

538
00:21:18,780 --> 00:21:20,890
standard perceptron learning rule.

539
00:21:21,050 --> 00:21:22,170
And the same thing,

540
00:21:22,330 --> 00:21:24,380
if you were using logistic regression

541
00:21:24,540 --> 00:21:27,140
you can then, again, after seeing each

542
00:21:27,300 --> 00:21:29,000
training example, just run, you know,

543
00:21:29,160 --> 00:21:31,470
essentially run one-step stochastic

544
00:21:31,680 --> 00:21:34,250
gradient descent on just the example

545
00:21:34,820 --> 00:21:37,200
you saw. Okay?

546
00:21:37,350 --> 00:21:41,360
And so the reason I've put this

547
00:21:41,510 --> 00:21:43,470
into the sort of "learning theory"

548
00:21:43,640 --> 00:21:44,630
section of this class was because

549
00:21:44,800 --> 00:21:46,510
it turns that sometimes you can prove

550
00:21:46,670 --> 00:21:47,760
fairly amazing results

551
00:21:47,910 --> 00:21:50,570
on your total online error

552
00:21:50,730 --> 00:21:52,420
using algorithms like these.

553
00:21:52,590 --> 00:21:55,900
I will actually I don't actually want

554
00:21:56,060 --> 00:21:57,700
to spend the time in the main lecture

555
00:21:57,850 --> 00:21:58,950
to prove this, but, for example,

556
00:21:59,120 --> 00:22:00,090
you can prove that

557
00:22:00,250 --> 00:22:03,070
when you use the perceptron algorithm,

558
00:22:03,230 --> 00:22:11,590
then even when the features XI, maybe

559
00:22:11,750 --> 00:22:13,570
infinite dimensional feature vectors,

560
00:22:13,730 --> 00:22:15,770
like we saw for simple vector machines.

561
00:22:15,930 --> 00:22:16,590
And sometimes,

562
00:22:16,770 --> 00:22:17,890
infinite feature dimensional vectors

563
00:22:18,050 --> 00:22:20,550
may use kernel representations. Okay?

564
00:22:20,710 --> 00:22:21,860
But so it turns out that you can prove

565
00:22:22,030 --> 00:22:23,320
that when you a perceptron algorithm,

566
00:22:23,480 --> 00:22:25,220
even when the data is

567
00:22:25,420 --> 00:22:28,490
maybe extremely high dimensional

568
00:22:28,650 --> 00:22:29,740
and it seems like you'd be prone to

569
00:22:29,890 --> 00:22:32,780
overfitting, right, you can prove that

570
00:22:32,950 --> 00:22:34,630
so as the long as the positive and

571
00:22:34,780 --> 00:22:39,050
negative examples are separated

572
00:22:39,220 --> 00:22:44,210
by a margin, right. So in this infinite

573
00:22:44,380 --> 00:22:49,930
dimensional space,so long as, you know,

574
00:22:50,100 --> 00:22:53,450
there is some margin down there

575
00:22:53,600 --> 00:22:54,810
separating the positive and negative

576
00:22:54,970 --> 00:22:59,670
examples,you can prove that perceptron

577
00:22:59,860 --> 00:23:02,770
algorithm will converge to a hypothesis

578
00:23:02,920 --> 00:23:04,540
that perfectly separates the positive

579
00:23:04,690 --> 00:23:07,320
and negative examples. Okay? And then

580
00:23:07,480 --> 00:23:09,060
so after seeing only a finite number of

581
00:23:09,230 --> 00:23:12,100
examples, it'll converge to digital

582
00:23:12,260 --> 00:23:13,560
boundary that perfectly separates

583
00:23:13,730 --> 00:23:14,760
the positive and negative examples,

584
00:23:14,920 --> 00:23:15,740
even though you may in an

585
00:23:15,910 --> 00:23:17,510
infinite dimensional space. Okay?

586
00:23:17,660 --> 00:23:21,190
So let's see.

587
00:23:21,340 --> 00:23:24,000
The proof itself would take me sort of

588
00:23:24,160 --> 00:23:25,280
almost an entire lecture to do,

589
00:23:25,450 --> 00:23:27,320
and there are sort of other things

590
00:23:27,460 --> 00:23:28,680
that I want to do more than that.

591
00:23:28,830 --> 00:23:30,000
So you want to see

592
00:23:30,150 --> 00:23:31,250
the proof of this yourself,

593
00:23:31,400 --> 00:23:32,650
it's actually written up in the

594
00:23:32,810 --> 00:23:33,890
lecture notes that I posted online.

595
00:23:34,040 --> 00:23:35,980
For the purposes of this class'

596
00:23:36,130 --> 00:23:38,280
syllabus, the proof of this result,

597
00:23:38,430 --> 00:23:39,510
you can treat this as optional reading.

598
00:23:39,680 --> 00:23:40,740
And by that, I mean, you know,

599
00:23:41,140 --> 00:23:42,550
it won't appear

600
00:23:42,710 --> 00:23:44,000
on the midterm and you won't be

601
00:23:44,160 --> 00:23:45,200
asked about this specifically in the

602
00:23:45,200 --> 00:23:46,200
problem sets, but I thought it'd be –

603
00:23:48,420 --> 00:23:49,600
I know some of you

604
00:23:49,760 --> 00:23:51,380
are curious after the previous lecture

605
00:23:51,530 --> 00:23:53,450
so why you can prove that, you know,

606
00:23:53,620 --> 00:23:55,870
SVMs can have bounded VC dimension,

607
00:23:56,030 --> 00:23:58,360
even in these infinite dimensional

608
00:23:58,530 --> 00:23:59,980
spaces, and how do you prove things in

609
00:24:00,150 --> 00:24:01,670
these how do you prove learning theory

610
00:24:01,830 --> 00:24:03,440
results in these infinite dimensional

611
00:24:03,600 --> 00:24:05,450
feature spaces. And so the perceptron

612
00:24:05,620 --> 00:24:06,830
bound that I just talked about was

613
00:24:06,980 --> 00:24:08,470
the simplest instance I know of

614
00:24:08,630 --> 00:24:10,180
that you can sort of read

615
00:24:10,350 --> 00:24:11,530
in like half an hour and understand it.

616
00:24:11,700 --> 00:24:12,550
So if you're interested,

617
00:24:12,710 --> 00:24:15,080
there are lecture notes online

618
00:24:15,240 --> 00:24:16,770
for how this perceptron bound

619
00:24:16,930 --> 00:24:17,870
is actually proved.

620
00:24:18,020 --> 00:24:19,400
It's a very [inaudible],

621
00:24:19,560 --> 00:24:20,950
you can prove it in like a page or so,

622
00:24:21,110 --> 00:24:22,610
so go ahead and take a look at that

623
00:24:22,780 --> 00:24:24,490
if you're interested. Okay?

624
00:24:24,660 --> 00:24:25,530
But regardless of

625
00:24:25,690 --> 00:24:28,560
the theoretical results, you know,

626
00:24:28,720 --> 00:24:29,630
the online learning setting

627
00:24:29,790 --> 00:24:30,620
is something that you

628
00:24:30,780 --> 00:24:32,050
that comes reasonably often.

629
00:24:32,210 --> 00:24:33,870
And so, these algorithms

630
00:24:34,180 --> 00:24:35,180
based on stochastic gradient descent

631
00:24:35,350 --> 00:24:36,270
often go very well.

632
00:24:36,440 --> 00:24:39,400
Okay, any questions about this

633
00:24:39,560 --> 00:24:40,470
before I move on?

634
00:24:47,430 --> 00:24:48,920
All right. Cool.

635
00:24:49,090 --> 00:24:51,350
So the last thing I want to do today,

636
00:24:51,540 --> 00:24:53,240
and was the majority of today's lecture,

637
00:24:53,400 --> 00:24:54,710
actually can I switch to PowerPoint

638
00:24:54,880 --> 00:24:57,240
slides, please, is I actually want to

639
00:24:57,410 --> 00:24:59,090
spend most of today's lecture sort of

640
00:24:59,250 --> 00:25:00,230
talking about advice for applying

641
00:25:00,400 --> 00:25:01,630
different machine learning algorithms.

642
00:25:10,350 --> 00:25:13,370
And so, you know, right now, already you

643
00:25:13,530 --> 00:25:14,870
have a, I think, a good understanding

644
00:25:15,040 --> 00:25:18,510
of really the most powerful tools known

645
00:25:18,660 --> 00:25:19,980
to humankind in machine learning.

646
00:25:20,140 --> 00:25:20,980
Right?

647
00:25:21,140 --> 00:25:23,030
And what I want to do today is to

648
00:25:23,200 --> 00:25:24,520
give you some advice

649
00:25:24,670 --> 00:25:25,910
on how to apply them really powerfully

650
00:25:26,070 --> 00:25:28,420
because, you know, the same tool

651
00:25:28,580 --> 00:25:30,220
it turns out that you can

652
00:25:30,390 --> 00:25:31,560
take the same machine learning tool,

653
00:25:31,720 --> 00:25:32,590
say logistic regression,

654
00:25:32,750 --> 00:25:34,510
and you can ask two different people

655
00:25:34,680 --> 00:25:35,630
to apply it to the same problem.

656
00:25:35,790 --> 00:25:38,510
And sometimes one person

657
00:25:38,660 --> 00:25:39,580
will do an amazing job

658
00:25:39,730 --> 00:25:40,750
and it'll work amazingly well,

659
00:25:40,910 --> 00:25:42,040
and the second person will sort of

660
00:25:42,200 --> 00:25:43,910
not really get it to work,

661
00:25:44,060 --> 00:25:45,190
even though it was exactly

662
00:25:45,350 --> 00:25:46,390
the same algorithm. Right?

663
00:25:46,550 --> 00:25:49,040
And so what I want to do today,

664
00:25:49,210 --> 00:25:50,650
in the rest of the time I have today,

665
00:25:50,810 --> 00:25:53,160
is try to convey to you, you know,

666
00:25:53,320 --> 00:25:55,250
some of the methods for how to make sure

667
00:25:55,410 --> 00:25:57,340
you're one of you really know how to

668
00:25:57,500 --> 00:25:58,320
get these learning algorithms

669
00:25:58,480 --> 00:25:59,580
to work well in problems.

670
00:25:59,740 --> 00:26:06,530
So just some caveats on what I'm gonna,

671
00:26:06,690 --> 00:26:07,790
I guess, talk about

672
00:26:07,960 --> 00:26:09,470
in the rest of today's lecture.

673
00:26:09,630 --> 00:26:12,660
Something I want to talk about is

674
00:26:12,820 --> 00:26:13,870
actually not very mathematical

675
00:26:14,020 --> 00:26:16,350
but is also some of the hardest,

676
00:26:16,530 --> 00:26:19,190
most conceptually most difficult

677
00:26:19,360 --> 00:26:20,630
material in this class to understand.

678
00:26:20,780 --> 00:26:21,390
All right?

679
00:26:21,540 --> 00:26:23,510
So this is not mathematical

680
00:26:23,670 --> 00:26:24,560
but this is not easy.

681
00:26:24,710 --> 00:26:27,350
And I want to say this caveat some of

682
00:26:27,500 --> 00:26:28,710
what I'll say today is debatable.

683
00:26:28,880 --> 00:26:30,270
I think most good machine learning

684
00:26:30,430 --> 00:26:31,560
people will agree with

685
00:26:31,730 --> 00:26:32,420
most of what I say

686
00:26:32,580 --> 00:26:33,540
but maybe not everything I say.

687
00:26:33,700 --> 00:26:36,340
And some of what I'll say

688
00:26:36,500 --> 00:26:37,270
is also not good advice

689
00:26:37,430 --> 00:26:38,610
for doing machine learning either,

690
00:26:38,780 --> 00:26:39,630
so I'll say more about this later.

691
00:26:39,780 --> 00:26:42,750
What I'm focusing on today is advice

692
00:26:42,910 --> 00:26:44,630
for how to just get stuff to work.

693
00:26:44,780 --> 00:26:45,770
If you work in the company

694
00:26:45,940 --> 00:26:47,110
and you want to deliver a product

695
00:26:47,260 --> 00:26:49,420
or you're, you know, building a system

696
00:26:49,570 --> 00:26:50,510
and you just want your

697
00:26:50,690 --> 00:26:52,160
machine learning system to work. Okay?

698
00:26:52,310 --> 00:26:53,190
Some of what I'm about to say today

699
00:26:53,340 --> 00:26:55,500
isn't great advice if you goal is to

700
00:26:55,650 --> 00:26:56,970
invent a new machine learning algorithm,

701
00:26:57,120 --> 00:26:58,250
but this is advice

702
00:26:58,410 --> 00:26:59,810
for how to make machine learning

703
00:26:59,970 --> 00:27:01,390
algorithm work and, you know,

704
00:27:01,540 --> 00:27:02,790
and deploy a working system.

705
00:27:02,950 --> 00:27:06,690
So three key areas I'm gonna talk about.

706
00:27:06,860 --> 00:27:09,640
One: diagnostics for debugging

707
00:27:09,790 --> 00:27:10,770
learning algorithms.

708
00:27:10,930 --> 00:27:13,540
Second: sort of talk briefly about

709
00:27:13,700 --> 00:27:15,150
error analyses and ablative analysis.

710
00:27:15,310 --> 00:27:18,970
And third, I want to talk about just

711
00:27:19,120 --> 00:27:20,710
advice for how to get started on a

712
00:27:20,870 --> 00:27:23,490
machine-learning problem.

713
00:27:23,650 --> 00:27:26,290
And one theme that'll come up later is

714
00:27:26,440 --> 00:27:28,610
it turns out you've heard about

715
00:27:28,790 --> 00:27:30,690
premature optimization, right,

716
00:27:30,860 --> 00:27:31,890
in writing software.

717
00:27:32,050 --> 00:27:32,590
This is when

718
00:27:32,760 --> 00:27:35,130
someone over-designs from the start,

719
00:27:35,290 --> 00:27:36,340
when someone, you know,

720
00:27:36,490 --> 00:27:37,330
is writing piece of code

721
00:27:37,500 --> 00:27:39,170
and they choose a subroutine

722
00:27:39,320 --> 00:27:41,250
to optimize heavily.

723
00:27:41,410 --> 00:27:42,760
And maybe you write the subroutine

724
00:27:42,920 --> 00:27:43,930
as assembly or something.

725
00:27:44,080 --> 00:27:46,300
And that's often and many of us have

726
00:27:46,480 --> 00:27:48,200
been guilty of premature optimization,

727
00:27:48,360 --> 00:27:49,570
where we're trying to

728
00:27:49,730 --> 00:27:50,840
get a piece of code to run faster.

729
00:27:50,990 --> 00:27:52,430
And we choose probably a piece of code

730
00:27:52,630 --> 00:27:54,150
and we implement it an assembly,

731
00:27:54,310 --> 00:27:55,250
and really tune and

732
00:27:55,400 --> 00:27:56,330
get to run really quickly.

733
00:27:56,500 --> 00:27:57,860
And it turns out that wasn't the

734
00:27:58,010 --> 00:27:59,480
bottleneck in the code at all. Right?

735
00:27:59,630 --> 00:28:00,500
And we call that premature

736
00:28:00,670 --> 00:28:01,560
optimization.

737
00:28:01,720 --> 00:28:03,320
And in undergraduate programming

738
00:28:03,470 --> 00:28:04,990
classes, we warn people all the time

739
00:28:05,150 --> 00:28:06,610
not to do premature optimization and

740
00:28:06,760 --> 00:28:08,350
people still do it all the time. Right?

741
00:28:08,510 --> 00:28:11,960
And turns out,

742
00:28:12,120 --> 00:28:13,290
a very similar thing happens in

743
00:28:13,460 --> 00:28:15,100
building machine-learning systems.

744
00:28:15,260 --> 00:28:17,300
That many people are often guilty of,

745
00:28:17,470 --> 00:28:18,870
what I call, premature statistical

746
00:28:19,030 --> 00:28:21,510
optimization, where they heavily

747
00:28:21,680 --> 00:28:23,880
optimize part of a machine learning

748
00:28:24,040 --> 00:28:25,880
system and that turns out

749
00:28:26,040 --> 00:28:27,630
not to be the important piece. Okay?

750
00:28:27,790 --> 00:28:28,880
So I'll talk about that later, as well.

751
00:28:29,050 --> 00:28:30,890
So let's first talk about

752
00:28:31,040 --> 00:28:32,130
debugging learning algorithms.

753
00:28:38,460 --> 00:28:40,560
As a motivating example,

754
00:28:40,720 --> 00:28:43,840
let's say you want to build an

755
00:28:44,000 --> 00:28:45,140
anti-spam system.

756
00:28:45,300 --> 00:28:47,900
And let's say you've carefully chosen,

757
00:28:48,070 --> 00:28:49,900
you know, a small set of 100 words

758
00:28:50,060 --> 00:28:51,640
to use as features. All right?

759
00:28:51,790 --> 00:28:52,930
So instead of using 50,000 words,

760
00:28:53,080 --> 00:28:53,590
you've chosen

761
00:28:53,740 --> 00:28:54,630
a small set of 100 features

762
00:28:54,790 --> 00:28:56,700
to use for your anti-spam system.

763
00:28:56,860 --> 00:28:59,660
And let's say you implement

764
00:28:59,830 --> 00:29:00,850
Bayesian logistic regression,

765
00:29:01,010 --> 00:29:02,210
implement gradient descent,

766
00:29:02,370 --> 00:29:04,010
and you get 20 percent test error,

767
00:29:04,170 --> 00:29:06,570
which is unacceptably high. Right?

768
00:29:06,740 --> 00:29:08,580
So this is Bayesian logistic

769
00:29:08,750 --> 00:29:10,290
regression, and so it's just like

770
00:29:10,480 --> 00:29:12,000
maximum likelihood but, you know,

771
00:29:12,170 --> 00:29:14,260
with that additional lambda squared

772
00:29:14,430 --> 00:29:16,110
term. And we're maximizing

773
00:29:16,260 --> 00:29:17,780
rather than minimizing as well,

774
00:29:17,950 --> 00:29:19,980
so there's a minus lambda theta square

775
00:29:20,130 --> 00:29:21,340
instead of plus lambda theta squared.

776
00:29:21,500 --> 00:29:24,200
So the question is,

777
00:29:24,360 --> 00:29:26,050
you implemented your Bayesian

778
00:29:26,220 --> 00:29:27,200
logistic regression algorithm,

779
00:29:27,360 --> 00:29:29,840
and you tested it on your test set

780
00:29:29,990 --> 00:29:31,180
and you got unacceptably high error,

781
00:29:31,340 --> 00:29:33,060
so what do you do next? Right?

782
00:29:35,220 --> 00:29:38,260
So, you know, one thing you could do is

783
00:29:38,420 --> 00:29:39,400
think about the ways

784
00:29:39,550 --> 00:29:40,470
you could improve this algorithm.

785
00:29:40,620 --> 00:29:41,430
And this is probably

786
00:29:41,590 --> 00:29:42,900
what most people will do instead of,

787
00:29:43,050 --> 00:29:44,240
"Well let's sit down and think

788
00:29:44,390 --> 00:29:45,630
what could've gone wrong, and then

789
00:29:45,790 --> 00:29:46,790
we'll try to improve the algorithm."

790
00:29:46,940 --> 00:29:49,410
Well obviously having more training

791
00:29:49,580 --> 00:29:50,360
data could only help,

792
00:29:50,510 --> 00:29:51,430
so one thing you can do is

793
00:29:51,570 --> 00:29:52,350
try to get more training examples.

794
00:29:54,190 --> 00:29:55,470
Maybe you suspect,

795
00:29:55,610 --> 00:29:57,060
that even 100 features was too many,

796
00:29:57,220 --> 00:29:58,180
so you might try to get

797
00:29:58,350 --> 00:29:59,950
a smaller set of features.

798
00:30:00,110 --> 00:30:03,000
What's more common is you might suspect

799
00:30:03,160 --> 00:30:04,110
your features aren't good enough,

800
00:30:04,270 --> 00:30:05,240
so you might spend some time,

801
00:30:05,390 --> 00:30:06,140
look at the email headers,

802
00:30:06,300 --> 00:30:07,510
see if you can figure out

803
00:30:07,670 --> 00:30:08,600
better features for, you know,

804
00:30:08,750 --> 00:30:11,950
finding spam emails or whatever. Right.

805
00:30:12,110 --> 00:30:15,320
And right, so and just sit around

806
00:30:15,470 --> 00:30:16,660
and come up with better features,

807
00:30:16,820 --> 00:30:18,700
such as for email headers.

808
00:30:18,870 --> 00:30:22,090
You may also suspect that gradient

809
00:30:22,250 --> 00:30:23,560
descent hasn't quite converged yet,

810
00:30:23,730 --> 00:30:25,030
and so let's try running gradient

811
00:30:25,190 --> 00:30:26,310
descent a bit longer to see

812
00:30:26,460 --> 00:30:27,140
if that works.

813
00:30:27,290 --> 00:30:28,320
And clearly, that can't hurt, right,

814
00:30:28,470 --> 00:30:30,280
just run gradient descent longer.

815
00:30:30,440 --> 00:30:33,040
Or maybe you remember, you know,

816
00:30:33,190 --> 00:30:34,710
you remember hearing from class that

817
00:30:34,850 --> 00:30:36,450
maybe Newton's method converges better,

818
00:30:36,610 --> 00:30:37,620
so let's try that instead.

819
00:30:39,250 --> 00:30:40,280
You may want to tune the

820
00:30:40,440 --> 00:30:41,190
value for lambda,

821
00:30:41,370 --> 00:30:42,120
because not sure

822
00:30:42,280 --> 00:30:42,950
if that was the right thing,

823
00:30:43,120 --> 00:30:45,350
or maybe you even want to use an SVM

824
00:30:45,510 --> 00:30:47,150
because maybe you think an SVM might

825
00:30:47,300 --> 00:30:48,830
work better than logistic regression.

826
00:30:48,990 --> 00:30:51,840
So I only listed eight things here,

827
00:30:51,990 --> 00:30:52,480
but you can imagine

828
00:30:52,630 --> 00:30:54,030
if you were actually sitting down,

829
00:30:54,180 --> 00:30:55,240
building machine-learning system,

830
00:30:55,390 --> 00:30:57,010
the options to you are endless.

831
00:30:57,170 --> 00:30:58,660
You can think of, you know, hundreds of

832
00:30:58,810 --> 00:31:00,150
ways to improve a learning system.

833
00:31:00,320 --> 00:31:01,930
And some of these things like,

834
00:31:02,100 --> 00:31:03,460
well getting more training examples,

835
00:31:03,620 --> 00:31:05,070
surely that's gonna help, so that seems

836
00:31:05,220 --> 00:31:06,760
like it's a good use of your time. Right?

837
00:31:06,920 --> 00:31:11,770
And it turns out that this approach

838
00:31:11,930 --> 00:31:13,910
of picking ways to improve the learning

839
00:31:14,060 --> 00:31:15,530
algorithm and picking one and going for it,

840
00:31:15,680 --> 00:31:17,410
it might work in the sense that

841
00:31:17,560 --> 00:31:18,850
it may eventually get you to

842
00:31:19,010 --> 00:31:20,090
a working system,

843
00:31:20,240 --> 00:31:21,630
but often it's very time-consuming.

844
00:31:21,820 --> 00:31:24,000
And I think it's often a largely

845
00:31:24,160 --> 00:31:25,410
largely a matter of luck,

846
00:31:25,580 --> 00:31:26,730
whether you end up fixing

847
00:31:26,880 --> 00:31:27,690
what the problem is.

848
00:31:27,850 --> 00:31:30,300
In particular, these eight improvements

849
00:31:30,460 --> 00:31:32,270
all fix very different problems.

850
00:31:32,440 --> 00:31:34,500
And some of them will be fixing problems

851
00:31:34,670 --> 00:31:35,560
that you don't have.

852
00:31:35,720 --> 00:31:38,330
And if you can rule out

853
00:31:38,490 --> 00:31:40,440
six of eight of these,

854
00:31:40,590 --> 00:31:41,420
say, you could

855
00:31:41,590 --> 00:31:42,570
if by somehow

856
00:31:42,730 --> 00:31:43,780
looking at the problem more deeply,

857
00:31:43,940 --> 00:31:44,670
you can figure out

858
00:31:44,830 --> 00:31:45,920
which one of these eight things

859
00:31:46,080 --> 00:31:47,140
is actually the right thing to do,

860
00:31:47,290 --> 00:31:49,480
you can save yourself a lot of time.

861
00:31:49,640 --> 00:31:51,260
So let's see

862
00:31:51,410 --> 00:31:52,330
how we can go about doing that.

863
00:31:55,390 --> 00:31:58,280
The people in industry and in research

864
00:31:58,440 --> 00:32:00,090
that I see that are really good,

865
00:32:00,260 --> 00:32:02,370
would not go and try to

866
00:32:02,530 --> 00:32:03,960
change a learning algorithm randomly.

867
00:32:04,120 --> 00:32:05,880
There are lots of things that obviously

868
00:32:06,080 --> 00:32:07,580
improve your learning algorithm,

869
00:32:07,760 --> 00:32:08,680
but the problem is

870
00:32:08,840 --> 00:32:10,050
there are so many of them

871
00:32:10,220 --> 00:32:11,060
it's hard to know what to do.

872
00:32:11,210 --> 00:32:14,030
So you find all the really good ones

873
00:32:14,190 --> 00:32:16,110
that run various diagnostics

874
00:32:16,270 --> 00:32:17,300
to figure out the problem is

875
00:32:17,450 --> 00:32:20,060
and they think where a problem is. Okay?

876
00:32:20,220 --> 00:32:25,550
So for our motivating story, right, we

877
00:32:25,710 --> 00:32:26,820
said let's say Bayesian logistic

878
00:32:26,990 --> 00:32:28,410
regression test error was 20 percent,

879
00:32:28,580 --> 00:32:30,380
which let's say is unacceptably high.

880
00:32:30,530 --> 00:32:33,030
And let's suppose you suspected

881
00:32:33,190 --> 00:32:35,300
the problem is either overfitting,

882
00:32:35,450 --> 00:32:37,210
so it's high bias,

883
00:32:37,370 --> 00:32:39,160
or you suspect that, you know,

884
00:32:39,310 --> 00:32:40,160
maybe you have two few features

885
00:32:40,300 --> 00:32:41,780
that classify as spam, so there's

886
00:32:41,930 --> 00:32:42,740
Oh excuse me;

887
00:32:42,900 --> 00:32:45,730
I think I wrote that wrong.

888
00:32:45,880 --> 00:32:48,410
Let's firstly so let's forget

889
00:32:48,570 --> 00:32:49,490
forget the tables.

890
00:32:49,640 --> 00:32:50,580
Suppose you suspect the problem is

891
00:32:50,730 --> 00:32:51,640
either high bias or high variance,

892
00:32:51,800 --> 00:32:52,850
and some of the text here

893
00:32:53,000 --> 00:32:53,820
doesn't make sense.

894
00:32:53,980 --> 00:32:55,820
And you want to know

895
00:32:55,980 --> 00:32:57,940
if you're overfitting,

896
00:32:58,090 --> 00:32:59,310
which would be high variance,

897
00:32:59,470 --> 00:33:00,700
or you have too few features

898
00:33:00,860 --> 00:33:01,860
classified as spam,

899
00:33:02,020 --> 00:33:02,840
it'd be high bias.

900
00:33:02,980 --> 00:33:04,910
I had those two reversed, sorry. Okay?

901
00:33:05,060 --> 00:33:06,720
So how can you figure out

902
00:33:06,880 --> 00:33:09,940
whether the problem is one of high bias

903
00:33:10,090 --> 00:33:12,600
or high variance? Right?

904
00:33:14,830 --> 00:33:16,460
So it turns out there's a simple

905
00:33:16,610 --> 00:33:18,050
diagnostic you can look at that will

906
00:33:18,210 --> 00:33:19,940
tell you whether the problem is high

907
00:33:20,100 --> 00:33:21,170
bias or high variance.

908
00:33:21,340 --> 00:33:25,070
If you remember the cartoon we'd seen

909
00:33:25,220 --> 00:33:26,860
previously for high variance problems,

910
00:33:27,010 --> 00:33:28,000
when you have high variance

911
00:33:28,150 --> 00:33:31,140
the training error will be

912
00:33:31,290 --> 00:33:32,450
much lower than the test error.

913
00:33:32,640 --> 00:33:33,380
All right?

914
00:33:33,550 --> 00:33:34,490
When you have a high variance problem,

915
00:33:34,650 --> 00:33:35,340
that's when you're

916
00:33:35,510 --> 00:33:36,980
fitting your training set very well.

917
00:33:37,140 --> 00:33:38,380
That's when you're fitting, you know,

918
00:33:38,550 --> 00:33:39,210
a tenth order polynomial

919
00:33:39,370 --> 00:33:41,520
to 11 data points. All right?

920
00:33:41,680 --> 00:33:42,510
And that's when you're just

921
00:33:42,660 --> 00:33:43,550
fitting the data set very well,

922
00:33:43,700 --> 00:33:44,720
and so your training error will be

923
00:33:44,870 --> 00:33:45,980
much lower than your test error.

924
00:33:46,130 --> 00:33:48,560
And in contrast, if you have high bias,

925
00:33:48,710 --> 00:33:50,680
that's when your training error

926
00:33:50,850 --> 00:33:52,720
will also be high. Right?

927
00:33:52,870 --> 00:33:54,260
That's when your data is quadratic, say,

928
00:33:54,430 --> 00:33:56,170
but you're fitting a linear function to

929
00:33:56,320 --> 00:33:57,380
it and so you aren't even

930
00:33:57,540 --> 00:33:58,580
fitting your training set well.

931
00:33:58,730 --> 00:34:03,180
So just in cartoons, I guess,

932
00:34:03,330 --> 00:34:05,860
this is a this is what a typical

933
00:34:06,030 --> 00:34:07,720
learning curve for high variance looks

934
00:34:07,880 --> 00:34:10,170
like. On your horizontal axis,

935
00:34:10,320 --> 00:34:12,340
I'm plotting the training set size M,

936
00:34:12,500 --> 00:34:14,730
and on vertical axis,

937
00:34:14,890 --> 00:34:15,590
I'm plotting the error.

938
00:34:15,750 --> 00:34:19,710
And so, let's see, you know,

939
00:34:19,870 --> 00:34:20,640
as you increase

940
00:34:20,800 --> 00:34:22,570
if you have a high variance problem,

941
00:34:22,730 --> 00:34:23,750
you'll notice

942
00:34:23,900 --> 00:34:25,470
as the training set size, M, increases,

943
00:34:25,630 --> 00:34:27,440
your test set error will

944
00:34:27,600 --> 00:34:28,330
keep on decreasing.

945
00:34:28,500 --> 00:34:30,770
And so this sort of suggests that, well,

946
00:34:30,940 --> 00:34:31,720
if you can increase

947
00:34:31,870 --> 00:34:32,840
the training set size even further,

948
00:34:33,010 --> 00:34:34,240
maybe if you extrapolate

949
00:34:34,410 --> 00:34:35,730
the green curve out,

950
00:34:35,910 --> 00:34:37,220
maybe that test set error will

951
00:34:37,380 --> 00:34:38,480
decrease even further.

952
00:34:38,640 --> 00:34:39,450
All right?

953
00:34:39,610 --> 00:34:41,470
Another thing that's useful to plot here

954
00:34:41,620 --> 00:34:44,090
is let's say the red horizontal line

955
00:34:44,250 --> 00:34:45,360
is the desired performance

956
00:34:45,530 --> 00:34:46,250
you're trying to reach,

957
00:34:46,420 --> 00:34:47,720
another useful thing to plot

958
00:34:47,880 --> 00:34:49,900
is actually the training error. Right?

959
00:34:50,060 --> 00:34:50,890
And it turns out that

960
00:34:51,040 --> 00:34:53,380
your training error will actually grow

961
00:34:53,540 --> 00:34:54,970
as a function of the training set size

962
00:34:55,130 --> 00:35:00,900
because the larger your training set,

963
00:35:01,060 --> 00:35:03,880
the harder it is to fit, you know,

964
00:35:04,040 --> 00:35:05,550
your training set perfectly. Right?

965
00:35:05,710 --> 00:35:06,860
So this is just a cartoon,

966
00:35:07,030 --> 00:35:08,030
don't take it too seriously,

967
00:35:08,190 --> 00:35:08,830
but in general,

968
00:35:08,990 --> 00:35:10,000
your training error will actually grow

969
00:35:10,160 --> 00:35:12,620
as a function of your training set size.

970
00:35:12,780 --> 00:35:13,910
Because smart training sets,

971
00:35:14,070 --> 00:35:15,110
if you have one data point,

972
00:35:15,260 --> 00:35:16,300
it's really easy to fit that perfectly,

973
00:35:16,450 --> 00:35:18,460
but if you have 10,000 data points,

974
00:35:18,620 --> 00:35:20,030
it's much harder to fit that perfectly.

975
00:35:20,200 --> 00:35:21,070
All right?

976
00:35:21,230 --> 00:35:24,580
And so another diagnostic

977
00:35:24,730 --> 00:35:25,640
for high variance,

978
00:35:25,800 --> 00:35:27,240
and the one that I tend to use more,

979
00:35:27,410 --> 00:35:28,420
is to just look at

980
00:35:28,580 --> 00:35:29,980
training versus test error.

981
00:35:30,140 --> 00:35:31,470
And if there's a large gap between them,

982
00:35:31,620 --> 00:35:34,210
then this suggests that, you know,

983
00:35:34,360 --> 00:35:35,180
getting more training data

984
00:35:35,330 --> 00:35:36,770
may allow you to help close that gap.

985
00:35:36,930 --> 00:35:38,380
Okay？

986
00:35:39,600 --> 00:35:43,010
So this is what the cartoon would look

987
00:35:43,170 --> 00:35:45,950
like when in the case of high variance.

988
00:35:46,110 --> 00:35:50,400
This is what the cartoon looks like

989
00:35:50,550 --> 00:35:51,810
for high bias. Right?

990
00:35:51,980 --> 00:35:53,730
If you look at the learning curve,

991
00:35:53,890 --> 00:35:55,860
you see that the curve for test error

992
00:35:57,500 --> 00:35:58,660
has flattened out already.

993
00:35:58,810 --> 00:36:01,670
And so this is a sign that, you know,

994
00:36:01,830 --> 00:36:02,900
if you get more training examples,

995
00:36:03,070 --> 00:36:04,850
if you extrapolate this curve

996
00:36:05,010 --> 00:36:05,830
further to the right,

997
00:36:06,000 --> 00:36:07,660
it's maybe not likely to

998
00:36:07,810 --> 00:36:08,780
go down much further.

999
00:36:08,930 --> 00:36:10,690
And this is a property of high bias:

1000
00:36:10,850 --> 00:36:11,970
that getting more training data

1001
00:36:12,140 --> 00:36:13,230
won't necessarily help.

1002
00:36:13,390 --> 00:36:16,390
But again, to me

1003
00:36:16,550 --> 00:36:17,800
the more useful diagnostic is

1004
00:36:17,960 --> 00:36:21,010
if you plot training errors well,

1005
00:36:21,170 --> 00:36:22,130
if you look at your training error

1006
00:36:22,300 --> 00:36:23,680
as well as your, you know,

1007
00:36:23,840 --> 00:36:24,770
hold out test set error.

1008
00:36:24,930 --> 00:36:27,140
If you find that

1009
00:36:27,300 --> 00:36:29,700
even your training error is high,

1010
00:36:31,220 --> 00:36:33,020
then that's a sign that getting more

1011
00:36:33,180 --> 00:36:34,930
training data is not going to help.

1012
00:36:35,090 --> 00:36:36,430
Right?

1013
00:36:36,590 --> 00:36:39,950
In fact, you know, think about it,

1014
00:36:40,130 --> 00:36:44,900
training error grows as

1015
00:36:45,060 --> 00:36:46,190
a function of your training set size.

1016
00:36:46,350 --> 00:36:50,670
And so if your training error

1017
00:36:50,830 --> 00:36:51,980
is already above your

1018
00:36:52,150 --> 00:36:53,480
level of desired performance,

1019
00:36:53,640 --> 00:36:58,020
then getting even more training data

1020
00:36:58,180 --> 00:36:59,440
is not going to reduce your

1021
00:36:59,590 --> 00:37:01,250
training error down to the

1022
00:37:01,410 --> 00:37:03,010
desired level of performance. Right?

1023
00:37:03,160 --> 00:37:04,410
Because, you know, your training error

1024
00:37:04,560 --> 00:37:05,770
sort of only gets worse as you get

1025
00:37:05,940 --> 00:37:06,880
more and more training examples.

1026
00:37:07,060 --> 00:37:08,860
So if you extrapolate further to

1027
00:37:09,020 --> 00:37:10,440
the right, it's not like this blue line

1028
00:37:10,600 --> 00:37:11,840
will come back down to the level of

1029
00:37:12,000 --> 00:37:13,260
desired performance. Right?

1030
00:37:13,420 --> 00:37:15,390
This will stay up there. Okay?

1031
00:37:15,550 --> 00:37:19,170
So for me personally, I actually,

1032
00:37:19,320 --> 00:37:21,040
when looking at a curve like the green

1033
00:37:21,190 --> 00:37:22,440
curve on test error,

1034
00:37:22,610 --> 00:37:23,790
I actually personally tend to find it

1035
00:37:23,960 --> 00:37:25,730
very difficult to tell if the curve is

1036
00:37:25,880 --> 00:37:27,570
still going down or if it's [inaudible].

1037
00:37:27,730 --> 00:37:28,590
Sometimes you can tell,

1038
00:37:28,740 --> 00:37:29,340
but very often,

1039
00:37:29,490 --> 00:37:30,240
it's somewhat ambiguous.

1040
00:37:30,390 --> 00:37:32,290
So for me personally,

1041
00:37:32,450 --> 00:37:34,380
the diagnostic I tend to use the most

1042
00:37:34,540 --> 00:37:35,930
often to tell if I have a bias problem

1043
00:37:36,080 --> 00:37:38,440
or a variance problem is to look at

1044
00:37:38,600 --> 00:37:39,460
training and test error and see

1045
00:37:39,610 --> 00:37:40,800
if they're very close together

1046
00:37:40,960 --> 00:37:42,100
or if they're relatively far apart.

1047
00:37:42,260 --> 00:37:43,580
Okay?

1048
00:37:43,730 --> 00:37:47,860
And so, going back to the list of fixes,

1049
00:37:52,370 --> 00:37:53,580
look at the first fix,

1050
00:37:53,750 --> 00:37:55,290
getting more training examples

1051
00:37:55,450 --> 00:37:58,850
is a way to fix high variance. Right?

1052
00:37:59,000 --> 00:38:00,140
If you have a high variance problem,

1053
00:38:00,290 --> 00:38:01,010
getting more training examples

1054
00:38:01,160 --> 00:38:02,030
will help.

1055
00:38:02,180 --> 00:38:04,340
Trying a smaller set of features:

1056
00:38:04,490 --> 00:38:06,940
that also fixes high variance.

1057
00:38:07,090 --> 00:38:09,920
All right?

1058
00:38:11,810 --> 00:38:13,050
Trying a larger set of features

1059
00:38:13,210 --> 00:38:14,430
or adding email features,

1060
00:38:15,860 --> 00:38:17,660
these are solutions that fix high bias.

1061
00:38:17,820 --> 00:38:19,010
Right?

1062
00:38:19,160 --> 00:38:21,740
So high bias being if

1063
00:38:21,890 --> 00:38:23,020
your hypothesis was too simple,

1064
00:38:23,170 --> 00:38:25,020
you didn't have enough features. Okay?

1065
00:38:26,500 --> 00:38:29,940
And so quite often you see

1066
00:38:30,110 --> 00:38:30,800
people working on

1067
00:38:30,970 --> 00:38:32,020
machine learning problems and

1068
00:38:32,180 --> 00:38:35,650
they'll remember that

1069
00:38:35,810 --> 00:38:37,060
getting more training examples helps.

1070
00:38:37,230 --> 00:38:38,480
And so, they'll build a learning

1071
00:38:38,640 --> 00:38:40,280
system, build an anti-spam system

1072
00:38:40,450 --> 00:38:41,190
and it doesn't work.

1073
00:38:41,370 --> 00:38:42,900
And then they go off and

1074
00:38:43,050 --> 00:38:44,580
spend lots of time and money and effort

1075
00:38:44,750 --> 00:38:45,770
collecting more training data

1076
00:38:45,940 --> 00:38:46,520
because they'll say,

1077
00:38:46,680 --> 00:38:47,650
"Oh well, getting more data's

1078
00:38:47,820 --> 00:38:48,540
obviously got to help."

1079
00:38:48,710 --> 00:38:51,550
But if they had a high bias problem

1080
00:38:51,720 --> 00:38:52,490
in the first place,

1081
00:38:52,640 --> 00:38:53,480
and not a high variance problem,

1082
00:38:53,640 --> 00:38:55,810
it's entirely possible to

1083
00:38:55,970 --> 00:38:57,500
spend three months or six months

1084
00:38:57,650 --> 00:38:59,080
collecting more and more training data,

1085
00:38:59,230 --> 00:39:00,520
not realizing that

1086
00:39:00,680 --> 00:39:02,980
it couldn't possibly help. Right?

1087
00:39:03,140 --> 00:39:06,880
And so, this actually happens a lot in,

1088
00:39:07,040 --> 00:39:09,000
you know, in Silicon Valley and

1089
00:39:09,150 --> 00:39:10,050
companies, this happens a lot.

1090
00:39:10,210 --> 00:39:12,830
There will often people building

1091
00:39:12,990 --> 00:39:14,320
various machine learning systems,

1092
00:39:14,480 --> 00:39:17,000
and they'll often you often see people

1093
00:39:17,140 --> 00:39:18,090
spending six months

1094
00:39:18,260 --> 00:39:20,010
working on fixing a learning algorithm

1095
00:39:20,160 --> 00:39:21,760
and you could've told them

1096
00:39:21,920 --> 00:39:22,880
six months ago that, you know,

1097
00:39:23,060 --> 00:39:25,060
that couldn't possibly have helped.

1098
00:39:25,220 --> 00:39:26,670
But because they didn't know

1099
00:39:26,830 --> 00:39:27,560
what the problem was,

1100
00:39:27,720 --> 00:39:30,090
and they'd easily spend six months

1101
00:39:30,240 --> 00:39:31,310
trying to invent new features

1102
00:39:31,470 --> 00:39:32,170
or something.

1103
00:39:32,330 --> 00:39:35,080
And this is you see this surprisingly

1104
00:39:35,240 --> 00:39:36,810
often and this is somewhat depressing.

1105
00:39:36,980 --> 00:39:38,030
You could've gone to them and told them,

1106
00:39:38,190 --> 00:39:39,710
"I could've told you six months ago that

1107
00:39:39,870 --> 00:39:41,300
this was not going to help."

1108
00:39:41,460 --> 00:39:43,250
And the six months is not a joke,

1109
00:39:43,400 --> 00:39:44,220
you actually see this.

1110
00:39:46,630 --> 00:39:48,460
And in contrast,

1111
00:39:48,610 --> 00:39:50,510
if you actually figure out the problem's

1112
00:39:50,670 --> 00:39:51,690
one of high bias or high variance,

1113
00:39:51,850 --> 00:39:53,610
then you can rule out

1114
00:39:53,770 --> 00:39:55,130
two of these solutions

1115
00:39:55,320 --> 00:39:56,100
and save yourself

1116
00:39:56,270 --> 00:39:58,550
many months of fruitless effort. Okay?

1117
00:39:58,700 --> 00:40:01,300
I actually want to talk about

1118
00:40:01,480 --> 00:40:02,480
these four at the bottom as well.

1119
00:40:02,640 --> 00:40:03,640
But before I move on,

1120
00:40:03,800 --> 00:40:04,490
let me just check

1121
00:40:04,660 --> 00:40:05,480
if there were questions about

1122
00:40:05,650 --> 00:40:06,340
what I've talked about so far.

1123
00:40:14,770 --> 00:40:17,880
No? Okay, great.

1124
00:40:18,020 --> 00:40:21,210
So bias versus variance is

1125
00:40:21,370 --> 00:40:23,440
one thing that comes up often.

1126
00:40:23,600 --> 00:40:24,920
This bias versus variance

1127
00:40:25,070 --> 00:40:26,190
is one common diagnostic.

1128
00:40:26,350 --> 00:40:30,360
And so, for other machine learning

1129
00:40:30,520 --> 00:40:31,690
problems, it's often up to your

1130
00:40:31,840 --> 00:40:32,820
own ingenuity to

1131
00:40:32,970 --> 00:40:34,200
figure out your own diagnostics

1132
00:40:34,360 --> 00:40:35,640
to figure out what's wrong. All right?

1133
00:40:35,800 --> 00:40:37,230
So if a machine-learning algorithm

1134
00:40:37,390 --> 00:40:38,010
isn't working,

1135
00:40:38,170 --> 00:40:40,440
very often it's up to you to figure out,

1136
00:40:40,600 --> 00:40:42,220
you know, to construct your own tests.

1137
00:40:42,380 --> 00:40:43,590
Like do you look at the

1138
00:40:43,750 --> 00:40:44,640
difference training and test errors

1139
00:40:44,810 --> 00:40:45,920
or do you look at something else?

1140
00:40:46,090 --> 00:40:47,610
It's often up to your own ingenuity

1141
00:40:47,760 --> 00:40:49,240
to construct your own diagnostics

1142
00:40:49,390 --> 00:40:50,530
to figure out what's going on.

1143
00:40:50,690 --> 00:40:53,800
What I want to do is go through another

1144
00:40:53,960 --> 00:40:55,050
example. All right?

1145
00:40:55,210 --> 00:40:56,480
And this one is slightly more contrived

1146
00:40:56,640 --> 00:40:58,860
but it'll illustrate another common

1147
00:40:59,010 --> 00:41:00,460
question that comes up, another

1148
00:41:00,630 --> 00:41:03,180
one of the most common issues that comes

1149
00:41:03,350 --> 00:41:05,420
up in applying learning algorithms.

1150
00:41:05,580 --> 00:41:06,600
So in this example,

1151
00:41:06,770 --> 00:41:07,820
it's slightly more contrived,

1152
00:41:07,970 --> 00:41:09,640
let's say you implement Bayesian

1153
00:41:09,820 --> 00:41:10,710
logistic regression

1154
00:41:10,880 --> 00:41:13,300
and you get 2 percent error

1155
00:41:13,460 --> 00:41:14,350
on spam mail

1156
00:41:14,510 --> 00:41:16,240
and 2 percent error non-spam mail.

1157
00:41:16,400 --> 00:41:18,720
Right? So it's rejecting, you know,

1158
00:41:18,890 --> 00:41:22,700
2 percent of it's rejecting 98 percent

1159
00:41:22,850 --> 00:41:23,700
of your spam mail,

1160
00:41:23,860 --> 00:41:25,040
which is fine, so 2 percent of all spam

1161
00:41:25,200 --> 00:41:27,700
gets through which is fine, but is also

1162
00:41:27,880 --> 00:41:29,440
rejecting 2 percent of your good email,

1163
00:41:29,620 --> 00:41:31,420
2 percent of the email

1164
00:41:31,580 --> 00:41:32,480
from your friends and

1165
00:41:32,630 --> 00:41:34,250
that's unacceptably high, let's say.

1166
00:41:34,410 --> 00:41:39,950
And let's say that a simple vector

1167
00:41:40,100 --> 00:41:41,220
machine using a linear kernel

1168
00:41:41,380 --> 00:41:43,090
gets 10 percent error on spam

1169
00:41:43,240 --> 00:41:45,700
and 0.01 percent error on non-spam,

1170
00:41:45,850 --> 00:41:47,300
which is more of

1171
00:41:47,460 --> 00:41:48,500
the acceptable performance you want.

1172
00:41:48,650 --> 00:41:50,180
And let's say for the sake of this

1173
00:41:50,350 --> 00:41:51,710
example, let's say you're trying to

1174
00:41:51,860 --> 00:41:52,920
build an anti-spam system. Right?

1175
00:41:53,080 --> 00:41:55,050
Let's say that you really want to deploy

1176
00:41:55,210 --> 00:41:58,430
logistic regression to your customers

1177
00:41:58,590 --> 00:41:59,890
because of computational efficiency

1178
00:42:00,040 --> 00:42:01,140
or because you need

1179
00:42:01,300 --> 00:42:02,660
retrain overnight every day,

1180
00:42:02,830 --> 00:42:04,430
and because logistic regression just

1181
00:42:04,600 --> 00:42:05,980
runs more easily and more

1182
00:42:06,140 --> 00:42:07,220
quickly or something. Okay?

1183
00:42:07,370 --> 00:42:07,940
So let's say you

1184
00:42:08,100 --> 00:42:09,480
want to deploy logistic regression,

1185
00:42:09,630 --> 00:42:10,550
but it's just not working out well.

1186
00:42:10,710 --> 00:42:14,080
So question is: What do you do next?

1187
00:42:14,240 --> 00:42:18,320
So it turns out that this

1188
00:42:18,480 --> 00:42:20,250
the issue that comes up here,

1189
00:42:20,410 --> 00:42:22,450
the one other common question that

1190
00:42:22,600 --> 00:42:25,440
comes up is a question of

1191
00:42:25,590 --> 00:42:26,980
is the algorithm converging.

1192
00:42:27,120 --> 00:42:28,690
So you might suspect that maybe the

1193
00:42:28,840 --> 00:42:31,080
problem with logistic regression

1194
00:42:31,250 --> 00:42:32,510
is that it's just not converging.

1195
00:42:32,660 --> 00:42:34,630
Maybe you need to run iterations.

1196
00:42:34,790 --> 00:42:37,860
And it turns out that, again if you

1197
00:42:38,000 --> 00:42:39,020
look at the optimization objective,

1198
00:42:39,180 --> 00:42:40,900
say, logistic regression is,

1199
00:42:41,070 --> 00:42:42,580
let's say, optimizing J of theta,

1200
00:42:42,740 --> 00:42:44,660
it actually turns out that if you

1201
00:42:44,810 --> 00:42:45,690
look at optimizing your objective as a

1202
00:42:45,860 --> 00:42:47,280
function of the number of iterations,

1203
00:42:50,060 --> 00:42:52,610
when you look at this curve, you know,

1204
00:42:52,760 --> 00:42:53,440
it sort of looks like

1205
00:42:53,600 --> 00:42:54,990
it's going up but it sort of looks like

1206
00:42:55,140 --> 00:42:56,210
there's absentiles.

1207
00:42:56,370 --> 00:42:58,380
And when you look at these curves,

1208
00:42:58,530 --> 00:42:59,900
it's often very hard to tell

1209
00:43:00,050 --> 00:43:02,420
if the curve has already flattened out.

1210
00:43:02,580 --> 00:43:03,220
All right?

1211
00:43:03,370 --> 00:43:04,940
And you look at these curves a lot

1212
00:43:05,100 --> 00:43:05,700
so you can ask:

1213
00:43:05,850 --> 00:43:07,050
Well has the algorithm converged?

1214
00:43:07,210 --> 00:43:08,370
When you look at the J of theta

1215
00:43:08,520 --> 00:43:09,500
like this, it's often hard to tell.

1216
00:43:09,650 --> 00:43:11,350
You can run this ten times as long

1217
00:43:11,500 --> 00:43:12,620
and see if it's flattened out.

1218
00:43:12,780 --> 00:43:14,520
And you can run this ten times as long

1219
00:43:14,670 --> 00:43:16,290
and it'll often still look like maybe

1220
00:43:16,450 --> 00:43:17,950
it's going up very slowly, or something.

1221
00:43:18,110 --> 00:43:22,480
Right? So a better diagnostic for

1222
00:43:22,630 --> 00:43:24,140
what logistic regression is converged

1223
00:43:24,300 --> 00:43:25,580
than looking at this curve.

1224
00:43:25,580 --> 00:43:26,580
The other question you might wonder –

1225
00:43:30,290 --> 00:43:32,060
the other thing you might suspect is

1226
00:43:32,220 --> 00:43:32,720
a problem is

1227
00:43:32,900 --> 00:43:34,940
are you optimizing the right function.

1228
00:43:35,100 --> 00:43:39,670
So what you care about,

1229
00:43:39,820 --> 00:43:42,850
right, in spam,say, is a

1230
00:43:43,010 --> 00:43:45,590
weighted accuracy function like that.

1231
00:43:45,750 --> 00:43:47,570
So A of theta is, you know,

1232
00:43:47,720 --> 00:43:49,600
sum over your examples of some

1233
00:43:49,760 --> 00:43:51,280
weights times whether you got it right.

1234
00:43:51,440 --> 00:43:53,840
And so the weight may be

1235
00:43:54,000 --> 00:43:55,680
higher for non-spam than for spam mail

1236
00:43:55,830 --> 00:43:57,850
because you care about getting your

1237
00:43:58,030 --> 00:43:59,440
predictions correct for spam email

1238
00:43:59,600 --> 00:44:01,160
much more than non-spam mail, say.

1239
00:44:01,320 --> 00:44:03,020
So let's say A of theta is

1240
00:44:03,220 --> 00:44:06,780
the optimization objective

1241
00:44:06,950 --> 00:44:08,060
that you really care about,

1242
00:44:08,230 --> 00:44:12,380
but Bayesian logistic regression is

1243
00:44:12,530 --> 00:44:14,970
that it optimizes a quantity like that. Right?

1244
00:44:15,130 --> 00:44:16,530
It's this sort of maximum likelihood

1245
00:44:16,690 --> 00:44:19,580
thing and then with this two-nom,

1246
00:44:19,740 --> 00:44:21,130
you know, penalty thing

1247
00:44:21,280 --> 00:44:22,120
that we saw previously.

1248
00:44:22,280 --> 00:44:23,380
And you might be wondering:

1249
00:44:23,530 --> 00:44:24,600
Is this the right optimization function

1250
00:44:24,760 --> 00:44:26,670
to be optimizing. Okay?

1251
00:44:26,830 --> 00:44:28,600
And: Or do I maybe need to

1252
00:44:28,760 --> 00:44:29,810
change the value for lambda

1253
00:44:29,970 --> 00:44:32,120
to change this parameter?

1254
00:44:32,280 --> 00:44:34,800
Or: Should I maybe really

1255
00:44:34,960 --> 00:44:36,460
be switching to support vector machine

1256
00:44:36,640 --> 00:44:37,660
optimization objective?

1257
00:44:37,800 --> 00:44:40,140
Okay? Does that make sense?

1258
00:44:40,290 --> 00:44:43,280
So the second diagnostic I'm gonna talk

1259
00:44:43,440 --> 00:44:45,640
about is let's say you want to figure out

1260
00:44:45,790 --> 00:44:48,440
is the algorithm converging, is

1261
00:44:48,600 --> 00:44:49,780
the optimization algorithm converging,

1262
00:44:49,940 --> 00:44:51,310
or is the problem with

1263
00:44:51,460 --> 00:44:52,850
the optimization objective

1264
00:44:53,020 --> 00:44:55,700
I chose in the first place? Okay?

1265
00:44:55,860 --> 00:44:59,060
So here's the diagnostic you can use.

1266
00:44:59,210 --> 00:45:04,940
Let me let right.

1267
00:45:05,100 --> 00:45:08,810
So to just reiterate the story, right,

1268
00:45:08,970 --> 00:45:10,460
let's say an SVM outperforms Bayesian

1269
00:45:10,620 --> 00:45:11,430
logistic regression

1270
00:45:11,590 --> 00:45:13,540
but you really want to deploy Bayesian

1271
00:45:13,700 --> 00:45:14,550
logistic regression to your problem.

1272
00:45:14,710 --> 00:45:18,390
Let me let theta subscript SVM,

1273
00:45:18,560 --> 00:45:20,430
be the parameters learned by an SVM,

1274
00:45:20,600 --> 00:45:23,190
and I'll let theta subscript BLR

1275
00:45:23,340 --> 00:45:24,600
be the parameters learned

1276
00:45:24,750 --> 00:45:25,710
by Bayesian logistic regression.

1277
00:45:25,860 --> 00:45:29,620
So the optimization objective you care

1278
00:45:29,770 --> 00:45:30,830
about is this, you know,

1279
00:45:30,990 --> 00:45:32,090
weighted accuracy criteria

1280
00:45:32,260 --> 00:45:33,530
that I talked about just now.

1281
00:45:33,680 --> 00:45:38,750
And the support vector machine

1282
00:45:38,910 --> 00:45:40,340
outperforms Bayesian logistic

1283
00:45:40,500 --> 00:45:41,910
regression. And so, you know,

1284
00:45:42,070 --> 00:45:43,110
the weighted accuracy on the

1285
00:45:43,280 --> 00:45:44,560
support vector-machine parameters is

1286
00:45:44,710 --> 00:45:46,790
better than the weighted accuracy for

1287
00:45:46,980 --> 00:45:48,280
Bayesian logistic regression.

1288
00:45:48,440 --> 00:45:54,850
So further, Bayesian logistic

1289
00:45:55,000 --> 00:45:56,220
regression tries to optimize an

1290
00:45:56,370 --> 00:45:58,460
optimization objective like that,

1291
00:45:58,610 --> 00:46:00,420
which I denoted J theta.

1292
00:46:00,580 --> 00:46:04,510
And so, the diagnostic I choose to use

1293
00:46:04,670 --> 00:46:08,700
is to see if J of SVM is bigger-than

1294
00:46:08,850 --> 00:46:11,900
or less-than J of BLR. Okay?

1295
00:46:12,050 --> 00:46:13,350
So I explain this on the next slide.

1296
00:46:13,510 --> 00:46:16,360
So we know two facts.

1297
00:46:16,510 --> 00:46:18,410
We know that well we know one fact.

1298
00:46:18,570 --> 00:46:19,590
We know that a weighted accuracy

1299
00:46:19,740 --> 00:46:22,460
of support vector machine, right,

1300
00:46:22,630 --> 00:46:25,140
is bigger than this weighted accuracy

1301
00:46:25,300 --> 00:46:26,680
of Bayesian logistic regression.

1302
00:46:26,830 --> 00:46:29,830
So in order for me to figure out

1303
00:46:29,990 --> 00:46:31,100
whether Bayesian logistic regression

1304
00:46:31,270 --> 00:46:32,140
is converging,

1305
00:46:32,300 --> 00:46:33,180
or whether I'm just optimizing

1306
00:46:33,340 --> 00:46:34,130
the wrong objective function,

1307
00:46:34,280 --> 00:46:36,230
the diagnostic I'm gonna use

1308
00:46:36,390 --> 00:46:37,200
and I'm gonna check

1309
00:46:37,370 --> 00:46:38,860
if this equality hold through.

1310
00:46:39,030 --> 00:46:40,230
Okay?

1311
00:46:40,390 --> 00:46:41,500
So let me explain this,

1312
00:46:41,660 --> 00:46:44,940
so in Case 1, right,it's

1313
00:46:45,120 --> 00:46:47,530
just those two equations copied over.

1314
00:46:47,690 --> 00:46:51,090
In Case 1, let's say that J of SVM is,

1315
00:46:51,250 --> 00:46:53,560
indeed, is greater than J of BLR or

1316
00:46:54,330 --> 00:46:57,930
J of theta SVM is greater than J of theta BLR

1317
00:46:58,090 --> 00:47:03,170
But we know that Bayesian logistic

1318
00:47:03,350 --> 00:47:05,340
regression was trying to maximize

1319
00:47:05,500 --> 00:47:07,890
J of theta; that's the definition

1320
00:47:08,050 --> 00:47:09,640
of Bayesian logistic regression.

1321
00:47:09,810 --> 00:47:16,610
So this means that theta

1322
00:47:16,760 --> 00:47:18,840
the value of theta output

1323
00:47:18,990 --> 00:47:20,060
that Bayesian logistic regression

1324
00:47:20,220 --> 00:47:22,230
actually fails to maximize J

1325
00:47:22,390 --> 00:47:25,160
because the support back to machine

1326
00:47:25,320 --> 00:47:26,480
actually returned the value of theta

1327
00:47:26,660 --> 00:47:28,970
that, you know does a better job

1328
00:47:29,140 --> 00:47:30,120
out-maximizing J.

1329
00:47:30,290 --> 00:47:32,220
And so, this tells me that

1330
00:47:32,390 --> 00:47:33,930
Bayesian logistic regression didn't

1331
00:47:34,090 --> 00:47:36,360
actually maximize J correctly,

1332
00:47:36,510 --> 00:47:38,630
and so the problem is with

1333
00:47:38,790 --> 00:47:40,240
the optimization algorithm.

1334
00:47:40,400 --> 00:47:41,600
The optimization algorithm hasn't

1335
00:47:41,760 --> 00:47:46,590
converged.The other case is as follows,

1336
00:47:46,750 --> 00:47:51,570
where J of theta SVM is less-than/equal

1337
00:47:51,730 --> 00:47:53,010
to J of theta BLR.

1338
00:47:53,160 --> 00:47:56,880
Okay? In this case, what does that mean?

1339
00:47:57,040 --> 00:47:59,590
This means that

1340
00:47:59,750 --> 00:48:00,680
Bayesian logistic regression

1341
00:48:00,850 --> 00:48:04,640
actually attains the higher value for

1342
00:48:04,800 --> 00:48:07,290
the optimization objective J then

1343
00:48:07,450 --> 00:48:08,850
doesn't support back to machine.

1344
00:48:09,020 --> 00:48:11,930
The support back to machine,

1345
00:48:12,090 --> 00:48:15,640
which does worse on your optimization

1346
00:48:15,800 --> 00:48:18,220
problem, actually does better

1347
00:48:18,380 --> 00:48:20,440
on the weighted accuracy measure.

1348
00:48:23,250 --> 00:48:25,180
So what this means is that

1349
00:48:25,340 --> 00:48:26,790
something that does worse on your

1350
00:48:26,950 --> 00:48:28,920
optimization objective, on J,

1351
00:48:29,070 --> 00:48:30,490
can actually do better

1352
00:48:30,650 --> 00:48:32,770
on the weighted accuracy objective.

1353
00:48:32,930 --> 00:48:34,690
And this really means that

1354
00:48:34,850 --> 00:48:38,590
maximizing J of theta, you know,

1355
00:48:38,750 --> 00:48:39,930
doesn't really correspond that well

1356
00:48:40,140 --> 00:48:41,600
to maximizing your weighted accuracy

1357
00:48:41,760 --> 00:48:44,260
criteria. And therefore, this tells you

1358
00:48:44,420 --> 00:48:46,120
that J of theta is maybe the wrong

1359
00:48:46,270 --> 00:48:47,560
optimization objective to be

1360
00:48:47,750 --> 00:48:49,640
maximizing. Right?

1361
00:48:49,800 --> 00:48:50,950
That just maximizing J of theta

1362
00:48:51,110 --> 00:48:52,210
just wasn't a good objective

1363
00:48:52,380 --> 00:48:54,340
to be choosing if you care about

1364
00:48:54,510 --> 00:48:55,460
the weighted accuracy.

1365
00:48:55,640 --> 00:48:58,480
Okay?

1366
00:49:00,520 --> 00:49:03,290
Can you raise your hand

1367
00:49:03,450 --> 00:49:06,330
if this made sense? Cool, good.

1368
00:49:10,520 --> 00:49:12,830
So that tells us whether the problem

1369
00:49:12,980 --> 00:49:14,330
is with the optimization objective

1370
00:49:14,490 --> 00:49:17,440
or whether it's with

1371
00:49:17,600 --> 00:49:18,520
the objective function.

1372
00:49:18,680 --> 00:49:20,490
And so going back to this slide,

1373
00:49:20,650 --> 00:49:23,390
the eight fixes we had, you notice that

1374
00:49:23,560 --> 00:49:25,610
if you run gradient descent for more

1375
00:49:25,770 --> 00:49:27,930
iterations that fixes the optimization

1376
00:49:28,090 --> 00:49:30,210
algorithm. You try and use this method

1377
00:49:30,380 --> 00:49:32,020
fixes the optimization algorithm,

1378
00:49:32,180 --> 00:49:34,460
whereas using a different value for

1379
00:49:34,610 --> 00:49:36,650
lambda, in that lambda times norm of

1380
00:49:36,810 --> 00:49:38,170
data squared, you know, in your

1381
00:49:38,330 --> 00:49:39,970
objective, fixes the optimization

1382
00:49:40,130 --> 00:49:42,810
objective. And changing to an SVM

1383
00:49:42,970 --> 00:49:44,560
is also another way of trying to

1384
00:49:44,710 --> 00:49:46,520
fix the optimization objective. Okay?

1385
00:49:46,680 --> 00:49:49,900
And so once again, you actually

1386
00:49:50,060 --> 00:49:51,180
see this quite often that

1387
00:49:51,350 --> 00:49:53,490
actually, you see it very often,

1388
00:49:53,650 --> 00:49:56,240
people will have a problem with

1389
00:49:56,400 --> 00:49:57,350
the optimization objective

1390
00:49:57,500 --> 00:49:59,480
and be working harder and harder

1391
00:49:59,640 --> 00:50:02,250
to fix the optimization algorithm.

1392
00:50:02,400 --> 00:50:05,550
That's another very common pattern that

1393
00:50:05,710 --> 00:50:06,850
the problem is in the

1394
00:50:07,010 --> 00:50:08,120
formula from your J of theta,

1395
00:50:08,280 --> 00:50:10,180
that often you see people, you know,

1396
00:50:10,330 --> 00:50:11,040
just running more and more

1397
00:50:11,200 --> 00:50:12,360
iterations of gradient descent.

1398
00:50:12,520 --> 00:50:13,700
Like trying Newton's method and trying

1399
00:50:13,850 --> 00:50:16,060
conjugate and then trying more and more

1400
00:50:16,220 --> 00:50:17,200
crazy optimization algorithms,

1401
00:50:17,350 --> 00:50:20,440
whereas the problem was, you know,

1402
00:50:20,600 --> 00:50:22,050
optimizing J of theta wasn't going to

1403
00:50:22,220 --> 00:50:23,150
fix the problem at all.

1404
00:50:23,300 --> 00:50:24,180
Okay?

1405
00:50:24,340 --> 00:50:25,640
So there's another example of

1406
00:50:25,800 --> 00:50:27,980
when these sorts of diagnostics

1407
00:50:28,150 --> 00:50:29,250
will help you figure out

1408
00:50:29,400 --> 00:50:30,280
whether you should be fixing your

1409
00:50:30,440 --> 00:50:31,990
optimization algorithm or

1410
00:50:32,140 --> 00:50:34,030
fixing the optimization objective.

1411
00:50:34,180 --> 00:50:36,980
Okay?

1412
00:50:37,140 --> 00:50:39,710
Let me think how much time I have.

1413
00:50:39,860 --> 00:50:46,090
Hmm, let's see. Well okay, we have time.

1414
00:50:46,250 --> 00:50:49,020
Let's do this. Show you

1415
00:50:49,170 --> 00:50:51,220
one last example of a diagnostic.

1416
00:50:51,380 --> 00:50:53,080
This is one that came up in, you know,

1417
00:50:53,230 --> 00:50:54,400
my students' and my work

1418
00:50:54,560 --> 00:50:55,660
on flying helicopters.

1419
00:50:55,830 --> 00:51:00,610
This one actually, this example is

1420
00:51:00,770 --> 00:51:03,250
the most complex of the three examples

1421
00:51:03,430 --> 00:51:04,370
I'm gonna do today.

1422
00:51:04,530 --> 00:51:06,780
I'm going to somewhat quickly,

1423
00:51:06,930 --> 00:51:08,910
and this actually

1424
00:51:09,060 --> 00:51:10,520
draws on reinforcement learning which

1425
00:51:10,690 --> 00:51:11,480
is something that I'm not gonna

1426
00:51:11,640 --> 00:51:12,680
talk about until towards

1427
00:51:12,830 --> 00:51:14,120
close to the end of the course here,

1428
00:51:14,300 --> 00:51:17,160
but this just a more complicated example

1429
00:51:17,320 --> 00:51:18,470
of a diagnostic we're gonna go over.

1430
00:51:18,610 --> 00:51:20,650
What I'll do is probably

1431
00:51:20,810 --> 00:51:21,770
go over this fairly quickly,

1432
00:51:21,920 --> 00:51:23,080
and then after we've talked about

1433
00:51:23,240 --> 00:51:24,410
reinforcement learning in the class,

1434
00:51:24,570 --> 00:51:25,590
I'll probably actually come back

1435
00:51:25,750 --> 00:51:26,840
and redo this exact same example

1436
00:51:26,990 --> 00:51:28,430
because you'll understand it

1437
00:51:28,590 --> 00:51:29,590
more deeply. Okay?

1438
00:51:29,740 --> 00:51:33,280
So some of you know that

1439
00:51:33,440 --> 00:51:34,150
my students and I

1440
00:51:34,310 --> 00:51:35,190
fly autonomous helicopters,

1441
00:51:35,350 --> 00:51:37,060
so how do you get a machine-learning

1442
00:51:37,230 --> 00:51:39,320
algorithm to design the controller for

1443
00:51:39,480 --> 00:51:42,670
helicopter? This is what we do.

1444
00:51:42,880 --> 00:51:44,830
All right? This first step was

1445
00:51:45,000 --> 00:51:46,240
you build a simulator for a helicopter,

1446
00:51:46,410 --> 00:51:47,490
so, you know,

1447
00:51:47,650 --> 00:51:48,750
there's a screenshot of our simulator.

1448
00:51:48,900 --> 00:51:50,330
This is just like a

1449
00:51:50,480 --> 00:51:51,870
it's like a joystick simulator;

1450
00:51:52,030 --> 00:51:53,320
you can fly a helicopter in simulation.

1451
00:51:53,490 --> 00:51:56,570
And then you choose a cost function,

1452
00:51:56,730 --> 00:51:57,780
it's actually called a

1453
00:51:57,940 --> 00:51:58,810
[inaudible] function,

1454
00:51:58,970 --> 00:51:59,590
but for this actually

1455
00:51:59,750 --> 00:52:00,390
I'll call it cost function.

1456
00:52:00,550 --> 00:52:02,860
Say J of theta is, you know,

1457
00:52:03,010 --> 00:52:04,360
the expected squared error

1458
00:52:04,520 --> 00:52:06,550
in your helicopter's position. Okay?

1459
00:52:06,710 --> 00:52:08,150
So this is J of theta is maybe

1460
00:52:08,310 --> 00:52:09,520
it's expected square error

1461
00:52:09,680 --> 00:52:10,680
or just the square error.

1462
00:52:10,850 --> 00:52:13,020
And then we run a

1463
00:52:13,180 --> 00:52:14,720
reinforcement-learning algorithm,

1464
00:52:14,880 --> 00:52:16,870
you'll learn about RL algorithms

1465
00:52:17,020 --> 00:52:17,840
in a few weeks.

1466
00:52:18,000 --> 00:52:19,280
You run reinforcement learning

1467
00:52:19,440 --> 00:52:21,550
algorithm in your simulator to

1468
00:52:21,710 --> 00:52:24,330
try to minimize this cost function;

1469
00:52:24,500 --> 00:52:25,840
try to minimize the squared error

1470
00:52:26,010 --> 00:52:27,610
of how well you're controlling

1471
00:52:27,760 --> 00:52:29,550
your helicopter's position. Okay?

1472
00:52:31,380 --> 00:52:32,620
The reinforcement learning algorithm

1473
00:52:32,780 --> 00:52:33,780
will output some parameters,

1474
00:52:33,930 --> 00:52:36,160
which I'm denoting theta subscript RL,

1475
00:52:36,320 --> 00:52:39,030
and then you'll use that

1476
00:52:39,200 --> 00:52:40,110
to fly your helicopter.

1477
00:52:41,590 --> 00:52:42,930
So suppose you run this learning

1478
00:52:43,090 --> 00:52:45,300
algorithm and you get out

1479
00:52:45,450 --> 00:52:46,660
a set of controller parameters,

1480
00:52:46,810 --> 00:52:48,000
theta subscript RL,

1481
00:52:48,160 --> 00:52:49,930
that gives much worse performance

1482
00:52:50,090 --> 00:52:50,970
than a human pilot.

1483
00:52:51,130 --> 00:52:52,820
Then what do you do next?

1484
00:52:52,980 --> 00:52:54,880
And in particular, you know,

1485
00:52:55,040 --> 00:52:56,390
corresponding to the three steps above,

1486
00:52:56,550 --> 00:52:58,140
there are three natural things

1487
00:52:58,300 --> 00:52:59,150
you can try. Right?

1488
00:52:59,310 --> 00:53:00,750
You can try to

1489
00:53:00,910 --> 00:53:02,870
oh, the bottom of the slide

1490
00:53:03,040 --> 00:53:03,790
got chopped off.

1491
00:53:03,950 --> 00:53:05,310
You can try to improve the simulator.

1492
00:53:05,470 --> 00:53:07,930
And maybe you think

1493
00:53:08,100 --> 00:53:09,170
your simulator's isn't that accurate,

1494
00:53:09,320 --> 00:53:10,570
you need to capture the

1495
00:53:10,720 --> 00:53:11,830
aerodynamic effects more accurately.

1496
00:53:11,990 --> 00:53:13,400
You need to capture

1497
00:53:13,560 --> 00:53:14,920
the airflow and the turbulence affects

1498
00:53:15,080 --> 00:53:16,190
around the helicopter more accurately.

1499
00:53:16,350 --> 00:53:18,610
Maybe you need to

1500
00:53:18,770 --> 00:53:19,630
modify the cost function.

1501
00:53:19,780 --> 00:53:20,980
Maybe your square error isn't cutting

1502
00:53:21,180 --> 00:53:22,500
it. Maybe what a human pilot does

1503
00:53:22,660 --> 00:53:23,720
isn't just optimizing square area

1504
00:53:23,880 --> 00:53:24,890
but it's something more subtle.

1505
00:53:25,050 --> 00:53:27,380
Or maybe the reinforcement-learning

1506
00:53:27,550 --> 00:53:28,300
algorithm isn't working;

1507
00:53:28,460 --> 00:53:29,660
maybe it's not quite converging or

1508
00:53:29,820 --> 00:53:30,930
something. Okay?

1509
00:53:31,110 --> 00:53:33,640
So these are the diagnostics

1510
00:53:33,800 --> 00:53:35,100
that I actually used,

1511
00:53:35,260 --> 00:53:36,190
and my students and I actually use

1512
00:53:36,410 --> 00:53:37,240
to figure out what's going on.

1513
00:53:38,660 --> 00:53:41,790
Actually, why don't you just

1514
00:53:41,950 --> 00:53:42,800
think about this for a second

1515
00:53:42,950 --> 00:53:44,030
and think what you'd do, and then

1516
00:53:44,180 --> 00:53:45,300
I'll go on and tell you what we do.

1517
00:53:45,300 --> 00:53:46,300
All right, so let me tell you what –

1518
00:54:48,320 --> 00:54:49,220
how we do this and see

1519
00:54:49,400 --> 00:54:51,030
whether it's the same as yours or not.

1520
00:54:51,180 --> 00:54:52,350
And if you have a better idea than I do,

1521
00:54:52,500 --> 00:54:53,320
let me know and

1522
00:54:53,480 --> 00:54:54,530
I'll let you try it on my helicopter.

1523
00:54:54,680 --> 00:54:58,360
So here's a reasoning

1524
00:54:58,500 --> 00:55:00,500
that I wanted to experiment, right.

1525
00:55:00,660 --> 00:55:02,000
So, yeah, let's say the controller

1526
00:55:02,150 --> 00:55:04,350
output by our reinforcement-learning

1527
00:55:04,510 --> 00:55:05,420
algorithm does poorly.

1528
00:55:05,570 --> 00:55:10,560
Well suppose the following

1529
00:55:10,710 --> 00:55:11,720
three things hold true.

1530
00:55:11,870 --> 00:55:13,600
Suppose the contrary, I guess.

1531
00:55:13,760 --> 00:55:15,960
Suppose that the helicopter simulator

1532
00:55:16,110 --> 00:55:18,190
is accurate, so let's assume we have an

1533
00:55:18,350 --> 00:55:19,980
accurate model of our helicopter.

1534
00:55:21,820 --> 00:55:23,050
And let's suppose that

1535
00:55:23,220 --> 00:55:24,580
the reinforcement learning algorithm,

1536
00:55:24,740 --> 00:55:26,400
you know, correctly controls the

1537
00:55:26,540 --> 00:55:28,080
helicopter in simulation,

1538
00:55:28,250 --> 00:55:30,080
so we tend to run a learning algorithm

1539
00:55:30,250 --> 00:55:31,310
in simulation so that, you know,

1540
00:55:31,480 --> 00:55:32,730
the learning algorithm can crash a

1541
00:55:32,900 --> 00:55:33,920
helicopter and it's fine. Right?

1542
00:55:34,070 --> 00:55:35,500
So let's assume our

1543
00:55:35,660 --> 00:55:37,010
reinforcement-learning algorithm

1544
00:55:37,180 --> 00:55:38,540
correctly controls the helicopter

1545
00:55:38,700 --> 00:55:39,260
so as to minimize

1546
00:55:39,420 --> 00:55:40,680
the cost function J of theta.

1547
00:55:40,840 --> 00:55:42,640
And let's suppose that

1548
00:55:42,790 --> 00:55:44,120
minimizing J of theta

1549
00:55:44,270 --> 00:55:46,340
does indeed correspond to accurate

1550
00:55:46,500 --> 00:55:47,640
or the correct autonomous flight.

1551
00:55:47,800 --> 00:55:50,330
If all of these things held true,

1552
00:55:50,490 --> 00:55:54,260
then that means that the parameters,

1553
00:55:54,460 --> 00:55:55,770
theta RL, should actually fly well

1554
00:55:55,920 --> 00:55:59,670
on my real helicopter. Right?

1555
00:55:59,830 --> 00:56:02,100
And so the fact that the

1556
00:56:02,260 --> 00:56:04,180
learning control parameters, theta RL,

1557
00:56:04,330 --> 00:56:06,680
does not fly well on my helicopter,

1558
00:56:06,840 --> 00:56:08,960
that sort of means that

1559
00:56:09,120 --> 00:56:10,200
ones of these three assumptions

1560
00:56:10,400 --> 00:56:11,190
must be wrong and

1561
00:56:11,350 --> 00:56:12,340
I'd like to figure out which of these

1562
00:56:12,500 --> 00:56:15,640
three assumptions is wrong. Okay?

1563
00:56:17,000 --> 00:56:20,680
So these are the diagnostics we use.

1564
00:56:20,870 --> 00:56:26,280
First one is we look at the controller

1565
00:56:26,450 --> 00:56:27,310
and see if it

1566
00:56:27,470 --> 00:56:28,760
even flies well in simulation.

1567
00:56:28,910 --> 00:56:30,560
Right?

1568
00:56:30,720 --> 00:56:32,360
So the simulator of the helicopter

1569
00:56:32,510 --> 00:56:33,560
that we did the learning on,

1570
00:56:33,720 --> 00:56:36,060
and so if the learning algorithm

1571
00:56:36,220 --> 00:56:37,530
flies well in the simulator

1572
00:56:37,690 --> 00:56:39,900
but it doesn't fly well on my real

1573
00:56:40,080 --> 00:56:42,580
helicopter, then that tells me the

1574
00:56:42,740 --> 00:56:44,290
problem is probably in the simulator.

1575
00:56:44,450 --> 00:56:45,240
Right?

1576
00:56:45,400 --> 00:56:47,780
My simulator predicts the helicopter's

1577
00:56:47,950 --> 00:56:49,040
controller will fly well

1578
00:56:49,200 --> 00:56:50,440
but it doesn't actually fly well in real

1579
00:56:50,590 --> 00:56:52,120
life，so could be the problem's

1580
00:56:52,290 --> 00:56:53,300
in the simulator and

1581
00:56:53,460 --> 00:56:54,110
we should spend out efforts

1582
00:56:54,280 --> 00:56:55,800
improving the accuracy of our simulator.

1583
00:56:55,960 --> 00:57:00,020
Otherwise,

1584
00:57:00,190 --> 00:57:02,460
let me write theta subscript human,

1585
00:57:02,610 --> 00:57:04,370
be the human control policy. All right?

1586
00:57:04,530 --> 00:57:08,100
So let's go ahead and ask a human

1587
00:57:08,270 --> 00:57:10,520
to fly the helicopter,

1588
00:57:10,700 --> 00:57:11,540
it could be in the simulator,

1589
00:57:11,690 --> 00:57:12,380
it could be in real life,

1590
00:57:12,530 --> 00:57:14,270
and let's measure, you know,

1591
00:57:14,440 --> 00:57:15,570
the means squared error

1592
00:57:15,720 --> 00:57:17,960
of the human pilot's flight.

1593
00:57:18,110 --> 00:57:21,450
And let's see if the human pilot

1594
00:57:21,610 --> 00:57:22,880
does better or worse

1595
00:57:23,040 --> 00:57:25,260
than the learned controller,

1596
00:57:25,410 --> 00:57:27,270
in terms of optimizing this

1597
00:57:27,440 --> 00:57:30,780
objective function J of theta. Okay?

1598
00:57:30,940 --> 00:57:33,900
So if the human does worse,

1599
00:57:34,050 --> 00:57:37,300
if even a very good human pilot attains

1600
00:57:37,470 --> 00:57:39,450
a worse value on my optimization

1601
00:57:39,610 --> 00:57:41,250
objective, on my cost function,

1602
00:57:41,400 --> 00:57:43,150
than my learning algorithm,

1603
00:57:47,690 --> 00:57:49,750
then the problem is in the

1604
00:57:49,910 --> 00:57:50,940
reinforcement-learning algorithm.

1605
00:57:51,110 --> 00:57:52,750
Because my reinforcement-learning

1606
00:57:52,910 --> 00:57:54,460
algorithm was trying to minimize J of

1607
00:57:54,620 --> 00:57:56,880
theta, but a human actually

1608
00:57:57,040 --> 00:57:58,530
attains a lower value for

1609
00:57:58,690 --> 00:58:00,140
J of theta than does my algorithm.

1610
00:58:00,300 --> 00:58:02,900
And so that tells me that clearly

1611
00:58:03,060 --> 00:58:05,570
my algorithm's not managing to

1612
00:58:05,720 --> 00:58:08,160
minimize J of theta and that tells me

1613
00:58:08,330 --> 00:58:09,300
the problem's in the

1614
00:58:09,470 --> 00:58:10,440
reinforcement learning algorithm.

1615
00:58:10,440 --> 00:58:11,440
And finally, if J of theta –

1616
00:58:14,280 --> 00:58:15,960
if the human actually

1617
00:58:16,130 --> 00:58:17,970
attains a larger value for theta

1618
00:58:18,140 --> 00:58:19,120
excuse me,

1619
00:58:19,270 --> 00:58:20,740
if the human actually attains

1620
00:58:20,900 --> 00:58:22,180
a larger value for J of theta,

1621
00:58:22,340 --> 00:58:23,900
the human actually has, you know,

1622
00:58:24,060 --> 00:58:25,680
larger mean squared error for the

1623
00:58:25,830 --> 00:58:27,730
helicopter position than does my

1624
00:58:27,880 --> 00:58:29,110
reinforcement learning algorithms,

1625
00:58:29,280 --> 00:58:31,890
that's I like but I like the way

1626
00:58:32,050 --> 00:58:33,070
the human flies much better than

1627
00:58:33,240 --> 00:58:34,640
my reinforcement learning algorithm.

1628
00:58:34,800 --> 00:58:35,950
So if that holds true,

1629
00:58:36,110 --> 00:58:37,880
then clearly the problem's

1630
00:58:38,030 --> 00:58:39,470
in the cost function, right,

1631
00:58:39,630 --> 00:58:40,870
because the human does worse

1632
00:58:41,050 --> 00:58:41,970
on my cost function

1633
00:58:42,120 --> 00:58:43,510
but flies much better

1634
00:58:43,670 --> 00:58:44,760
than my learning algorithm.

1635
00:58:44,920 --> 00:58:46,930
And so that means the problem's

1636
00:58:47,080 --> 00:58:47,910
in the cost function.

1637
00:58:48,080 --> 00:58:48,750
It means

1638
00:58:48,910 --> 00:58:50,580
oh excuse me, I meant minimizing it,

1639
00:58:50,730 --> 00:58:51,870
not maximizing it,

1640
00:58:52,030 --> 00:58:53,180
there's a typo on the slide,

1641
00:58:53,330 --> 00:58:54,290
because that means that

1642
00:58:54,450 --> 00:58:55,700
minimizing the cost function

1643
00:58:55,840 --> 00:58:58,380
my learning algorithm does a better job

1644
00:58:58,540 --> 00:58:59,580
minimizing the cost function

1645
00:58:59,740 --> 00:59:01,230
but doesn't fly as well as a human pilot.

1646
00:59:01,380 --> 00:59:02,240
So that tells you that

1647
00:59:02,400 --> 00:59:03,910
minimizing the cost function doesn't

1648
00:59:04,070 --> 00:59:06,250
correspond to good autonomous flight.

1649
00:59:06,410 --> 00:59:07,680
And what you should do is go back and

1650
00:59:07,840 --> 00:59:09,300
see if you can change J of theta.

1651
00:59:09,450 --> 00:59:10,680
Okay?

1652
00:59:10,880 --> 00:59:13,540
And so for those

1653
00:59:13,690 --> 00:59:14,770
reinforcement learning problems,

1654
00:59:14,930 --> 00:59:16,290
you know, if something doesn't work

1655
00:59:16,450 --> 00:59:19,020
often reinforcement learning

1656
00:59:19,180 --> 00:59:20,040
algorithms just work

1657
00:59:20,190 --> 00:59:21,180
but when they don't work,

1658
00:59:21,340 --> 00:59:22,400
these are the sorts of diagnostics

1659
00:59:22,560 --> 00:59:23,620
you use to figure out

1660
00:59:23,780 --> 00:59:25,010
should we be focusing on the simulator,

1661
00:59:25,170 --> 00:59:27,190
or changing the cost function,

1662
00:59:27,350 --> 00:59:28,580
or on changing the

1663
00:59:28,750 --> 00:59:29,690
reinforcement learning algorithm.

1664
00:59:29,850 --> 00:59:32,400
And again,

1665
00:59:32,550 --> 00:59:33,540
if you don't know

1666
00:59:33,700 --> 00:59:34,960
which of your three problems it is,

1667
00:59:35,120 --> 00:59:37,100
it's entirely possible, you know,

1668
00:59:37,260 --> 00:59:39,620
to spend two years, whatever, changing,

1669
00:59:39,780 --> 00:59:41,000
building a better simulator

1670
00:59:41,180 --> 00:59:41,990
for your helicopter.

1671
00:59:42,150 --> 00:59:43,040
But it turns out that

1672
00:59:43,200 --> 00:59:44,850
modeling helicopter aerodynamics

1673
00:59:45,010 --> 00:59:46,610
is an active area of research.

1674
00:59:46,770 --> 00:59:47,600
There are people, you know, writing

1675
00:59:47,750 --> 00:59:48,940
entire PhD theses on this still.

1676
00:59:49,100 --> 00:59:51,350
So it's entirely possible to go out

1677
00:59:51,510 --> 00:59:52,100
and spend six years

1678
00:59:52,270 --> 00:59:53,100
and write a PhD thesis

1679
00:59:53,270 --> 00:59:54,180
and build a much better helicopter

1680
00:59:54,330 --> 00:59:55,650
simulator, but if you're

1681
00:59:55,800 --> 00:59:56,530
fixing the wrong problem

1682
00:59:56,700 --> 00:59:57,480
it's not gonna help.

1683
01:00:03,410 --> 01:00:05,760
So quite often,

1684
01:00:05,910 --> 01:00:06,710
you need to come up with

1685
01:00:06,860 --> 01:00:07,760
your own diagnostics to figure out

1686
01:00:07,920 --> 01:00:09,020
what's happening in an algorithm

1687
01:00:09,180 --> 01:00:09,980
when something is going wrong.

1688
01:00:10,140 --> 01:00:14,690
And unfortunately I don't know of

1689
01:00:14,850 --> 01:00:16,730
what I've described are sort of maybe

1690
01:00:16,890 --> 01:00:18,460
some of the most common diagnostics

1691
01:00:18,610 --> 01:00:20,750
that I've used, that I've seen, you know,

1692
01:00:20,910 --> 01:00:21,950
to be useful for many problems.

1693
01:00:22,110 --> 01:00:22,760
But very often,

1694
01:00:22,920 --> 01:00:24,200
you need to come up with your own

1695
01:00:24,370 --> 01:00:25,630
for your own specific learning problem.

1696
01:00:25,800 --> 01:00:28,790
And I just want to point out that

1697
01:00:28,940 --> 01:00:30,140
even when the learning algorithm is

1698
01:00:30,310 --> 01:00:32,520
working well, it's often a good idea

1699
01:00:32,690 --> 01:00:33,990
to run diagnostics,

1700
01:00:34,160 --> 01:00:35,150
like the ones I talked about,

1701
01:00:35,310 --> 01:00:36,420
to make sure you really

1702
01:00:36,580 --> 01:00:38,000
understand what's going on. All right?

1703
01:00:38,160 --> 01:00:39,080
And this is useful for a couple of

1704
01:00:39,230 --> 01:00:40,060
reasons. One is that

1705
01:00:40,230 --> 01:00:41,790
diagnostics like these

1706
01:00:41,950 --> 01:00:43,880
will often help you to understand

1707
01:00:44,040 --> 01:00:45,570
your application problem better.

1708
01:00:45,730 --> 01:00:49,070
So some of you will, you know,

1709
01:00:49,240 --> 01:00:49,990
graduate from Stanford

1710
01:00:50,130 --> 01:00:50,940
and go on to get some

1711
01:00:51,120 --> 01:00:52,450
amazingly high-paying job to

1712
01:00:52,640 --> 01:00:53,900
apply machine-learning algorithms to

1713
01:00:54,060 --> 01:00:56,350
some application problem of, you know,

1714
01:00:56,500 --> 01:00:58,330
significant economic interest. Right?

1715
01:00:58,490 --> 01:01:01,460
And you're gonna be working on one

1716
01:01:01,620 --> 01:01:03,390
specific important machine learning

1717
01:01:03,560 --> 01:01:04,510
application for many months,

1718
01:01:04,660 --> 01:01:05,690
or even for years.

1719
01:01:05,840 --> 01:01:09,160
One of the most valuable things for you

1720
01:01:09,160 --> 01:01:10,160
personally will be for you to get in –

1721
01:01:11,220 --> 01:01:13,500
for you personally to get in an

1722
01:01:13,690 --> 01:01:15,310
intuitive understanding of what works

1723
01:01:15,460 --> 01:01:16,580
and what doesn't work your problem.

1724
01:01:16,750 --> 01:01:18,950
Sort of right now in the industry,

1725
01:01:19,120 --> 01:01:20,550
in Silicon Valley or around the world,

1726
01:01:20,720 --> 01:01:22,550
there are many companies with important

1727
01:01:22,710 --> 01:01:23,940
machine learning problems and there are

1728
01:01:24,110 --> 01:01:24,820
often people working on

1729
01:01:25,010 --> 01:01:26,190
the same machine learning problem,

1730
01:01:26,350 --> 01:01:27,430
you know, for many months

1731
01:01:27,580 --> 01:01:28,600
or for years on end.

1732
01:01:28,760 --> 01:01:32,310
And when you're doing that, I mean

1733
01:01:32,470 --> 01:01:33,390
solving a really important problem

1734
01:01:33,540 --> 01:01:34,440
using learning algorithms,

1735
01:01:34,610 --> 01:01:35,860
one of the most valuable things is

1736
01:01:36,020 --> 01:01:37,460
just your own personal intuitive

1737
01:01:37,610 --> 01:01:39,960
understanding of the problem. Okay?

1738
01:01:40,120 --> 01:01:43,930
And diagnostics, like the sort I talked

1739
01:01:44,080 --> 01:01:45,400
about, will be one way for you

1740
01:01:45,560 --> 01:01:46,580
to get a better and better

1741
01:01:46,730 --> 01:01:48,400
understanding of these problems.

1742
01:01:48,560 --> 01:01:50,640
It turns out, by the way,

1743
01:01:50,800 --> 01:01:51,990
there are some of Silicon Valley

1744
01:01:52,160 --> 01:01:53,270
companies that outsource

1745
01:01:53,440 --> 01:01:54,560
their machine learning.

1746
01:01:54,720 --> 01:01:55,590
So there's sometimes, you know,

1747
01:01:55,750 --> 01:01:57,620
whatever. They're a company in Silicon

1748
01:01:57,780 --> 01:01:58,930
Valley and they'll, you know,

1749
01:01:59,110 --> 01:02:00,570
hire a firm in New York to run

1750
01:02:00,720 --> 01:02:02,960
all their learning algorithms for them.

1751
01:02:03,110 --> 01:02:04,530
And I'm not a businessman,

1752
01:02:04,690 --> 01:02:06,020
but I personally think

1753
01:02:06,190 --> 01:02:07,940
that's often a terrible idea because

1754
01:02:08,100 --> 01:02:11,130
if your expertise,if your understanding

1755
01:02:11,290 --> 01:02:13,650
of your data is given, you know,

1756
01:02:13,800 --> 01:02:15,920
to an outsource agency, then if you

1757
01:02:16,080 --> 01:02:17,210
don't maintain that expertise,

1758
01:02:17,360 --> 01:02:18,410
if there's a problem you really care

1759
01:02:18,570 --> 01:02:21,360
about then it'll be your own, you know,

1760
01:02:21,520 --> 01:02:23,290
understanding of the problem that you

1761
01:02:23,460 --> 01:02:24,680
build up over months that'll be really

1762
01:02:24,850 --> 01:02:26,630
valuable. And if that knowledge is

1763
01:02:26,790 --> 01:02:27,770
outsourced, you don't get to

1764
01:02:27,930 --> 01:02:28,830
keep that knowledge yourself.

1765
01:02:28,990 --> 01:02:30,070
I personally think

1766
01:02:30,220 --> 01:02:30,970
that's a terrible idea.

1767
01:02:31,120 --> 01:02:32,380
But I'm not a businessman,

1768
01:02:32,540 --> 01:02:33,750
but I just see people do that a lot,

1769
01:02:33,900 --> 01:02:37,800
and just. Let's see.

1770
01:02:37,960 --> 01:02:40,490
Another reason for running diagnostics

1771
01:02:40,640 --> 01:02:41,990
like these is actually in

1772
01:02:42,160 --> 01:02:43,200
writing research papers, right?

1773
01:02:43,350 --> 01:02:46,980
So diagnostics and error analyses,

1774
01:02:47,160 --> 01:02:48,310
which I'll talk about in a minute,

1775
01:02:48,470 --> 01:02:50,330
often help to convey insight

1776
01:02:50,490 --> 01:02:51,530
about the problem and

1777
01:02:51,690 --> 01:02:53,110
help justify your research claims.

1778
01:02:55,130 --> 01:02:56,990
So for example,

1779
01:02:57,150 --> 01:02:58,890
rather than writing a research paper,

1780
01:02:59,080 --> 01:03:00,110
say, that's says, you know,

1781
01:03:00,280 --> 01:03:01,480
"Oh well here's an algorithm that works.

1782
01:03:01,640 --> 01:03:02,790
I built this helicopter and it flies,"

1783
01:03:02,950 --> 01:03:04,210
or whatever, it's often

1784
01:03:04,370 --> 01:03:05,100
much more interesting to say,

1785
01:03:05,260 --> 01:03:06,440
"Here's an algorithm that works,

1786
01:03:06,590 --> 01:03:08,830
and it works because of a specific

1787
01:03:08,990 --> 01:03:11,180
component X. And moreover, here's the

1788
01:03:11,340 --> 01:03:12,490
diagnostic that gives you justification

1789
01:03:12,860 --> 01:03:14,170
that shows X was the thing that

1790
01:03:14,320 --> 01:03:15,170
fixed this problem,"

1791
01:03:15,330 --> 01:03:16,460
and that's where you made it work.

1792
01:03:16,620 --> 01:03:17,380
Okay?

1793
01:03:17,540 --> 01:03:21,790
So that leads me into a discussion

1794
01:03:21,940 --> 01:03:22,920
on error analysis,

1795
01:03:23,070 --> 01:03:24,710
which is often good machine learning

1796
01:03:24,870 --> 01:03:27,000
practice, is a way for understanding

1797
01:03:27,170 --> 01:03:28,450
what your sources of errors are.

1798
01:03:28,450 --> 01:03:29,450
So what I call error analyses –

1799
01:03:33,210 --> 01:03:34,940
and let's check questions about this.

1800
01:03:39,550 --> 01:03:41,350
Yeah?

1801
01:03:41,500 --> 01:03:42,240
Student:What ended up

1802
01:03:42,410 --> 01:03:43,340
being wrong with the helicopter?

1803
01:03:43,500 --> 01:03:46,260
Instructor (Andrew Ng):Oh, don't know.

1804
01:03:46,410 --> 01:03:47,210
Let's see.

1805
01:03:47,380 --> 01:03:48,550
We've flown so many times.

1806
01:03:48,710 --> 01:03:51,360
The thing that is most difficult a

1807
01:03:51,510 --> 01:03:52,920
helicopter is actually building a very

1808
01:03:53,070 --> 01:03:55,410
I don't know. It changes all the time.

1809
01:03:55,590 --> 01:03:56,800
Quite often, it's actually the

1810
01:03:56,970 --> 01:03:58,000
simulator. Building an accurate

1811
01:03:58,160 --> 01:03:59,370
simulator of a helicopter is very hard.

1812
01:03:59,530 --> 01:04:04,280
Yeah. Okay. So for error analyses,

1813
01:04:04,440 --> 01:04:07,080
this is a way for figuring out

1814
01:04:07,250 --> 01:04:08,360
what is working in your algorithm

1815
01:04:08,510 --> 01:04:09,320
and what isn't working.

1816
01:04:09,470 --> 01:04:12,450
And we're gonna talk about two specific

1817
01:04:12,450 --> 01:04:13,450
examples. So there are many learning –

1818
01:04:18,220 --> 01:04:19,930
there are many sort of AI systems,

1819
01:04:20,090 --> 01:04:21,070
many machine learning systems,

1820
01:04:21,220 --> 01:04:23,190
that combine many different components

1821
01:04:23,340 --> 01:04:24,490
into a pipeline.

1822
01:04:24,650 --> 01:04:26,060
So here's sort of a contrived example

1823
01:04:26,210 --> 01:04:28,540
for this, not dissimilar in many ways

1824
01:04:28,690 --> 01:04:29,970
from the actual machine learning

1825
01:04:30,130 --> 01:04:30,880
systems you see.

1826
01:04:31,040 --> 01:04:31,870
So let's say you want to

1827
01:04:32,030 --> 01:04:33,960
recognize people from images.

1828
01:04:34,120 --> 01:04:35,540
This is a picture of one of my friends.

1829
01:04:35,700 --> 01:04:39,010
So you take this input in camera image,

1830
01:04:39,160 --> 01:04:41,080
say, and you often run it through a long

1831
01:04:41,240 --> 01:04:42,230
pipeline. So for example,

1832
01:04:42,400 --> 01:04:44,020
the first thing you may do

1833
01:04:44,180 --> 01:04:45,600
may be preprocess the image

1834
01:04:45,770 --> 01:04:46,630
and remove the background,

1835
01:04:46,780 --> 01:04:48,200
so you remove the background.

1836
01:04:48,360 --> 01:04:51,940
And then you run a face

1837
01:04:52,100 --> 01:04:52,950
detection algorithm,

1838
01:04:53,110 --> 01:04:53,950
so a machine learning algorithm

1839
01:04:54,100 --> 01:04:55,470
to detect people's faces. Right?

1840
01:04:55,630 --> 01:04:57,130
And then, you know, let's say you want

1841
01:04:57,290 --> 01:04:58,350
to recognize the identity of the

1842
01:04:58,560 --> 01:04:59,230
person, right,

1843
01:04:59,400 --> 01:05:00,180
this is your application.

1844
01:05:00,340 --> 01:05:02,410
You then segment of the eyes,

1845
01:05:02,560 --> 01:05:04,760
segment of the nose, and have

1846
01:05:04,920 --> 01:05:05,710
different learning algorithms

1847
01:05:05,880 --> 01:05:07,710
to detect the mouth and so on. I know;

1848
01:05:07,860 --> 01:05:09,210
she might not want to be friend

1849
01:05:09,360 --> 01:05:10,700
after she sees this.

1850
01:05:10,860 --> 01:05:14,430
And then having found all these

1851
01:05:14,590 --> 01:05:16,040
features, based on, you know, what the

1852
01:05:16,200 --> 01:05:17,050
nose looks like, what the eyes looks

1853
01:05:17,210 --> 01:05:18,540
like, whatever, then you feed

1854
01:05:18,700 --> 01:05:19,670
all the features into

1855
01:05:19,830 --> 01:05:20,600
a logistic regression algorithm.

1856
01:05:20,750 --> 01:05:22,610
And your logistic regression

1857
01:05:22,770 --> 01:05:24,040
or soft match regression, or whatever,

1858
01:05:24,200 --> 01:05:26,370
will tell you the identity of this

1859
01:05:26,570 --> 01:05:27,780
person. Okay?

1860
01:05:27,950 --> 01:05:33,660
So this is what error analysis is.

1861
01:05:33,820 --> 01:05:37,050
You have a long complicated pipeline

1862
01:05:37,200 --> 01:05:39,850
combining many machine learning

1863
01:05:40,000 --> 01:05:42,050
components. Many of these would be used

1864
01:05:42,220 --> 01:05:43,180
in learning algorithms.

1865
01:05:43,350 --> 01:05:46,890
And so, it's often very useful to figure

1866
01:05:47,040 --> 01:05:48,890
out how much of your error can be

1867
01:05:49,150 --> 01:05:51,080
attributed to each of these components.

1868
01:05:54,840 --> 01:05:57,950
So what we'll do in a typical error

1869
01:05:58,110 --> 01:06:00,280
analysis procedure is we'll repeatedly

1870
01:06:00,430 --> 01:06:02,030
plug in the ground-truth for each

1871
01:06:02,170 --> 01:06:03,350
component and see how the accuracy

1872
01:06:03,510 --> 01:06:05,680
changes.So what I mean by that

1873
01:06:05,850 --> 01:06:08,280
is the figure on the bottom left

1874
01:06:08,440 --> 01:06:10,020
bottom right, let's say the overall

1875
01:06:10,180 --> 01:06:11,830
accuracy of the system is 85 percent.

1876
01:06:11,990 --> 01:06:14,610
Right? Then I want to know where

1877
01:06:14,770 --> 01:06:16,200
my 15 percent of error comes from.

1878
01:06:16,360 --> 01:06:18,070
And so what I'll do is

1879
01:06:18,230 --> 01:06:20,200
I'll go to my test set and

1880
01:06:20,360 --> 01:06:23,410
I'll actually code it and

1881
01:06:23,570 --> 01:06:24,890
oh, instead of actually implement

1882
01:06:25,040 --> 01:06:27,840
my correct background removal.

1883
01:06:27,990 --> 01:06:29,380
So actually, go in and give it,

1884
01:06:29,540 --> 01:06:30,790
give my algorithm what is the

1885
01:06:30,960 --> 01:06:32,410
correct background versus foreground.

1886
01:06:32,580 --> 01:06:33,880
And if I do that,

1887
01:06:34,030 --> 01:06:36,140
let's color that blue to denote that

1888
01:06:36,310 --> 01:06:37,670
I'm giving that ground-truth data

1889
01:06:37,830 --> 01:06:38,780
in the test set,

1890
01:06:38,940 --> 01:06:40,040
let's assume our accuracy

1891
01:06:40,200 --> 01:06:42,840
increases to 85.1 percent. Okay?

1892
01:06:42,990 --> 01:06:45,530
And now I'll go in and, you know,

1893
01:06:45,690 --> 01:06:47,280
give my algorithm the ground-truth,

1894
01:06:47,440 --> 01:06:48,780
face detection output.

1895
01:06:48,950 --> 01:06:51,190
So I'll go in and actually on my test set

1896
01:06:51,340 --> 01:06:52,330
I'll just tell the algorithm

1897
01:06:52,490 --> 01:06:53,090
where the face is.

1898
01:06:53,250 --> 01:06:54,090
And if I do that,

1899
01:06:54,250 --> 01:06:56,010
let's say my algorithm's accuracy

1900
01:06:56,170 --> 01:06:58,340
increases to 91 percent, and so on.

1901
01:06:58,510 --> 01:07:00,030
And then I'll go for each of these

1902
01:07:00,190 --> 01:07:05,920
components and just give it the

1903
01:07:06,070 --> 01:07:07,570
ground-truth label for each of the

1904
01:07:07,720 --> 01:07:08,960
components, because say, like,

1905
01:07:09,120 --> 01:07:10,800
the nose segmentation algorithm's

1906
01:07:10,970 --> 01:07:12,130
trying to figure out where the nose is.

1907
01:07:12,280 --> 01:07:13,780
I just in and tell it where the nose is

1908
01:07:13,950 --> 01:07:15,650
so that it doesn't have to figure that

1909
01:07:15,820 --> 01:07:17,550
out. And as I do this, one component

1910
01:07:17,710 --> 01:07:19,040
through the other, you know, I end up

1911
01:07:19,210 --> 01:07:20,630
giving it the correct output label

1912
01:07:20,790 --> 01:07:22,210
and end up with 100 percent accuracy.

1913
01:07:23,400 --> 01:07:24,950
And now you can look at this table

1914
01:07:25,100 --> 01:07:26,570
I'm sorry this is cut off on the bottom,

1915
01:07:26,730 --> 01:07:27,940
it says logistic regression

1916
01:07:28,090 --> 01:07:28,890
100 percent.

1917
01:07:29,050 --> 01:07:31,750
Now you can look at this table and see,

1918
01:07:31,920 --> 01:07:34,060
you know, how much giving the

1919
01:07:34,210 --> 01:07:35,710
round-truth labels for each of

1920
01:07:35,870 --> 01:07:36,820
these components could help

1921
01:07:36,980 --> 01:07:37,910
boost your final performance.

1922
01:07:38,060 --> 01:07:39,430
In particular,

1923
01:07:39,590 --> 01:07:40,500
if you look at this table,

1924
01:07:40,660 --> 01:07:42,510
you notice that when I added

1925
01:07:42,670 --> 01:07:44,000
the face detection ground-truth,

1926
01:07:44,170 --> 01:07:45,810
my performance jumped

1927
01:07:45,970 --> 01:07:47,250
from 85.1 percent accuracy

1928
01:07:47,420 --> 01:07:50,090
to 91 percent accuracy. Right?

1929
01:07:50,240 --> 01:07:51,360
So this tells me that

1930
01:07:51,520 --> 01:07:53,250
if only I can get better face detection,

1931
01:07:53,420 --> 01:07:55,350
maybe I can boost my accuracy

1932
01:07:55,510 --> 01:07:59,080
by 6 percent. Whereas in contrast,

1933
01:07:59,240 --> 01:08:01,520
when I, you know, say plugged in better,

1934
01:08:01,680 --> 01:08:04,590
I don't know, background removal,

1935
01:08:04,740 --> 01:08:06,790
my accuracy improved from 85 to 85.1

1936
01:08:06,950 --> 01:08:09,290
percent. And so, this sort of

1937
01:08:09,460 --> 01:08:10,440
diagnostic also tells you that

1938
01:08:10,600 --> 01:08:12,040
if your goal is to

1939
01:08:12,200 --> 01:08:12,820
improve the system,

1940
01:08:12,980 --> 01:08:14,400
it's probably a waste of your time

1941
01:08:14,570 --> 01:08:16,190
to try to improve your background

1942
01:08:16,340 --> 01:08:18,070
subtraction. Because if even if you got

1943
01:08:18,220 --> 01:08:19,510
the ground-truth, this is gives you,

1944
01:08:19,670 --> 01:08:20,720
at most, 0.1 percent accuracy,

1945
01:08:20,880 --> 01:08:22,760
whereas if you do better face

1946
01:08:22,930 --> 01:08:24,240
detection, maybe there's a much larger

1947
01:08:24,420 --> 01:08:26,350
potential for gains there. Okay?

1948
01:08:26,510 --> 01:08:27,750
So this sort of diagnostic, again,

1949
01:08:27,910 --> 01:08:31,290
is very useful because if

1950
01:08:31,450 --> 01:08:32,320
your goal is to improve the system,

1951
01:08:32,480 --> 01:08:34,040
there are so many different pieces

1952
01:08:34,210 --> 01:08:35,220
you can easily choose

1953
01:08:35,360 --> 01:08:36,360
to spend the next three months on.

1954
01:08:36,520 --> 01:08:37,650
Right? And choosing the right piece

1955
01:08:37,810 --> 01:08:39,970
is critical, and

1956
01:08:40,120 --> 01:08:41,150
this sort of diagnostic tells you

1957
01:08:41,320 --> 01:08:42,750
what's the piece that may actually

1958
01:08:42,910 --> 01:08:43,670
be worth your time to work on.

1959
01:08:47,080 --> 01:08:49,760
There's sort of another type of analyses

1960
01:08:49,930 --> 01:08:51,150
that's sort of the opposite of

1961
01:08:51,300 --> 01:08:52,050
what I just talked about.

1962
01:08:52,220 --> 01:08:54,650
The error analysis I just talked about

1963
01:08:54,810 --> 01:08:56,520
tries to explain the difference between

1964
01:08:56,690 --> 01:08:57,890
the current performance and perfect

1965
01:08:58,050 --> 01:09:00,820
performance, whereas this sort of

1966
01:09:00,970 --> 01:09:02,770
ablative analysis tries to explain

1967
01:09:02,920 --> 01:09:04,590
the difference between some baselines,

1968
01:09:04,740 --> 01:09:05,520
some really bad performance

1969
01:09:05,680 --> 01:09:06,730
and your current performance.

1970
01:09:06,890 --> 01:09:09,590
So for this example,

1971
01:09:09,750 --> 01:09:10,960
let's suppose you've built

1972
01:09:11,120 --> 01:09:12,770
a very good anti-spam classifier for

1973
01:09:12,920 --> 01:09:14,450
adding lots of clever features to your

1974
01:09:14,620 --> 01:09:16,100
logistic regression algorithm. Right?

1975
01:09:16,250 --> 01:09:17,360
So you added features for

1976
01:09:17,550 --> 01:09:18,740
spam correction, for, you know,

1977
01:09:18,900 --> 01:09:19,680
sender host features,

1978
01:09:19,840 --> 01:09:20,530
for email header features,

1979
01:09:20,690 --> 01:09:22,380
email text parser features,

1980
01:09:22,540 --> 01:09:23,770
JavaScript parser features,

1981
01:09:23,920 --> 01:09:25,460
features for embedded images,

1982
01:09:25,620 --> 01:09:26,490
and so on.

1983
01:09:26,650 --> 01:09:27,430
So now let's say

1984
01:09:27,580 --> 01:09:28,150
you preview the system

1985
01:09:28,320 --> 01:09:29,670
and you want to figure out, you know,

1986
01:09:29,840 --> 01:09:31,310
how well did each of these how much

1987
01:09:31,500 --> 01:09:32,330
did each of these components actually

1988
01:09:32,480 --> 01:09:34,280
contribute? Maybe you want to

1989
01:09:34,440 --> 01:09:35,670
write a research paper and

1990
01:09:35,840 --> 01:09:36,790
claim this was the piece

1991
01:09:36,940 --> 01:09:37,590
that made the big difference.

1992
01:09:37,750 --> 01:09:39,180
Can you actually document that claim

1993
01:09:39,340 --> 01:09:39,870
and justify it?

1994
01:09:40,030 --> 01:09:41,850
So in ablative analysis,

1995
01:09:42,010 --> 01:09:43,800
here's what we do.

1996
01:09:43,950 --> 01:09:46,700
So in this example, let's say that

1997
01:09:46,860 --> 01:09:47,790
simple logistic regression

1998
01:09:47,950 --> 01:09:49,610
without any of your clever improvements

1999
01:09:49,780 --> 01:09:50,890
get 94 percent performance.

2000
01:09:51,050 --> 01:09:53,070
And you want to figure out what

2001
01:09:53,230 --> 01:09:54,310
accounts for your improvement

2002
01:09:54,480 --> 01:09:56,350
from 94 to 99.9 percent performance.

2003
01:09:58,230 --> 01:10:00,900
So in ablative analysis and so instead

2004
01:10:01,060 --> 01:10:02,420
of adding components one at a time,

2005
01:10:02,570 --> 01:10:03,850
we'll instead remove components one

2006
01:10:04,010 --> 01:10:05,390
at a time to see how it rates.

2007
01:10:05,550 --> 01:10:08,880
So start with your overall system,

2008
01:10:09,040 --> 01:10:10,320
which is 99 percent accuracy.

2009
01:10:10,470 --> 01:10:12,740
And then we remove spelling correction

2010
01:10:12,900 --> 01:10:14,200
and see how much performance drops.

2011
01:10:14,370 --> 01:10:15,800
Then we'll remove

2012
01:10:15,970 --> 01:10:17,180
the sender host features and see

2013
01:10:17,340 --> 01:10:18,500
how much performance drops, and so on.

2014
01:10:18,660 --> 01:10:23,910
All right? And so,

2015
01:10:24,070 --> 01:10:25,150
in this contrived example,

2016
01:10:25,300 --> 01:10:29,540
you see that, I guess,

2017
01:10:29,700 --> 01:10:31,510
the biggest drop occurred when you

2018
01:10:31,670 --> 01:10:33,640
remove the text parser features.

2019
01:10:33,800 --> 01:10:36,200
And so you can then make a credible case

2020
01:10:36,370 --> 01:10:37,830
that, you know, the text parser

2021
01:10:38,000 --> 01:10:39,430
features where what really made the

2022
01:10:39,580 --> 01:10:41,340
biggest difference here. Okay?

2023
01:10:41,510 --> 01:10:42,230
And you can also tell,

2024
01:10:42,390 --> 01:10:45,080
for instance, that, I don't know,

2025
01:10:45,240 --> 01:10:46,700
removing the sender host features

2026
01:10:46,860 --> 01:10:47,730
on this line, right,

2027
01:10:47,890 --> 01:10:51,310
performance dropped from 99.9 to 98.9.

2028
01:10:51,460 --> 01:10:52,470
And so this also means that

2029
01:10:52,630 --> 01:10:53,750
in case you want to

2030
01:10:53,920 --> 01:10:55,100
get rid of the sender host features

2031
01:10:55,280 --> 01:10:56,580
to speed up computational something

2032
01:10:56,740 --> 01:10:57,900
that would be a good candidate

2033
01:10:58,060 --> 01:10:59,310
for elimination. Okay?

2034
01:10:59,480 --> 01:11:01,080
Student:Are there any guarantees

2035
01:11:01,250 --> 01:11:02,340
that if you shuffle around the order

2036
01:11:02,510 --> 01:11:03,810
in which you drop those features that

2037
01:11:03,960 --> 01:11:04,960
you'll get the same

2038
01:11:05,110 --> 01:11:06,400
Instructor (Andrew Ng):Yeah. Let's

2039
01:11:06,570 --> 01:11:07,170
address the question:

2040
01:11:07,350 --> 01:11:08,110
What if you shuffle

2041
01:11:08,270 --> 01:11:08,780
in which you remove things?

2042
01:11:08,880 --> 01:11:09,480
The answer is no.

2043
01:11:09,560 --> 01:11:10,240
There's no guarantee

2044
01:11:10,320 --> 01:11:11,160
you'd get the similar result.

2045
01:11:11,240 --> 01:11:12,750
So in practice,

2046
01:11:12,840 --> 01:11:15,040
sometimes there's a fairly natural of

2047
01:11:15,220 --> 01:11:16,970
ordering for both types of analyses,

2048
01:11:17,140 --> 01:11:18,260
the error analyses and ablative

2049
01:11:18,410 --> 01:11:19,680
analysis, sometimes there's a fairly

2050
01:11:19,850 --> 01:11:21,070
natural ordering in which you add

2051
01:11:21,220 --> 01:11:22,130
things or remove things,

2052
01:11:22,310 --> 01:11:23,330
sometimes there's isn't.

2053
01:11:23,500 --> 01:11:25,380
And quite often, you either choose

2054
01:11:25,540 --> 01:11:27,450
one ordering and just go for it or

2055
01:11:27,620 --> 01:11:29,430
And don't think of these analyses as

2056
01:11:29,620 --> 01:11:31,160
sort of formulas that are constants,

2057
01:11:31,320 --> 01:11:32,070
though; I mean

2058
01:11:32,230 --> 01:11:33,270
feel free to invent your own, as well.

2059
01:11:33,430 --> 01:11:35,910
You know one of the things that's done

2060
01:11:36,070 --> 01:11:38,910
quite often is take the overall system

2061
01:11:39,070 --> 01:11:40,170
and just remove one

2062
01:11:40,320 --> 01:11:41,030
and then put it back,

2063
01:11:41,180 --> 01:11:41,990
then remove a different one

2064
01:11:42,150 --> 01:11:42,860
then put it back

2065
01:11:43,020 --> 01:11:44,300
until all of these things are done.

2066
01:11:44,460 --> 01:11:46,420
Okay.

2067
01:11:46,580 --> 01:11:49,490
So the very last thing I want to talk

2068
01:11:49,650 --> 01:11:51,200
about is sort of this general advice

2069
01:11:51,360 --> 01:11:52,260
for how to get started

2070
01:11:52,430 --> 01:11:53,180
on a learning problem.

2071
01:11:53,350 --> 01:11:59,490
So here's a cartoon description on

2072
01:11:59,640 --> 01:12:01,570
two broad to get started on learning

2073
01:12:01,730 --> 01:12:04,490
problem. The first one is

2074
01:12:04,670 --> 01:12:07,020
carefully design your system,

2075
01:12:07,170 --> 01:12:08,220
so you spend a long time

2076
01:12:08,400 --> 01:12:09,980
designing exactly the right features,

2077
01:12:10,140 --> 01:12:11,300
collecting the right data set,

2078
01:12:11,460 --> 01:12:12,540
and designing the right algorithmic

2079
01:12:12,700 --> 01:12:14,230
structure, then you implement it

2080
01:12:14,390 --> 01:12:15,650
and hope it works. All right?

2081
01:12:15,800 --> 01:12:18,760
The benefit of this sort of approach is

2082
01:12:18,930 --> 01:12:19,860
you get maybe nicer,

2083
01:12:20,010 --> 01:12:21,200
maybe more scalable algorithms,

2084
01:12:21,360 --> 01:12:23,190
and maybe you come up with

2085
01:12:23,350 --> 01:12:24,720
new elegant learning algorithms.

2086
01:12:24,890 --> 01:12:26,260
And if your goal is to, you know,

2087
01:12:26,420 --> 01:12:28,080
contribute to basic research

2088
01:12:28,240 --> 01:12:28,880
in machine learning,

2089
01:12:29,080 --> 01:12:29,760
if your goal is to invent

2090
01:12:29,930 --> 01:12:30,660
new machine learning algorithms,

2091
01:12:30,850 --> 01:12:32,360
this process of slowing down

2092
01:12:32,530 --> 01:12:34,570
and thinking deeply about the problem,

2093
01:12:34,730 --> 01:12:35,940
you know, that is sort of the right way

2094
01:12:36,090 --> 01:12:37,210
to go about is think deeply about

2095
01:12:37,370 --> 01:12:38,780
a problem and invent new solutions.

2096
01:12:38,940 --> 01:12:44,030
Second sort of approach is

2097
01:12:44,180 --> 01:12:45,780
what I call build-and-fix,

2098
01:12:45,940 --> 01:12:47,140
which is we input something

2099
01:12:47,290 --> 01:12:47,980
quick and dirty

2100
01:12:48,140 --> 01:12:49,660
and then you run error analyses

2101
01:12:49,820 --> 01:12:50,580
and diagnostics

2102
01:12:50,730 --> 01:12:51,660
to figure out what's wrong

2103
01:12:51,820 --> 01:12:52,970
and you fix those errors.

2104
01:12:53,150 --> 01:12:55,550
The benefit of this second type of

2105
01:12:55,720 --> 01:12:57,260
approach is that it'll often get your

2106
01:12:57,420 --> 01:12:59,060
application working much more quickly.

2107
01:12:59,220 --> 01:13:01,730
And especially with those of you,

2108
01:13:01,880 --> 01:13:02,880
if you end up working in a company,

2109
01:13:03,040 --> 01:13:04,330
and sometimes if you end up working

2110
01:13:04,500 --> 01:13:06,060
in a company, you know, very often

2111
01:13:06,220 --> 01:13:08,460
it's not the best product that wins;

2112
01:13:08,610 --> 01:13:10,710
it's the first product to market that

2113
01:13:10,870 --> 01:13:11,800
wins. And so there's

2114
01:13:11,960 --> 01:13:12,950
especially in the industry.

2115
01:13:13,110 --> 01:13:14,080
There's really something to be said for,

2116
01:13:14,230 --> 01:13:15,680
you know, building a system quickly

2117
01:13:15,850 --> 01:13:16,910
and getting it deployed quickly.

2118
01:13:17,070 --> 01:13:19,750
And the second approach of

2119
01:13:19,910 --> 01:13:21,300
building a quick-and-dirty,

2120
01:13:21,470 --> 01:13:23,380
I'm gonna say hack and then fixing

2121
01:13:23,540 --> 01:13:24,780
the problems will actually get you

2122
01:13:24,930 --> 01:13:26,920
to a system that works well

2123
01:13:27,080 --> 01:13:28,300
much more quickly.

2124
01:13:28,450 --> 01:13:33,240
And the reason is very often it's really

2125
01:13:33,420 --> 01:13:34,670
not clear what parts of a system

2126
01:13:34,840 --> 01:13:35,980
are easier to think of to build

2127
01:13:36,130 --> 01:13:37,730
and therefore what you need

2128
01:13:37,890 --> 01:13:38,890
to spends lot of time focusing on.

2129
01:13:39,050 --> 01:13:41,190
So there's that example

2130
01:13:41,360 --> 01:13:43,180
I talked about just now. Right?

2131
01:13:43,340 --> 01:13:47,670
For identifying people, say.

2132
01:13:47,830 --> 01:13:49,640
And with a big complicated

2133
01:13:49,810 --> 01:13:50,740
learning system like this,

2134
01:13:50,910 --> 01:13:52,130
a big complicated pipeline like this,

2135
01:13:52,290 --> 01:13:54,650
it's really not obvious at the outset

2136
01:13:54,810 --> 01:13:56,730
which of these components you should

2137
01:13:56,910 --> 01:13:58,300
spend lots of time working on. Right?

2138
01:13:58,590 --> 01:13:59,670
And if you didn't know that

2139
01:13:59,840 --> 01:14:00,910
preprocessing wasn't

2140
01:14:01,070 --> 01:14:01,970
the right component,

2141
01:14:02,120 --> 01:14:03,660
you could easily have spent

2142
01:14:03,820 --> 01:14:04,580
three months working on

2143
01:14:04,740 --> 01:14:05,810
better background subtraction,

2144
01:14:05,970 --> 01:14:07,250
not knowing that it's just

2145
01:14:07,400 --> 01:14:08,290
not gonna ultimately matter.

2146
01:14:08,440 --> 01:14:11,220
And so the only way to find out

2147
01:14:11,390 --> 01:14:12,420
what really works was

2148
01:14:12,580 --> 01:14:13,420
inputting something quickly

2149
01:14:13,590 --> 01:14:14,350
and you find out what parts

2150
01:14:14,510 --> 01:14:18,100
and find out what parts are really

2151
01:14:18,250 --> 01:14:19,260
the hard parts to implement,

2152
01:14:19,420 --> 01:14:20,620
or what parts are hard parts that

2153
01:14:20,790 --> 01:14:22,100
could make a difference in performance.

2154
01:14:22,260 --> 01:14:24,040
In fact, say that

2155
01:14:24,190 --> 01:14:25,310
if your goal is to

2156
01:14:25,470 --> 01:14:27,160
build a people recognition system,

2157
01:14:27,320 --> 01:14:28,250
a system like this

2158
01:14:28,410 --> 01:14:29,450
is actually far too complicated

2159
01:14:29,610 --> 01:14:30,780
as your initial system.

2160
01:14:30,950 --> 01:14:32,450
Maybe after you're prototyped

2161
01:14:32,620 --> 01:14:33,780
a few systems, and you

2162
01:14:33,940 --> 01:14:34,700
converged a system like this.

2163
01:14:34,870 --> 01:14:35,900
But if this is your first system

2164
01:14:36,060 --> 01:14:37,070
you're designing,

2165
01:14:37,230 --> 01:14:37,950
this is much too complicated.

2166
01:14:41,040 --> 01:14:43,600
Also, this is a very

2167
01:14:43,760 --> 01:14:44,680
concrete piece of advice, and

2168
01:14:44,830 --> 01:14:46,720
this applies to your projects as well.

2169
01:14:46,880 --> 01:14:48,530
If your goal is to

2170
01:14:48,700 --> 01:14:49,970
build a working application,

2171
01:14:50,130 --> 01:14:52,260
Step 1 is actually probably not

2172
01:14:52,420 --> 01:14:53,650
to design a system like this.

2173
01:14:53,810 --> 01:14:54,830
Step 1 is where you would

2174
01:14:54,990 --> 01:14:55,680
plot your data.

2175
01:14:55,840 --> 01:14:57,800
And very often,

2176
01:14:57,960 --> 01:14:58,990
and if you just take the data

2177
01:14:59,160 --> 01:15:00,360
you're trying to predict and

2178
01:15:00,540 --> 01:15:02,430
just plot your data, plot X, plot Y,

2179
01:15:02,600 --> 01:15:03,590
plot your data

2180
01:15:03,750 --> 01:15:05,910
everywhere you can think of, you know,

2181
01:15:06,080 --> 01:15:07,950
half the time you look at it and go,

2182
01:15:08,110 --> 01:15:09,300
"Gee, how come all those numbers

2183
01:15:09,450 --> 01:15:10,150
are negative?

2184
01:15:10,320 --> 01:15:11,090
I thought they should be positive.

2185
01:15:11,260 --> 01:15:12,280
Something's wrong with this dataset."

2186
01:15:12,440 --> 01:15:14,250
And it's about half the time

2187
01:15:14,410 --> 01:15:15,010
you find something

2188
01:15:15,170 --> 01:15:16,360
obviously wrong with your data

2189
01:15:16,520 --> 01:15:17,380
or something very surprising.

2190
01:15:17,530 --> 01:15:19,270
And this is something you find out

2191
01:15:19,490 --> 01:15:20,540
just by plotting your data,

2192
01:15:20,690 --> 01:15:22,630
and that you won't find out be

2193
01:15:22,790 --> 01:15:23,530
implementing these big

2194
01:15:23,700 --> 01:15:24,800
complicated learning algorithms on it.

2195
01:15:26,540 --> 01:15:29,190
Plotting the data sounds so simple,

2196
01:15:29,330 --> 01:15:30,520
it was one of the pieces of advice that

2197
01:15:30,680 --> 01:15:31,380
lots of us give

2198
01:15:31,550 --> 01:15:32,630
but hardly anyone follows,

2199
01:15:32,800 --> 01:15:34,760
so you can take that for what it's worth.

2200
01:15:38,270 --> 01:15:39,360
Let me just reiterate,

2201
01:15:39,530 --> 01:15:41,050
what I just said here may be bad advice

2202
01:15:41,210 --> 01:15:43,230
if your goal is to come up with

2203
01:15:43,380 --> 01:15:45,270
new machine learning algorithms.

2204
01:15:45,430 --> 01:15:47,270
All right? So for me personally,

2205
01:15:47,430 --> 01:15:49,230
the learning algorithm I use the most

2206
01:15:49,410 --> 01:15:50,870
often is probably logistic regression

2207
01:15:51,040 --> 01:15:52,260
because I have code lying around.

2208
01:15:52,420 --> 01:15:54,110
So give me a learning problem,

2209
01:15:54,260 --> 01:15:55,150
I probably won't try anything

2210
01:15:55,300 --> 01:15:55,920
more complicated than

2211
01:15:56,070 --> 01:15:57,470
logistic regression on it first.

2212
01:15:57,620 --> 01:15:58,750
And it's only after trying

2213
01:15:58,900 --> 01:15:59,860
something really simple and

2214
01:16:00,020 --> 01:16:01,110
figure our what's easy,

2215
01:16:01,270 --> 01:16:02,110
what's hard, then you

2216
01:16:02,270 --> 01:16:03,130
know where to focus your efforts.

2217
01:16:03,280 --> 01:16:04,770
But again, if your goal is to invent

2218
01:16:04,920 --> 01:16:06,330
new machine learning algorithms,

2219
01:16:06,490 --> 01:16:07,500
then you sort of

2220
01:16:07,660 --> 01:16:08,420
don't want to hack up something

2221
01:16:08,580 --> 01:16:09,790
and then add another hack to fix it,

2222
01:16:09,970 --> 01:16:11,700
and hack it even more to fix it. Right?

2223
01:16:11,860 --> 01:16:12,790
So if your goal is to

2224
01:16:12,930 --> 01:16:14,020
do novel machine learning research,

2225
01:16:14,180 --> 01:16:15,220
then it pays to

2226
01:16:15,380 --> 01:16:16,520
think more deeply about the problem

2227
01:16:16,680 --> 01:16:17,790
and not gonna follow this specifically.

2228
01:16:20,180 --> 01:16:22,850
Shoot, you know what? All right,

2229
01:16:23,000 --> 01:16:23,750
sorry if I'm

2230
01:16:23,910 --> 01:16:25,070
late but I just have two more slides

2231
01:16:25,230 --> 01:16:26,430
so I'm gonna go through these quickly.

2232
01:16:26,590 --> 01:16:30,780
And so, this is what I think of as

2233
01:16:30,940 --> 01:16:32,510
premature statistical optimization,

2234
01:16:32,670 --> 01:16:35,080
where quite often, just like

2235
01:16:35,250 --> 01:16:36,690
premature optimization of code,

2236
01:16:36,860 --> 01:16:39,310
quite often people will prematurely

2237
01:16:39,470 --> 01:16:41,510
optimize one component of a big

2238
01:16:41,660 --> 01:16:43,000
complicated machine learning system.

2239
01:16:43,170 --> 01:16:45,830
Okay? Just two more slides.

2240
01:16:45,830 --> 01:16:46,830
This was –this is a sort of cartoon that

2241
01:16:50,120 --> 01:16:51,520
highly influenced my own thinking.

2242
01:16:51,690 --> 01:16:52,520
It was based on a paper

2243
01:16:52,680 --> 01:16:53,900
written by Christos Papadimitriou.

2244
01:16:54,050 --> 01:16:58,350
This is how progress this is how

2245
01:16:58,510 --> 01:17:00,320
developmental progress of research

2246
01:17:00,470 --> 01:17:01,360
often happens. Right?

2247
01:17:01,530 --> 01:17:02,870
Let's say you want to

2248
01:17:03,040 --> 01:17:03,710
build a mail delivery robot,

2249
01:17:03,870 --> 01:17:05,090
so I've drawn a circle there

2250
01:17:05,250 --> 01:17:06,280
that says mail delivery robot.

2251
01:17:06,440 --> 01:17:07,470
And it seems like a useful thing to

2252
01:17:07,620 --> 01:17:08,360
have. Right?

2253
01:17:08,520 --> 01:17:09,440
You know free up people,

2254
01:17:09,620 --> 01:17:10,400
don't have to deliver mail.

2255
01:17:10,550 --> 01:17:13,270
So what to deliver mail,

2256
01:17:13,420 --> 01:17:14,960
obviously you need a robot

2257
01:17:15,130 --> 01:17:16,790
to wander around indoor environments

2258
01:17:16,940 --> 01:17:19,090
and you need a robot to manipulate

2259
01:17:19,250 --> 01:17:20,430
objects and pickup envelopes.

2260
01:17:20,590 --> 01:17:21,760
And so, you need to

2261
01:17:21,920 --> 01:17:22,830
build those two components

2262
01:17:22,990 --> 01:17:24,230
in order to get a mail delivery robot.

2263
01:17:24,390 --> 01:17:26,350
And so I've drawing those two components

2264
01:17:26,510 --> 01:17:28,020
and little arrows to denote that,

2265
01:17:28,180 --> 01:17:30,430
you know, obstacle avoidance is needed

2266
01:17:30,590 --> 01:17:32,880
or would help build your mail delivery

2267
01:17:33,040 --> 01:17:36,360
robot.Well for obstacle for avoidance,

2268
01:17:36,520 --> 01:17:38,010
clearly, you need a robot

2269
01:17:38,170 --> 01:17:39,070
that can navigate and

2270
01:17:39,240 --> 01:17:40,180
you need to detect objects

2271
01:17:40,350 --> 01:17:41,220
so you can avoid the obstacles.

2272
01:17:41,400 --> 01:17:43,560
Now we're gonna use computer vision

2273
01:17:43,710 --> 01:17:45,230
to detect the objects.

2274
01:17:45,400 --> 01:17:48,110
And so, we know that, you know,

2275
01:17:48,270 --> 01:17:49,550
lighting sometimes changes, right,

2276
01:17:49,710 --> 01:17:51,130
depending on whether it's the morning

2277
01:17:51,290 --> 01:17:51,920
or noontime or evening.

2278
01:17:52,110 --> 01:17:53,690
This is lighting causes

2279
01:17:53,850 --> 01:17:55,080
the color of things to change,

2280
01:17:55,240 --> 01:17:57,240
and so you need an object detection

2281
01:17:57,390 --> 01:17:58,800
system that's invariant to the

2282
01:17:58,970 --> 01:18:01,070
specific colors of an object. Right?

2283
01:18:01,230 --> 01:18:02,340
Because lighting changes, say.

2284
01:18:02,500 --> 01:18:06,120
Well color, or RGB values,

2285
01:18:06,300 --> 01:18:08,970
is represented by three-dimensional

2286
01:18:09,130 --> 01:18:10,200
vectors. And so you need to learn

2287
01:18:10,360 --> 01:18:12,470
when two colors might be the same thing,

2288
01:18:12,640 --> 01:18:15,400
when two, you know, visual appearance

2289
01:18:15,560 --> 01:18:16,740
of two colors may be the same thing as

2290
01:18:16,890 --> 01:18:18,110
just the lighting change or something.

2291
01:18:18,270 --> 01:18:21,490
And to understand that properly,

2292
01:18:21,640 --> 01:18:22,530
we can go out and study

2293
01:18:22,690 --> 01:18:24,120
differential geometry of 3d manifolds

2294
01:18:24,280 --> 01:18:25,320
because that helps us

2295
01:18:25,470 --> 01:18:26,460
build a sound theory

2296
01:18:26,620 --> 01:18:27,960
on which to develop our

2297
01:18:28,140 --> 01:18:29,340
3d similarity learning algorithms.

2298
01:18:31,640 --> 01:18:33,120
And to really understand the

2299
01:18:33,270 --> 01:18:34,600
fundamental aspects of this problem,

2300
01:18:34,750 --> 01:18:36,870
we have to study the

2301
01:18:37,040 --> 01:18:37,990
complexity of non-Riemannian

2302
01:18:38,150 --> 01:18:41,040
geometries. And on and on it goes until

2303
01:18:41,210 --> 01:18:42,250
eventually you're proving

2304
01:18:42,400 --> 01:18:43,830
convergence bounds for sampled of

2305
01:18:44,000 --> 01:18:44,870
non-monotonic logic.

2306
01:18:45,020 --> 01:18:46,240
I don't even know what this is

2307
01:18:46,410 --> 01:18:46,920
because I just made it up.

2308
01:18:47,090 --> 01:18:50,830
Whereas in reality, you know,

2309
01:18:50,990 --> 01:18:53,250
chances are that link isn't real.

2310
01:18:53,420 --> 01:18:56,090
Color variance just barely helped

2311
01:18:56,240 --> 01:18:57,160
object recognition maybe.

2312
01:18:57,310 --> 01:18:58,200
I'm making this up.

2313
01:18:58,370 --> 01:19:00,910
Maybe differential geometry was hardly

2314
01:19:01,070 --> 01:19:02,320
gonna help 3d similarity learning

2315
01:19:02,480 --> 01:19:03,850
and that link's also gonna fail.

2316
01:19:04,000 --> 01:19:04,610
Okay?

2317
01:19:04,760 --> 01:19:05,970
So, each of these circles

2318
01:19:06,120 --> 01:19:06,900
can represent a person,

2319
01:19:07,060 --> 01:19:08,010
or a research community,

2320
01:19:08,190 --> 01:19:09,270
or a thought in your head.

2321
01:19:09,440 --> 01:19:10,990
And there's a very real chance that

2322
01:19:11,150 --> 01:19:12,590
maybe there are all these papers

2323
01:19:12,750 --> 01:19:14,210
written on differential geometry of 3d

2324
01:19:14,370 --> 01:19:15,460
manifolds, and they are written because

2325
01:19:15,620 --> 01:19:17,300
some guy once told someone else that

2326
01:19:17,470 --> 01:19:18,730
it'll help 3d similarity learning.

2327
01:19:18,890 --> 01:19:20,830
And, you know, it's like

2328
01:19:20,990 --> 01:19:21,990
"A friend of mine told me that

2329
01:19:22,140 --> 01:19:23,120
color invariance would help

2330
01:19:23,270 --> 01:19:24,010
in object recognition,

2331
01:19:24,160 --> 01:19:25,110
so I'm working on color invariance.

2332
01:19:25,270 --> 01:19:26,250
And now I'm gonna tell a friend of mine

2333
01:19:26,420 --> 01:19:28,070
that his thing will help my problem.

2334
01:19:28,240 --> 01:19:29,490
And he'll tell a friend of his that

2335
01:19:29,650 --> 01:19:30,810
his thing will help with his problem."

2336
01:19:30,970 --> 01:19:32,510
And pretty soon, you're working on

2337
01:19:32,670 --> 01:19:34,130
convergence bound for sampled

2338
01:19:34,290 --> 01:19:35,900
non-monotonic logic, when in reality

2339
01:19:36,080 --> 01:19:37,880
none of these will see the light of day

2340
01:19:38,040 --> 01:19:39,790
of your mail delivery robot. Okay?

2341
01:19:39,950 --> 01:19:43,960
I'm not criticizing the role of theory.

2342
01:19:44,120 --> 01:19:45,840
There are very powerful theories,

2343
01:19:46,000 --> 01:19:48,070
like the theory of VC dimension, which

2344
01:19:48,240 --> 01:19:49,950
is far, far, far to the right of this.

2345
01:19:50,110 --> 01:19:51,430
So VC dimension is about

2346
01:19:51,610 --> 01:19:52,630
as theoretical as it can get.

2347
01:19:52,790 --> 01:19:54,860
And it's clearly had a huge impact

2348
01:19:55,010 --> 01:19:56,090
on many applications.

2349
01:19:56,250 --> 01:19:57,570
And there's, you know, dramatically

2350
01:19:57,730 --> 01:19:58,960
advanced data machine learning.

2351
01:19:59,110 --> 01:20:00,530
And another example is theory of

2352
01:20:00,690 --> 01:20:01,770
NP-hardness as again, you know,

2353
01:20:01,930 --> 01:20:03,480
is about theoretical as it can get.

2354
01:20:03,660 --> 01:20:04,820
It's like a huge application

2355
01:20:04,970 --> 01:20:07,220
on all of computer science,

2356
01:20:07,380 --> 01:20:08,330
the theory of NP-hardness.

2357
01:20:08,490 --> 01:20:11,520
But when you are off working on

2358
01:20:11,680 --> 01:20:13,240
highly theoretical things, I guess,

2359
01:20:13,390 --> 01:20:13,990
to me personally

2360
01:20:14,140 --> 01:20:15,320
it's important to keep in mind

2361
01:20:15,480 --> 01:20:17,690
are you working on something like

2362
01:20:17,850 --> 01:20:19,340
VC dimension, which is high impact,

2363
01:20:19,510 --> 01:20:20,420
or are you working on something like

2364
01:20:20,570 --> 01:20:21,700
convergence bound for sampled

2365
01:20:21,850 --> 01:20:22,550
non-monotonic logic,

2366
01:20:22,710 --> 01:20:23,800
which you're only hoping

2367
01:20:23,950 --> 01:20:25,650
has some peripheral relevance

2368
01:20:25,800 --> 01:20:27,830
to some application. Okay?

2369
01:20:28,250 --> 01:20:31,170
For me personally, I tend to

2370
01:20:31,330 --> 01:20:33,210
work on an application only if I

2371
01:20:33,360 --> 01:20:35,400
excuse me. For me personally,

2372
01:20:35,550 --> 01:20:36,530
and this is a personal choice,

2373
01:20:36,680 --> 01:20:38,900
I tend to trust something only if

2374
01:20:39,050 --> 01:20:40,420
I personally can see a link

2375
01:20:40,570 --> 01:20:41,830
from the theory I'm working on

2376
01:20:41,990 --> 01:20:43,630
all the way back to an application.

2377
01:20:43,790 --> 01:20:47,100
And if I don't personally see

2378
01:20:47,260 --> 01:20:48,020
a direct link from

2379
01:20:48,170 --> 01:20:49,170
what I'm doing to an application then,

2380
01:20:49,330 --> 01:20:51,230
you know, then that's fine.

2381
01:20:51,400 --> 01:20:52,820
Then I can choose to work on theory,

2382
01:20:52,980 --> 01:20:54,810
but I wouldn't necessarily trust that

2383
01:20:54,960 --> 01:20:56,500
what the theory I'm working on

2384
01:20:56,650 --> 01:20:57,860
will relate to an application,

2385
01:20:58,010 --> 01:20:58,940
if I don't personally

2386
01:20:59,100 --> 01:21:00,080
see a link all the way back.

2387
01:21:01,460 --> 01:21:02,800
Just to summarize.

2388
01:21:04,630 --> 01:21:07,510
One lesson to take away from today is

2389
01:21:07,680 --> 01:21:09,260
I think time spent coming up with

2390
01:21:09,440 --> 01:21:10,640
diagnostics for learning algorithms

2391
01:21:10,800 --> 01:21:11,690
is often time well spent.

2392
01:21:11,850 --> 01:21:13,820
It's often up to your own ingenuity

2393
01:21:13,970 --> 01:21:15,340
to come up with great diagnostics.

2394
01:21:15,490 --> 01:21:17,090
And just when I personally, when I

2395
01:21:17,250 --> 01:21:18,060
work on machine learning algorithm,

2396
01:21:18,230 --> 01:21:20,350
it's not uncommon for me to be spending

2397
01:21:20,510 --> 01:21:22,140
like between a third and often half of

2398
01:21:22,290 --> 01:21:24,300
my time just writing diagnostics

2399
01:21:24,450 --> 01:21:25,400
and trying to figure out

2400
01:21:25,550 --> 01:21:26,590
what's going right and what's going wrong.

2401
01:21:26,750 --> 01:21:29,510
Sometimes it's tempting not to, right,

2402
01:21:29,680 --> 01:21:30,620
because you want to be implementing

2403
01:21:30,780 --> 01:21:31,450
learning algorithms and making

2404
01:21:31,610 --> 01:21:32,730
progress. You don't want to be spending

2405
01:21:32,890 --> 01:21:33,860
all this time, you know, implementing

2406
01:21:34,020 --> 01:21:35,040
tests on your learning algorithms;

2407
01:21:35,200 --> 01:21:36,340
it doesn't feel like when you're doing

2408
01:21:36,510 --> 01:21:38,400
anything. But when I implement

2409
01:21:38,550 --> 01:21:39,300
learning algorithms,

2410
01:21:39,470 --> 01:21:40,860
at least a third, and quite often

2411
01:21:41,010 --> 01:21:42,100
half of my time, is actually spent

2412
01:21:42,260 --> 01:21:43,880
implementing those tests and you can

2413
01:21:44,050 --> 01:21:44,920
figure out what to work on.

2414
01:21:45,110 --> 01:21:46,130
And I think it's actually one of

2415
01:21:46,280 --> 01:21:47,260
the best uses of your time.

2416
01:21:48,950 --> 01:21:50,760
Talked about error analyses

2417
01:21:50,920 --> 01:21:51,620
and ablative analyses,

2418
01:21:51,780 --> 01:21:54,910
and lastly talked about, you know,

2419
01:21:55,060 --> 01:21:56,120
different approaches and the risks of

2420
01:21:56,280 --> 01:21:58,620
premature statistical optimization.

2421
01:21:58,780 --> 01:22:00,230
Okay.

2422
01:22:00,400 --> 01:22:01,400
Sorry I ran you over.

2423
01:22:01,570 --> 01:22:03,150
I'll be here for a few more minutes

2424
01:22:03,340 --> 01:22:04,100
for your questions.

2425
01:22:04,250 --> 01:22:06,490
That's [inaudible] today.

