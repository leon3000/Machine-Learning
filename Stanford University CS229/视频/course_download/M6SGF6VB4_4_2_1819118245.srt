1
00:00:24,050 --> 00:00:25,530
Okay, so welcome back.

2
00:00:25,830 --> 00:00:29,180
And what I want to do today is talk about

3
00:00:29,480 --> 00:00:32,340
new test methods algorith for

4
00:00:32,530 --> 00:00:34,940
fitting models like logistic regression,

5
00:00:35,180 --> 00:00:38,040
and then we'll talk about exponential family

6
00:00:38,260 --> 00:00:40,240
distributions and generalized linear models.

7
00:00:40,440 --> 00:00:43,260
It's a very nice class of ideas that will tie together,

8
00:00:43,460 --> 00:00:44,500
the logistic regression

9
00:00:44,500 --> 00:00:45,720
and the ordinary V

10
00:00:45,990 --> 00:00:47,530
squares models that we'll see.

11
00:00:47,710 --> 00:00:49,580
So hopefully I'll get to that today.

12
00:00:49,760 --> 00:00:52,530
So throughout the previous lecture

13
00:00:52,710 --> 00:00:53,410
and this lecture,

14
00:00:53,410 --> 00:00:54,540
we're starting to use increasingly

15
00:00:54,730 --> 00:00:56,550
large amounts of material on probability.

16
00:00:56,780 --> 00:01:01,850
So if you'd like to see a refresher on sort of the

17
00:01:02,030 --> 00:01:04,370
foundations of probability if you're not sure

18
00:01:04,570 --> 00:01:05,990
if you quite had your prerequisites for this

19
00:01:06,180 --> 00:01:07,900
class in terms of a background in probability

20
00:01:08,140 --> 00:01:10,210
and statistics, then the discussion section

21
00:01:10,430 --> 00:01:14,100
taught this week by the TA's will go over

22
00:01:14,300 --> 00:01:16,380
so they can review a probability.

23
00:01:16,610 --> 00:01:18,880
At the same discussion sections also for the TA's,

24
00:01:19,090 --> 00:01:20,770
we'll also briefly go over sort of

25
00:01:20,990 --> 00:01:23,280
matlab and octave notation

26
00:01:24,470 --> 00:01:26,690
which you need to use for your problem sets.

27
00:01:26,910 --> 00:01:29,110
And so if you any of you want to see a review

28
00:01:29,360 --> 00:01:31,170
of the probability and statistics pre-reqs,

29
00:01:31,350 --> 00:01:32,950
or if you want to [inaudible] octave,

30
00:01:33,240 --> 00:01:36,650
please come to this the next discussion section.

31
00:01:39,200 --> 00:01:43,240
All right. So just to recap briefly,

32
00:01:43,500 --> 00:01:46,980
towards the end of the last lecture I talked about

33
00:01:47,180 --> 00:01:49,900
the logistic regression model where we had

34
00:01:50,110 --> 00:01:53,010
which was an algorithm for [inaudible]

35
00:01:53,210 --> 00:01:55,620
We had that [inaudible] of [inaudible]

36
00:01:55,800 --> 00:02:02,270
if an XFY equals one, give an X theta

37
00:02:02,790 --> 00:02:05,440
by theta under this model, all right.

38
00:02:05,750 --> 00:02:08,360
If this was one over one [inaudible] theta,

39
00:02:09,120 --> 00:02:13,370
transpose X. And then you can write down

40
00:02:13,590 --> 00:02:17,200
the log likelihood like given the training sets,

41
00:02:30,250 --> 00:02:31,700
which was that.

42
00:02:32,040 --> 00:02:34,930
And by taking the derivitive of this,

43
00:02:35,130 --> 00:02:39,010
you can derive sort of a gradient ascent rule

44
00:02:39,220 --> 00:02:41,510
for finding the maximum likelihood estimate

45
00:02:41,750 --> 00:02:43,370
of the parameter theta

46
00:02:43,730 --> 00:02:46,310
for this logistic regression model.

47
00:02:46,660 --> 00:02:49,480
And so last time I wrote down the learning rule

48
00:02:49,690 --> 00:02:52,920
for batch gradient ascent, but the [inaudible] has to

49
00:02:53,220 --> 00:02:56,710
be gradient ascent where you look at just

50
00:02:56,920 --> 00:02:58,830
one training example at a time,

51
00:02:59,080 --> 00:03:01,750
would be like this, okay.

52
00:03:06,950 --> 00:03:09,720
So last time I wrote down batch gradient ascent.

53
00:03:09,980 --> 00:03:11,650
This is stochastic gradient ascent.

54
00:03:12,030 --> 00:03:13,520
So if you want to fit

55
00:03:13,770 --> 00:03:15,590
a logistic regression model,

56
00:03:16,790 --> 00:03:19,090
meaning find the value of theta that

57
00:03:19,370 --> 00:03:21,240
maximizes this log likelihood,

58
00:03:21,490 --> 00:03:23,630
gradient ascent or stochastic gradient ascent

59
00:03:23,870 --> 00:03:25,070
or batch gradient ascent

60
00:03:25,250 --> 00:03:27,280
is a perfectly fine algorithm to use.

61
00:03:27,540 --> 00:03:30,100
But what I want to do is talk about a different

62
00:03:30,330 --> 00:03:33,090
algorithm for fitting models

63
00:03:33,330 --> 00:03:35,580
like logistic regression. And this would be

64
00:03:35,810 --> 00:03:38,580
an algorithm that will, I guess,

65
00:03:39,550 --> 00:03:40,810
often run much faster

66
00:03:40,810 --> 00:03:42,360
than gradient ascent.

67
00:03:42,570 --> 00:03:46,400
And this algorithm is called Newton's Method.

68
00:03:47,560 --> 00:03:49,880
And when we describe Newton's Method

69
00:03:50,080 --> 00:03:53,130
let me ask you I'm actually going to ask you

70
00:03:56,120 --> 00:03:59,470
to consider a different problem first,

71
00:04:01,770 --> 00:04:07,420
which is let's say you have a function F of theta,

72
00:04:08,860 --> 00:04:12,060
and let's say you want to find the

73
00:04:12,310 --> 00:04:21,570
value of theta so that F of theta is equal to zero.

74
00:04:22,500 --> 00:04:24,880
Let's start the [inaudible], and then we'll

75
00:04:25,110 --> 00:04:27,640
sort of slowly change this until it becomes

76
00:04:27,890 --> 00:04:29,520
an algorithm for fitting

77
00:04:29,740 --> 00:04:32,620
maximum likelihood models, like logistic regression.

78
00:04:32,890 --> 00:04:43,350
So let's see. I guess that works. Okay,

79
00:04:51,650 --> 00:04:55,030
so let's say that's my function F.

80
00:04:55,530 --> 00:04:58,840
This is my horizontal axis of theta,

81
00:04:59,080 --> 00:05:01,710
and so we're really trying to find this value for theta,

82
00:05:01,910 --> 00:05:05,340
and which F of theta is equal to zero.

83
00:05:05,930 --> 00:05:07,760
This is a horizontal axis.

84
00:05:07,960 --> 00:05:10,060
So here's the algorithm we use.

85
00:05:10,460 --> 00:05:17,160
I'm going to initialize theta as some value.

86
00:05:18,750 --> 00:05:21,870
We'll call theta superscript zero.

87
00:05:22,260 --> 00:05:24,430
And then here's what Newton's Method does.

88
00:05:24,660 --> 00:05:27,150
We're going to evaluate the function F

89
00:05:27,410 --> 00:05:31,750
at a value of theta, and then we'll compute

90
00:05:31,940 --> 00:05:33,720
the derivitive of that, and we'll use

91
00:05:33,920 --> 00:05:35,420
the linear approximation

92
00:05:35,560 --> 00:05:37,780
to the function F of that value of theta.

93
00:05:38,060 --> 00:05:46,800
So in particular, I'm going to take the tangents

94
00:05:47,080 --> 00:05:53,340
to my function hope that makes sense

95
00:05:53,540 --> 00:05:56,620
starting the function [inaudible] work out nicely.

96
00:05:56,870 --> 00:05:59,120
I'm going to take the tangent to my function

97
00:05:59,340 --> 00:06:01,760
at that point theta zero, and I'm going to

98
00:06:01,990 --> 00:06:03,860
sort of extend this tangent down until

99
00:06:04,120 --> 00:06:06,430
it intersects the horizontal axis.

100
00:06:06,660 --> 00:06:08,630
I want to see what value this is.

101
00:06:08,840 --> 00:06:11,090
And I'm going to call this theta one, okay.

102
00:06:11,300 --> 00:06:14,360
And then so that's one iteration

103
00:06:14,600 --> 00:06:16,120
of Newton's Method.

104
00:06:16,290 --> 00:06:17,970
And what I'll do then is the same thing

105
00:06:18,140 --> 00:06:22,900
with this point. Take the tangent down here,

106
00:06:23,080 --> 00:06:25,830
and that's two iterations of the algorithm.

107
00:06:26,030 --> 00:06:28,120
And then just sort of keep going,

108
00:06:28,300 --> 00:06:30,900
that's theta three and so on, okay.

109
00:06:31,480 --> 00:06:34,370
So let's just go ahead and write down

110
00:06:34,640 --> 00:06:38,190
what this algorithm actually does.

111
00:06:38,450 --> 00:06:41,180
To go from theta zero to theta one,

112
00:06:41,390 --> 00:06:46,420
let me call that length

113
00:06:46,620 --> 00:06:48,770
let me just call that capital delta.

114
00:06:49,020 --> 00:06:52,620
So capital so if you remember the definition

115
00:06:53,180 --> 00:06:55,240
of a derivative [inaudible],

116
00:06:55,470 --> 00:06:58,350
derivative of F evaluated at theta zero.

117
00:06:59,050 --> 00:07:01,980
In other words, the gradient of this first line,

118
00:07:02,240 --> 00:07:04,520
by the definition of gradient is going to be equal

119
00:07:04,790 --> 00:07:06,560
to this vertical length,

120
00:07:06,770 --> 00:07:08,640
divided by this horizontal length.

121
00:07:08,880 --> 00:07:11,730
A gradient of this so the slope of this function

122
00:07:11,920 --> 00:07:13,600
is defined as the ratio between this vertical

123
00:07:13,830 --> 00:07:16,190
height and this width of triangle.

124
00:07:16,930 --> 00:07:20,640
So that's just equal to F of theta zero,

125
00:07:23,030 --> 00:07:25,090
divided by delta,

126
00:07:25,390 --> 00:07:31,300
which implies that delta is equal to F of theta zero,

127
00:07:32,440 --> 00:07:38,990
divided by a prime of theta zero, okay.

128
00:07:44,590 --> 00:07:48,580
And so theta one is therefore theta zero

129
00:07:48,800 --> 00:07:51,980
minus delta, minus capital delta,

130
00:07:52,230 --> 00:07:55,240
which is therefore just F theta zero

131
00:07:56,020 --> 00:07:59,800
over F prime of theta zero, all right.

132
00:08:00,540 --> 00:08:02,220
And more generally,

133
00:08:02,440 --> 00:08:05,250
one iteration of Newton's Method precedes this,

134
00:08:05,450 --> 00:08:10,860
theta T plus one equals theta T minus F of theta T

135
00:08:11,060 --> 00:08:14,380
divided by F prime of theta T.

136
00:08:15,360 --> 00:08:18,080
So that's one iteration of Newton's Method.

137
00:08:24,580 --> 00:08:26,950
Now, this is an algorithm for finding

138
00:08:27,140 --> 00:08:29,890
a value of theta for which F of theta equals zero.

139
00:08:30,090 --> 00:08:32,010
And so we apply the same idea

140
00:08:32,180 --> 00:08:35,210
to maximizing the log likelihood, right.

141
00:08:35,430 --> 00:08:37,800
So we have a function L of theta,

142
00:08:38,500 --> 00:08:40,500
and we ant to maximize this function.

143
00:08:40,700 --> 00:08:42,320
Well, how do you maximize the function?

144
00:08:42,490 --> 00:08:44,400
You set the derivative to zero.

145
00:08:44,600 --> 00:08:47,160
So we want theta such that.

146
00:08:48,060 --> 00:08:51,710
L prime of theta is equal to zero,

147
00:08:52,060 --> 00:08:53,690
so to maximize this function we

148
00:08:53,860 --> 00:08:55,520
want to find the place where the derivative

149
00:08:55,750 --> 00:08:57,840
of the function is equal to zero,

150
00:08:58,030 --> 00:09:00,410
and so we just apply the same idea.

151
00:09:02,020 --> 00:09:06,280
So we get theta one equals theta T minus L

152
00:09:06,490 --> 00:09:18,880
prime of theta T over L double prime of T,

153
00:09:20,080 --> 00:09:25,950
L double prime of theta T, okay.

154
00:09:26,120 --> 00:09:28,200
Because to maximize this function,

155
00:09:28,470 --> 00:09:30,550
we just let F be equal to L prime.

156
00:09:30,780 --> 00:09:32,830
Let F be the first derivitive  of L, and then we want to

157
00:09:33,090 --> 00:09:35,030
find the value of theta for which

158
00:09:35,190 --> 00:09:37,640
the derivative of L is zero,

159
00:09:37,840 --> 00:09:39,970
and therefore must be a local optimum.

160
00:09:48,530 --> 00:09:51,660
Does this make sense? Any questions about this?

161
00:09:51,910 --> 00:09:54,810
Student:[Inaudible]

162
00:09:58,980 --> 00:10:01,020
Instructor (Andrew Ng):The answer to that

163
00:10:01,230 --> 00:10:02,640
is fairly complicated.

164
00:10:02,910 --> 00:10:04,960
There are conditions on F that

165
00:10:05,170 --> 00:10:06,920
would guarantee that this will work.

166
00:10:07,120 --> 00:10:08,850
They are fairly complicated,

167
00:10:09,070 --> 00:10:10,910
and this is more complex than

168
00:10:11,130 --> 00:10:12,630
I want to go into now. In practice,

169
00:10:12,840 --> 00:10:14,780
this works very well for logistic regression,

170
00:10:15,020 --> 00:10:17,000
and for sort of

171
00:10:17,230 --> 00:10:19,180
generalizing linear models I'll talk about later.

172
00:10:19,460 --> 00:10:22,700
Student:[Inaudible]

173
00:10:23,450 --> 00:10:24,940
Instructor (Andrew Ng):Yeah, it usually

174
00:10:25,180 --> 00:10:26,620
doesn't matter. When I implement this,

175
00:10:26,820 --> 00:10:29,100
I usually just initialize theta zero to zero

176
00:10:29,290 --> 00:10:31,160
to just initialize the parameters to the

177
00:10:31,440 --> 00:10:34,040
vector of all zeros, and usually this works fine.

178
00:10:34,230 --> 00:10:36,010
It's usually not a huge deal

179
00:10:36,180 --> 00:10:38,740
how you initialize theta.

180
00:10:39,980 --> 00:10:47,180
Student:[Inaudible]

181
00:10:48,730 --> 00:10:51,290
or is it just different conversions?

182
00:10:51,600 --> 00:10:55,690
Instructor (Andrew Ng):Let me say some

183
00:10:55,910 --> 00:10:58,720
things about that that'll sort of answer it.

184
00:10:58,880 --> 00:11:00,980
All of these algorithms tend not to

185
00:11:01,170 --> 00:11:03,230
converges problems, and all of these algorithms

186
00:11:03,390 --> 00:11:05,850
will generally converge, unless you choose too

187
00:11:06,020 --> 00:11:07,600
large a linear rate for

188
00:11:07,600 --> 00:11:09,180
gradient ascent or something.

189
00:11:09,370 --> 00:11:11,440
But the speeds of conversions

190
00:11:11,610 --> 00:11:14,180
of these algorithms are very different.

191
00:11:15,110 --> 00:11:20,170
So it turns out that Newton's Method

192
00:11:20,350 --> 00:11:22,990
is an algorithm that enjoys extremely fast

193
00:11:23,160 --> 00:11:26,440
conversions. The technical term is that it enjoys

194
00:11:26,640 --> 00:11:28,840
a property called quadratic conversions.

195
00:11:29,040 --> 00:11:31,680
Don't know about what that means,

196
00:11:31,850 --> 00:11:34,140
but just stated informally,

197
00:11:34,340 --> 00:11:36,850
it means that [inaudible] every iteration

198
00:11:37,090 --> 00:11:39,070
of Newton's Method will double the number

199
00:11:39,210 --> 00:11:41,580
of significant digits that your solution

200
00:11:41,850 --> 00:11:46,300
is accurate to. Just lots of constant factors.

201
00:11:46,480 --> 00:11:52,270
Suppose that on a certain iteration your solution

202
00:11:52,720 --> 00:11:54,470
is within 0.01 at the optimum,

203
00:11:55,090 --> 00:11:58,910
so you have 0.01 error. Then after one iteration,

204
00:11:59,100 --> 00:12:01,910
your error will be on the order of 0.001,

205
00:12:04,480 --> 00:12:06,610
and after another iteration, your error

206
00:12:06,810 --> 00:12:12,890
will be on the order of 0.0000001.

207
00:12:13,990 --> 00:12:16,550
So this is called quadratic conversions

208
00:12:16,760 --> 00:12:18,420
because you essentially get to square

209
00:12:18,790 --> 00:12:21,270
the error on every iteration of Newton's Method.

210
00:12:21,410 --> 00:12:24,290
[Inaudible] result that holds only

211
00:12:24,480 --> 00:12:26,780
when your [inaudible] cause the optimum anyway,

212
00:12:26,930 --> 00:12:30,010
so this is the theoretical result that says it's true,

213
00:12:30,200 --> 00:12:32,210
but because of constant factors and so on,

214
00:12:32,420 --> 00:12:34,370
may paint a slightly rosier picture

215
00:12:34,610 --> 00:12:36,490
than might be accurate.

216
00:12:36,660 --> 00:12:38,870
But the fact is, when you implement

217
00:12:39,110 --> 00:12:41,260
when I implement Newton's Method

218
00:12:41,450 --> 00:12:43,780
for logistic regression, usually converges

219
00:12:43,970 --> 00:12:45,890
like a dozen iterations or so

220
00:12:46,040 --> 00:12:47,540
for most reasonable size problems

221
00:12:47,720 --> 00:12:49,800
of tens of hundreds of features.

222
00:12:50,050 --> 00:12:52,560
So one thing I should talk about,

223
00:12:52,810 --> 00:12:55,860
which is what I wrote down over

224
00:12:56,000 --> 00:12:58,560
there was actually Newton's Method for the

225
00:12:58,760 --> 00:13:01,200
case of theta being a single-row number.

226
00:13:01,410 --> 00:13:04,450
The generalization to Newton's Method for

227
00:13:04,610 --> 00:13:06,580
when theta is a vector rather than

228
00:13:06,790 --> 00:13:08,270
when theta is just a row number

229
00:13:08,270 --> 00:13:09,610
is the following,

230
00:13:10,990 --> 00:13:15,060
which is that theta T plus one is theta T plus

231
00:13:15,340 --> 00:13:18,170
and then we have the second derivative divided

232
00:13:18,360 --> 00:13:20,670
by the first the first derivative divided

233
00:13:20,890 --> 00:13:23,490
by the second derivative.

234
00:13:25,550 --> 00:13:28,410
And the appropriate generalization is this,

235
00:13:28,610 --> 00:13:32,190
where this is the usual gradient of your

236
00:13:33,810 --> 00:13:39,580
objective, and H here is a matrix

237
00:13:39,890 --> 00:13:48,940
called a Hessian, which is just a matrix of

238
00:13:51,680 --> 00:14:00,550
second derivative where HIJ equals okay.

239
00:14:06,500 --> 00:14:12,190
So just to sort of the first derivative divided

240
00:14:12,370 --> 00:14:14,610
by the second derivative, now you have a

241
00:14:14,780 --> 00:14:19,300
vector of first derivatives times sort of

242
00:14:19,480 --> 00:14:22,150
the inverse of the matrix of second derivatives.

243
00:14:22,360 --> 00:14:24,530
So this is sort of just the same thing

244
00:14:24,770 --> 00:14:27,250
[inaudible] of multiple dimensions.

245
00:14:27,510 --> 00:14:31,080
So for logistic regression, again, use the

246
00:14:31,360 --> 00:14:34,770
for a reasonable number of features and

247
00:14:34,950 --> 00:14:37,810
training examples when I run this algorithm,

248
00:14:38,080 --> 00:14:41,100
usually you see a conversion anywhere

249
00:14:41,310 --> 00:14:43,580
from sort of [inaudible] to like

250
00:14:43,740 --> 00:14:45,550
a dozen or so other [inaudible].

251
00:14:45,750 --> 00:14:48,000
To compare to gradient ascent,

252
00:14:48,170 --> 00:14:49,620
it's [inaudible] to gradient ascent,

253
00:14:49,790 --> 00:14:51,710
this usually means far fewer iterations

254
00:14:51,930 --> 00:14:54,310
to converge. Compared to gradient ascent,

255
00:14:54,480 --> 00:14:56,120
let's say batch gradient ascent,

256
00:14:56,330 --> 00:14:58,410
the disadvantage of Newton's Method is that

257
00:14:58,650 --> 00:14:59,910
on every iteration you

258
00:14:59,910 --> 00:15:01,860
need to invert the Hessian.

259
00:15:02,140 --> 00:15:04,660
So the Hessian will be an N-by-N matrix,

260
00:15:04,880 --> 00:15:06,930
or an N plus one by N plus one-dimensional

261
00:15:07,100 --> 00:15:09,450
matrix if N is the number of features.

262
00:15:09,650 --> 00:15:11,670
And so if you have a large number of features in

263
00:15:11,890 --> 00:15:14,000
your learning problem, if you have tens of

264
00:15:14,180 --> 00:15:16,600
thousands of features, then inverting H could be

265
00:15:16,740 --> 00:15:18,690
a slightly computationally expensive step.

266
00:15:19,010 --> 00:15:21,180
But for smaller, more reasonable numbers of

267
00:15:21,390 --> 00:15:23,140
features, this is usually a very fast algorithm.

268
00:15:23,350 --> 00:15:24,960
Question?

269
00:15:25,220 --> 00:15:27,560
Student:[Inaudible]

270
00:15:30,860 --> 00:15:33,400
Instructor (Andrew Ng):Let's see.

271
00:15:33,780 --> 00:15:35,680
I think you're right.

272
00:15:35,930 --> 00:15:38,730
That should probably be a minus.

273
00:15:39,190 --> 00:15:44,890
Do you have [inaudible]? Yeah, thanks.

274
00:15:45,220 --> 00:15:47,590
Yeah, X to a minus.

275
00:15:47,900 --> 00:15:49,910
Thank you. [Inaudible] problem also.

276
00:15:50,140 --> 00:15:52,640
I wrote down this algorithm to find the maximum

277
00:15:52,940 --> 00:15:54,860
likely estimate of the parameters

278
00:15:55,080 --> 00:15:56,710
for logistic regression.

279
00:15:56,910 --> 00:15:58,930
I wrote this down for maximizing a function.

280
00:15:59,180 --> 00:16:01,440
So I'll leave you to think about this yourself.

281
00:16:01,660 --> 00:16:03,880
If I wanted to use Newton's Method to minimize

282
00:16:04,070 --> 00:16:06,230
the function, how does the algorithm change?

283
00:16:06,470 --> 00:16:08,900
All right. So I'll leave you to think about that.

284
00:16:09,220 --> 00:16:11,750
So in other words, it's not the maximizations.

285
00:16:11,940 --> 00:16:13,460
How does the algorithm change

286
00:16:14,140 --> 00:16:16,310
if you want to use it for minimization?

287
00:16:28,500 --> 00:16:34,510
Actually, the answer is that it doesn't change.

288
00:16:34,960 --> 00:16:37,930
I'll leave you to work that out yourself why, okay.

289
00:16:39,610 --> 00:16:42,380
All right. Let's talk about generalized linear models.

290
00:16:42,640 --> 00:16:49,930
Let me just say, just to give a recap

291
00:16:50,140 --> 00:16:53,680
of both of the algorithms we've talked about

292
00:16:53,900 --> 00:16:56,380
so far. We've talked about two different

293
00:16:56,570 --> 00:16:58,480
algorithms for modeling PFY

294
00:16:58,680 --> 00:17:00,930
given X and parameterized by theta.

295
00:17:01,130 --> 00:17:04,970
And one of them R was a real number

296
00:17:05,240 --> 00:17:08,050
and we assume that. And we sort of the

297
00:17:08,360 --> 00:17:10,300
[inaudible] has a Gaussian distribution,

298
00:17:10,560 --> 00:17:12,080
then we got, you know,

299
00:17:12,080 --> 00:17:14,680
ordinary least squares of linear regression.

300
00:17:14,950 --> 00:17:19,120
In the other case, we saw that if

301
00:17:19,340 --> 00:17:22,730
was a classification problem where Y took

302
00:17:22,950 --> 00:17:27,920
on a value of either zero or one. In that case,

303
00:17:28,190 --> 00:17:30,680
well, what's the most natural distribution

304
00:17:31,110 --> 00:17:33,030
of zeros and ones is the Bernoulli.

305
00:17:33,360 --> 00:17:35,610
The Bernoulli distribution models random

306
00:17:35,790 --> 00:17:37,900
variables with two values,

307
00:17:38,120 --> 00:17:41,640
and in that case we got logistic regression.

308
00:17:45,140 --> 00:17:48,070
So along the way, some of the questions that

309
00:17:48,300 --> 00:17:50,970
came up were for logistic regression,

310
00:17:51,640 --> 00:17:53,730
where on earth did I get the

311
00:17:53,960 --> 00:17:56,280
sigmoid function from? And then so there are the choices

312
00:17:56,510 --> 00:17:59,030
you can use for, sort of, just where

313
00:17:59,290 --> 00:18:01,010
did this function come from?

314
00:18:01,240 --> 00:18:03,280
And there are other functions I could've

315
00:18:03,500 --> 00:18:05,400
plugged in, but the sigmoid function turns out

316
00:18:05,650 --> 00:18:07,980
to be a natural default choice that lead us

317
00:18:08,220 --> 00:18:10,570
to logistic regression. And what I want to do

318
00:18:10,790 --> 00:18:13,340
now is take both of these algorithms and

319
00:18:13,610 --> 00:18:17,740
show that they are special cases of

320
00:18:17,980 --> 00:18:20,540
a broader class of algorithms

321
00:18:20,800 --> 00:18:23,750
called generalized linear models, and there will be

322
00:18:24,020 --> 00:18:27,850
pauses for it will be as a broader class

323
00:18:28,010 --> 00:18:29,980
of algorithms that think that the sigmoid

324
00:18:30,200 --> 00:18:32,410
function will fall out very naturally as well.

325
00:18:32,640 --> 00:18:34,920
So, let's see just looking for a longer piece

326
00:18:35,230 --> 00:18:39,250
of chalk. I should warn you, the ideas in

327
00:18:39,460 --> 00:18:42,030
generalized linear models are somewhat complex,

328
00:18:42,290 --> 00:18:44,910
so what I'm going to do today is try to

329
00:18:45,070 --> 00:18:47,190
sort of point you point out the key ideas and

330
00:18:47,400 --> 00:18:49,250
give you a gist of the entire story.

331
00:18:49,440 --> 00:18:51,360
And then some of the details in the map

332
00:18:51,580 --> 00:18:53,290
and the derivations I'll leave you to work through

333
00:18:53,480 --> 00:18:55,880
by yourselves in the intellection [inaudible],

334
00:18:56,110 --> 00:18:58,440
which posts online.

335
00:19:04,330 --> 00:19:07,680
So [inaudible] these two distributions,

336
00:19:08,400 --> 00:19:11,240
the Bernoulli and the Gaussian.

337
00:19:11,480 --> 00:19:16,440
So suppose we have data that is zero-one valued,

338
00:19:16,970 --> 00:19:18,820
and we and we want to model it

339
00:19:18,820 --> 00:19:21,070
with a Bernoulli random variable

340
00:19:21,900 --> 00:19:24,340
variable parameterized by phi.

341
00:19:24,600 --> 00:19:28,380
So the Bernoulli distribution has

342
00:19:28,680 --> 00:19:31,030
the probability of Y equals one,

343
00:19:31,440 --> 00:19:33,270
which just equals the phi, right.

344
00:19:33,520 --> 00:19:35,940
So the parameter phi in the Bernoulli

345
00:19:36,150 --> 00:19:38,390
specifies the probability of Y being one.

346
00:19:38,700 --> 00:19:41,980
Now, as you vary the parameter theta,

347
00:19:42,230 --> 00:19:45,190
you get you sort of get different Bernoulli distributions

348
00:19:45,410 --> 00:19:48,400
distributions. As you vary the value of theta

349
00:19:48,730 --> 00:19:51,460
you get different probability distributions on

350
00:19:51,640 --> 00:19:53,380
Y that have different probabilities of being

351
00:19:53,570 --> 00:19:56,070
equal to one. And so I want you to think of

352
00:19:56,310 --> 00:19:58,650
this as not one fixed distribution,

353
00:19:58,870 --> 00:20:01,020
but as a set where there are a class

354
00:20:01,260 --> 00:20:03,870
of distributions that you get as you vary theta.

355
00:20:04,110 --> 00:20:05,950
And in the same way, if you consider

356
00:20:09,110 --> 00:20:11,570
Gaussian distribution, as you vary [inaudible] you

357
00:20:11,780 --> 00:20:15,530
would get different Gaussian distributions.

358
00:20:15,810 --> 00:20:17,810
So think of this again as a class

359
00:20:18,030 --> 00:20:20,500
or as a set to distributions.

360
00:20:20,680 --> 00:20:23,330
And what I want to do now is show that

361
00:20:26,410 --> 00:20:30,690
both of these are special cases of the class of

362
00:20:30,890 --> 00:20:33,270
distribution that's called the exponential

363
00:20:33,490 --> 00:20:36,060
family distribution. And in particular,

364
00:20:36,270 --> 00:20:38,410
we'll say that the class of distributions,

365
00:20:38,600 --> 00:20:40,480
like the Bernoulli distributions that you get

366
00:20:40,690 --> 00:20:42,690
as you vary theta, we'll say the class

367
00:20:42,890 --> 00:20:45,160
of distributions is in the exponential family

368
00:20:45,390 --> 00:20:48,270
if it can be written in the following form.

369
00:20:49,620 --> 00:20:51,370
P of Y parameterized by theta is

370
00:20:51,540 --> 00:21:00,800
equal to B of Y [inaudible], okay.

371
00:21:04,520 --> 00:21:08,510
Let me just get some of these terms, names,

372
00:21:08,760 --> 00:21:14,540
and then let me I'll say a bit more about

373
00:21:14,720 --> 00:21:21,190
what this means. So [inaudible] is called

374
00:21:21,370 --> 00:21:24,570
the natural parameter of the distribution,

375
00:21:24,770 --> 00:21:33,840
and T of Y is called the sufficient statistic.

376
00:21:35,050 --> 00:21:39,760
Usually, for many of the examples we'll see,

377
00:21:39,940 --> 00:21:43,280
including the [inaudible] and the Gaussian,

378
00:21:43,870 --> 00:21:47,270
T of Y is just equal to Y. So for most of this

379
00:21:47,480 --> 00:21:50,050
lecture you can mentally replace T of Y

380
00:21:50,280 --> 00:21:52,760
to be equal to Y, although this won't be true for

381
00:21:53,020 --> 00:21:55,810
the very fine example we do today,

382
00:21:56,490 --> 00:21:59,630
but mentally, you think of T of Y as equal to Y.

383
00:22:07,080 --> 00:22:11,780
And so for a given choice of these functions,

384
00:22:12,000 --> 00:22:18,070
A, B and T, all right so we're gonna sort

385
00:22:18,290 --> 00:22:21,430
of fix the forms of the functions A, B and T.

386
00:22:21,640 --> 00:22:25,750
Then this formula defines, again, a set of

387
00:22:25,950 --> 00:22:30,550
distributions. It defines the class of distributions

388
00:22:30,790 --> 00:22:33,130
that is now parameterized by [inaudible].

389
00:22:33,330 --> 00:22:36,260
So again, let's write down specific formulas for A,

390
00:22:36,450 --> 00:22:38,500
B and T, true specific choices of A, B

391
00:22:38,700 --> 00:22:40,690
and T. Then as I vary [inaudible]

392
00:22:40,870 --> 00:22:43,180
I get different distributions.

393
00:22:43,420 --> 00:22:46,150
And I'm going to show that

394
00:22:46,420 --> 00:22:50,970
the [inaudible] I'm going to show that

395
00:22:51,150 --> 00:22:53,450
the Bernoulli and the Gaussians are special

396
00:22:53,640 --> 00:22:56,070
cases of exponential family distributions.

397
00:22:56,270 --> 00:22:58,980
And by that I mean that I can choose specific

398
00:22:59,160 --> 00:23:01,700
functions, A, B and T, so that this becomes

399
00:23:01,850 --> 00:23:04,120
the formula of the distributions of

400
00:23:04,300 --> 00:23:06,930
either a Bernoulli or a Gaussian.

401
00:23:07,140 --> 00:23:09,340
And then again, as I vary [inaudible],

402
00:23:09,510 --> 00:23:11,750
I'll get [inaudible], distributions with different

403
00:23:11,990 --> 00:23:14,540
means, or as I vary [inaudible],

404
00:23:14,800 --> 00:23:17,290
I'll get Gaussian distributions with different means

405
00:23:17,460 --> 00:23:20,020
for my fixed values of A, B and T.

406
00:23:22,490 --> 00:23:25,220
And for those of you that know

407
00:23:25,370 --> 00:23:28,710
what a sufficient statistic and statistics is,

408
00:23:28,880 --> 00:23:31,310
T of Y actually is a sufficient statistic

409
00:23:31,490 --> 00:23:35,540
in the formal sense of sufficient statistic

410
00:23:35,730 --> 00:23:37,610
for a probability distribution.

411
00:23:37,830 --> 00:23:40,140
They may have seen it in a statistics class.

412
00:23:40,290 --> 00:23:42,240
If you don't know what a sufficient statistic is,

413
00:23:42,440 --> 00:23:44,170
don't worry about.

414
00:23:44,360 --> 00:23:46,820
We sort of don't need that property today.

415
00:23:49,650 --> 00:23:56,950
Okay. So oh, one last comment. Often,

416
00:23:57,210 --> 00:24:02,910
T of Y is equal to Y, and in many of these cases,

417
00:24:03,140 --> 00:24:05,550
[inaudible] is also just a real number.

418
00:24:05,840 --> 00:24:07,320
So in many cases,

419
00:24:07,530 --> 00:24:08,890
the parameter of this distribution

420
00:24:09,100 --> 00:24:11,340
is just a real number, and [inaudible] transposed

421
00:24:11,490 --> 00:24:13,280
T of Y is just a product of real numbers.

422
00:24:13,510 --> 00:24:15,130
So again, that would be true

423
00:24:15,310 --> 00:24:16,530
for our first two examples,

424
00:24:16,730 --> 00:24:18,920
but now for the last example I'll do today.

425
00:24:19,160 --> 00:24:23,310
So now we'll show that the [inaudible] and

426
00:24:23,500 --> 00:24:25,350
the Gaussian are examples

427
00:24:25,630 --> 00:24:27,170
of exponential family distributions.

428
00:24:27,420 --> 00:24:28,870
We'll start with the [inaudible].

429
00:24:29,090 --> 00:24:30,440
So the [inaudible] distribution with

430
00:24:30,660 --> 00:24:32,960
[inaudible] I guess I wrote this down already.

431
00:24:33,190 --> 00:24:35,670
PFY equals one [inaudible] by phi,

432
00:24:35,920 --> 00:24:37,410
[inaudible] equal to phi.

433
00:24:37,550 --> 00:24:39,530
So the parameter of phi specifies

434
00:24:39,800 --> 00:24:42,630
the probability that Y equals one.

435
00:24:42,860 --> 00:24:46,490
And so my goal now is to choose T, A and B,

436
00:24:46,690 --> 00:24:49,660
or is to choose A, B and T so that my formula

437
00:24:49,890 --> 00:24:51,980
for the exponential family becomes identical

438
00:24:52,220 --> 00:24:54,760
to my formula for the distribution of a [inaudible].

439
00:25:02,620 --> 00:25:10,400
So probability of Y parameterized by phi

440
00:25:10,750 --> 00:25:19,390
is equal to that, all right. And you already saw

441
00:25:19,660 --> 00:25:23,480
sort of a similar exponential notation

442
00:25:23,730 --> 00:25:25,880
where we talked about logistic regression.

443
00:25:26,090 --> 00:25:28,430
The probability of Y being one is phi,

444
00:25:28,650 --> 00:25:31,280
the probability of Y being zero is one minus phi,

445
00:25:31,480 --> 00:25:32,920
so we can write this compactly as phi

446
00:25:33,130 --> 00:25:35,410
to the Y times one minus phi to the one minus Y.

447
00:25:35,630 --> 00:25:42,100
So I'm gonna take the exponent of the log of this,

448
00:25:42,340 --> 00:25:44,290
an exponentiation in taking log

449
00:25:44,530 --> 00:25:46,890
[inaudible] cancel each other out [inaudible].

450
00:25:47,130 --> 00:25:54,740
And this is equal to E to the Y.

451
00:26:31,290 --> 00:26:35,630
And so [inaudible] is to be T of Y,

452
00:26:37,160 --> 00:26:44,410
and this will be minus A of [inaudible].

453
00:26:45,720 --> 00:26:47,630
And then B of Y is just one,

454
00:26:47,910 --> 00:26:51,260
so B of Y doesn't matter.

455
00:26:56,110 --> 00:26:59,570
Just take a second to look through this

456
00:26:59,790 --> 00:27:01,660
and make sure it makes sense.

457
00:27:01,850 --> 00:27:05,160
I'll clean another board while you do that.

458
00:27:43,120 --> 00:27:45,580
So now let's write down a few more things.

459
00:27:47,490 --> 00:27:49,210
Just copying from the previous board,

460
00:27:49,460 --> 00:27:53,150
we had that [inaudible] zero four equal

461
00:27:53,420 --> 00:27:56,970
to log phi over one minus phi.

462
00:27:57,190 --> 00:27:58,320
It turns out, so if I want

463
00:27:58,320 --> 00:27:59,340
to do the algebra, it turns out we

464
00:27:59,580 --> 00:28:02,070
take this formula, and if you invert it,

465
00:28:02,300 --> 00:28:05,190
if you solve for phi excuse me, if you solve

466
00:28:05,490 --> 00:28:08,060
for theta as a function of phi, which is really

467
00:28:08,240 --> 00:28:10,280
[inaudible] is the function of phi.

468
00:28:10,530 --> 00:28:13,610
Just invert this formula. You find that phi

469
00:28:13,870 --> 00:28:18,270
is one over one plus [inaudible] minus [inaudible].

470
00:28:18,480 --> 00:28:21,640
And so somehow the logistic function

471
00:28:21,910 --> 00:28:23,810
magically falls out of this.

472
00:28:24,110 --> 00:28:26,200
We'll take this even this even further later.

473
00:28:26,440 --> 00:28:29,560
Again, copying definitions from the board on

474
00:28:29,830 --> 00:28:32,690
from the previous board, A of [inaudible]

475
00:28:32,890 --> 00:28:39,980
I said is minus log of one minus phi.

476
00:28:40,320 --> 00:28:43,300
So again, phi and [inaudible] are function of each

477
00:28:43,550 --> 00:28:45,420
other, all right. So [inaudible] depends on phi,

478
00:28:45,650 --> 00:28:47,430
and phi depends on [inaudible].

479
00:28:47,670 --> 00:28:51,140
So if I plug in this definition for [inaudible]

480
00:28:51,380 --> 00:28:53,330
into this excuse me, plug in this definition

481
00:28:53,580 --> 00:28:55,840
for phi into that, I'll find that A of [inaudible]

482
00:28:56,070 --> 00:28:58,270
is therefore equal to log one plus

483
00:28:58,560 --> 00:29:00,550
[inaudible] to [inaudible]. And again,

484
00:29:00,750 --> 00:29:02,270
this is just algebra.

485
00:29:02,270 --> 00:29:02,910
This is not terribly interesting. .

486
00:29:03,170 --> 00:29:05,420
And just to complete excuse me.

487
00:29:08,240 --> 00:29:10,800
And just to complete the rest of this,

488
00:29:11,120 --> 00:29:13,210
T of Y is equal to Y,

489
00:29:13,410 --> 00:29:17,410
and B of Y is equal to one, okay.

490
00:29:18,610 --> 00:29:21,160
So just to recap what we've done, we've come up

491
00:29:21,400 --> 00:29:24,210
with a certain choice of functions A,

492
00:29:24,480 --> 00:29:27,440
T and B, so then my formula

493
00:29:27,670 --> 00:29:29,740
for the exponential family distribution

494
00:29:30,050 --> 00:29:31,950
now becomes exactly the formula

495
00:29:32,220 --> 00:29:34,620
for the distributions, or for the probability

496
00:29:34,860 --> 00:29:37,300
mass function of the [inaudible] distribution.

497
00:29:37,510 --> 00:29:39,350
And the natural parameter [inaudible]

498
00:29:39,580 --> 00:29:41,810
has a certain relationship of the original parameter

499
00:29:42,030 --> 00:29:45,050
of the [inaudible]. Question?

500
00:29:45,340 --> 00:29:47,940
Student:[Inaudible]

501
00:29:48,610 --> 00:29:50,280
Instructor (Andrew Ng):Let's see. [Inaudible].

502
00:29:50,490 --> 00:29:53,110
Student:The second to the last one.

503
00:29:54,540 --> 00:29:58,130
Instructor (Andrew Ng):Oh, this answer is fine.

504
00:29:58,410 --> 00:30:01,020
Student:Okay.

505
00:30:01,510 --> 00:30:03,900
Instructor (Andrew Ng):Let's see. Yeah,

506
00:30:04,130 --> 00:30:06,240
so this is well, if you expand this term out,

507
00:30:06,500 --> 00:30:08,240
one minus Y times log Y minus phi, and

508
00:30:08,420 --> 00:30:11,980
so one times log one minus phi becomes this.

509
00:30:12,650 --> 00:30:15,280
And the other term is minus Y times log Y

510
00:30:15,580 --> 00:30:19,810
minus phi. And then so the minus of a log is

511
00:30:20,110 --> 00:30:22,510
log one over X, or is just log one over whatever.

512
00:30:22,730 --> 00:30:26,460
So minus Y times log one minus phi

513
00:30:26,720 --> 00:30:29,900
becomes sort of Y times log,

514
00:30:30,150 --> 00:30:32,990
one over one minus phi. Does that make sense?

515
00:30:33,310 --> 00:30:34,980
Student:Yeah.

516
00:30:35,290 --> 00:30:38,200
Instructor (Andrew Ng):Yeah, cool.

517
00:30:38,690 --> 00:30:40,450
Anything else? Yes?

518
00:30:41,030 --> 00:30:45,110
Student:[Inaudible] is a scalar, isn't it? Up there

519
00:30:45,300 --> 00:30:46,980
Instructor (Andrew Ng):Yes.

520
00:30:46,980 --> 00:30:47,980
Student:– it's a [inaudible] transposed,

521
00:30:49,680 --> 00:30:50,950
so it can be a vector or

522
00:30:51,210 --> 00:30:52,380
Instructor (Andrew Ng):Yes, [inaudible].

523
00:30:52,600 --> 00:30:54,170
So let's see. In most in this and the next

524
00:30:54,380 --> 00:30:56,330
example, [inaudible] will turn out

525
00:30:56,440 --> 00:30:58,750
to be a scalar. And so well, on this board.

526
00:30:58,950 --> 00:31:01,390
And so if [inaudible] is a scalar and T of Y

527
00:31:01,560 --> 00:31:04,840
is a scalar, then this is just a real number times

528
00:31:05,110 --> 00:31:07,140
a real number. So this would be like a

529
00:31:07,320 --> 00:31:09,250
one-dimensional vector transposed

530
00:31:09,460 --> 00:31:11,380
times a one-dimensional vector.

531
00:31:11,570 --> 00:31:14,120
And so this is just real number times real number.

532
00:31:14,300 --> 00:31:16,330
Towards the end of today's lecture,

533
00:31:16,530 --> 00:31:18,240
we'll go with just one example where

534
00:31:18,420 --> 00:31:19,700
both of these are vectors.

535
00:31:19,910 --> 00:31:21,460
But for many distributions,

536
00:31:21,630 --> 00:31:23,490
these will turn out to be scalars.

537
00:31:23,640 --> 00:31:27,340
Student:[Inaudible] distribution [inaudible].

538
00:31:27,560 --> 00:31:29,580
I mean, it doesn't have the zero probability

539
00:31:29,900 --> 00:31:33,820
or [inaudible] zero and one.

540
00:31:40,740 --> 00:31:44,340
Instructor (Andrew Ng):I see. So yeah.

541
00:31:44,610 --> 00:31:47,700
Let's for this, let's imagine that we're

542
00:31:47,980 --> 00:31:53,390
restricting the domain of the input of the

543
00:31:53,560 --> 00:31:55,650
function to be Y equals zero or one.

544
00:31:57,050 --> 00:31:58,930
So think of that as maybe

545
00:31:59,560 --> 00:32:01,660
in implicit constraint on it. [Inaudible].

546
00:32:01,920 --> 00:32:03,630
But so this is a probability mass

547
00:32:03,860 --> 00:32:06,200
function for Y equals zero or Y equals one.

548
00:32:06,430 --> 00:32:08,120
So write down Y equals zero one.

549
00:32:08,320 --> 00:32:11,090
Let's think of that as an implicit constraint.

550
00:32:12,330 --> 00:32:16,610
So cool. So this takes the Bernoulli

551
00:32:17,510 --> 00:32:22,630
distribution and writes in the form of

552
00:32:22,850 --> 00:32:25,490
exponential family distribution. Let me just

553
00:32:25,700 --> 00:32:28,110
do that very quickly for the Gaussian.

554
00:32:28,370 --> 00:32:30,740
I won't do the algebra for the Gaussian.

555
00:32:30,890 --> 00:32:33,290
I'll basically just write out the answers.

556
00:32:33,540 --> 00:32:36,600
So with a normal distribution

557
00:32:36,600 --> 00:32:37,600
with mean μ and variance

558
00:32:40,110 --> 00:32:43,070
sigma squared, and so you remember,

559
00:32:43,350 --> 00:32:47,910
was it two lectures ago, when we were

560
00:32:48,150 --> 00:32:51,060
deriving the maximum likelihood excuse me,

561
00:32:51,220 --> 00:32:53,680
oh, no, just the previous lecture when we were

562
00:32:53,910 --> 00:32:56,360
deriving the maximum likelihood estimate

563
00:32:56,570 --> 00:32:59,570
for the parameters of ordinary least squares.

564
00:32:59,810 --> 00:33:02,200
We showed that the parameter for

565
00:33:02,470 --> 00:33:04,480
sigma squared didn't matter.

566
00:33:04,690 --> 00:33:06,690
When we derive the probabilistic model for

567
00:33:06,850 --> 00:33:08,510
least square regression, we said that

568
00:33:08,660 --> 00:33:10,140
no matter what sigma square was,

569
00:33:10,310 --> 00:33:12,310
we end up with the same value of the parameters.

570
00:33:12,460 --> 00:33:15,320
So for the purposes of just writing lesson,

571
00:33:15,500 --> 00:33:18,030
today's lecture, and not taking account

572
00:33:18,210 --> 00:33:19,610
sigma squared, I'm just going

573
00:33:19,800 --> 00:33:23,070
to set sigma squared to be for the one,

574
00:33:23,250 --> 00:33:25,520
okay, so as to not worry about it.

575
00:33:25,640 --> 00:33:28,220
Lecture [inaudible] talks a little bit more

576
00:33:28,340 --> 00:33:31,230
about this, but I'm just gonna just to make

577
00:33:31,450 --> 00:33:34,040
[inaudible] in class a bit easier and simpler today,

578
00:33:34,220 --> 00:33:35,940
let's just say that sigma square

579
00:33:36,200 --> 00:33:38,070
equals one. Sigma square is essentially

580
00:33:38,310 --> 00:33:41,820
just a scaling factor on the variable Y.

581
00:33:44,010 --> 00:33:46,870
So in that case, the Gaussian density is given

582
00:33:47,080 --> 00:33:56,420
by this, [inaudible] squared. And well, by

583
00:33:59,510 --> 00:34:02,710
a couple of steps of algebra, which I'm not

584
00:34:02,980 --> 00:34:06,130
going to do, but is written out in [inaudible] in

585
00:34:06,360 --> 00:34:10,080
the lecture notes so you can download.

586
00:34:10,240 --> 00:34:12,800
This is one root two pie,

587
00:34:13,140 --> 00:34:17,560
E to the minus one-half Y squared times E to E.

588
00:34:17,730 --> 00:34:22,050
New Y minus one-half [inaudible] squared,

589
00:34:22,380 --> 00:34:24,870
okay. So I'm just not doing the algebra.

590
00:34:25,170 --> 00:34:34,400
And so that's B of Y, we have [inaudible]

591
00:34:34,620 --> 00:34:40,450
that's equal to [inaudible]. T(y)equals Y,

592
00:34:40,650 --> 00:34:46,910
and well, A of [inaudible] is equal

593
00:34:47,180 --> 00:34:54,200
to minus one-half actually, I think that

594
00:34:54,470 --> 00:34:59,840
should be plus one-half. Have I got that right?

595
00:35:00,130 --> 00:35:02,410
Yeah, sorry. Let's see excuse me.

596
00:35:02,620 --> 00:35:04,830
Plus sign there, okay. If you minus

597
00:35:05,070 --> 00:35:07,650
one-half [inaudible] squared, and because

598
00:35:07,910 --> 00:35:10,370
[inaudible] is equal to [inaudible], this is just

599
00:35:10,620 --> 00:35:13,450
minus one-half [inaudible] squared, okay.

600
00:35:13,800 --> 00:35:17,980
And so this would be a specific choice again of A,

601
00:35:18,200 --> 00:35:22,380
B and T that expresses the Gaussian density

602
00:35:22,570 --> 00:35:25,630
in the form of an exponential family distribution.

603
00:35:25,850 --> 00:35:28,580
And in this case, the relationship between

604
00:35:28,790 --> 00:35:31,240
[inaudible] and [inaudible] is that [inaudible]

605
00:35:31,560 --> 00:35:33,440
is just equal to [inaudible],

606
00:35:33,600 --> 00:35:35,300
so the [inaudible] of the Gaussian

607
00:35:35,490 --> 00:35:37,430
is just equal to the natural parameter of

608
00:35:37,660 --> 00:35:39,280
the exponential family distribution.

609
00:35:39,460 --> 00:35:40,980
Student:Minus half.

610
00:35:41,210 --> 00:35:43,200
Instructor (Andrew Ng):Oh, this is minus half?

611
00:35:43,390 --> 00:35:45,120
Student:[Inaudible]

612
00:35:45,340 --> 00:35:46,950
Instructor (Andrew Ng):Oh, okay, thanks.

613
00:35:47,290 --> 00:35:50,150
And so guessing that should be plus then.

614
00:35:50,420 --> 00:35:52,530
Is that right? Okay. Oh, yes, you're right.

615
00:35:52,740 --> 00:35:55,470
Thank you. All right.

616
00:36:02,700 --> 00:36:06,640
And so that turns out that if you've

617
00:36:06,780 --> 00:36:08,880
taken a look in undergraduate statistics class,

618
00:36:09,070 --> 00:36:11,970
turns out that most of the "textbook distributions,

619
00:36:12,160 --> 00:36:14,440
not all, but most of them, can be written in the

620
00:36:14,630 --> 00:36:17,050
form of an exponential family distribution.

621
00:36:17,230 --> 00:36:18,440
So you saw the Gaussian,

622
00:36:18,440 --> 00:36:19,470
the normal distribution.

623
00:36:19,660 --> 00:36:21,430
It turns out the multivariate normal

624
00:36:21,650 --> 00:36:23,740
distribution, which is a generalization

625
00:36:23,960 --> 00:36:25,890
of Gaussian random variables,

626
00:36:26,090 --> 00:36:28,090
so it's a high dimension to vectors.

627
00:36:28,230 --> 00:36:30,240
The [inaudible] normal distribution

628
00:36:30,380 --> 00:36:32,420
is also in the exponential family.

629
00:36:32,470 --> 00:36:34,710
You saw the Bernoulli as an exponential family.

630
00:36:34,880 --> 00:36:37,190
It turns out the multinomial distribution is too,

631
00:36:37,410 --> 00:36:39,590
all right. So the Bernoulli models outcomes

632
00:36:39,730 --> 00:36:40,900
over zero and one.

633
00:36:41,130 --> 00:36:43,360
They'll be coin tosses with two outcomes.

634
00:36:43,470 --> 00:36:44,840
The multinomial models outcomes

635
00:36:44,840 --> 00:36:46,330
over K possible values.

636
00:36:46,540 --> 00:36:48,920
That's also an exponential families distribution.

637
00:36:49,090 --> 00:36:51,720
You may have heard of the Poisson distribution.

638
00:36:51,940 --> 00:36:53,610
And so the Poisson distribution

639
00:36:53,660 --> 00:36:55,750
is often used for modeling counts.

640
00:36:55,940 --> 00:36:58,370
Things like the number of radioactive decays

641
00:36:58,560 --> 00:37:00,280
in a sample,

642
00:37:00,510 --> 00:37:02,630
or the number of customers to your website,

643
00:37:02,860 --> 00:37:04,890
the numbers of visitors arriving in a store.

644
00:37:05,040 --> 00:37:06,630
The Poisson distribution

645
00:37:06,880 --> 00:37:08,550
is also in the exponential family.

646
00:37:08,720 --> 00:37:11,070
So are the gamma and the exponential

647
00:37:11,240 --> 00:37:13,260
distributions, if you've heard of them.

648
00:37:13,450 --> 00:37:15,660
So the gamma and the exponential distributions

649
00:37:15,810 --> 00:37:17,820
are distributions of the positive numbers.

650
00:37:17,930 --> 00:37:20,180
So they're often used in model intervals,

651
00:37:20,410 --> 00:37:22,520
like if you're standing at the bus stop

652
00:37:22,670 --> 00:37:24,110
and you want to ask,

653
00:37:24,290 --> 00:37:26,340
"When is the next bus likely to arrive?

654
00:37:26,500 --> 00:37:28,400
How long do I have to wait for my bus to arrive?"

655
00:37:28,620 --> 00:37:31,140
Often you model that with sort of gamma

656
00:37:31,340 --> 00:37:34,040
distribution or exponential families,

657
00:37:34,120 --> 00:37:36,310
or the exponential distribution.

658
00:37:36,470 --> 00:37:38,650
Those are also in the exponential family.

659
00:37:38,770 --> 00:37:40,290
Even more [inaudible] distributions,

660
00:37:40,460 --> 00:37:42,610
like the beta and the Dirichlet distributions,

661
00:37:42,800 --> 00:37:44,950
these are probably distributions over fractions,

662
00:37:45,120 --> 00:37:47,220
are already probability distributions

663
00:37:47,330 --> 00:37:48,940
over probability distributions.

664
00:37:49,060 --> 00:37:52,270
And also things like the Wishart distribution,

665
00:37:52,390 --> 00:37:54,620
which is the distribution over covariance matrices.

666
00:37:54,730 --> 00:37:57,110
So all of these, it turns out, can be written in the

667
00:37:57,230 --> 00:37:59,120
form of exponential family distributions.

668
00:37:59,200 --> 00:38:05,660
Well, and in the problem set where

669
00:38:05,790 --> 00:38:08,520
he asks you to take one of these distributions and

670
00:38:08,760 --> 00:38:10,720
write it in the form of the exponential

671
00:38:10,900 --> 00:38:12,920
family distribution, and derive

672
00:38:13,080 --> 00:38:15,280
a generalized linear model for it, okay.

673
00:38:15,420 --> 00:38:17,260
Which brings me to the next topic of

674
00:38:29,800 --> 00:38:32,710
having chosen and exponential family distribution,

675
00:38:32,890 --> 00:38:35,330
how do you use it

676
00:38:35,500 --> 00:38:37,950
to derive a generalized linear model?

677
00:38:38,130 --> 00:38:43,110
So generalized linear models

678
00:38:43,350 --> 00:38:49,320
are often abbreviated GLM's. And I'm going to

679
00:38:49,440 --> 00:38:51,500
write down the three assumptions.

680
00:38:51,620 --> 00:38:53,820
You can think of them as assumptions,

681
00:38:54,050 --> 00:38:56,270
or you can think of them as design choices,

682
00:38:56,480 --> 00:38:58,600
that will then allow me to sort of turn a crank

683
00:38:58,830 --> 00:39:01,460
and come up with a generalized linear model.

684
00:39:01,630 --> 00:39:04,010
So the first one is I'm going to assume

685
00:39:04,180 --> 00:39:09,000
that given my input X and my parameters theta,

686
00:39:09,140 --> 00:39:12,720
I'm going to assume that the variable Y,

687
00:39:12,940 --> 00:39:16,750
the output Y, or the response variable Y I'm

688
00:39:16,940 --> 00:39:22,070
trying to predict is distributed exponential family

689
00:39:23,960 --> 00:39:26,220
with some natural parameter [inaudible].

690
00:39:26,460 --> 00:39:29,060
And so this means that there is some

691
00:39:29,210 --> 00:39:32,130
specific choice of those functions, A, B and T

692
00:39:32,400 --> 00:39:35,710
so that the conditional distribution of Y

693
00:39:36,180 --> 00:39:39,030
given X and parameterized by theta, those

694
00:39:39,230 --> 00:39:42,520
exponential families with parameter [inaudible].

695
00:39:42,740 --> 00:39:44,170
Where here,

696
00:39:44,370 --> 00:39:47,030
[inaudible] may depend on X in some way.

697
00:39:47,160 --> 00:39:49,540
So for example, if you're trying to predict

698
00:39:49,690 --> 00:39:52,010
if you want to predict how many customers

699
00:39:52,180 --> 00:39:54,420
have arrived at your website, you may choose

700
00:39:54,650 --> 00:39:56,620
to model the number of people

701
00:39:56,810 --> 00:39:59,060
the number of hits on your website

702
00:39:59,200 --> 00:40:01,390
by Poisson Distribution, since Poisson Distribution

703
00:40:01,530 --> 00:40:03,790
is natural for modeling count data.

704
00:40:04,000 --> 00:40:06,590
And so you may choose the exponential family

705
00:40:06,770 --> 00:40:08,640
distribution here to be the Parson distribution.

706
00:40:14,170 --> 00:40:21,630
[Inaudible] that given X, our goal is to output

707
00:40:24,210 --> 00:40:28,970
the expective value of Y given X.

708
00:40:29,210 --> 00:40:32,630
So given the features in the website examples,

709
00:40:32,760 --> 00:40:35,840
I've given a set of features about

710
00:40:36,090 --> 00:40:38,280
whether there were any proportions,

711
00:40:38,430 --> 00:40:40,210
whether there were sales,

712
00:40:40,330 --> 00:40:42,170
how many people linked to your website,

713
00:40:42,310 --> 00:40:44,680
or whatever. I'm going to assume that our goal

714
00:40:44,810 --> 00:40:46,860
in our [inaudible] problem is to estimate the

715
00:40:46,980 --> 00:40:49,870
expected number of people that will arrive

716
00:40:50,040 --> 00:40:51,760
at your website on a given day.

717
00:40:52,000 --> 00:40:54,470
So in other words, you're saying that

718
00:40:54,650 --> 00:40:58,950
I want HFX to be equal to oh, excuse me.

719
00:40:59,080 --> 00:41:01,980
I actually meant to write T of Y here.

720
00:41:02,320 --> 00:41:07,930
My goal is to get my learning algorithms

721
00:41:08,150 --> 00:41:10,720
hypothesis to output the expected

722
00:41:10,910 --> 00:41:12,780
value of TFY given X.

723
00:41:12,960 --> 00:41:15,200
But again, for most of the examples,

724
00:41:15,360 --> 00:41:17,190
T of Y is just equal to Y.

725
00:41:17,380 --> 00:41:19,070
And so for most of the examples,

726
00:41:19,280 --> 00:41:21,400
our goal is to get our learning algorithms output,

727
00:41:21,670 --> 00:41:23,730
T expected value of Y given X

728
00:41:23,910 --> 00:41:26,900
because T of Y is usually equal to Y. Yes?

729
00:41:30,160 --> 00:41:32,550
Student:[Inaudible]

730
00:41:33,060 --> 00:41:35,710
Instructor (Andrew Ng):Yes, same thing, right.

731
00:41:36,390 --> 00:41:39,120
T of Y is a sufficient statistic. Same T of Y.

732
00:41:39,580 --> 00:41:43,100
And lastly, this last one I wrote down

733
00:41:43,310 --> 00:41:46,410
these are assumptions. This last one you might

734
00:41:46,550 --> 00:41:48,700
maybe wanna think of this as a design choice.

735
00:41:48,940 --> 00:41:52,090
Which is [inaudible] assume that the distribution

736
00:41:52,290 --> 00:41:55,350
of Y given X is a distributed exponential family

737
00:41:55,570 --> 00:41:57,330
with some parameter [inaudible].

738
00:41:57,530 --> 00:41:59,890
So the number of visitors on the website on any

739
00:42:00,040 --> 00:42:02,560
given day will be Poisson or some parameter

740
00:42:02,760 --> 00:42:05,490
And the last decision I need to make

741
00:42:05,630 --> 00:42:08,210
is was the relationship between my input features

742
00:42:08,510 --> 00:42:11,930
and this parameter [inaudible] parameterizing

743
00:42:12,100 --> 00:42:14,760
my Parson distribution or whatever.

744
00:42:15,000 --> 00:42:17,710
And this last step, I'm going

745
00:42:17,890 --> 00:42:21,150
to make the assumption, or really a design choice,

746
00:42:22,790 --> 00:42:25,410
that I'm going to assume the relationship between

747
00:42:25,560 --> 00:42:27,820
[inaudible] and my [inaudible] axi s linear,

748
00:42:27,990 --> 00:42:30,170
and in particular that they're governed by this

749
00:42:30,330 --> 00:42:32,530
that [inaudible] is equal to theta, transpose X.

750
00:42:32,730 --> 00:42:35,080
And the reason I make this design choice

751
00:42:35,240 --> 00:42:37,370
is it will allow me to turn the crank of the

752
00:42:37,530 --> 00:42:39,790
generalized linear model of machinery and

753
00:42:39,980 --> 00:42:42,150
come off with very nice algorithms for fitting

754
00:42:42,290 --> 00:42:46,510
say Poisson Regression models or performed

755
00:42:46,820 --> 00:42:49,810
regression with a gamma distribution outputs

756
00:42:50,090 --> 00:42:53,540
or exponential distribution outputs and so on.

757
00:42:53,770 --> 00:43:00,310
So let's work through an example.

758
00:43:17,380 --> 00:43:20,970
[Inaudible] equals theta transpose X works for the

759
00:43:21,200 --> 00:43:23,750
case where [inaudible] is a real number.

760
00:43:23,980 --> 00:43:27,260
For the more general case, you would have

761
00:43:27,450 --> 00:43:30,930
[inaudible] I equals theta I, transpose X

762
00:43:35,310 --> 00:43:37,250
if [inaudible] is a vector rather than a real

763
00:43:37,450 --> 00:43:39,930
number. But again, most of the examples

764
00:43:40,140 --> 00:43:43,380
[inaudible] will just be a real number.

765
00:43:43,750 --> 00:43:56,120
All right. So let's work through the Bernoulli

766
00:44:00,600 --> 00:44:03,500
example. You'll see where Y given X

767
00:44:03,740 --> 00:44:07,000
parameterized by theta this is a

768
00:44:07,250 --> 00:44:10,540
distributed exponential family with

769
00:44:10,780 --> 00:44:13,220
natural parameter [inaudible].

770
00:44:13,440 --> 00:44:16,450
And for the [inaudible] distribution,

771
00:44:16,700 --> 00:44:20,040
I'm going to choose A, B and T to be the

772
00:44:20,220 --> 00:44:23,530
specific forms that cause those exponential

773
00:44:23,760 --> 00:44:25,670
families to become the Bernoulli distribution.

774
00:44:25,940 --> 00:44:29,310
This is the example we worked through just now,

775
00:44:30,070 --> 00:44:33,080
the first example we worked through just now.

776
00:44:33,390 --> 00:44:38,010
So oh, and we also have so for any fixed

777
00:44:41,740 --> 00:44:45,800
value of X and theta, my hypothesis,

778
00:44:46,180 --> 00:44:52,350
my learning algorithm will make a prediction,

779
00:44:53,320 --> 00:44:56,830
or will make will sort of output [inaudible]

780
00:44:57,130 --> 00:45:03,150
of X, which is by my,

781
00:45:03,490 --> 00:45:06,850
I guess, assumption [inaudible].

782
00:45:07,050 --> 00:45:09,050
Watch our learning algorithm to

783
00:45:09,300 --> 00:45:11,490
output the expected value of Y given X and

784
00:45:11,750 --> 00:45:14,090
parameterized by theta, where Y can take

785
00:45:14,480 --> 00:45:17,960
on only the value zero and one,

786
00:45:18,210 --> 00:45:21,410
then the expected value of Y is just equal

787
00:45:21,630 --> 00:45:24,930
to the probability that Y is equal to one.

788
00:45:25,160 --> 00:45:28,540
So the expected value of a Bernoulli random variable is

789
00:45:28,760 --> 00:45:30,790
just equal to the probability that it's equal to one.

790
00:45:36,540 --> 00:45:40,100
And so the probability that Y equals one

791
00:45:40,450 --> 00:45:44,400
is just equal to phi because that's the parameter

792
00:45:45,060 --> 00:45:47,870
of my Bernoulli distribution. Phi is,

793
00:45:48,050 --> 00:45:50,390
by definition, I guess, is the probability of my

794
00:45:50,620 --> 00:45:53,090
Bernoulli distribution [inaudible] value of one.

795
00:45:57,370 --> 00:45:59,330
Which we worked out previously,

796
00:45:59,660 --> 00:46:02,000
phi was one over one plus E to the negative

797
00:46:02,260 --> 00:46:04,620
[inaudible]. So we worked this out

798
00:46:04,850 --> 00:46:06,980
on our previous board. This is the relationship

799
00:46:07,170 --> 00:46:09,120
so when we wrote down the Bernoulli

800
00:46:09,310 --> 00:46:12,150
distribution in the form of an exponential family,

801
00:46:12,310 --> 00:46:14,650
we worked out what the relationship was

802
00:46:14,880 --> 00:46:17,290
between phi and [inaudible], and it was this.

803
00:46:17,490 --> 00:46:19,970
So we worked out the relationship

804
00:46:20,270 --> 00:46:22,230
between the expected value of Y

805
00:46:22,430 --> 00:46:24,260
and [inaudible] was this relationship.

806
00:46:24,420 --> 00:46:27,730
And lastly, because we made the design choice,

807
00:46:27,970 --> 00:46:31,030
or the assumption that [inaudible] and theta are

808
00:46:31,210 --> 00:46:35,580
linearly related. This is therefore equal to one

809
00:46:36,750 --> 00:46:40,690
over one plus E to the minus theta, transpose X.

810
00:46:42,590 --> 00:46:47,240
And so that's how I come up with the

811
00:46:47,440 --> 00:46:52,580
logistic regression algorithm when you have a

812
00:46:52,800 --> 00:46:55,110
variable Y when you have a [inaudible]

813
00:46:55,300 --> 00:46:57,940
variable Y, or also response variable Y that

814
00:46:58,160 --> 00:47:00,480
takes on two values, and then you choose to

815
00:47:00,640 --> 00:47:02,860
model variable [inaudible] distribution.

816
00:47:07,820 --> 00:47:09,820
Are you sure this does make sense?

817
00:47:10,010 --> 00:47:12,740
Raise your hand if this makes sense.

818
00:47:12,980 --> 00:47:15,010
Yeah, okay, cool.

819
00:47:15,220 --> 00:47:18,140
So I hope you get the ease of use of this,

820
00:47:18,300 --> 00:47:20,720
or sort of the power of this. The only decision I

821
00:47:20,900 --> 00:47:23,860
made was really, I said Y let's say I have

822
00:47:24,030 --> 00:47:26,340
a new machine-learning problem and I'm

823
00:47:26,490 --> 00:47:28,800
trying to predict the value of a variable

824
00:47:28,940 --> 00:47:31,970
Y that happens to take on two values.

825
00:47:32,280 --> 00:47:34,530
Then the only decision I need to make

826
00:47:34,720 --> 00:47:36,300
is I chose [inaudible] distribution.

827
00:47:36,490 --> 00:47:38,470
I say I want to model I want to assume

828
00:47:38,640 --> 00:47:41,140
that given X and theta, I'm going to assume Y is

829
00:47:41,390 --> 00:47:43,020
distributed [inaudible].

830
00:47:43,250 --> 00:47:45,080
That's the only decision I made.

831
00:47:45,240 --> 00:47:46,900
And then everything else follows automatically

832
00:47:47,110 --> 00:47:50,600
having made the decision to model Y given X

833
00:47:50,810 --> 00:47:53,640
and parameterized by theta as being [inaudible].

834
00:47:54,720 --> 00:47:56,490
In the same way you can choose a

835
00:47:56,650 --> 00:47:58,700
different distribution, you can choose Y

836
00:47:58,890 --> 00:47:59,740
as Parson or Y as

837
00:47:59,740 --> 00:48:01,020
gamma or Y as whatever,

838
00:48:01,090 --> 00:48:03,050
and follow a similar process and come up with

839
00:48:03,210 --> 00:48:05,400
a different model and different learning algorithm.

840
00:48:05,610 --> 00:48:07,680
Come up with a different generalized linear model

841
00:48:07,850 --> 00:48:10,620
for whatever learning algorithm you're faced with.

842
00:48:11,590 --> 00:48:16,780
This tiny little notation, the function G that

843
00:48:17,010 --> 00:48:22,620
relates G of [inaudible] that relates the natural

844
00:48:22,790 --> 00:48:26,970
parameter to the expected value of Y,

845
00:48:27,150 --> 00:48:32,740
which in this case, one over one plus [inaudible]

846
00:48:32,830 --> 00:48:35,050
minus [inaudible], this is called the

847
00:48:35,150 --> 00:48:42,610
canonical response function. And G inverse

848
00:48:42,870 --> 00:48:46,660
is called the canonical link function.

849
00:48:53,770 --> 00:48:56,230
These aren't a huge deal. I won't use

850
00:48:56,380 --> 00:48:59,230
this terminology a lot. I'm just mentioning

851
00:48:59,390 --> 00:49:01,680
those in case you hear about people talk about

852
00:49:01,810 --> 00:49:04,040
generalized linear models, and if they talk about

853
00:49:04,190 --> 00:49:06,390
canonical response functions or canonical

854
00:49:06,560 --> 00:49:07,630
link functions,

855
00:49:07,630 --> 00:49:09,520
just so you know there's all of this.

856
00:49:09,670 --> 00:49:12,880
Actually, many techs actually use the reverse way.

857
00:49:13,050 --> 00:49:15,180
This is G inverse and this is G, but

858
00:49:15,450 --> 00:49:17,590
this notation turns out to be more consistent

859
00:49:17,820 --> 00:49:20,010
with other algorithms in machine learning.

860
00:49:20,220 --> 00:49:23,400
So I'm going to use this notation.

861
00:49:23,590 --> 00:49:25,790
But I probably won't use the terms

862
00:49:25,970 --> 00:49:28,120
canonical response functions and canonical

863
00:49:28,320 --> 00:49:30,620
link functions in lecture a lot, so just

864
00:49:30,800 --> 00:49:33,170
I don't know. I'm not big on memorizing

865
00:49:33,300 --> 00:49:35,590
lots of names of things. I'm just tossing those

866
00:49:35,780 --> 00:49:38,200
out there in case you see it elsewhere.

867
00:49:44,110 --> 00:49:52,850
Okay. You know what, I think in the interest

868
00:49:53,070 --> 00:49:56,070
of time, I'm going to skip over the Gaussian

869
00:49:56,340 --> 00:49:58,510
example. But again, just like I said, [inaudible],

870
00:49:58,660 --> 00:50:00,930
Y is [inaudible], different variation I get

871
00:50:01,180 --> 00:50:03,260
of logistic regression. You can do the same

872
00:50:03,410 --> 00:50:05,630
thing with the Gaussian distribution and end

873
00:50:05,830 --> 00:50:08,300
up with ordinary [inaudible] squares model.

874
00:50:08,480 --> 00:50:11,540
The problem with Gaussian is that it's almost

875
00:50:11,760 --> 00:50:14,750
so simple that when you see it for the first

876
00:50:14,920 --> 00:50:16,860
time that it's sometimes more confusing

877
00:50:16,990 --> 00:50:17,830
than the [inaudible] model

878
00:50:17,830 --> 00:50:18,950
because it looks so

879
00:50:19,160 --> 00:50:20,920
simple, it looks like it has to be more complicated.

880
00:50:21,130 --> 00:50:23,850
So let me just skip that and leave you to read

881
00:50:24,070 --> 00:50:25,090
about the Gaussian example

882
00:50:25,090 --> 00:50:26,630
in the lecture notes.

883
00:50:26,750 --> 00:50:29,130
And what I want to do is actually go through

884
00:50:29,370 --> 00:50:31,490
a more complex example. Question?

885
00:50:31,690 --> 00:50:33,150
Student:[Inaudible]

886
00:50:33,340 --> 00:50:35,410
Instructor (Andrew Ng):Okay, right.

887
00:50:35,640 --> 00:50:37,900
So how do choose what theta will be?

888
00:50:38,050 --> 00:50:41,740
We'll get to that in the end. What you have

889
00:50:41,930 --> 00:50:44,730
there is the logistic regression model,

890
00:50:44,900 --> 00:50:47,000
which is a probabilistic model that assumes

891
00:50:47,250 --> 00:50:50,730
the probability of Y given X

892
00:50:50,960 --> 00:50:53,060
is given by a certain form.

893
00:50:53,230 --> 00:50:56,170
And so what you do is you can write down

894
00:50:56,380 --> 00:50:58,590
the log likelihood of your training set,

895
00:50:58,700 --> 00:51:01,210
and find the value of theta that maximizes

896
00:51:01,360 --> 00:51:03,320
the log likelihood of the parameters.

897
00:51:03,620 --> 00:51:05,770
Does that make sense?

898
00:51:05,950 --> 00:51:08,250
So I'll say that again towards

899
00:51:08,490 --> 00:51:10,020
the end of today's lecture.

900
00:51:10,160 --> 00:51:12,430
But for logistic regression, the way you

901
00:51:12,580 --> 00:51:14,590
choose theta is exactly maximum likelihood,

902
00:51:14,700 --> 00:51:16,400
as we worked out in the previous lecture,

903
00:51:16,520 --> 00:51:19,120
using Newton's Method or gradient ascent or

904
00:51:19,250 --> 00:51:23,080
whatever. I'll sort of try to do that again

905
00:51:23,270 --> 00:51:25,350
for one more example

906
00:51:25,530 --> 00:51:28,500
towards the end of today's lecture.

907
00:51:28,670 --> 00:51:33,000
So what I want to do is actually use the remaining,

908
00:51:33,140 --> 00:51:36,140
I don't know, 19 minutes or so of this class,

909
00:51:36,220 --> 00:51:39,140
to go through the one of the more

910
00:51:39,310 --> 00:51:42,300
it's probably the most complex example of a

911
00:51:42,430 --> 00:51:44,480
generalized linear model that I've used.

912
00:51:44,650 --> 00:51:47,030
This one I want to go through because it's

913
00:51:47,180 --> 00:51:50,560
a little bit trickier than many of the other

914
00:51:50,790 --> 00:51:53,780
textbook examples of generalized linear models.

915
00:51:53,980 --> 00:51:58,740
So again, what I'm going to do is go through

916
00:51:58,900 --> 00:52:01,840
the derivation reasonably quickly and give

917
00:52:02,020 --> 00:52:03,310
you the gist of it,

918
00:52:03,310 --> 00:52:04,260
and if there are steps I skip

919
00:52:04,420 --> 00:52:06,610
or details omitted, I'll leave you to read

920
00:52:06,780 --> 00:52:09,870
about them more carefully in the lecture notes.

921
00:52:10,120 --> 00:52:14,730
And what I want to do is talk about Multinomial.

922
00:52:17,010 --> 00:52:26,300
And Multinomial is the distribution over K

923
00:52:26,890 --> 00:52:30,510
possible outcomes. Imagine you're now in

924
00:52:30,640 --> 00:52:33,140
a machine-learning problem where the value of Y

925
00:52:33,320 --> 00:52:36,070
that you're trying to predict can take on K possible

926
00:52:36,220 --> 00:52:38,100
outcomes, so rather than only two outcomes.

927
00:52:38,240 --> 00:52:40,080
So obviously, this example's already

928
00:52:40,190 --> 00:52:42,750
if you want to have a learning algorithm,

929
00:52:42,920 --> 00:52:45,180
or to magically send emails for you into your

930
00:52:45,340 --> 00:52:47,210
right email folder, and you may have a dozen

931
00:52:47,400 --> 00:52:49,900
email folders you want your algorithm

932
00:52:50,160 --> 00:52:52,020
to classify emails into.

933
00:52:52,200 --> 00:52:54,800
Or predicting if the patient either has

934
00:52:55,050 --> 00:52:57,570
a disease or does not have a disease, which

935
00:52:57,770 --> 00:52:59,760
would be a binary classification problem.

936
00:52:59,950 --> 00:53:01,930
If you think that the patient may have

937
00:53:02,120 --> 00:53:04,090
one of K diseases, and you want other than have a

938
00:53:04,260 --> 00:53:06,020
learning algorithm figure out which

939
00:53:06,170 --> 00:53:07,850
one of K diseases your patient has is all.

940
00:53:08,000 --> 00:53:09,690
So lots of multi-class classification problems

941
00:53:09,820 --> 00:53:11,590
where you have more than two classes.

942
00:53:11,790 --> 00:53:13,660
You model that with Multinomial.

943
00:53:13,790 --> 00:53:18,720
And eventually so for logistic regression,

944
00:53:18,880 --> 00:53:21,560
I had [inaudible] like these where you

945
00:53:21,640 --> 00:53:23,450
have a training set and you find

946
00:53:23,540 --> 00:53:26,220
a decision boundary that separates them.

947
00:53:26,340 --> 00:53:28,350
[Inaudible], we're going to entertain the

948
00:53:28,480 --> 00:53:30,680
value of predicting, taking on multiple values,

949
00:53:30,800 --> 00:53:33,860
so you now have three classes,

950
00:53:34,010 --> 00:53:37,290
and the learning algorithm will learn some way

951
00:53:37,490 --> 00:53:39,570
to separate out three classes or more,

952
00:53:39,720 --> 00:53:41,940
rather than just two classes.

953
00:53:44,090 --> 00:53:46,550
So let's write Multinomial in the form of

954
00:53:47,050 --> 00:53:49,930
an exponential family distribution.

955
00:53:50,200 --> 00:53:55,710
So the parameters of a Multinomial are phi one,

956
00:53:58,150 --> 00:54:00,860
phi two [inaudible] phi K.

957
00:54:01,570 --> 00:54:04,720
I'll actually change this in a second

958
00:54:04,890 --> 00:54:07,260
where the probability of Y equals I is phi I,

959
00:54:07,590 --> 00:54:10,700
right, because there are K possible outcomes.

960
00:54:11,040 --> 00:54:13,010
But if I choose this

961
00:54:13,010 --> 00:54:15,430
as my parameterization

962
00:54:15,560 --> 00:54:17,780
of the [inaudible], then my parameter's

963
00:54:18,000 --> 00:54:20,730
actually redundant because if these are

964
00:54:20,920 --> 00:54:23,370
probabilities, then you have to sum up the one.

965
00:54:23,590 --> 00:54:25,980
And therefore for example,

966
00:54:26,500 --> 00:54:29,020
I can derive the last parameter, phi K,

967
00:54:29,130 --> 00:54:33,440
as one minus phi one, up to phi K minus one.

968
00:54:33,900 --> 00:54:36,980
So this would be a redundant

969
00:54:37,140 --> 00:54:40,020
parameterization from [inaudible].

970
00:54:40,150 --> 00:54:42,320
The result is over-parameterized.

971
00:54:44,130 --> 00:54:49,800
And so for purposes of this [inaudible],

972
00:54:49,980 --> 00:54:52,750
I'm going to treat my parameters of my

973
00:54:52,890 --> 00:54:56,880
[inaudible] as phi one, phi two, up to phi

974
00:54:57,280 --> 00:55:00,490
K minus one. And I won't think of phi K as a

975
00:55:00,710 --> 00:55:03,350
parameter. I'll just so my parameters are just

976
00:55:03,520 --> 00:55:05,720
I just have K minus one parameters,

977
00:55:05,790 --> 00:55:07,750
parameterizing my [inaudible].

978
00:55:07,900 --> 00:55:11,400
And sometimes I write phi K in my derivations

979
00:55:11,620 --> 00:55:14,620
as well, and you should think of phi K as

980
00:55:14,720 --> 00:55:16,720
just a shorthand for this, for one minus

981
00:55:16,860 --> 00:55:20,630
the rest of the parameters, okay.

982
00:55:37,340 --> 00:55:39,960
So it turns out the [inaudible] is one of the

983
00:55:40,110 --> 00:55:42,300
few examples where T of Y it's one of the

984
00:55:42,500 --> 00:55:44,730
examples where T of Y is not equal to Y.

985
00:55:45,190 --> 00:55:49,780
So in this case, Y is on of K possible values.

986
00:55:50,020 --> 00:55:54,270
And so T of Y would be defined as follows;

987
00:55:54,670 --> 00:55:57,970
T of one is going to be a vector with a one

988
00:55:58,230 --> 00:56:01,000
and zeros everywhere else.

989
00:56:02,200 --> 00:56:08,160
T of two is going to be a zero, one, zero and so on.

990
00:56:08,390 --> 00:56:10,740
Except that these are going to

991
00:56:10,910 --> 00:56:14,070
be K minus one-dimensional vectors.

992
00:56:14,280 --> 00:56:19,340
And so T of K minus one is going to be zero,

993
00:56:19,690 --> 00:56:24,530
zero, zero, one. And T of K

994
00:56:24,730 --> 00:56:28,960
is going to be the vector of all zeros.

995
00:56:29,680 --> 00:56:32,350
So this is just how I'm choosing to define T

996
00:56:32,580 --> 00:56:36,650
of Y to write down the [inaudible] in the form

997
00:56:36,890 --> 00:56:40,900
of an exponential family distribution. Again,

998
00:56:41,090 --> 00:56:44,060
these are K minus one-dimensional vectors.

999
00:56:44,300 --> 00:56:48,780
So this is a good point to introduce one more

1000
00:56:49,060 --> 00:56:51,910
useful piece of notation, which is called

1001
00:56:52,050 --> 00:56:54,870
indicator function notation. So I'm going

1002
00:56:55,010 --> 00:56:59,060
to write one, and then curly braces.

1003
00:56:59,210 --> 00:57:01,140
And if I write a true statement inside, then

1004
00:57:01,240 --> 00:57:04,300
the indicator of that statement is going to be one.

1005
00:57:04,480 --> 00:57:06,620
Then I write one, and then I write a false

1006
00:57:06,750 --> 00:57:09,770
statement inside, then the value of this indicator

1007
00:57:10,060 --> 00:57:12,510
function is going to be a zero.

1008
00:57:12,650 --> 00:57:16,070
For example, if I write indicator two equals

1009
00:57:16,250 --> 00:57:18,490
three [inaudible] that's false,

1010
00:57:18,720 --> 00:57:20,980
and so this is equal to zero.

1011
00:57:21,050 --> 00:57:23,890
Whereas indicator [inaudible] plus one equals two,

1012
00:57:24,090 --> 00:57:27,170
I wrote down a true statement inside.

1013
00:57:27,260 --> 00:57:29,660
And so the indicator of the statement

1014
00:57:29,760 --> 00:57:32,020
was equal to one. So the indicator function

1015
00:57:32,160 --> 00:57:34,640
is just a very useful notation for indicating

1016
00:57:34,750 --> 00:57:37,850
sort of truth or falsehood of the statement inside.

1017
00:57:39,050 --> 00:57:46,460
And so actually, let's do this here.

1018
00:57:46,680 --> 00:57:49,500
To combine both of these, right,

1019
00:57:49,810 --> 00:57:57,770
if I carve out a bit of space here so if I use

1020
00:57:57,980 --> 00:58:06,440
so TY is a vector. Y is one of K values,

1021
00:58:06,580 --> 00:58:10,320
and so TY is one of these K vectors.

1022
00:58:10,460 --> 00:58:13,820
If I use TY as [inaudible] to denote

1023
00:58:13,970 --> 00:58:16,840
the [inaudible] element of the vector TY, then TY

1024
00:58:17,130 --> 00:58:21,650
the [inaudible] element of the vector TY is

1025
00:58:21,950 --> 00:58:27,240
just equal to indicator for whether Y is equal to I.

1026
00:58:27,360 --> 00:58:32,320
Just take a let me clean a couple more boards.

1027
00:58:32,500 --> 00:58:34,870
Take a look at this for a second and make

1028
00:58:35,060 --> 00:58:38,070
sure you understand why that make sure you

1029
00:58:38,200 --> 00:58:41,060
understand all that notation and why this is true.

1030
00:59:10,420 --> 00:59:12,670
All right. Actually, raise your hand if this

1031
00:59:12,790 --> 00:59:15,170
equation makes sense to you.

1032
00:59:15,380 --> 00:59:18,380
Most of you, not all, okay. [Inaudible].

1033
00:59:18,570 --> 00:59:20,790
Just as one kind of [inaudible],

1034
00:59:21,010 --> 00:59:29,760
suppose Y is equal to one let's say let me see.

1035
00:59:30,490 --> 00:59:35,910
Suppose Y is equal to one, right,

1036
00:59:36,100 --> 00:59:38,930
so TY is equal to this vector,

1037
00:59:39,140 --> 00:59:43,630
and therefore the first element of this vector

1038
00:59:43,840 --> 00:59:46,990
will be one, and the rest of the elements

1039
00:59:47,200 --> 00:59:50,160
will be equal to zero.

1040
00:59:50,520 --> 00:59:53,360
And so let me try that again, I'm sorry.

1041
00:59:53,540 --> 00:59:56,020
Let's say I want to ask I want to look at the

1042
00:59:56,160 --> 00:59:57,990
[inaudible] element of the vector TY,

1043
00:59:58,150 --> 01:00:00,800
and I want to know is this one or zero. All right.

1044
01:00:00,910 --> 01:00:02,870
Well, this will be one.

1045
01:00:03,020 --> 01:00:05,390
The [inaudible] element of the vector TY

1046
01:00:05,520 --> 01:00:08,750
will be equal to one if, and only if Y is equal to I.

1047
01:00:08,940 --> 01:00:11,190
Because for example, if Y is equal to one,

1048
01:00:11,430 --> 01:00:13,790
then only the first element of this vector will

1049
01:00:13,970 --> 01:00:15,940
be zero. If Y is equal to two,

1050
01:00:16,080 --> 01:00:17,990
then only the second element of the vector

1051
01:00:18,150 --> 01:00:20,190
will be zero and so on. So the question of

1052
01:00:20,300 --> 01:00:22,680
whether or not whether the [inaudible] element

1053
01:00:22,810 --> 01:00:25,010
of this vector, TY, is equal to one

1054
01:00:25,180 --> 01:00:29,450
is answered by just asking is Y equal to I.

1055
01:00:32,010 --> 01:00:35,100
Okay. If you're still not

1056
01:00:35,100 --> 01:00:38,190
quite sure why that's true,

1057
01:00:38,450 --> 01:00:40,800
go home and think about it a bit more.

1058
01:00:40,960 --> 01:00:43,290
And I think I and take a look at the

1059
01:00:43,460 --> 01:00:46,140
lecture notes as well, maybe that'll help.

1060
01:00:46,280 --> 01:00:49,320
At least for now, only just take my word for it.

1061
01:00:49,530 --> 01:00:54,030
So let's go ahead and write out the distribution

1062
01:00:54,190 --> 01:00:57,060
for the [inaudible] in an exponential family form.

1063
01:00:57,180 --> 01:01:07,020
So PFY is equal to phi one. Indicator Y

1064
01:01:07,220 --> 01:01:10,660
equals one times phi two.

1065
01:01:10,920 --> 01:01:17,730
Indicator Y equals to up to phi K times indicator

1066
01:01:17,810 --> 01:01:21,090
Y equals K. And again, phi K is not a parameter

1067
01:01:21,240 --> 01:01:25,110
of the distribution. Phi K is a shorthand for

1068
01:01:25,230 --> 01:01:27,700
one minus phi one minus phi two minus the rest.

1069
01:01:27,810 --> 01:01:33,230
And so using this equation on the left as well,

1070
01:01:33,390 --> 01:01:36,800
I can also write this as phi one times TY

1071
01:01:36,890 --> 01:01:41,490
one, phi two, TY two, dot, dot, dot.

1072
01:01:41,630 --> 01:01:49,430
Phi K minus one, TY, K minus one times phi K.

1073
01:01:49,640 --> 01:02:01,790
And then one minus [inaudible]. That should be j.

1074
01:02:06,100 --> 01:02:12,080
And it turns out it takes some of the

1075
01:02:12,200 --> 01:02:15,880
steps of algebra that I don't have time to show.

1076
01:02:16,060 --> 01:02:20,760
It turns out, you can simplify this into well,

1077
01:02:20,880 --> 01:02:24,180
the exponential family form where [inaudible]

1078
01:02:31,490 --> 01:02:42,190
is a vector, this is a K minus

1079
01:02:53,640 --> 01:03:00,780
one-dimensional vector, and well, okay.

1080
01:03:03,100 --> 01:03:07,720
So deriving this is a few steps of algebra

1081
01:03:08,980 --> 01:03:13,270
that you can work out yourself, but I won't do

1082
01:03:13,360 --> 01:03:17,250
here. And so using my definition for TY,

1083
01:03:17,870 --> 01:03:21,850
and by choosing [inaudible] A and B this way,

1084
01:03:22,050 --> 01:03:23,940
I can take my distribution

1085
01:03:24,030 --> 01:03:25,880
from [inaudible] and write it out

1086
01:03:26,040 --> 01:03:28,860
in a form of an exponential family distribution.

1087
01:03:29,060 --> 01:03:35,650
It turns out also that let's see. [Inaudible],

1088
01:03:36,980 --> 01:03:40,760
right. One of the things we did was we also

1089
01:03:40,900 --> 01:03:43,830
had [inaudible] as a function of phi, and then

1090
01:03:43,920 --> 01:03:46,980
we inverted that to write out phi as a function of

1091
01:03:47,120 --> 01:03:50,170
[inaudible]. So it turns out you can do that as well.

1092
01:03:50,350 --> 01:03:53,410
So this defines [inaudible] as a function

1093
01:03:53,530 --> 01:03:56,300
of the [inaudible] distributions parameters phi.

1094
01:03:56,670 --> 01:03:58,680
So you can take this relationship between

1095
01:03:58,810 --> 01:04:01,280
[inaudible] and phi and invert it, and write out

1096
01:04:01,380 --> 01:04:05,070
phi as a function of [inaudible]. And it turns out,

1097
01:04:05,180 --> 01:04:09,420
you get that phi I is equal to [inaudible]

1098
01:04:10,080 --> 01:04:17,660
excuse me. And you get that phi I

1099
01:04:17,780 --> 01:04:26,970
is equal to [inaudible] I of one plus that.

1100
01:04:33,080 --> 01:04:37,830
And the way you do this is you just

1101
01:04:38,000 --> 01:04:41,210
this defines [inaudible] as a function of the phi,

1102
01:04:41,420 --> 01:04:43,540
so if you take this and solve for [inaudible],

1103
01:04:43,790 --> 01:04:45,870
you end up with this. And this is again,

1104
01:04:46,040 --> 01:04:47,660
there are a couple of steps of algebra

1105
01:04:47,840 --> 01:04:49,320
that I'm just not showing.

1106
01:04:49,510 --> 01:04:52,100
And then lastly, using our assumption

1107
01:04:52,230 --> 01:04:55,740
that the [inaudible] are a linear function of the

1108
01:04:55,850 --> 01:04:59,510
[inaudible] axis, phi I is therefore equal to E

1109
01:04:59,610 --> 01:05:02,060
to the theta I, transpose X, divided by one

1110
01:05:02,160 --> 01:05:09,990
plus sum over J equals one, to K minus one,

1111
01:05:10,210 --> 01:05:18,550
E to the theta J, transpose X. And this is just

1112
01:05:18,900 --> 01:05:25,750
using the fact that [inaudible] I equals theta I,

1113
01:05:26,820 --> 01:05:29,490
transpose X, which was our earlier design

1114
01:05:29,620 --> 01:05:31,590
choice from generalized linear models.

1115
01:05:43,710 --> 01:05:46,250
So we're just about down.

1116
01:05:46,520 --> 01:05:52,150
So my learning algorithm [inaudible].

1117
01:05:52,320 --> 01:05:56,720
I'm going to think of it as [inaudible] the expected

1118
01:05:56,890 --> 01:06:01,400
value of TY given X and [inaudible] by theta.

1119
01:06:02,640 --> 01:06:07,810
So TY was this vector indicator function.

1120
01:06:07,960 --> 01:06:11,390
So T one was indicator Y equals one,

1121
01:06:11,590 --> 01:06:17,000
down to indicator Y equals K minus one.

1122
01:06:19,770 --> 01:06:21,480
All right. So I want

1123
01:06:21,480 --> 01:06:22,900
my learning algorithm

1124
01:06:23,100 --> 01:06:26,590
to output this; the expected value of this vector

1125
01:06:26,750 --> 01:06:28,990
of indicator functions.

1126
01:06:31,670 --> 01:06:38,220
The expected value of indicator Y equals one

1127
01:06:38,370 --> 01:06:42,650
is just the probability that Y equals one,

1128
01:06:42,780 --> 01:06:45,090
which is given by phi one.

1129
01:06:45,250 --> 01:06:47,730
So I have a random variable that's one

1130
01:06:47,850 --> 01:06:50,300
whenever Y is equal to one and zero otherwise,

1131
01:06:50,450 --> 01:06:52,760
so the expected value of that,

1132
01:06:52,880 --> 01:06:54,910
of this indicator Y equals one is just

1133
01:06:55,050 --> 01:06:57,280
the probability that Y equals one,

1134
01:06:57,410 --> 01:06:59,740
which is given by phi one.

1135
01:06:59,910 --> 01:07:03,970
And therefore, by what we were taught earlier,

1136
01:07:04,070 --> 01:07:09,130
this is therefore [inaudible] to the theta

1137
01:07:09,260 --> 01:07:18,020
one, transpose X over well okay.

1138
01:07:46,210 --> 01:07:49,540
And so my learning algorithm will output the

1139
01:07:49,700 --> 01:07:51,990
probability that Y equals one, Y equals two,

1140
01:07:52,100 --> 01:07:54,600
up to Y equals K minus one.

1141
01:07:54,730 --> 01:07:57,350
And these probabilities are going

1142
01:07:57,460 --> 01:08:00,760
to be parameterized by these functions like these.

1143
01:08:21,540 --> 01:08:25,390
And so just to give this algorithm a name,

1144
01:08:25,550 --> 01:08:33,530
this algorithm is called softmax regression,

1145
01:08:35,090 --> 01:08:38,390
and is widely thought of as the generalization

1146
01:08:38,590 --> 01:08:41,430
of logistic regression, which is regression of

1147
01:08:41,540 --> 01:08:43,680
two classes. Is widely thought of

1148
01:08:43,800 --> 01:08:46,060
as a generalization of logistic regression

1149
01:08:46,180 --> 01:08:49,510
to the case of K classes rather than two classes.

1150
01:08:49,600 --> 01:08:55,040
And so just to be very concrete about what you do,

1151
01:08:55,170 --> 01:08:57,500
right. So you have a machine-learning

1152
01:08:57,610 --> 01:08:59,730
problem, and you want to apply

1153
01:08:59,810 --> 01:09:01,760
softmax regression to it. So generally,

1154
01:09:01,860 --> 01:09:03,710
work for the entire derivation [inaudible].

1155
01:09:03,830 --> 01:09:05,730
I think the question you had is about

1156
01:09:05,820 --> 01:09:07,300
how to fit parameters.

1157
01:09:07,410 --> 01:09:10,320
So let's say you have a machine-learning problem,

1158
01:09:10,410 --> 01:09:13,220
and Y takes on one of K classes.

1159
01:09:13,330 --> 01:09:15,390
What you do is you sit down and say,

1160
01:09:15,510 --> 01:09:17,360
"Okay, I wanna model Y

1161
01:09:17,470 --> 01:09:19,430
as being [inaudible] given any

1162
01:09:19,580 --> 01:09:21,880
X and then theta." And so you chose [inaudible]

1163
01:09:21,950 --> 01:09:24,470
as the exponential family.

1164
01:09:24,580 --> 01:09:26,640
Then you sort of turn the crank.

1165
01:09:26,740 --> 01:09:28,950
And everything else I wrote down

1166
01:09:29,040 --> 01:09:30,460
follows automatically from you have

1167
01:09:30,560 --> 01:09:32,640
made the choice of using [inaudible]

1168
01:09:32,750 --> 01:09:35,460
distribution as your choice of exponential family.

1169
01:09:35,540 --> 01:09:37,700
And then what you do is you then have

1170
01:09:37,780 --> 01:09:44,550
this training set, X, I, Y, I up to X, M, Y, M.

1171
01:09:44,700 --> 01:09:47,780
So you're doing the training set.

1172
01:09:47,920 --> 01:09:49,810
We're now [inaudible] the value of

1173
01:09:49,880 --> 01:09:52,290
Y takes on one of K possible values.

1174
01:09:52,360 --> 01:09:56,390
And what you do is you then find the

1175
01:09:56,560 --> 01:09:58,810
parameters of the model by maximum likelihood.

1176
01:09:58,940 --> 01:10:01,130
So you write down the likelihood of

1177
01:10:01,230 --> 01:10:03,610
the parameters, and you maximize the likelihood.

1178
01:10:03,690 --> 01:10:05,790
So what's the likelihood? Well, the likelihood,

1179
01:10:05,910 --> 01:10:08,360
as usual, is the product of your training set

1180
01:10:08,470 --> 01:10:12,260
of P of YI given XI parameterized by theta.

1181
01:10:12,350 --> 01:10:18,580
That's the likelihood, same as we had before.

1182
01:10:18,640 --> 01:10:25,440
And that's product of your training set of

1183
01:10:25,530 --> 01:10:31,370
let me write these down now. YI equals one

1184
01:10:31,470 --> 01:10:34,640
times phi two of indicator YI equals two, dot,

1185
01:10:34,780 --> 01:10:44,350
dot, dot, to phi K of indicator YI equals K.

1186
01:10:47,470 --> 01:10:51,230
Where, for example, phi one depends on

1187
01:10:51,370 --> 01:10:54,470
theta through this formula. It is E to the theta

1188
01:10:54,570 --> 01:10:58,000
one, transpose X over one plus sum over J

1189
01:10:58,090 --> 01:11:03,780
well, that formula I had just now. And so phi

1190
01:11:03,920 --> 01:11:10,490
one here is really a shorthand for this formula,

1191
01:11:11,100 --> 01:11:14,470
and similarly for phi two and so on,

1192
01:11:14,610 --> 01:11:17,950
up to phi K, where phi K is one minus

1193
01:11:18,050 --> 01:11:20,490
all of these things. All right.

1194
01:11:20,490 --> 01:11:21,490
So this is a –this formula looks more

1195
01:11:24,320 --> 01:11:26,790
complicated than it really is.

1196
01:11:26,920 --> 01:11:29,210
What you really do is you write this down,

1197
01:11:29,270 --> 01:11:31,690
then you take logs, compute a derivative

1198
01:11:31,780 --> 01:11:34,810
of this formula [inaudible] theta, and apply

1199
01:11:34,930 --> 01:11:37,480
say gradient ascent to maximize the likelihood.

1200
01:11:37,660 --> 01:11:41,500
Student:What are the rows of theta?

1201
01:11:41,610 --> 01:11:43,310
[Inaudible] it's just been a vector, right?

1202
01:11:43,430 --> 01:11:45,060
And now it looks like it's two-dimensional.

1203
01:11:45,200 --> 01:11:47,340
Instructor (Andrew Ng):Yeah. In the notation

1204
01:11:47,440 --> 01:11:50,140
of the [inaudible] I think have theta one through

1205
01:11:50,290 --> 01:11:54,860
theta K minus one. I've been thinking of each

1206
01:11:55,000 --> 01:11:58,150
of these as and N plus one-dimensional vector.

1207
01:11:58,540 --> 01:12:01,280
If X is N plus one-dimensional,

1208
01:12:01,440 --> 01:12:05,770
then I've been see, I think if you have a set of

1209
01:12:05,900 --> 01:12:08,520
parameters comprising K minus one vectors,

1210
01:12:08,620 --> 01:12:12,110
and each of these is a you could group all

1211
01:12:12,240 --> 01:12:13,720
of these together a matrix,

1212
01:12:13,820 --> 01:12:15,870
but I just haven't been doing that. [Inaudible]

1213
01:12:15,960 --> 01:12:18,480
the derivative of K minus one parameter vectors.

1214
01:12:19,440 --> 01:12:21,100
Student:[Inaudible], what do they correspond to?

1215
01:12:21,260 --> 01:12:23,130
Instructor (Andrew Ng):[Inaudible].

1216
01:12:23,260 --> 01:12:27,660
We're sort of out of time. Let me take that offline.

1217
01:12:27,820 --> 01:12:29,760
It's hard to answer in the same way that

1218
01:12:29,910 --> 01:12:32,250
the logistic regression what does theta

1219
01:12:32,340 --> 01:12:34,180
correspond to in logistic regression?

1220
01:12:34,310 --> 01:12:36,250
You can sort of answer that as sort of

1221
01:12:36,270 --> 01:12:38,730
Student:Yeah.

1222
01:12:40,020 --> 01:12:41,670
It's kind of like the [inaudible] feature

1223
01:12:41,930 --> 01:12:43,300
Instructor (Andrew Ng):Yeah.

1224
01:12:43,390 --> 01:12:45,490
Sort of similar interpretation, yeah.

1225
01:12:45,600 --> 01:12:47,990
That's good. I think I'm running a little bit late.

1226
01:12:48,090 --> 01:12:49,580
Why don't I

1227
01:12:49,650 --> 01:12:51,350
why don't we officially close for the day,

1228
01:12:51,500 --> 01:12:53,700
but you can come up if you more questions

