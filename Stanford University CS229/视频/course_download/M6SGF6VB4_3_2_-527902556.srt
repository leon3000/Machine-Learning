1
00:00:21,250 --> 00:00:24,320
Instructor (Andrew Ng):Okay. Good morning

2
00:00:24,420 --> 00:00:28,180
and welcome back to the third lecture of this class.

3
00:00:28,260 --> 00:00:31,720
So here's what I want to do today,

4
00:00:31,800 --> 00:00:35,190
and some of the topics I do today

5
00:00:35,280 --> 00:00:36,930
may seem a little bit like I'm jumping,

6
00:00:37,040 --> 00:00:38,710
sort of, from topic to topic,

7
00:00:38,790 --> 00:00:40,860
but here's, sort of, the outline for today

8
00:00:40,940 --> 00:00:42,580
and the illogical flow of ideas.

9
00:00:42,660 --> 00:00:46,000
In the last lecture, we talked about linear regression

10
00:00:46,080 --> 00:00:48,450
and today I want to talk about sort of

11
00:00:48,540 --> 00:00:51,420
an adaptation of that called locally weighted regression.

12
00:00:51,500 --> 00:00:52,820
It's very a popular algorithm

13
00:00:52,880 --> 00:00:55,530
that's actually one of my former mentors

14
00:00:55,550 --> 00:00:57,940
probably favorite machine learning algorithm.

15
00:00:58,030 --> 00:01:01,190
We'll then talk about a probable second interpretation

16
00:01:01,270 --> 00:01:03,270
of linear regression and use that

17
00:01:03,350 --> 00:01:07,390
to move onto our first classification algorithm,

18
00:01:07,480 --> 00:01:08,660
which is logistic regression;

19
00:01:08,740 --> 00:01:10,850
take a brief digression to tell you

20
00:01:10,940 --> 00:01:12,610
about something called the perceptron algorithm,

21
00:01:12,690 --> 00:01:13,850
which is something we'll come back to,

22
00:01:13,930 --> 00:01:15,180
again, later this quarter;

23
00:01:15,260 --> 00:01:16,060
and time allowing I hope

24
00:01:16,060 --> 00:01:19,280
to get to Newton's method,

25
00:01:19,360 --> 00:01:20,590
which is an algorithm

26
00:01:20,670 --> 00:01:22,450
for fitting logistic regression models.

27
00:01:22,530 --> 00:01:25,730
So this is recap

28
00:01:25,750 --> 00:01:27,990
where we're talking about in the previous lecture,

29
00:01:28,070 --> 00:01:32,470
remember the notation I defined was that

30
00:01:32,540 --> 00:01:36,330
I used this X superscript I,

31
00:01:36,440 --> 00:01:40,420
Y superscript I to denote the I training example.

32
00:01:40,530 --> 00:01:51,290
And when we're talking about linear regression

33
00:01:51,390 --> 00:01:54,670
or linear least squares, we use this

34
00:01:54,760 --> 00:01:57,660
to denote the predicted value

35
00:01:57,740 --> 00:02:02,250
of "by my hypothesis H" on the input XI.

36
00:02:02,340 --> 00:02:04,930
And my hypothesis was franchised

37
00:02:05,010 --> 00:02:08,740
by the vector of grams as theta and so

38
00:02:08,810 --> 00:02:15,960
we said that this was equal to some from theta J, si J,

39
00:02:16,060 --> 00:02:21,550
and more theta transpose X.

40
00:02:21,640 --> 00:02:24,720
And we had the convention that

41
00:02:24,810 --> 00:02:28,110
X subscript Z is equal to one so this accounts for

42
00:02:28,210 --> 00:02:29,960
the intercept term in our linear regression model.

43
00:02:30,050 --> 00:02:34,330
And lowercase n here was the notation I was using

44
00:02:34,390 --> 00:02:38,620
for the number of features in my training set. Okay?

45
00:02:38,720 --> 00:02:40,160
So in the example

46
00:02:40,260 --> 00:02:41,960
when trying to predict housing prices,

47
00:02:42,070 --> 00:02:43,370
we had two features,

48
00:02:43,450 --> 00:02:45,060
the size of the house and the number of bedrooms.

49
00:02:45,160 --> 00:02:47,160
We had two features and there was

50
00:02:47,250 --> 00:02:49,170
little n was equal to two.

51
00:02:49,300 --> 00:02:56,190
So just to finish recapping the previous lecture,

52
00:02:56,300 --> 00:02:58,900
we defined this quadratic cos function J

53
00:02:59,010 --> 00:03:00,570
of theta equals one-half,

54
00:03:00,660 --> 00:03:02,540
something I equals one to m,

55
00:03:02,630 --> 00:03:12,490
theta of XI minus YI squared where

56
00:03:12,580 --> 00:03:15,400
this is the sum over our m training examples

57
00:03:15,480 --> 00:03:16,380
and my training set.

58
00:03:16,480 --> 00:03:19,060
So lowercase m was the notation I've been using

59
00:03:19,150 --> 00:03:20,840
to denote the number of training examples I have

60
00:03:20,930 --> 00:03:22,160
and the size of my training set.

61
00:03:22,270 --> 00:03:24,770
And at the end of the last lecture,

62
00:03:24,870 --> 00:03:27,960
we derive the value of theta that

63
00:03:28,050 --> 00:03:29,360
minimizes this enclosed form,

64
00:03:29,430 --> 00:03:36,640
which was X transpose X inverse X transpose Y. Okay?

65
00:03:36,730 --> 00:03:43,830
So as we move on in today's lecture,

66
00:03:43,920 --> 00:03:46,420
I'll continue to use this notation and, again,

67
00:03:46,490 --> 00:03:48,430
I realize this is a fair amount of

68
00:03:48,430 --> 00:03:49,540
notation to all remember,

69
00:03:49,640 --> 00:03:52,810
so if partway through this lecture you forgot

70
00:03:52,890 --> 00:03:55,010
if you're having trouble remembering

71
00:03:55,110 --> 00:03:57,630
what lowercase m is or what lowercase n is

72
00:03:57,720 --> 00:03:59,220
or something please raise your hand and ask.

73
00:03:59,300 --> 00:04:06,030
When we talked about linear regression

74
00:04:06,120 --> 00:04:08,700
last time we used two features.

75
00:04:08,760 --> 00:04:10,750
One of the features was the size

76
00:04:10,850 --> 00:04:12,020
of the houses in square feet,

77
00:04:12,100 --> 00:04:13,570
so the living area of the house,

78
00:04:13,650 --> 00:04:15,180
and the other feature was

79
00:04:15,290 --> 00:04:16,690
the number of bedrooms in the house.

80
00:04:16,780 --> 00:04:20,760
In general, we apply a machine-learning algorithm

81
00:04:20,840 --> 00:04:22,130
to some problem that you care about.

82
00:04:22,240 --> 00:04:24,820
The choice of the features will very much

83
00:04:24,910 --> 00:04:26,070
be up to you, right?

84
00:04:26,160 --> 00:04:29,440
And the way you choose your features

85
00:04:29,530 --> 00:04:31,270
to give the learning algorithm will often have

86
00:04:31,420 --> 00:04:33,530
a large impact on how it actually does.

87
00:04:33,650 --> 00:04:40,820
So just for example, the choice we made

88
00:04:40,870 --> 00:04:42,720
last time was X1 equal this size,

89
00:04:42,820 --> 00:04:45,270
and let's leave this idea of the feature of the number

90
00:04:45,360 --> 00:04:47,120
of bedrooms for now, let's say we don't have data

91
00:04:47,200 --> 00:04:49,300
that tells us how many bedrooms are in these houses.

92
00:04:49,390 --> 00:04:52,970
One thing you could do is actually define

93
00:04:53,070 --> 00:05:03,090
oh, let's draw this out. And so, right?

94
00:05:03,160 --> 00:05:05,930
So say that was the size of the house

95
00:05:06,030 --> 00:05:07,240
and that's the price of the house.

96
00:05:07,300 --> 00:05:11,780
So if you use this as a feature maybe you get

97
00:05:11,880 --> 00:05:16,310
theta zero plus theta 1, X1, this, sort of, linear model.

98
00:05:16,390 --> 00:05:20,520
If you choose

99
00:05:20,600 --> 00:05:26,080
let me just copy the same data set over, right?

100
00:05:26,170 --> 00:05:28,350
You can define the set of features

101
00:05:28,440 --> 00:05:30,090
where X1 is equal to the size of the house

102
00:05:30,190 --> 00:05:38,500
and X2 is the square of the size of the house. Okay?

103
00:05:38,600 --> 00:05:41,210
So X1 is the size of the house in say square footage

104
00:05:41,320 --> 00:05:44,210
and X2 is just take whatever the square footage

105
00:05:44,290 --> 00:05:46,460
of the house is and just square that number,

106
00:05:46,540 --> 00:05:47,830
and this would be another way

107
00:05:47,910 --> 00:05:49,070
to come up with a feature,

108
00:05:49,150 --> 00:05:52,370
and if you do that then the same algorithm will end up

109
00:05:52,440 --> 00:05:57,350
fitting a quadratic function for you.

110
00:05:57,430 --> 00:06:01,640
Theta 2, XM squared. Okay?

111
00:06:01,720 --> 00:06:07,100
Because this is actually X2.

112
00:06:07,200 --> 00:06:11,330
And depending on what the data looks like,

113
00:06:11,400 --> 00:06:13,190
maybe this is a slightly better fit to the data.

114
00:06:13,280 --> 00:06:24,570
You can actually take this even further, right?

115
00:06:24,630 --> 00:06:26,710
Which is let's see.

116
00:06:26,810 --> 00:06:28,780
I have seven training examples here,

117
00:06:28,890 --> 00:06:30,900
so you can actually maybe fit up

118
00:06:30,980 --> 00:06:32,270
to six for the polynomial.

119
00:06:32,360 --> 00:06:34,440
You can actually fill a model theta zero plus theta one,

120
00:06:34,500 --> 00:06:42,640
X1 plus theta two, X squared plus up to theta six.

121
00:06:42,740 --> 00:06:50,360
X to the power of six and theta six

122
00:06:50,430 --> 00:06:54,640
are the polynomial to these seven data points.

123
00:06:54,740 --> 00:06:57,610
And if you do that you find that

124
00:06:57,690 --> 00:06:58,780
you come up with a model

125
00:06:58,780 --> 00:07:00,280
that fits your data exactly.

126
00:07:00,380 --> 00:07:03,090
This is where, I guess, in this example I drew,

127
00:07:03,180 --> 00:07:04,790
we have seven data points,

128
00:07:04,880 --> 00:07:08,840
so if you fit a six model polynomial you can, sort of,

129
00:07:08,930 --> 00:07:09,880
fit a line that passes through

130
00:07:09,980 --> 00:07:11,290
these seven points perfectly.

131
00:07:11,380 --> 00:07:13,370
And you probably find that

132
00:07:13,450 --> 00:07:18,020
the curve you get will look something like that.

133
00:07:18,100 --> 00:07:19,080
And on the one hand,

134
00:07:19,080 --> 00:07:22,430
this is a great model in a sense

135
00:07:22,510 --> 00:07:24,610
that it fits your training data perfectly.

136
00:07:24,700 --> 00:07:27,160
On the other hand, this is probably not

137
00:07:27,260 --> 00:07:28,500
a very good model in the sense that

138
00:07:28,600 --> 00:07:30,440
none of us seriously think that

139
00:07:30,550 --> 00:07:32,140
this is a very good predictor of housing prices

140
00:07:32,220 --> 00:07:34,480
as a function of the size of the house, right?

141
00:07:34,560 --> 00:07:37,660
So we'll actually come back to this later.

142
00:07:37,770 --> 00:07:40,960
It turns out of the models we have here;

143
00:07:41,040 --> 00:07:44,680
I feel like maybe the quadratic model fits the data best.

144
00:07:44,770 --> 00:07:47,800
Whereas the linear model looks like

145
00:07:47,890 --> 00:07:51,360
there's actually a bit of a quadratic component

146
00:07:51,440 --> 00:07:54,900
in this data that the linear function is not capturing.

147
00:07:54,990 --> 00:07:58,440
So we'll actually come back to this a little bit later

148
00:07:58,520 --> 00:08:00,840
and talk about the problems associated with

149
00:08:00,910 --> 00:08:02,480
fitting models that are either too simple,

150
00:08:02,570 --> 00:08:04,040
use two small a set of features,

151
00:08:04,120 --> 00:08:06,210
or on the models that are too complex

152
00:08:06,280 --> 00:08:09,920
and maybe use too large a set of features.

153
00:08:10,020 --> 00:08:12,490
Just to give these a name,

154
00:08:12,560 --> 00:08:19,890
we call this the problem of underfitting and,

155
00:08:20,000 --> 00:08:22,760
very informally, this refers to a setting where

156
00:08:22,840 --> 00:08:24,720
there are obvious patterns that

157
00:08:24,800 --> 00:08:25,480
where there are

158
00:08:25,480 --> 00:08:26,600
patterns in the data that

159
00:08:26,700 --> 00:08:28,240
the algorithm is just failing to fit.

160
00:08:28,320 --> 00:08:33,240
And this problem here we refer to as overfitting

161
00:08:33,330 --> 00:08:35,780
and, again, very informally,

162
00:08:35,860 --> 00:08:38,550
this is when the algorithm is fitting

163
00:08:38,630 --> 00:08:39,940
the idiosyncrasies of

164
00:08:39,940 --> 00:08:41,400
this specific data set, right?

165
00:08:41,490 --> 00:08:44,390
It just so happens that of the seven houses

166
00:08:44,470 --> 00:08:46,480
we sampled in Portland,

167
00:08:46,530 --> 00:08:47,880
or wherever you collect data from,

168
00:08:47,940 --> 00:08:50,120
that house happens to be a bit more expensive,

169
00:08:50,240 --> 00:08:51,610
that house happened on the less expensive,

170
00:08:51,690 --> 00:08:56,010
and by fitting six for the polynomial we're, sort of,

171
00:08:56,080 --> 00:08:58,400
fitting the idiosyncratic properties of this data set,

172
00:08:58,490 --> 00:09:00,640
rather than the true underlying trends of

173
00:09:00,730 --> 00:09:03,120
how housing prices vary

174
00:09:03,220 --> 00:09:04,720
as the function of the size of house. Okay?

175
00:09:04,830 --> 00:09:06,320
So these are two very different problems.

176
00:09:06,410 --> 00:09:07,990
We'll define them more formally me later

177
00:09:08,080 --> 00:09:10,800
and talk about how to address each of these problems,

178
00:09:10,880 --> 00:09:14,970
but for now I hope you appreciate that

179
00:09:15,050 --> 00:09:16,270
there is this issue of selecting features.

180
00:09:16,360 --> 00:09:24,650
So if you want to just teach us the learning problems

181
00:09:24,750 --> 00:09:26,120
there are a few ways to do so.

182
00:09:26,200 --> 00:09:30,430
We'll talk about feature selection algorithms later

183
00:09:30,500 --> 00:09:31,470
this quarter as well.

184
00:09:31,550 --> 00:09:33,180
So automatic algorithms for choosing

185
00:09:33,290 --> 00:09:36,990
what features you use in a regression problem like this.

186
00:09:37,080 --> 00:09:40,880
What I want to do today is talk about a class of algorithms

187
00:09:40,960 --> 00:09:43,930
called non-parametric learning algorithms that

188
00:09:43,980 --> 00:09:46,480
will help to alleviate the need somewhat

189
00:09:46,560 --> 00:09:49,580
for you to choose features very carefully. Okay?

190
00:09:49,820 --> 00:09:51,380
And this leads us into our discussion

191
00:09:51,450 --> 00:09:53,310
of locally weighted regression.

192
00:09:53,400 --> 00:10:13,130
And just to define the term, linear regression,

193
00:10:13,200 --> 00:10:14,180
as we've defined it so far,

194
00:10:14,280 --> 00:10:16,770
is an example of a parametric learning algorithm.

195
00:10:16,840 --> 00:10:20,270
Parametric learning algorithm is one

196
00:10:20,350 --> 00:10:22,490
that's defined as an algorithm that

197
00:10:22,570 --> 00:10:24,180
has a fixed number of parameters that

198
00:10:24,270 --> 00:10:26,620
fit to the data. Okay?

199
00:10:26,710 --> 00:10:28,800
So in linear regression

200
00:10:28,880 --> 00:10:33,130
we have a fix set of parameters theta, right?

201
00:10:33,190 --> 00:10:34,700
That must fit to the data.

202
00:10:34,790 --> 00:10:41,160
In contrast, what I'm gonna talk about now

203
00:10:41,240 --> 00:10:43,550
is our first non-parametric learning algorithm.

204
00:10:43,630 --> 00:11:02,010
The formal definition, which is not very intuitive,

205
00:11:02,090 --> 00:11:04,390
so I've replaced it with a second, say, more intuitive.

206
00:11:04,490 --> 00:11:08,260
The, sort of, formal definition of the non-

207
00:11:08,350 --> 00:11:09,950
parametric learning algorithm is that

208
00:11:10,030 --> 00:11:11,080
it's an algorithm where

209
00:11:11,120 --> 00:11:22,660
the number of parameters goes with M,

210
00:11:22,730 --> 00:11:24,190
with the size of the training set.

211
00:11:24,290 --> 00:11:26,620
And usually it's defined as a number of parameters

212
00:11:26,700 --> 00:11:28,650
grows linearly with the size of the training set.

213
00:11:28,740 --> 00:11:31,720
This is the formal definition.

214
00:11:31,820 --> 00:11:35,900
A slightly less formal definition is that

215
00:11:36,020 --> 00:11:39,660
the amount of stuff that your learning algorithm

216
00:11:39,740 --> 00:11:41,800
needs to keep around will grow linearly

217
00:11:41,890 --> 00:11:43,130
with the training sets or,

218
00:11:43,200 --> 00:11:44,330
in another way of saying it,

219
00:11:44,410 --> 00:11:46,420
is that this is an algorithm that

220
00:11:46,520 --> 00:11:48,340
we'll need to keep around an entire training set,

221
00:11:48,440 --> 00:11:49,930
even after learning. Okay?

222
00:11:50,010 --> 00:11:52,970
So don't worry too much about this definition.

223
00:11:53,070 --> 00:11:56,650
But what I want to do now is describe a specific

224
00:11:56,750 --> 00:11:58,450
non-parametric learning algorithm

225
00:11:58,540 --> 00:12:00,450
called locally weighted regression.

226
00:12:00,550 --> 00:12:11,850
Which also goes by a couple of other names

227
00:12:11,970 --> 00:12:18,640
which also goes by the name of Loess

228
00:12:18,740 --> 00:12:20,370
for self-hysterical reasons.

229
00:12:20,460 --> 00:12:22,590
Loess is usually spelled L-O-E-S-S,

230
00:12:22,700 --> 00:12:24,720
sometimes spelled like that, too.

231
00:12:24,800 --> 00:12:26,340
I just call it locally weighted regression.

232
00:12:26,410 --> 00:12:28,100
So here's the idea.

233
00:12:28,180 --> 00:12:36,450
This will be an algorithm that

234
00:12:36,520 --> 00:12:39,310
allows us to worry a little bit less about

235
00:12:39,410 --> 00:12:41,430
having to choose features very carefully.

236
00:12:41,520 --> 00:12:49,850
So for my motivating example,

237
00:12:49,940 --> 00:12:59,650
let's say that I have a training site

238
00:12:59,740 --> 00:13:02,390
that looks like this, okay? So this is X and that's Y.

239
00:13:02,490 --> 00:13:08,250
If you run linear regression on this

240
00:13:08,350 --> 00:13:10,930
and you fit maybe a linear function to this

241
00:13:11,020 --> 00:13:13,650
and you end up with a more or less flat,

242
00:13:13,740 --> 00:13:15,650
straight line, which is not a very good fit to this data.

243
00:13:15,750 --> 00:13:18,410
You can sit around and stare at this and try to

244
00:13:18,500 --> 00:13:20,170
decide whether the features are used right.

245
00:13:20,280 --> 00:13:21,970
So maybe you want to toss in a quadratic function,

246
00:13:22,060 --> 00:13:24,170
but this isn't really quadratic either.

247
00:13:24,280 --> 00:13:28,810
So maybe you want to model this as a X plus X squared

248
00:13:28,900 --> 00:13:31,210
plus maybe some function of sin of X or something.

249
00:13:31,310 --> 00:13:32,740
You actually sit around and fiddle with features.

250
00:13:32,850 --> 00:13:36,080
And after a while you can probably come up with

251
00:13:36,170 --> 00:13:37,430
a set of features that the model is okay,

252
00:13:37,540 --> 00:13:39,230
but let's talk about an algorithm that

253
00:13:39,310 --> 00:13:41,410
you can use without needing to do that.

254
00:13:41,500 --> 00:13:55,220
So if well, suppose you want to

255
00:13:55,330 --> 00:14:00,160
evaluate your hypothesis H at a certain point

256
00:14:00,270 --> 00:14:06,560
with a certain query point low K is X. Okay?

257
00:14:06,670 --> 00:14:09,050
And let's say you want to know what's

258
00:14:09,150 --> 00:14:13,650
the predicted value of Y at this position of X, right?

259
00:14:13,770 --> 00:14:19,510
So for linear regression, what we were doing

260
00:14:19,620 --> 00:14:29,290
was we would fit theta to minimize sum over I,

261
00:14:29,390 --> 00:14:35,540
YI minus theta, transpose XI squared,

262
00:14:35,660 --> 00:14:42,290
and return theta transpose X. Okay?

263
00:14:42,410 --> 00:14:43,810
So that was linear regression.

264
00:14:43,920 --> 00:14:48,720
In contrast, in locally weighted linear regression

265
00:14:48,830 --> 00:14:50,380
you're going to do things slightly different.

266
00:14:50,500 --> 00:14:52,770
You're going to look at this point X

267
00:14:52,870 --> 00:14:55,790
and then I'm going to look in my data set

268
00:14:55,890 --> 00:15:00,240
and take into account only the data points that are,

269
00:15:00,330 --> 00:15:03,020
sort of, in the little vicinity of X. Okay?

270
00:15:03,130 --> 00:15:05,940
So we'll look at where I want to value my hypothesis.

271
00:15:06,020 --> 00:15:10,980
I'm going to look only in the vicinity of this point

272
00:15:11,060 --> 00:15:12,730
where I want to value my hypothesis,

273
00:15:12,830 --> 00:15:16,900
and then I'm going to take, let's say,

274
00:15:17,010 --> 00:15:18,130
just these few points,

275
00:15:18,240 --> 00:15:23,730
and I will apply linear regression to fit a straight line

276
00:15:23,820 --> 00:15:26,230
just to this sub-set of the data. Okay?

277
00:15:26,330 --> 00:15:27,730
I'm using this sub-term sub-set

278
00:15:27,830 --> 00:15:29,460
well let's come back to that later.

279
00:15:29,560 --> 00:15:30,910
So we take this data set

280
00:15:30,910 --> 00:15:32,140
and I fit a straight line to it

281
00:15:32,250 --> 00:15:34,760
and maybe I get a straight line like that.

282
00:15:34,890 --> 00:15:40,820
And what I'll do is then evaluate

283
00:15:40,920 --> 00:15:42,280
this particular value of straight line

284
00:15:42,390 --> 00:15:47,200
and that will be the value I return for my algorithm.

285
00:15:47,310 --> 00:15:50,570
I think this would be the predicted value for

286
00:15:50,660 --> 00:15:55,480
this would be the value of then my hypothesis outputs

287
00:15:55,570 --> 00:15:59,120
in locally weighted regression. Okay?

288
00:15:59,220 --> 00:16:06,390
So we're gonna fall one up.

289
00:16:06,440 --> 00:16:08,060
Let me go ahead and formalize that.

290
00:16:08,130 --> 00:16:11,360
In locally weighted regression,

291
00:16:11,460 --> 00:16:31,090
we're going to fit theta to minimize sum over I

292
00:16:31,190 --> 00:16:34,240
to minimize that where these terms W

293
00:16:34,330 --> 00:16:36,290
superscript I are called weights.

294
00:16:36,380 --> 00:16:39,120
There are many possible choice for ways,

295
00:16:39,200 --> 00:16:40,370
I'm just gonna write one down.

296
00:16:40,440 --> 00:16:46,240
So this E's and minus, XI minus X squared over two.

297
00:16:46,320 --> 00:16:52,030
So let's look at what these weights really are, right?

298
00:16:52,130 --> 00:16:53,010
So notice that

299
00:16:53,120 --> 00:16:57,000
suppose you have a training example XI.

300
00:16:57,090 --> 00:17:02,560
So that XI is very close to X. So this is small, right?

301
00:17:02,660 --> 00:17:08,240
Then if XI minus X is small,

302
00:17:08,320 --> 00:17:10,040
so if XI minus X is close to zero,

303
00:17:10,130 --> 00:17:12,400
then this is E's to the minus zero

304
00:17:12,480 --> 00:17:17,640
and E to the zero is one. So if XI is close to X,

305
00:17:17,740 --> 00:17:23,290
then WI will be close to one. In other words,

306
00:17:23,370 --> 00:17:25,040
the weight associated with the,

307
00:17:25,130 --> 00:17:27,180
I training example be close to one if XI

308
00:17:27,260 --> 00:17:29,150
and X are close to each other.

309
00:17:29,250 --> 00:17:40,370
Conversely if XI minus X is large then

310
00:17:40,450 --> 00:17:44,710
I don't know, what would WI be?

311
00:17:44,800 --> 00:17:48,000
Student:Zero.

312
00:17:48,090 --> 00:17:49,550
Instructor (Andrew Ng):Zero, right. Close to zero.

313
00:17:49,640 --> 00:17:53,280
Right. So if XI is very far from X

314
00:17:53,350 --> 00:17:55,810
then this is E to the minus of some large number

315
00:17:55,930 --> 00:17:58,330
and E to the minus some large number

316
00:17:58,410 --> 00:18:00,540
will be close to zero. Okay?

317
00:18:00,640 --> 00:18:14,570
So the picture is, if I'm quarrying at a certain point X,

318
00:18:14,660 --> 00:18:22,540
shown on the X axis, and if my data set, say,

319
00:18:22,620 --> 00:18:25,900
look like that, then I'm going to give the points

320
00:18:25,980 --> 00:18:27,610
close to this a large weight

321
00:18:27,720 --> 00:18:30,090
and give the points far away a small weight.

322
00:18:30,200 --> 00:18:37,330
So for the points that are far away,

323
00:18:37,410 --> 00:18:39,180
WI will be close to zero.

324
00:18:39,280 --> 00:18:41,610
And so as if for the points that are far away,

325
00:18:41,720 --> 00:18:45,570
they will not contribute much

326
00:18:45,690 --> 00:18:47,340
at all to this summation, right?

327
00:18:47,410 --> 00:18:49,030
So I think this is sum over I

328
00:18:49,120 --> 00:18:52,450
of one times this quadratic term for points by points

329
00:18:52,540 --> 00:18:55,830
plus zero times this quadratic term for faraway points.

330
00:18:55,910 --> 00:18:58,060
And so the effect of using this weighting is that

331
00:18:58,120 --> 00:19:00,140
locally weighted linear regression

332
00:19:00,210 --> 00:19:01,820
fits a set of parameters theta,

333
00:19:01,890 --> 00:19:04,110
paying much more attention

334
00:19:04,170 --> 00:19:07,390
to fitting the points close by accurately.

335
00:19:07,460 --> 00:19:11,930
Whereas ignoring the contribution from faraway points.

336
00:19:12,000 --> 00:19:13,410
Okay? Yeah?

337
00:19:13,490 --> 00:19:17,060
Student:Your Y is exponentially  [inaudible]?

338
00:19:17,130 --> 00:19:18,330
Instructor (Andrew Ng):Yeah. Let's see.

339
00:19:18,380 --> 00:19:20,230
So it turns out there are many other

340
00:19:20,320 --> 00:19:21,710
weighting functions you can use.

341
00:19:21,830 --> 00:19:24,520
It turns out that there are definitely different

342
00:19:24,640 --> 00:19:26,360
communities of researchers that tend to

343
00:19:26,450 --> 00:19:28,400
choose different choices by default.

344
00:19:28,500 --> 00:19:31,740
There is somewhat of a literature

345
00:19:31,820 --> 00:19:33,010
on debating what point

346
00:19:33,010 --> 00:19:34,670
exactly what function to use.

347
00:19:34,770 --> 00:19:35,990
This, sort of,

348
00:19:35,990 --> 00:19:37,350
exponential decay function is

349
00:19:37,460 --> 00:19:39,910
this happens to be a reasonably common one that

350
00:19:39,980 --> 00:19:42,120
seems to be a more reasonable choice on many problems,

351
00:19:42,210 --> 00:19:43,950
but you can actually plug in other functions as well.

352
00:19:44,060 --> 00:19:47,420
Did I mention what [inaudible] is it at?

353
00:19:47,530 --> 00:19:48,780
For those of you that are familiar

354
00:19:48,890 --> 00:19:52,270
with the normal distribution,

355
00:19:52,380 --> 00:19:54,340
or the Gaussian distribution, say this

356
00:19:54,450 --> 00:19:55,720
what this formula I've written out here,

357
00:19:55,720 --> 00:19:57,000
it cosmetically looks a bit like a Gaussian distribution.

358
00:19:57,110 --> 00:20:00,880
Okay? But this actually has absolutely nothing

359
00:20:00,970 --> 00:20:03,720
to do with Gaussian distribution.

360
00:20:03,800 --> 00:20:05,320
So this is not that a problem with XI is Gaussian or whatever.

361
00:20:05,420 --> 00:20:10,810
This is no such interpretation.

362
00:20:10,890 --> 00:20:12,540
This is just a convenient function that

363
00:20:12,630 --> 00:20:14,820
happens to be a bell-shaped function,

364
00:20:14,890 --> 00:20:19,440
but don't endow this of any Gaussian semantics.

365
00:20:19,520 --> 00:20:20,910
Okay?

366
00:20:20,980 --> 00:20:23,730
So, in fact well, if you remember

367
00:20:23,810 --> 00:20:26,740
the familiar bell-shaped Gaussian, again,

368
00:20:26,820 --> 00:20:29,950
it's just the ways of associating

369
00:20:30,000 --> 00:20:34,450
with these points is that if you imagine putting this

370
00:20:34,510 --> 00:20:37,390
on a bell-shaped bump, centered around the position

371
00:20:37,450 --> 00:20:39,430
of where you want to value your hypothesis H,

372
00:20:39,500 --> 00:20:43,560
then there's a saying this point here I'll give a weight

373
00:20:43,640 --> 00:20:46,010
that's proportional to the height of the Gaussian

374
00:20:46,090 --> 00:20:48,050
excuse me, to the height of the bell-shaped function

375
00:20:48,110 --> 00:20:49,920
evaluated at this point.

376
00:20:50,000 --> 00:20:52,520
And the way to get to this point will be,

377
00:20:52,610 --> 00:20:53,990
to this training example,

378
00:20:54,080 --> 00:20:58,290
will be proportionate to that height and so on. Okay?

379
00:20:58,360 --> 00:20:59,840
And so training examples that

380
00:20:59,900 --> 00:21:02,500
are really far away get a very small weight.

381
00:21:02,580 --> 00:21:09,890
One last small generalization to this is that

382
00:21:09,970 --> 00:21:13,940
normally there's one other parameter to this algorithm,

383
00:21:14,020 --> 00:21:16,810
which I'll denote as tow.

384
00:21:16,900 --> 00:21:19,380
Again, this looks suspiciously

385
00:21:19,450 --> 00:21:20,970
like the variants of a Gaussian,

386
00:21:21,050 --> 00:21:22,280
but this is not a Gaussian.

387
00:21:22,360 --> 00:21:24,030
This is a convenient form or function.

388
00:21:24,120 --> 00:21:33,250
This parameter tow is called the bandwidth parameter

389
00:21:41,790 --> 00:21:43,370
and informally it controls

390
00:21:43,460 --> 00:21:46,410
how fast the weights fall of with distance. Okay?

391
00:21:46,500 --> 00:21:49,990
So just copy my diagram from the other side, I guess.

392
00:21:53,760 --> 00:21:58,450
So if tow is very small, if that's a query X,

393
00:21:58,530 --> 00:22:01,860
then you end up choosing a fairly narrow Gaussian

394
00:22:01,940 --> 00:22:04,030
excuse me, a fairly narrow bell shape,

395
00:22:04,100 --> 00:22:06,540
so that the weights of the points

396
00:22:06,600 --> 00:22:08,030
are far away fall off rapidly.

397
00:22:08,110 --> 00:22:10,330
Whereas if tow is large

398
00:22:20,920 --> 00:22:22,770
then you'd end up choosing a weighting function

399
00:22:22,860 --> 00:22:24,580
that falls of relatively slowly

400
00:22:24,640 --> 00:22:27,520
with distance from your query. Okay?

401
00:22:29,590 --> 00:22:34,280
So I hope you can, therefore,

402
00:22:34,360 --> 00:22:43,570
see that if you apply locally weighted linear regression

403
00:22:43,660 --> 00:22:45,010
to a data set that looks like this,

404
00:22:45,080 --> 00:22:48,270
then to ask what your hypothesis output

405
00:22:48,370 --> 00:22:50,120
is at a point like this you end up

406
00:22:50,220 --> 00:22:52,230
having a straight line making that prediction.

407
00:22:52,330 --> 00:22:54,560
To ask what kind of class this output

408
00:22:54,630 --> 00:22:57,540
at that value you put a straight line there

409
00:22:57,620 --> 00:22:59,880
and you predict that value.

410
00:22:59,960 --> 00:23:01,650
It turns out that every time

411
00:23:01,740 --> 00:23:03,690
you try to vary your hypothesis,

412
00:23:03,750 --> 00:23:05,330
every time you ask your learning algorithm

413
00:23:05,400 --> 00:23:07,130
to make a prediction for how much

414
00:23:07,210 --> 00:23:08,410
a new house costs or whatever,

415
00:23:08,490 --> 00:23:11,800
you need to run a new fitting procedure

416
00:23:11,890 --> 00:23:15,860
and then evaluate this line that you fit just

417
00:23:15,930 --> 00:23:18,980
at the position of the value of X.

418
00:23:19,060 --> 00:23:20,400
So the position of the query where

419
00:23:20,480 --> 00:23:22,850
you're trying to make a prediction. Okay?

420
00:23:22,920 --> 00:23:24,020
But if you do this for

421
00:23:24,020 --> 00:23:25,220
every point along the X-axis

422
00:23:25,290 --> 00:23:28,210
then you find that locally weighted regression

423
00:23:28,290 --> 00:23:29,880
is able to trace on this, sort of,

424
00:23:29,960 --> 00:23:33,760
very non-linear curve for a data set like this. Okay?

425
00:23:33,830 --> 00:23:39,340
So in the problem set we're actually gonna

426
00:23:39,430 --> 00:23:41,580
let you play around more with this algorithm.

427
00:23:41,670 --> 00:23:43,120
So I won't say too much more about it here.

428
00:23:43,200 --> 00:23:45,890
But to finally move on to the next topic

429
00:23:45,970 --> 00:23:48,140
let me check the questions you have. Yeah?

430
00:23:48,210 --> 00:23:50,760
Student:It seems like you still have

431
00:23:50,820 --> 00:23:52,510
the same problem of overfitting and underfitting,

432
00:23:52,580 --> 00:23:54,160
like when you had a Q's tow.

433
00:23:54,240 --> 00:23:57,820
Like you make it too small in your

434
00:23:57,880 --> 00:24:00,140
Instructor (Andrew Ng):Yes, absolutely. Yes.

435
00:24:00,210 --> 00:24:03,650
So locally weighted regression can run into

436
00:24:03,710 --> 00:24:06,130
locally weighted regression is not a penancier

437
00:24:06,220 --> 00:24:08,090
for the problem of overfitting or underfitting.

438
00:24:08,170 --> 00:24:12,070
You can still run into the same problems

439
00:24:12,150 --> 00:24:13,460
with locally weighted regression.

440
00:24:13,550 --> 00:24:15,450
What you just said about

441
00:24:16,340 --> 00:24:19,110
and so some of these things I'll leave you

442
00:24:19,190 --> 00:24:20,890
to discover for yourself in the homework problem.

443
00:24:20,950 --> 00:24:23,910
You'll actually see what you just mentioned. Yeah?

444
00:24:23,910 --> 00:24:31,690
Student: [inaudible]

445
00:24:31,780 --> 00:24:32,610
Instructor (Andrew Ng):Yeah.

446
00:24:32,690 --> 00:24:34,290
Student:I'm just trying to think of  [inaudible]

447
00:24:34,340 --> 00:24:36,930
the original data points.

448
00:24:37,010 --> 00:24:37,960
Instructor (Andrew Ng):Right.

449
00:24:38,030 --> 00:24:41,030
So the question is, sort of, this

450
00:24:41,090 --> 00:24:42,690
it's almost as if you're not building a model,

451
00:24:42,760 --> 00:24:44,320
because you need the entire data set.

452
00:24:44,390 --> 00:24:46,780
And the other way of saying that is that

453
00:24:46,850 --> 00:24:48,220
this is a non-parametric learning algorithm.

454
00:24:48,220 --> 00:24:49,220
So this –I don't know.

455
00:24:52,730 --> 00:24:55,570
I won't debate whether, you know,

456
00:24:55,650 --> 00:24:56,970
are we really building a model or not.

457
00:24:57,060 --> 00:24:59,840
But this is a perfectly fine

458
00:24:59,930 --> 00:25:03,230
so if I think when you write a code implementing

459
00:25:03,320 --> 00:25:07,400
locally weighted linear regression on the data set

460
00:25:07,500 --> 00:25:09,450
I think of that code as a whole

461
00:25:09,540 --> 00:25:11,220
as building your model.

462
00:25:11,320 --> 00:25:14,060
So it actually uses

463
00:25:14,140 --> 00:25:16,810
we've actually used this quite successfully to model,

464
00:25:16,880 --> 00:25:18,170
sort of, the dynamics of

465
00:25:18,230 --> 00:25:21,450
this autonomous helicopter this is. Yeah?

466
00:25:21,530 --> 00:25:23,550
Student:I ask if this algorithm that

467
00:25:23,620 --> 00:25:25,610
learn the weights based on the data?

468
00:25:25,700 --> 00:25:28,580
Instructor (Andrew Ng):Learn what weights?

469
00:25:28,660 --> 00:25:29,770
Oh, the weights WI.

470
00:25:29,840 --> 00:25:32,430
Student:Instead of using [inaudible].

471
00:25:32,480 --> 00:25:33,400
Instructor (Andrew Ng):I see, yes.

472
00:25:33,480 --> 00:25:35,700
So it turns out there are a few things you can do.

473
00:25:35,770 --> 00:25:37,020
One thing that is quite common is

474
00:25:37,080 --> 00:25:40,490
how to choose this band with parameter tow, right?

475
00:25:40,540 --> 00:25:42,560
As using the data. We'll actually talk about that

476
00:25:42,620 --> 00:25:44,390
a bit later when we talk about model selection.

477
00:25:44,470 --> 00:25:46,370
Yes? One last question.

478
00:25:46,460 --> 00:25:48,840
Student:I used [inaudible] Gaussian sometimes

479
00:25:48,950 --> 00:25:52,300
if you [inaudible] Gaussian and then

480
00:25:52,410 --> 00:25:59,230
Instructor (Andrew Ng):Oh, I guess. Lt's see. Boy.

481
00:25:59,320 --> 00:26:03,570
The weights are not random variables and it's not,

482
00:26:03,680 --> 00:26:04,950
for the purpose of this algorithm,

483
00:26:05,040 --> 00:26:07,810
it is not useful to endow it with probable semantics.

484
00:26:07,920 --> 00:26:10,660
So you could choose to define things as Gaussian,

485
00:26:10,730 --> 00:26:12,280
but it, sort of, doesn't lead anywhere.

486
00:26:12,360 --> 00:26:18,680
In fact, it turns out that I happened to choose this,

487
00:26:18,740 --> 00:26:22,680
sort of, bell-shaped function to define my weights.

488
00:26:22,760 --> 00:26:24,960
It's actually fine to choose a function that

489
00:26:25,030 --> 00:26:26,350
doesn't even integrate to one,

490
00:26:26,470 --> 00:26:27,660
that integrates to infinity, say,

491
00:26:27,760 --> 00:26:29,020
as you're weighting function.

492
00:26:29,230 --> 00:26:31,890
So in that sense, I mean,

493
00:26:31,990 --> 00:26:34,290
you could force in the definition of a Gaussian,

494
00:26:34,390 --> 00:26:35,620
but it's, sort of, not useful.

495
00:26:35,700 --> 00:26:38,260
Especially since you use other functions that

496
00:26:38,320 --> 00:26:41,700
integrate to infinity and don't integrate to one. Okay?

497
00:26:41,790 --> 00:26:43,900
It's the last question and let's move on

498
00:26:43,990 --> 00:26:46,670
Student:Assume that we have a very huge [inaudible],

499
00:26:46,770 --> 00:26:49,600
for example. A very huge set of houses and

500
00:26:49,690 --> 00:26:52,810
want to predict the linear for each house

501
00:26:52,900 --> 00:26:56,760
and so should the end result for each input

502
00:26:56,830 --> 00:26:59,360
I'm seeing this very constantly for

503
00:26:59,460 --> 00:27:00,570
Instructor (Andrew Ng):Yes, you're right.

504
00:27:00,640 --> 00:27:03,490
So because locally weighted regression

505
00:27:03,580 --> 00:27:07,400
is a non-parametric algorithm every time

506
00:27:07,510 --> 00:27:09,720
you make a prediction you need to fit theta to

507
00:27:09,800 --> 00:27:10,910
your entire training set again.

508
00:27:11,000 --> 00:27:12,850
So you're actually right.

509
00:27:12,940 --> 00:27:14,950
If you have a very large training set

510
00:27:15,040 --> 00:27:17,950
then this is a somewhat expensive algorithm to use.

511
00:27:18,040 --> 00:27:19,480
Because every time you want to

512
00:27:19,540 --> 00:27:22,040
make a prediction you need to fit a straight line

513
00:27:22,130 --> 00:27:24,470
to a huge data set again.

514
00:27:24,560 --> 00:27:26,970
Turns out there are algorithms that

515
00:27:27,050 --> 00:27:29,740
turns out there are ways to make this

516
00:27:29,840 --> 00:27:32,100
much more efficient for large data sets as well.

517
00:27:32,200 --> 00:27:33,410
So don't want to talk about that.

518
00:27:33,500 --> 00:27:35,140
If you're interested, look up the work

519
00:27:35,200 --> 00:27:37,320
of Andrew Moore on KD-trees.

520
00:27:37,410 --> 00:27:40,280
He, sort of, figured out ways

521
00:27:40,330 --> 00:27:41,640
to fit these models much more efficiently.

522
00:27:41,720 --> 00:27:44,070
That's not something I want to go into today. Okay?

523
00:27:44,170 --> 00:27:45,830
Let me move one.

524
00:27:45,910 --> 00:27:46,990
Let's take more questions later.

525
00:28:00,750 --> 00:28:02,320
So, okay. So that's locally weighted regression.

526
00:28:02,410 --> 00:28:07,990
Remember the outline I had, I guess,

527
00:28:08,070 --> 00:28:09,260
at the beginning of this lecture.

528
00:28:09,340 --> 00:28:12,030
What I want to do now is talk about a

529
00:28:12,110 --> 00:28:15,200
probabilistic interpretation of linear regression, all right?

530
00:28:15,290 --> 00:28:16,780
And in particular of the

531
00:28:16,870 --> 00:28:19,340
it'll be this probabilistic interpretation that

532
00:28:19,440 --> 00:28:23,350
let's us move on to talk about logistic regression,

533
00:28:23,440 --> 00:28:25,100
which will be our first classification algorithm.

534
00:28:39,830 --> 00:28:42,130
So let's put aside locally weighted regression for now.

535
00:28:42,170 --> 00:28:43,280
We'll just talk about

536
00:28:43,360 --> 00:28:45,410
ordinary unweighted linear regression.

537
00:28:45,490 --> 00:28:47,800
Let's ask the question of

538
00:28:47,800 --> 00:28:49,730
why least squares, right?

539
00:28:49,830 --> 00:28:51,280
Of all the things we could optimize

540
00:28:51,370 --> 00:28:54,040
how do we come up with this criteria

541
00:28:54,130 --> 00:28:56,630
for minimizing the square of the area between

542
00:28:56,740 --> 00:28:58,330
the predictions of the hypotheses

543
00:28:58,400 --> 00:29:00,640
and the values Y predicted.

544
00:29:00,720 --> 00:29:04,270
So why not minimize the absolute value ofthe areas

545
00:29:04,360 --> 00:29:06,330
or the areas to the power of four or something?

546
00:29:06,410 --> 00:29:11,440
What I'm going to do now is present one set of assumptions

547
00:29:11,520 --> 00:29:13,900
that will serve to "justify"

548
00:29:13,990 --> 00:29:17,270
why we're minimizing the sum of square zero. Okay?

549
00:29:17,380 --> 00:29:20,360
It turns out that there are many assumptions

550
00:29:20,460 --> 00:29:23,610
that are sufficient to justify why we do least squares

551
00:29:23,700 --> 00:29:24,760
and this is just one of them.

552
00:29:24,880 --> 00:29:29,330
So just because I present one set of assumptions

553
00:29:29,440 --> 00:29:31,400
under which least squares regression make sense,

554
00:29:31,490 --> 00:29:34,010
but this is not the only set of assumptions.

555
00:29:34,090 --> 00:29:36,480
So even if the assumptions I describe don't hold,

556
00:29:36,590 --> 00:29:38,510
least squares actually still makes sense

557
00:29:38,580 --> 00:29:39,580
in many circumstances.

558
00:29:39,670 --> 00:29:41,770
But this sort of new help, you know,

559
00:29:41,840 --> 00:29:43,320
give one rationalization, like,

560
00:29:43,390 --> 00:29:44,610
one reason for doing least squares regression.

561
00:29:44,710 --> 00:29:49,670
And, in particular, what I'm going to do

562
00:29:49,770 --> 00:29:53,010
is endow the least squares model

563
00:29:53,110 --> 00:29:54,590
with probabilistic semantics.

564
00:29:54,670 --> 00:29:59,480
So let's assume in our example of

565
00:29:59,560 --> 00:30:00,690
predicting housing prices,

566
00:30:00,760 --> 00:30:05,050
that the price of the house it's sold four,

567
00:30:05,100 --> 00:30:09,610
and there's going to be some linear function

568
00:30:09,690 --> 00:30:16,840
of the features, plus some term epsilon I. Okay?

569
00:30:16,920 --> 00:30:20,990
And epsilon I will be our error term.

570
00:30:21,080 --> 00:30:23,880
You can think of the error term

571
00:30:23,970 --> 00:30:26,840
as capturing unmodeled effects, like,

572
00:30:26,930 --> 00:30:29,430
that maybe there's some other features of a house,

573
00:30:29,510 --> 00:30:31,720
like, maybe how many fireplaces it has or whether

574
00:30:31,800 --> 00:30:33,010
there's a garden or whatever,

575
00:30:33,100 --> 00:30:36,320
that there are additional features that

576
00:30:36,380 --> 00:30:37,600
we jut fail to capture

577
00:30:37,670 --> 00:30:39,750
or you can think of epsilon as random noise.

578
00:30:39,800 --> 00:30:41,280
Epsilon is our error term that captures

579
00:30:41,360 --> 00:30:43,320
both these unmodeled effects.

580
00:30:43,400 --> 00:30:45,150
Just things we forgot to model.

581
00:30:45,210 --> 00:30:46,980
Maybe the function isn't quite linear or something.

582
00:30:47,070 --> 00:30:53,670
As well as random noise, like maybe that day the seller

583
00:30:53,760 --> 00:30:56,010
was in a really bad mood and so he sold it,

584
00:30:56,120 --> 00:30:57,880
just refused to go for a reasonable price

585
00:30:57,960 --> 00:30:59,760
or something.

586
00:30:59,840 --> 00:31:09,160
And now I will assume that the errors have a probabilistic

587
00:31:09,200 --> 00:31:10,590
have a probability distribution.

588
00:31:10,680 --> 00:31:13,770
I'll assume that the errors epsilon I are distributed

589
00:31:13,850 --> 00:31:19,510
just till they denote epsilon I is distributive

590
00:31:19,600 --> 00:31:21,580
according to a probability distribution.

591
00:31:21,650 --> 00:31:26,130
That's a Gaussian distribution with mean zero

592
00:31:26,220 --> 00:31:28,590
and variance sigma squared. Okay?

593
00:31:28,700 --> 00:31:30,480
So let me just scripts in here,

594
00:31:30,560 --> 00:31:32,510
n stands for normal, right?

595
00:31:32,600 --> 00:31:34,320
To denote a normal distribution,

596
00:31:34,400 --> 00:31:35,540
also known as the Gaussian distribution,

597
00:31:35,620 --> 00:31:38,700
with mean zero and covariance sigma squared.

598
00:31:38,780 --> 00:31:42,670
Actually, just quickly raise your hand

599
00:31:42,760 --> 00:31:44,680
if you've seen a Gaussian distribution before.

600
00:31:44,770 --> 00:31:46,950
Okay, cool. Most of you. Great.

601
00:31:47,010 --> 00:31:52,030
Almost everyone. So, in other words,

602
00:31:52,120 --> 00:31:55,630
the density for Gaussian is what you've seen before.

603
00:31:55,720 --> 00:31:56,910
The density for epsilon I would

604
00:31:56,990 --> 00:32:03,620
be one over root 2 pi sigma, E to the negative,

605
00:32:03,700 --> 00:32:08,860
epsilon I squared over 2 sigma squared, right?

606
00:32:08,950 --> 00:32:17,270
And the density of our epsilon I will be

607
00:32:17,360 --> 00:32:23,630
this bell-shaped curve with one standard

608
00:32:23,710 --> 00:32:28,990
deviation being a, sort of, sigma. Okay?

609
00:32:29,090 --> 00:32:34,320
This is form for that bell-shaped curve.

610
00:32:34,400 --> 00:32:37,530
So, let's see. I can erase that.

611
00:32:37,620 --> 00:32:42,860
Can I erase the board?

612
00:32:42,920 --> 00:33:05,820
So this implies that the probability distribution

613
00:33:05,890 --> 00:33:10,820
of a price of a house given in si

614
00:33:10,910 --> 00:33:14,010
and the parameters theta, that

615
00:33:14,070 --> 00:33:33,300
this is going to be Gaussian with that density. Okay?

616
00:33:33,390 --> 00:33:37,870
In other words, saying goes as that the price

617
00:33:37,960 --> 00:33:43,590
of a house given the features of the house

618
00:33:43,650 --> 00:33:45,090
and my parameters theta,

619
00:33:45,170 --> 00:33:48,220
this is going to be a random variable

620
00:33:48,290 --> 00:33:54,060
that's distributed Gaussian with mean theta

621
00:33:54,140 --> 00:33:58,410
transpose XI and variance sigma squared. Right?

622
00:33:58,490 --> 00:34:02,100
Because we imagine that the way the housing prices

623
00:34:02,200 --> 00:34:05,930
are generated is that the price of a house is equal to

624
00:34:06,000 --> 00:34:08,360
theta transpose XI and then plus some random

625
00:34:08,450 --> 00:34:10,600
Gaussian noise with variance sigma squared.

626
00:34:10,710 --> 00:34:14,190
So the price of a house is going to have mean

627
00:34:14,260 --> 00:34:19,130
theta transpose XI, again, and sigma squared, right?

628
00:34:19,210 --> 00:34:21,000
Does this make sense?

629
00:34:21,080 --> 00:34:25,090
Raise your hand if this makes sense. Yeah, okay.

630
00:34:37,330 --> 00:34:40,340
Lots of you. In point of notation oh, yes?

631
00:34:40,430 --> 00:34:44,740
Student:Assuming we don't know anything

632
00:34:44,790 --> 00:34:46,910
about the error,

633
00:34:46,940 --> 00:34:47,950
why do you assume here the error is a Gaussian?

634
00:34:48,040 --> 00:34:49,220
Instructor (Andrew Ng):Right. So, boy.

635
00:34:49,330 --> 00:34:55,500
Why do I see the error as Gaussian?

636
00:34:55,610 --> 00:34:57,610
Two reasons, right?

637
00:34:57,700 --> 00:34:59,530
One is that it turns out to be mathematically

638
00:34:59,630 --> 00:35:03,610
convenient to do so and the other is, I don't know,

639
00:35:03,710 --> 00:35:05,660
I can also mumble about justifications,

640
00:35:05,750 --> 00:35:07,550
such as things to the central limit theorem.

641
00:35:07,650 --> 00:35:10,060
It turns out that if you, for the vast majority of problems,

642
00:35:10,160 --> 00:35:11,530
if you apply a linear regression model like this

643
00:35:11,650 --> 00:35:14,460
and try to measure the distribution of the errors,

644
00:35:14,560 --> 00:35:17,590
not all the time, but very often you find that

645
00:35:17,650 --> 00:35:18,460
the errors really

646
00:35:18,520 --> 00:35:19,480
are Gaussian.

647
00:35:19,520 --> 00:35:21,650
That this Gaussian model is a good assumption

648
00:35:21,750 --> 00:35:25,250
for the error in regression problems like these.

649
00:35:25,330 --> 00:35:27,090
Some of you may have heard of

650
00:35:27,140 --> 00:35:28,510
the central limit theorem, which says that

651
00:35:28,590 --> 00:35:31,390
the sum of many independent random variables

652
00:35:31,450 --> 00:35:32,590
will tend towards a Gaussian.

653
00:35:32,680 --> 00:35:35,610
So if the error is caused by many effects,

654
00:35:35,700 --> 00:35:38,630
like the mood of the seller, the mood of the buyer,

655
00:35:38,710 --> 00:35:40,350
some other features that we miss,

656
00:35:40,430 --> 00:35:42,900
whether the place has a garden or not,

657
00:35:42,980 --> 00:35:44,310
and if all of these effects are independent,

658
00:35:44,360 --> 00:35:47,320
then by the central limit theorem you might

659
00:35:47,370 --> 00:35:50,720
be inclined to believe that the sum of all these effects

660
00:35:50,780 --> 00:35:52,120
will be approximately Gaussian.

661
00:35:52,180 --> 00:35:54,980
If in practice, I guess, the two real answers are that,

662
00:35:55,030 --> 00:35:56,670
1.) In practice this is actually a

663
00:35:56,770 --> 00:35:58,930
reasonably accurate assumption, and

664
00:35:59,010 --> 00:36:01,500
2.)Is it turns out

665
00:36:01,610 --> 00:36:02,810
to be mathematically convenient to do so.

666
00:36:02,920 --> 00:36:04,620
Okay? Yeah?

667
00:36:04,700 --> 00:36:08,720
Student:It seems like we're saying if we assume that

668
00:36:08,770 --> 00:36:11,090
area around model has zero mean,

669
00:36:11,170 --> 00:36:14,420
then the area is centered around our model.

670
00:36:14,510 --> 00:36:17,680
Which it seems almost like we're trying to

671
00:36:17,750 --> 00:36:18,900
assume what we're trying to prove.

672
00:36:18,990 --> 00:36:22,530
Instructor: That's the [inaudible] but, yes.

673
00:36:22,630 --> 00:36:24,850
You are assuming that the error has zero mean.

674
00:36:24,960 --> 00:36:30,210
Which is, yeah, right. I think later this quarter

675
00:36:30,270 --> 00:36:31,530
we get to some of the other things,

676
00:36:31,580 --> 00:36:33,610
but for now just think of this as a mathematically

677
00:36:33,690 --> 00:36:36,160
it's actually not an unreasonable assumption.

678
00:36:36,250 --> 00:36:42,210
I guess, in machine learning all the assumptions

679
00:36:42,300 --> 00:36:46,690
we make are almost never true in the absence sense, right?

680
00:36:46,760 --> 00:36:50,280
Because, for instance, housing prices are priced to

681
00:36:50,380 --> 00:36:53,810
dollars and cents, so the error will be

682
00:36:53,890 --> 00:36:56,720
errors in prices are not continued

683
00:36:56,780 --> 00:36:57,780
as value random variables,

684
00:36:57,890 --> 00:37:01,340
because houses can only be priced at a certain number

685
00:37:01,400 --> 00:37:03,330
of dollars and a certain number of cents and you never

686
00:37:03,430 --> 00:37:05,560
have fractions of cents in housing prices.

687
00:37:05,670 --> 00:37:07,650
Whereas a Gaussian random variable would.

688
00:37:07,720 --> 00:37:10,230
So in that sense, assumptions we make

689
00:37:10,330 --> 00:37:11,610
are never "absolutely true,"

690
00:37:11,720 --> 00:37:12,730
but for practical purposes this

691
00:37:12,840 --> 00:37:15,910
is a accurate enough assumption that

692
00:37:15,960 --> 00:37:18,420
it'll be useful to make. Okay?

693
00:37:18,510 --> 00:37:21,140
I think in a week or two,

694
00:37:21,250 --> 00:37:25,410
we'll actually come back to selected more about

695
00:37:25,510 --> 00:37:26,980
the assumptions we make and when they help our

696
00:37:27,060 --> 00:37:27,950
learning algorithms and when

697
00:37:28,030 --> 00:37:29,240
they hurt our learning algorithms.

698
00:37:29,310 --> 00:37:31,210
We'll say a bit more about it when

699
00:37:31,300 --> 00:37:32,650
we talk about generative and discriminative

700
00:37:32,730 --> 00:37:35,680
learning algorithms, like, in a week or two. Okay?

701
00:37:35,770 --> 00:37:42,720
So let's point out one bit of notation,

702
00:37:42,920 --> 00:37:46,040
which is that when I wrote this down I actually

703
00:37:46,120 --> 00:37:48,910
wrote P of YI given XI and then semicolon theta

704
00:37:49,030 --> 00:37:51,300
and I'm going to use this notation

705
00:37:51,370 --> 00:37:55,940
when we are not thinking of theta as a random variable.

706
00:37:56,000 --> 00:37:58,200
So in statistics, though,

707
00:37:58,280 --> 00:38:01,100
sometimes it's called the frequentist's point of view,

708
00:38:01,210 --> 00:38:04,210
where you think of there as being some, sort of,

709
00:38:04,250 --> 00:38:05,280
true value of theta that's out there

710
00:38:05,390 --> 00:38:06,780
that's generating the data say,

711
00:38:06,850 --> 00:38:09,020
but we don't know what theta is,

712
00:38:09,130 --> 00:38:11,650
but theta is not a random variable, right?

713
00:38:11,740 --> 00:38:13,680
So it's not like there's some random

714
00:38:13,900 --> 00:38:16,000
value of theta out there. It's that theta is

715
00:38:16,090 --> 00:38:18,550
there's some true value of theta out there.

716
00:38:18,590 --> 00:38:19,510
It's just that we don't know

717
00:38:19,590 --> 00:38:20,720
what the true value of theta is.

718
00:38:20,810 --> 00:38:25,110
So if theta is not a random variable, then

719
00:38:25,190 --> 00:38:29,950
I'm going to avoid writing P of YI given XI comma theta,

720
00:38:30,030 --> 00:38:33,700
because this would mean that probably of YI

721
00:38:33,780 --> 00:38:35,560
conditioned on X and theta and you can

722
00:38:35,640 --> 00:38:37,760
only condition on random variables.

723
00:38:37,860 --> 00:38:41,550
So at this part of the class where

724
00:38:41,670 --> 00:38:44,250
we're taking sort of frequentist's viewpoint

725
00:38:44,340 --> 00:38:45,620
rather than the Bayesian viewpoint,

726
00:38:45,710 --> 00:38:47,390
in this part of class we're thinking of theta

727
00:38:47,470 --> 00:38:48,530
not as a random variable,

728
00:38:48,620 --> 00:38:50,390
but just as something we're trying to estimate

729
00:38:50,440 --> 00:38:51,960
and use the semicolon notation.

730
00:38:52,060 --> 00:38:55,460
So the way to read this is this is the probability

731
00:38:55,520 --> 00:39:00,410
of YI given XI and parameterized by theta. Okay?

732
00:39:00,490 --> 00:39:02,910
So you read the semicolon as parameterized by.

733
00:39:03,030 --> 00:39:04,750
And in the same way here,

734
00:39:04,830 --> 00:39:07,750
I'll say YI given XI parameterized by theta

735
00:39:07,840 --> 00:39:10,280
is distributed Gaussian with that. All right.

736
00:39:36,150 --> 00:39:37,800
So we're gonna make one more assumption.

737
00:39:37,920 --> 00:39:50,670
Let's assume that the error terms are IID, okay?

738
00:39:50,790 --> 00:39:52,950
Which stands for Independently and Identically Distributed.

739
00:39:53,060 --> 00:39:55,890
So it's going to assume that the error terms

740
00:39:56,000 --> 00:39:58,980
are independent of each other, right?

741
00:39:59,050 --> 00:40:13,630
The identically distributed part just means that

742
00:40:13,670 --> 00:40:15,460
I'm assuming the outcome for the same Gaussian

743
00:40:15,530 --> 00:40:17,130
distribution or the same variance,

744
00:40:18,890 --> 00:40:20,440
but the more important part of is this is that

745
00:40:20,550 --> 00:40:22,410
I'm assuming that the epsilon I's

746
00:40:22,460 --> 00:40:23,730
are independent of each other.

747
00:40:23,840 --> 00:40:26,910
Now, let's talk about how to fit a model.

748
00:40:27,010 --> 00:40:33,530
The probability of Y given X parameterized by theta

749
00:40:33,640 --> 00:40:38,100
I'm actually going to give this another name.

750
00:40:38,210 --> 00:40:39,480
I'm going to write this down

751
00:40:39,560 --> 00:40:41,790
and we'll call this the likelihood of theta

752
00:40:41,870 --> 00:40:44,950
as the probability of Y given X parameterized by theta.

753
00:40:45,000 --> 00:40:49,680
And so this is going to be the product

754
00:40:49,750 --> 00:40:57,490
over my training set like that.

755
00:40:57,560 --> 00:41:04,380
Which is, in turn, going to be a product

756
00:41:04,450 --> 00:41:05,650
of those Gaussian densities that

757
00:41:05,690 --> 00:41:08,650
I wrote down just now, right? Okay?

758
00:41:08,770 --> 00:41:22,600
Then in parts of notation, I guess,

759
00:41:22,680 --> 00:41:25,560
I define this term here to be the likelihood of theta.

760
00:41:25,640 --> 00:41:27,570
And the likely of theta is just

761
00:41:27,640 --> 00:41:29,720
the probability of the data Y, right?

762
00:41:29,810 --> 00:41:31,380
Given X and prioritized by theta.

763
00:41:31,460 --> 00:41:35,400
To test the likelihood and probability are often confused.

764
00:41:35,440 --> 00:41:40,160
So the likelihood of theta is the same thing

765
00:41:40,240 --> 00:41:41,910
as the probability of the data you saw.

766
00:41:41,980 --> 00:41:43,140
So likely and probably are,

767
00:41:43,140 --> 00:41:44,880
sort of, the same thing.

768
00:41:44,970 --> 00:41:47,190
Except that when I use the term likelihood

769
00:41:47,590 --> 00:41:50,850
I'm trying to emphasize that I'm taking this thing

770
00:41:50,960 --> 00:41:54,990
and viewing it as a function of theta. Okay?

771
00:41:55,070 --> 00:41:56,640
So likelihood

772
00:41:56,640 --> 00:41:58,800
and for probability,

773
00:41:58,910 --> 00:42:00,830
they're really the same thing except that

774
00:42:00,950 --> 00:42:03,180
when I want to view this thing as a function of

775
00:42:03,280 --> 00:42:06,910
theta holding X and Y fix are then called likelihood.

776
00:42:07,010 --> 00:42:11,030
Okay? So hopefully you hear me say

777
00:42:11,120 --> 00:42:12,360
the likelihood of the parameters

778
00:42:12,470 --> 00:42:15,490
and the probability of the data, right?

779
00:42:15,570 --> 00:42:16,890
Rather than the likelihood of the data

780
00:42:17,000 --> 00:42:18,120
or probability of parameters.

781
00:42:18,230 --> 00:42:19,870
So try to be consistent in that terminology.

782
00:42:19,960 --> 00:42:25,600
So given that the probability

783
00:42:25,610 --> 00:42:32,990
of the data is this

784
00:42:33,100 --> 00:42:35,390
and this is also the likelihood of the parameters,

785
00:42:35,500 --> 00:42:37,000
how do you estimate

786
00:42:37,000 --> 00:42:39,260
the parameters theta?

787
00:42:39,370 --> 00:42:42,310
So given a training set, what parameters theta

788
00:42:42,360 --> 00:42:43,510
do you want to choose for your model?

789
00:42:57,690 --> 00:43:00,980
Well, the principle of maximum likelihood

790
00:43:01,070 --> 00:43:10,070
estimation says that, right?

791
00:43:10,170 --> 00:43:12,130
You can choose the value of theta that

792
00:43:12,240 --> 00:43:15,660
makes the data as probable as possible, right?

793
00:43:15,760 --> 00:43:24,750
So choose theta to maximize the likelihood.

794
00:43:24,870 --> 00:43:29,430
Or in other words choose the parameters that

795
00:43:29,630 --> 00:43:32,030
make the data as probable as possible, right?

796
00:43:32,140 --> 00:43:35,460
So this is massive likely your estimation

797
00:43:35,580 --> 00:43:37,040
from six to six. So it's choose the parameters that

798
00:43:37,130 --> 00:43:39,910
makes it as likely as probable as possible for me

799
00:43:40,000 --> 00:43:41,460
to have seen the data I just did.

800
00:43:41,560 --> 00:43:48,150
So for mathematical convenience,

801
00:43:48,220 --> 00:43:50,760
let me define lower case l of theta.

802
00:43:50,830 --> 00:43:55,010
This is called the log likelihood function

803
00:43:55,100 --> 00:43:59,650
and it's just log of capital L of theta.

804
00:43:59,740 --> 00:44:04,490
So this is log over product over I

805
00:44:04,490 --> 00:44:10,030
to find sigma E to that.

806
00:44:10,110 --> 00:44:11,630
I won't bother to write out

807
00:44:11,690 --> 00:44:13,040
what's in the exponent for now.

808
00:44:13,110 --> 00:44:15,460
It's just saying this from the previous board.

809
00:44:15,500 --> 00:44:18,350
Log and a product is the same

810
00:44:18,390 --> 00:44:19,690
as the sum of over logs, right?

811
00:44:20,760 --> 00:44:25,450
So it's a sum of the logs of

812
00:44:25,560 --> 00:44:39,280
which simplifies to m times one over root

813
00:44:39,370 --> 00:44:48,680
two pi sigma plus and then log of

814
00:44:48,720 --> 00:44:51,340
exponentiation cancel each other, right?

815
00:44:51,460 --> 00:44:52,390
So if log of E of something

816
00:44:52,480 --> 00:44:54,220
is just whatever's inside the exponent.

817
00:44:54,320 --> 00:45:02,440
So, you know what, let me write this on the next board.

818
00:45:02,520 --> 00:45:51,430
Okay. So maximizing the likelihood

819
00:45:51,520 --> 00:45:58,740
or maximizing the log likelihood is the same as

820
00:45:58,820 --> 00:46:03,910
minimizing that term over there.

821
00:46:19,620 --> 00:46:23,010
Well, you get it, right? Because there's a minus sign.

822
00:46:23,090 --> 00:46:25,180
So maximizing this because of the minus sign

823
00:46:25,250 --> 00:46:28,000
is the same as minimizing this as a function of theta.

824
00:46:28,130 --> 00:46:36,170
And this is, of course, just the same

825
00:46:36,270 --> 00:46:38,250
quadratic cos function that we had last time,

826
00:46:38,370 --> 00:46:40,860
J of theta, right?

827
00:46:40,950 --> 00:46:45,960
So what we've just shown is that

828
00:46:46,040 --> 00:46:47,600
the ordinary least squares algorithm,

829
00:46:47,680 --> 00:46:50,020
that we worked on the previous lecture,

830
00:46:50,080 --> 00:46:55,220
is just maximum likelihood assuming

831
00:46:55,310 --> 00:46:57,420
this probabilistic model,

832
00:46:57,510 --> 00:47:03,430
assuming IID Gaussian errors on our data. Okay?

833
00:47:03,540 --> 00:47:13,900
One thing that we'll actually leave is that,

834
00:47:13,980 --> 00:47:15,750
in the next lecture notice that the value of

835
00:47:15,810 --> 00:47:17,540
sigma squared doesn't matter, right?

836
00:47:17,590 --> 00:47:19,200
That somehow no matter

837
00:47:19,260 --> 00:47:20,580
what the value of sigma squared is, I mean,

838
00:47:20,700 --> 00:47:22,250
sigma squared has to be a positive number.

839
00:47:22,270 --> 00:47:23,450
It's a variance of a Gaussian.

840
00:47:23,540 --> 00:47:24,900
So that no matter what sigma squared

841
00:47:24,950 --> 00:47:28,800
is since it's a positive number the value of theta

842
00:47:28,900 --> 00:47:31,420
we end up with will be the same, right?

843
00:47:31,540 --> 00:47:35,700
So because minimizing this you get the same value

844
00:47:35,790 --> 00:47:37,900
of theta no matter what sigma squared is.

845
00:47:38,010 --> 00:47:40,650
So it's as if in this model the value

846
00:47:40,720 --> 00:47:42,020
of sigma squared doesn't really matter.

847
00:47:42,110 --> 00:47:45,260
Just remember that for the next lecture.

848
00:47:45,340 --> 00:47:46,540
We'll come back to this again.

849
00:47:46,620 --> 00:47:48,780
Any questions about this?

850
00:47:48,860 --> 00:47:53,600
Actually, let me clean up another couple of boards

851
00:47:53,670 --> 00:47:55,080
and then I'll see what questions you have.

852
00:47:55,180 --> 00:48:41,940
Okay. Any questions? Yeah?

853
00:48:42,030 --> 00:48:44,470
Student:You are, I think here you try to

854
00:48:44,540 --> 00:48:50,260
measure the likelihood of your nice of theta

855
00:48:50,340 --> 00:48:53,390
by a fraction of error,

856
00:48:53,490 --> 00:48:56,720
but I think it's that you measure because

857
00:48:56,770 --> 00:49:00,000
it depends on the family of theta too, for example.

858
00:49:00,100 --> 00:49:01,610
If you have a lot of parameters

859
00:49:01,610 --> 00:49:04,040
[inaudible] or fitting in?

860
00:49:04,130 --> 00:49:06,030
Instructor (Andrew Ng):Yeah, yeah. I mean,

861
00:49:06,100 --> 00:49:07,140
you're asking about overfitting,

862
00:49:07,220 --> 00:49:09,390
whether this is a good model. I think let's

863
00:49:09,460 --> 00:49:12,990
the thing's you're mentioning are maybe deeper

864
00:49:13,100 --> 00:49:15,420
questions about learning algorithms that

865
00:49:15,510 --> 00:49:18,240
we'll just come back to later,

866
00:49:18,350 --> 00:49:20,890
so don't really want to get into that right now.

867
00:49:20,980 --> 00:49:27,770
Any more questions? Okay.

868
00:49:27,850 --> 00:49:36,010
So this endows linear regression

869
00:49:36,100 --> 00:49:37,540
with a probabilistic interpretation.

870
00:49:37,620 --> 00:49:40,910
I'm actually going to use this probabil use this,

871
00:49:41,000 --> 00:49:44,470
sort of, probabilistic interpretation in order to

872
00:49:44,560 --> 00:49:46,220
derive our next learning algorithm,

873
00:49:46,330 --> 00:49:49,820
which will be our first classification algorithm. Okay?

874
00:49:49,910 --> 00:49:56,620
So you'll recall that I said that regression problems

875
00:49:56,690 --> 00:49:58,410
are where the variable Y that

876
00:49:58,490 --> 00:50:00,150
you're trying to predict is continuous values.

877
00:50:00,260 --> 00:50:02,790
Now I'm actually gonna talk about

878
00:50:02,930 --> 00:50:05,550
our first classification problem, where the value

879
00:50:05,640 --> 00:50:09,040
Y you're trying to predict will be discreet value.

880
00:50:09,100 --> 00:50:10,530
You can take on only a small number of

881
00:50:10,610 --> 00:50:11,950
discrete values and in this case

882
00:50:12,030 --> 00:50:13,810
I'll talk about binary classification

883
00:50:13,910 --> 00:50:18,160
where Y takes on only two values, right?

884
00:50:18,220 --> 00:50:20,890
So you come up with classification problems

885
00:50:20,980 --> 00:50:22,280
if you're trying to do, say,

886
00:50:22,380 --> 00:50:24,000
a medical diagnosis and try to decide based on

887
00:50:24,090 --> 00:50:25,600
some features that the patient has a disease

888
00:50:25,690 --> 00:50:28,250
or does not have a disease.

889
00:50:28,350 --> 00:50:31,290
Or if in the housing example,

890
00:50:31,300 --> 00:50:33,560
maybe you're trying to decide will

891
00:50:33,560 --> 00:50:35,160
this house sell in the next six months

892
00:50:35,240 --> 00:50:36,710
or not and the answer is either yes or no.

893
00:50:36,800 --> 00:50:38,440
It'll either be sold in the next six months

894
00:50:38,490 --> 00:50:42,260
or it won't be. Other standing examples,

895
00:50:42,350 --> 00:50:43,890
if you want to build a spam filter.

896
00:50:43,970 --> 00:50:45,090
Is this e-mail spam or not?

897
00:50:45,090 --> 00:50:45,930
It's yes or no.

898
00:50:46,010 --> 00:50:49,860
Or if you, you know, some of my colleagues sit in

899
00:50:49,940 --> 00:50:51,290
whether predicting whether

900
00:50:51,340 --> 00:50:52,510
a computer system will crash.

901
00:50:52,610 --> 00:50:54,610
So you have a learning algorithm

902
00:50:54,700 --> 00:50:56,560
to predict will this computing cluster

903
00:50:56,630 --> 00:50:58,540
crash over the next 24 hours?

904
00:50:58,610 --> 00:51:00,440
And, again, it's a yes or no answer.

905
00:51:00,570 --> 00:51:09,840
So there's X, there's Y. And in a classification problem

906
00:51:09,900 --> 00:51:16,600
Y takes on two values, zero and one.

907
00:51:16,670 --> 00:51:18,420
That's it in binding the classification.

908
00:51:18,500 --> 00:51:19,960
So what can you do?

909
00:51:20,020 --> 00:51:22,810
Well, one thing you could do is take linear regression,

910
00:51:22,890 --> 00:51:24,280
as we've described it so far,

911
00:51:24,340 --> 00:51:25,740
and apply it to this problem, right?

912
00:51:25,830 --> 00:51:27,030
So you, you know,

913
00:51:27,100 --> 00:51:28,010
given this data set

914
00:51:28,010 --> 00:51:28,920
you can fit a straight line to it.

915
00:51:29,000 --> 00:51:31,940
Maybe you get that straight line, right?

916
00:51:32,000 --> 00:51:34,290
But this data set I've drawn, right?

917
00:51:34,350 --> 00:51:36,690
This is an amazingly easy classification problem.

918
00:51:36,770 --> 00:51:39,690
It's pretty obvious to all of us that, right?

919
00:51:39,760 --> 00:51:41,010
The relationship between X and Y is

920
00:51:41,080 --> 00:51:42,390
well, you just look at

921
00:51:42,390 --> 00:51:43,000
a value around here and

922
00:51:43,060 --> 00:51:45,580
it's the right is one, it's the left and Y is zero.

923
00:51:45,660 --> 00:51:49,450
So you apply linear regressions to this data set

924
00:51:49,520 --> 00:51:51,370
and you get a reasonable fit and you can

925
00:51:51,440 --> 00:51:52,230
then maybe take your

926
00:51:52,310 --> 00:51:53,870
linear regression hypothesis

927
00:51:53,940 --> 00:51:57,670
to this straight line and threshold it at 0.5.

928
00:51:57,730 --> 00:52:00,310
If you do that you'll certainly get the right answer.

929
00:52:00,370 --> 00:52:05,300
You predict that if X is to the right of, sort of,

930
00:52:05,370 --> 00:52:07,710
the mid-point here then Y is one

931
00:52:07,760 --> 00:52:09,950
and then next to the left of that mid-point then Y is zero.

932
00:52:10,030 --> 00:52:13,080
So some people actually do this.

933
00:52:13,150 --> 00:52:15,550
Apply linear regression to classification problems

934
00:52:15,630 --> 00:52:18,160
and sometimes it'll work okay,

935
00:52:18,210 --> 00:52:21,350
but in general it's actually a pretty bad idea

936
00:52:21,410 --> 00:52:26,960
to apply linear regression to classification problems

937
00:52:26,990 --> 00:52:29,110
like these and here's why.

938
00:52:29,190 --> 00:52:33,880
Let's say I change my training set by

939
00:52:33,970 --> 00:52:35,770
giving you just one more training example

940
00:52:35,870 --> 00:52:36,990
all the way up there, right?

941
00:52:37,080 --> 00:52:41,270
Imagine if given this training set is actually still

942
00:52:41,360 --> 00:52:43,150
entirely obvious what the relationship

943
00:52:43,260 --> 00:52:44,670
between X and Y is, right?

944
00:52:44,750 --> 00:52:47,320
It's just take this value as greater than

945
00:52:47,410 --> 00:52:49,410
Y is one and it's less then Y is zero.

946
00:52:49,500 --> 00:52:54,270
By giving you this additional training example

947
00:52:54,360 --> 00:52:56,050
it really shouldn't change anything.

948
00:52:56,140 --> 00:52:58,510
I mean, I didn't really convey much new information.

949
00:52:58,600 --> 00:52:59,410
There's no surprise that

950
00:52:59,470 --> 00:53:00,780
this corresponds to Y equals one.

951
00:53:00,860 --> 00:53:02,490
But if you now fit

952
00:53:02,500 --> 00:53:04,130
linear regression to this data

953
00:53:04,210 --> 00:53:07,880
set you end up with a line that, I don't know,

954
00:53:07,950 --> 00:53:09,910
maybe looks like that, right?

955
00:53:09,990 --> 00:53:13,790
And now the predictions of your hypothesis

956
00:53:13,870 --> 00:53:14,950
have changed completely

957
00:53:14,960 --> 00:53:16,720
if your threshold

958
00:53:16,790 --> 00:53:20,200
your hypothesis at Y equal both 0.5. Okay?

959
00:53:20,290 --> 00:53:24,500
So Student:In between there might

960
00:53:24,560 --> 00:53:27,470
be an interval where it's zero, right?

961
00:53:27,560 --> 00:53:28,500
For that far off point?

962
00:53:28,580 --> 00:53:29,860
Instructor (Andrew Ng):Oh, you mean, like that?

963
00:53:29,940 --> 00:53:30,690
Student: Right.

964
00:53:30,750 --> 00:53:32,170
Instructor (Andrew Ng):Yeah, yeah, fine.

965
00:53:32,270 --> 00:53:34,230
Yeah, sure. A theta set like that so.

966
00:53:34,280 --> 00:53:39,750
So, I guess, these just yes, you're right,

967
00:53:39,840 --> 00:53:40,790
but this is an example

968
00:53:40,800 --> 00:53:43,570
and this example works.

969
00:53:43,570 --> 00:53:44,570
This–

970
00:53:44,570 --> 00:53:47,660
Student:[Inaudible]

971
00:53:47,660 --> 00:53:48,660
that will change it even more if you gave it all –

972
00:53:50,000 --> 00:53:50,700
Instructor (Andrew Ng):Yeah.

973
00:53:50,770 --> 00:53:51,640
Then I think this actually would make it even worse.

974
00:53:51,720 --> 00:53:53,100
You would actually get a line that

975
00:53:53,190 --> 00:53:56,490
pulls out even further, right? So this is my example.

976
00:53:56,580 --> 00:53:57,540
I get to make it whatever I want, right?

977
00:53:57,600 --> 00:53:58,980
But the point of this is that

978
00:53:59,060 --> 00:54:00,630
there's not a deep meaning to this.

979
00:54:00,710 --> 00:54:02,230
The point of this is just that

980
00:54:02,300 --> 00:54:04,670
it could be a really bad idea to apply linear regression

981
00:54:04,780 --> 00:54:05,880
to classification algorithm.

982
00:54:05,880 --> 00:54:06,780
Sometimes it work fine,

983
00:54:06,870 --> 00:54:08,510
but usually I wouldn't do it.

984
00:54:08,510 --> 00:54:09,510
So a couple of problems with this. One is that, well –

985
00:54:14,220 --> 00:54:16,260
so what do you want to do for classification?

986
00:54:16,320 --> 00:54:21,420
If you know the value of Y lies between

987
00:54:21,480 --> 00:54:24,330
zero and one then to kind of fix this problem

988
00:54:24,390 --> 00:54:30,260
let's just start by changing the form of

989
00:54:30,330 --> 00:54:34,720
our hypothesis so that my hypothesis always lies

990
00:54:34,780 --> 00:54:37,390
in the unit interval between zero and one. Okay?

991
00:54:37,460 --> 00:54:42,780
So if I know Y is either zero or one

992
00:54:42,840 --> 00:54:44,750
then let's at least not

993
00:54:44,750 --> 00:54:45,870
have my hypothesis predict

994
00:54:45,960 --> 00:54:48,340
values much larger than one and much smaller than zero.

995
00:54:48,420 --> 00:54:53,660
And so I'm going to instead of choosing

996
00:54:53,720 --> 00:54:56,210
a linear function for my hypothesis I'm going to choose

997
00:54:56,270 --> 00:54:57,380
something slightly different.

998
00:54:57,470 --> 00:55:03,980
And, in particular, I'm going to choose this function,

999
00:55:04,060 --> 00:55:08,190
H subscript theta of X is going to equal to G of theta

1000
00:55:08,240 --> 00:55:15,580
transpose X where G is going to be this function

1001
00:55:15,650 --> 00:55:20,260
and so this becomes more than one plus

1002
00:55:20,320 --> 00:55:23,920
theta X of theta transpose X.

1003
00:55:24,000 --> 00:55:28,750
And G of Z is called the sigmoid function

1004
00:55:28,800 --> 00:55:35,110
and it is often also called the logistic function.

1005
00:55:35,190 --> 00:55:39,760
It goes by either of these names.

1006
00:55:39,820 --> 00:55:43,620
And what G of Z looks like is the following.

1007
00:55:43,680 --> 00:55:49,200
So when you have your horizontal axis

1008
00:55:49,280 --> 00:55:58,070
I'm going to plot Z and so G of Z will look like this. Okay?

1009
00:55:58,130 --> 00:56:04,270
I didn't draw that very well. Okay.

1010
00:56:04,350 --> 00:56:08,990
So G of Z tends towards zero as Z becomes very small

1011
00:56:09,080 --> 00:56:13,910
and G of Z will ascend towards one as Z becomes

1012
00:56:13,990 --> 00:56:19,370
large and it crosses the vertical axis at 0.5.

1013
00:56:19,430 --> 00:56:20,930
So this is what sigmoid function,

1014
00:56:21,010 --> 00:56:23,140
also called the logistic function of. Yeah? Question?

1015
00:56:23,220 --> 00:56:25,030
Student:What sort of sigmoid in other step five?

1016
00:56:25,110 --> 00:56:26,270
Instructor (Andrew Ng):Say that again.

1017
00:56:26,330 --> 00:56:28,990
Student:Why we cannot chose this

1018
00:56:29,050 --> 00:56:31,430
at five for some reason, like, that's better binary.

1019
00:56:31,510 --> 00:56:32,380
Instructor (Andrew Ng):Yeah.

1020
00:56:32,440 --> 00:56:33,920
Let me come back to that later. So it turns out that Y

1021
00:56:33,990 --> 00:56:36,100
where did I get this function from, right?

1022
00:56:36,160 --> 00:56:38,370
I just wrote down this function.

1023
00:56:38,480 --> 00:56:40,670
It actually turns out that there are two reasons

1024
00:56:40,750 --> 00:56:42,460
for using this function that we'll come to.

1025
00:56:42,540 --> 00:56:45,280
One is we talked about generalized linear models.

1026
00:56:45,370 --> 00:56:46,870
We'll see that this falls out naturally

1027
00:56:46,970 --> 00:56:48,700
as part of the broader class of models.

1028
00:56:48,770 --> 00:56:53,030
And another reason that we'll talk about next week,

1029
00:56:53,140 --> 00:56:55,910
it turns out there are a couple of, I think, very

1030
00:56:55,990 --> 00:56:58,830
beautiful reasons for why we choose logistic functions.

1031
00:56:58,910 --> 00:57:00,190
We'll see that in a little bit.

1032
00:57:00,270 --> 00:57:02,750
But for now let me just define it

1033
00:57:02,840 --> 00:57:04,360
and just take my word for it

1034
00:57:04,440 --> 00:57:06,510
for now that this is a reasonable choice. Okay?

1035
00:57:06,580 --> 00:57:08,570
But notice now that my

1036
00:57:08,690 --> 00:57:11,880
the values output by my hypothesis will always

1037
00:57:11,940 --> 00:57:13,220
be between zero and one.

1038
00:57:13,290 --> 00:57:16,450
Furthermore, just like we did

1039
00:57:16,460 --> 00:57:18,830
for linear regression,

1040
00:57:18,870 --> 00:57:22,090
I'm going to endow the outputs and my hypothesis

1041
00:57:22,160 --> 00:57:24,350
with a probabilistic interpretation, right?

1042
00:57:24,370 --> 00:57:28,660
So I'm going to assume that the probability that

1043
00:57:28,710 --> 00:57:32,710
Y is equal to one given X and parameterized by theta

1044
00:57:32,760 --> 00:57:38,600
that's equal to H subscript theta of X, all right?

1045
00:57:38,660 --> 00:57:43,240
So in other words I'm going to imagine that

1046
00:57:43,300 --> 00:57:44,560
my hypothesis is outputting all these numbers that

1047
00:57:44,640 --> 00:57:45,670
lie between zero and one.

1048
00:57:45,750 --> 00:57:48,620
I'm going to think of my hypothesis as trying to

1049
00:57:48,670 --> 00:57:51,550
estimate the probability that Y is equal to one. Okay?

1050
00:57:51,640 --> 00:58:01,920
And because Y has to be either zero or one

1051
00:58:02,000 --> 00:58:05,550
then the probability of Y equals zero is going to be that.

1052
00:58:05,600 --> 00:58:13,670
All right? So more simply it turns out actually,

1053
00:58:13,750 --> 00:58:16,910
take these two equations and write them more compactly.

1054
00:58:16,990 --> 00:58:22,900
Write P of Y given X parameterized by theta.

1055
00:58:22,990 --> 00:58:29,260
This is going to be H subscript theta of X to the power

1056
00:58:29,340 --> 00:58:33,840
of Y times one minus H of X

1057
00:58:33,940 --> 00:58:36,890
to the power of one minus Y. Okay?

1058
00:58:36,990 --> 00:58:39,140
So I know this looks somewhat bizarre,

1059
00:58:39,200 --> 00:58:41,840
but this actually makes the variation much nicer.

1060
00:58:41,920 --> 00:58:46,440
So Y is equal to one then this equation is H of X

1061
00:58:46,540 --> 00:58:48,620
to the power of one times

1062
00:58:48,700 --> 00:58:50,300
something to the power of zero.

1063
00:58:50,380 --> 00:58:54,590
So anything to the power of zero is just one, right?

1064
00:58:54,660 --> 00:58:57,620
So Y equals one then this is something to the power

1065
00:58:57,680 --> 00:58:59,330
of zero and so this is just one.

1066
00:58:59,430 --> 00:59:02,360
So if Y equals one this is just saying P of Y

1067
00:59:02,440 --> 00:59:06,190
equals one is equal to H subscript theta of X. Okay?

1068
00:59:06,260 --> 00:59:08,260
And in the same way, if Y is equal to zero then

1069
00:59:08,350 --> 00:59:12,490
this is P of Y equals zero equals this thing to the power

1070
00:59:12,600 --> 00:59:14,390
of zero and so this disappears.

1071
00:59:14,460 --> 00:59:17,330
This is just one times this thing power of one. Okay?

1072
00:59:17,440 --> 00:59:20,630
So this is a compact way of writing both of

1073
00:59:20,740 --> 00:59:24,460
these equations to gather them to one line.

1074
00:59:24,530 --> 00:59:33,100
So let's hope our parameter fitting, right?

1075
00:59:33,180 --> 00:59:34,770
And, again, you can ask

1076
00:59:34,860 --> 00:59:37,040
well, given this model by data,

1077
00:59:37,150 --> 00:59:40,570
how do I fit the parameters theta of my model?

1078
00:59:40,640 --> 00:59:45,360
So the likelihood of the parameters is, as before,

1079
00:59:45,440 --> 00:59:48,610
it's just the probability of data, right?

1080
00:59:48,710 --> 00:59:51,510
Which is product over I,

1081
00:59:51,600 --> 00:59:57,990
PFYI given XI parameterized by theta.

1082
00:59:58,040 --> 01:00:09,130
Which is just plugging those in. Okay?

1083
01:00:09,230 --> 01:00:11,380
I dropped this theta subscript just so

1084
01:00:11,430 --> 01:00:18,230
you can write a little bit less. Oh, excuse me.

1085
01:00:18,330 --> 01:00:27,990
These should be XI's and YI's. Okay?

1086
01:00:52,970 --> 01:00:55,460
So, as before, let's say we want to find a maximum

1087
01:00:55,550 --> 01:00:57,800
likelihood estimate of the parameters theta.

1088
01:00:57,890 --> 01:00:59,470
So we want to find the

1089
01:00:59,530 --> 01:01:01,230
setting the parameters theta that

1090
01:01:01,280 --> 01:01:04,660
maximizes the likelihood L of theta.

1091
01:01:04,740 --> 01:01:10,460
It turns out that very often

1092
01:01:10,540 --> 01:01:13,490
just when you work with the derivations,

1093
01:01:13,530 --> 01:01:15,340
it turns out that it is often much easier to

1094
01:01:15,430 --> 01:01:17,370
maximize the log of the likelihood rather than

1095
01:01:17,460 --> 01:01:18,530
maximize the likelihood.

1096
01:01:18,630 --> 01:01:24,420
So the log likelihood L of theta is just log of capital L.

1097
01:01:24,500 --> 01:01:48,490
This will, therefore, be sum of this. Okay?

1098
01:01:48,600 --> 01:02:04,240
And so to fit the parameters theta of our model

1099
01:02:04,310 --> 01:02:06,370
we'll find the value of theta that

1100
01:02:06,470 --> 01:02:09,150
maximizes this log likelihood. Yeah?

1101
01:02:09,240 --> 01:02:12,110
Student:[Inaudible]

1102
01:02:12,200 --> 01:02:12,770
Instructor (Andrew Ng):Say that again.

1103
01:02:12,840 --> 01:02:14,230
Student:YI is [inaudible].

1104
01:02:14,310 --> 01:02:18,720
Instructor (Andrew Ng):Oh, yes. Thanks.

1105
01:02:18,820 --> 01:02:23,080
So having maximized this function

1106
01:02:23,180 --> 01:02:25,910
well, it turns out we can actually apply

1107
01:02:25,980 --> 01:02:30,350
the same gradient descent algorithm that we learned.

1108
01:02:30,430 --> 01:02:33,060
That was the first algorithm we used to

1109
01:02:33,140 --> 01:02:34,380
minimize the quadratic function.

1110
01:02:34,460 --> 01:02:35,980
And you remember,

1111
01:02:36,090 --> 01:02:37,430
when we talked about least squares,

1112
01:02:37,510 --> 01:02:39,040
the first algorithm we used to minimize

1113
01:02:39,120 --> 01:02:41,880
the quadratic error function was great in descent.

1114
01:02:41,960 --> 01:02:44,930
So can actually use exactly the same algorithm to

1115
01:02:45,020 --> 01:02:47,080
maximize the log likelihood.

1116
01:02:47,140 --> 01:02:51,600
And you remember, that algorithm was just

1117
01:02:51,660 --> 01:02:55,750
repeatedly take the value of theta and you replace it

1118
01:02:55,830 --> 01:03:00,720
with the previous value of theta plus a learning rate

1119
01:03:00,780 --> 01:03:06,350
alpha times the gradient of the cos function.

1120
01:03:06,440 --> 01:03:10,050
The log likelihood will respect the theta. Okay?

1121
01:03:10,100 --> 01:03:11,000
One small change is that

1122
01:03:11,070 --> 01:03:12,980
because previously we were trying to

1123
01:03:13,060 --> 01:03:16,620
minimize the quadratic error term.

1124
01:03:16,700 --> 01:03:19,080
Today we're trying to maximize rather than minimize.

1125
01:03:19,160 --> 01:03:20,680
So rather than having a minus sign we have a plus sign.

1126
01:03:20,770 --> 01:03:22,160
So this is just great in ascents,

1127
01:03:22,220 --> 01:03:27,470
but for the maximization rather than the minimization.

1128
01:03:27,570 --> 01:03:29,750
So we actually call this gradient ascent and

1129
01:03:29,840 --> 01:03:30,990
it's really the same algorithm.

1130
01:03:31,100 --> 01:03:37,960
So to figure out what this gradient

1131
01:03:38,070 --> 01:03:40,560
so in order to derive gradient descent,

1132
01:03:40,630 --> 01:03:45,660
what you need to do is compute the partial derivatives

1133
01:03:45,740 --> 01:03:48,770
of your objective function with respect to each of

1134
01:03:48,830 --> 01:03:50,310
your parameters theta I, right?

1135
01:03:50,380 --> 01:03:53,830
It turns out that

1136
01:03:53,920 --> 01:04:01,740
if you actually compute this partial derivative

1137
01:04:01,820 --> 01:04:05,690
so you take this formula, this L of theta, which is

1138
01:04:05,770 --> 01:04:08,950
oh, got that wrong too.

1139
01:04:09,070 --> 01:04:11,660
If you take this lower case l theta,

1140
01:04:11,740 --> 01:04:13,270
if you take the log likelihood of theta,

1141
01:04:13,350 --> 01:04:15,530
and if you take it's partial derivative

1142
01:04:15,600 --> 01:04:23,150
with respect to theta I you find that this is equal to

1143
01:04:45,580 --> 01:04:48,100
And, I don't know,

1144
01:04:48,100 --> 01:04:49,860
the derivation isn't terribly complicated,

1145
01:04:49,920 --> 01:04:52,840
but in the interest of saving you watching me

1146
01:04:52,920 --> 01:04:54,010
write down a couple of

1147
01:04:54,010 --> 01:04:55,230
blackboards full of math

1148
01:04:55,280 --> 01:04:57,210
I'll just write down the final answer.

1149
01:04:57,280 --> 01:04:59,950
But the way you get this is you just take those,

1150
01:05:00,030 --> 01:05:02,570
plug in the definition for F subscript theta

1151
01:05:02,640 --> 01:05:05,150
as function of XI, and take derivatives,

1152
01:05:05,230 --> 01:05:06,940
and work through the algebra it turns out

1153
01:05:07,030 --> 01:05:11,030
it'll simplify down to this formula. Okay?

1154
01:05:11,100 --> 01:05:16,700
And so what that gives you is that

1155
01:05:16,790 --> 01:05:20,620
gradient ascent is the following rule.

1156
01:05:20,690 --> 01:05:28,450
Theta J gets updated as theta J plus alpha gives this.

1157
01:05:44,810 --> 01:05:46,250
Okay? Does this

1158
01:05:46,250 --> 01:05:48,400
look familiar to anyone?

1159
01:05:48,500 --> 01:05:50,920
Did you remember seeing

1160
01:05:50,980 --> 01:05:55,460
this formula at the last lecture? Right.

1161
01:05:55,500 --> 01:05:57,910
So when I worked up Bastrian descent

1162
01:05:57,960 --> 01:06:00,300
for least squares regression I, actually,

1163
01:06:00,370 --> 01:06:04,020
wrote down exactly the same thing,

1164
01:06:04,100 --> 01:06:09,790
or maybe there's a minus sign and this is also fit.

1165
01:06:09,870 --> 01:06:12,390
But I, actually, had exactly the same learning rule

1166
01:06:12,460 --> 01:06:17,000
last time for least squares regression, right?

1167
01:06:17,090 --> 01:06:21,720
Is this the same learning algorithm then?

1168
01:06:21,800 --> 01:06:22,850
So what's different?

1169
01:06:22,910 --> 01:06:23,900
How come I was making

1170
01:06:23,900 --> 01:06:24,660
all that noise earlier about

1171
01:06:24,770 --> 01:06:27,820
least squares regression being a bad idea for

1172
01:06:27,880 --> 01:06:30,590
classification problems and then I did a bunch of math

1173
01:06:30,660 --> 01:06:33,100
and I skipped some steps, but I'm, sort of, claiming

1174
01:06:33,150 --> 01:06:33,950
at the end they're really

1175
01:06:33,950 --> 01:06:34,820
the same learning algorithm?

1176
01:06:34,890 --> 01:06:38,440
Student:[Inaudible] constants?

1177
01:06:38,520 --> 01:06:39,260
Instructor (Andrew Ng):Say that again.

1178
01:06:39,360 --> 01:06:41,150
Student:[Inaudible]

1179
01:06:41,380 --> 01:06:42,910
Instructor (Andrew Ng):Oh, right. Okay, cool.

1180
01:06:43,000 --> 01:06:44,510
Student:It's the lowest it

1181
01:06:44,580 --> 01:06:45,300
Instructor (Andrew Ng):No, exactly. Right.

1182
01:06:45,380 --> 01:06:47,260
So zero to the same, this is not the same, right?

1183
01:06:47,320 --> 01:06:50,230
And the reason is, in logistic regression

1184
01:06:50,310 --> 01:06:54,560
this is different from before, right?

1185
01:06:54,610 --> 01:06:58,080
The definition of this H subscript theta of XI

1186
01:06:58,140 --> 01:07:00,370
is not the same as the definition

1187
01:07:00,450 --> 01:07:02,030
I was using in the previous lecture.

1188
01:07:02,080 --> 01:07:04,800
And in particular this is no longer

1189
01:07:04,890 --> 01:07:07,830
theta transpose XI. This is not a linear function anymore.

1190
01:07:07,890 --> 01:07:12,880
This is a logistic function

1191
01:07:12,880 --> 01:07:13,710
of theta transpose XI. Okay?

1192
01:07:13,820 --> 01:07:16,930
So even though this looks cosmetically similar,

1193
01:07:17,010 --> 01:07:19,490
even though this is similar on the surface,

1194
01:07:19,560 --> 01:07:23,110
to the Bastrian descent rule I derived last time

1195
01:07:23,170 --> 01:07:25,500
for least squares regression this is

1196
01:07:25,580 --> 01:07:26,680
actually a totally

1197
01:07:26,680 --> 01:07:28,730
different learning algorithm. Okay?

1198
01:07:28,810 --> 01:07:31,670
And it turns out that there's actually no coincidence

1199
01:07:31,750 --> 01:07:33,270
that you ended up with the same learning rule.

1200
01:07:33,360 --> 01:07:36,560
We'll actually talk a bit more about this later

1201
01:07:36,640 --> 01:07:38,270
when we talk about generalized learning models.

1202
01:07:38,350 --> 01:07:42,060
But this is one of the most elegant generalized

1203
01:07:42,160 --> 01:07:43,810
learning models that we'll see later.

1204
01:07:43,900 --> 01:07:45,940
That even though we're using a different model,

1205
01:07:46,030 --> 01:07:49,870
you actually ended up with what looks like the same

1206
01:07:49,960 --> 01:07:51,860
learning algorithm and it's actually no coincidence.

1207
01:07:51,920 --> 01:07:56,230
Cool.

1208
01:07:56,330 --> 01:08:00,490
One last comment as part of a sort of learning process,

1209
01:08:00,580 --> 01:08:04,420
over here I said I take the derivatives

1210
01:08:04,470 --> 01:08:06,070
and I ended up with this line.

1211
01:08:06,150 --> 01:08:09,840
I didn't want to make you sit

1212
01:08:09,890 --> 01:08:11,740
through a long algebraic derivation,

1213
01:08:11,830 --> 01:08:14,880
but later today or later this week, please,

1214
01:08:14,960 --> 01:08:17,180
do go home and look at our lecture notes,

1215
01:08:17,240 --> 01:08:19,360
where I wrote out the entirety

1216
01:08:19,430 --> 01:08:20,600
of this derivation in full,

1217
01:08:20,680 --> 01:08:22,950
and make sure you can follow every single step of

1218
01:08:23,050 --> 01:08:26,250
how we take partial derivatives of this log likelihood

1219
01:08:26,340 --> 01:08:28,630
to get this formula over here. Okay?

1220
01:08:28,680 --> 01:08:33,240
By the way, for those who are interested in

1221
01:08:33,300 --> 01:08:35,530
seriously masking machine learning material,

1222
01:08:35,600 --> 01:08:38,030
when you go home and look at the lecture notes

1223
01:08:38,090 --> 01:08:39,970
it will actually be very easy for most of you

1224
01:08:40,060 --> 01:08:41,230
to look through the lecture notes

1225
01:08:41,320 --> 01:08:43,000
and read through every line and go yep,

1226
01:08:43,090 --> 01:08:44,180
that makes sense, that makes sense,

1227
01:08:44,240 --> 01:08:46,180
that makes sense, and, sort of, say cool.

1228
01:08:46,260 --> 01:08:47,520
I see how you get this line.

1229
01:08:47,600 --> 01:08:51,890
You want to make sure you really understand the material.

1230
01:08:51,990 --> 01:08:54,580
My concrete suggestion to you would be to you to go home,

1231
01:08:54,660 --> 01:08:56,870
read through the lecture notes, check every line,

1232
01:08:56,950 --> 01:08:59,650
and then to cover up the derivation

1233
01:08:59,720 --> 01:09:02,820
and see if you can derive this example, right?

1234
01:09:02,910 --> 01:09:05,390
So in general, that's usually good advice for studying

1235
01:09:05,450 --> 01:09:06,460
technical material

1236
01:09:06,460 --> 01:09:07,300
like machine learning.

1237
01:09:07,360 --> 01:09:09,970
Which is if you work through a proof

1238
01:09:10,030 --> 01:09:11,400
and you think you understood every line,

1239
01:09:11,510 --> 01:09:13,280
the way to make sure you really understood it is

1240
01:09:13,350 --> 01:09:14,490
to cover it up and see

1241
01:09:14,590 --> 01:09:16,090
if you can rederive the entire thing itself.

1242
01:09:16,200 --> 01:09:17,240
This is actually a great way

1243
01:09:17,340 --> 01:09:18,840
because I did this a lot when I was trying to

1244
01:09:18,940 --> 01:09:20,940
study various pieces of

1245
01:09:21,040 --> 01:09:22,840
machine learning theory and various proofs.

1246
01:09:22,950 --> 01:09:24,960
And this is actually a great way to study

1247
01:09:25,030 --> 01:09:26,040
because cover up the derivations

1248
01:09:26,140 --> 01:09:28,000
and see if you can do it yourself without

1249
01:09:28,110 --> 01:09:35,110
looking at the original derivation. All right.

1250
01:09:35,200 --> 01:09:38,540
I probably won't get to Newton's Method today.

1251
01:09:38,630 --> 01:09:41,390
I just want to say take one quick digression

1252
01:09:57,170 --> 01:09:58,950
to talk about one more algorithm,

1253
01:09:59,060 --> 01:10:02,490
which was the discussion sort of alluding to this earlier,

1254
01:10:02,590 --> 01:10:10,760
which is the perceptron algorithm, right?

1255
01:10:10,850 --> 01:10:14,630
So I'm not gonna say a whole lot

1256
01:10:14,700 --> 01:10:15,970
about the perceptron algorithm,

1257
01:10:16,060 --> 01:10:17,770
but this is something that we'll come back to later.

1258
01:10:17,880 --> 01:10:21,350
Later this quarter we'll talk about learning theory.

1259
01:10:21,420 --> 01:10:28,570
So in logistic regression we said that G of Z are, sort of,

1260
01:10:28,660 --> 01:10:30,540
my hypothesis output values that

1261
01:10:30,630 --> 01:10:32,100
were low numbers between zero and one.

1262
01:10:32,200 --> 01:10:35,110
The question is what if you want to force G of Z

1263
01:10:35,220 --> 01:10:40,980
to up the value to either zero one?

1264
01:10:41,000 --> 01:10:46,940
So the perceptron algorithm defines G of Z to be this.

1265
01:10:47,060 --> 01:10:55,270
So the picture is or the cartoon is,

1266
01:10:55,300 --> 01:10:57,090
rather than this sigmoid function.

1267
01:10:57,190 --> 01:11:04,530
E of Z now looks like this step function that

1268
01:11:04,620 --> 01:11:06,000
you were asking about earlier.

1269
01:11:06,080 --> 01:11:10,230
In saying this before, we can use H subscript

1270
01:11:10,330 --> 01:11:14,780
theta of X equals G of theta transpose X. Okay?

1271
01:11:14,860 --> 01:11:15,780
So this is actually

1272
01:11:15,880 --> 01:11:17,070
everything is exactly the same as before,

1273
01:11:17,170 --> 01:11:19,190
except that G of Z is now the step function.

1274
01:11:19,310 --> 01:11:23,220
It turns out there's this learning

1275
01:11:23,320 --> 01:11:24,840
called the perceptron learning rule

1276
01:11:24,920 --> 01:11:26,490
that's actually even the same as

1277
01:11:26,580 --> 01:11:29,520
the classic gradient ascent for logistic regression.

1278
01:11:29,620 --> 01:11:43,820
And the learning rule is given by this. Okay?

1279
01:11:43,920 --> 01:11:50,630
So it looks just like the classic gradient ascent rule

1280
01:11:50,720 --> 01:11:55,470
for logistic regression.

1281
01:11:55,530 --> 01:11:58,750
So this is very different flavor of algorithm

1282
01:11:58,840 --> 01:12:01,230
than least squares regression and logistic regression,

1283
01:12:01,340 --> 01:12:03,560
and, in particular, because it outputs

1284
01:12:03,650 --> 01:12:06,300
only values are either zero or one it turns out

1285
01:12:06,390 --> 01:12:08,390
it's very difficult to endow this algorithm

1286
01:12:08,480 --> 01:12:10,160
with probabilistic semantics.

1287
01:12:10,280 --> 01:12:16,370
And this is, again, even though oh, excuse me.

1288
01:12:16,470 --> 01:12:19,510
Right there. Okay.

1289
01:12:19,600 --> 01:12:22,150
And even though this learning rule looks, again,

1290
01:12:22,240 --> 01:12:23,420
looks cosmetically very similar to what

1291
01:12:23,500 --> 01:12:25,400
we have in logistics regression

1292
01:12:25,500 --> 01:12:27,860
this is actually a very different type of learning rule

1293
01:12:27,950 --> 01:12:30,440
than the others that were seen in this class.

1294
01:12:30,550 --> 01:12:36,520
So because this is such a simple learning algorithm, right?

1295
01:12:36,580 --> 01:12:39,220
It just computes theta transpose X

1296
01:12:39,290 --> 01:12:40,160
and then you threshold

1297
01:12:40,250 --> 01:12:42,960
and then your output is zero or one. This is right.

1298
01:12:43,020 --> 01:12:44,410
So these are a simpler algorithm than

1299
01:12:44,500 --> 01:12:45,430
logistic regression, I think.

1300
01:12:45,520 --> 01:12:48,440
When we talk about learning theory later in this class,

1301
01:12:48,490 --> 01:12:51,260
the simplicity of this algorithm will let us come back

1302
01:12:51,350 --> 01:12:54,160
and use it as a building block. Okay?

1303
01:12:54,240 --> 01:12:55,800
But that's all I want to say

1304
01:12:55,800 --> 01:12:56,730
about this algorithm for now.

