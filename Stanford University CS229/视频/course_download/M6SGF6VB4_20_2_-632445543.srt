1
00:00:24,060 --> 00:00:26,220
So welcome to the last lecture of this course.

2
00:00:26,930 --> 00:00:29,200
What I want to do today

3
00:00:29,400 --> 00:00:30,250
Is tell you

4
00:00:30,410 --> 00:00:34,070
about one final class of reinforcement learning algorithms.

5
00:00:34,280 --> 00:00:36,860
I just want to say a little bit about POMDPs,

6
00:00:37,050 --> 00:00:39,730
The partially observable MDPs,  and then  the main

7
00:00:40,030 --> 00:00:43,000
technical topic for today will be policy search algorithms.

8
00:00:43,190 --> 00:00:45,260
I'll talk about two specific algorithms

9
00:00:45,540 --> 00:00:48,010
essentially called reinforced and called Pegasus,

10
00:00:48,270 --> 00:00:50,530
and then we'll wrap up the class.

11
00:00:50,890 --> 00:00:54,360
So if you recall from the last lecture

12
00:00:54,540 --> 00:00:57,040
I actually started to talk about

13
00:00:57,260 --> 00:00:58,830
one specific example of a POMDP

14
00:00:59,100 --> 00:01:02,530
which was this sort of linear dynamical system.

15
00:01:02,740 --> 00:01:10,190
This is sort of LQR, linear quadratic revelation problem

16
00:01:10,380 --> 00:01:13,040
but I changed it and said what if we

17
00:01:13,200 --> 00:01:19,710
only have observations YT.

18
00:01:19,910 --> 00:01:28,010
And what if we couldn't observe the state of the system

19
00:01:28,170 --> 00:01:31,300
directly but had to choose an action only based on some

20
00:01:31,810 --> 00:01:35,390
noisy observations that maybe some function of the state?

21
00:01:35,550 --> 00:01:40,110
So our strategy last time was that we said that

22
00:01:40,270 --> 00:01:42,230
in the fully observable case,

23
00:01:42,410 --> 00:01:50,930
we could choose actions AT equals two, that matrix LT

24
00:01:51,100 --> 00:01:56,010
times ST.So LT was this matrix of parameters

25
00:01:56,160 --> 00:01:59,430
that [inaudible] describe the dynamic programming algorithm

26
00:01:59,610 --> 00:02:01,780
for finite horizon MDPs in the LQR problem.

27
00:02:01,980 --> 00:02:05,480
And so we said if only we knew what the state was,we choose

28
00:02:05,700 --> 00:02:09,110
actions according to some matrix LT times the state.

29
00:02:09,290 --> 00:02:14,880
And then I said in the partially observable case, we would

30
00:02:15,050 --> 00:02:22,600
compute these estimates.I wrote them as S of T given T,

31
00:02:22,810 --> 00:02:33,090
which were our best estimate for

32
00:02:33,250 --> 00:02:35,140
what the state is given all the observations.

33
00:02:35,330 --> 00:02:39,150
And in particular, I'm gonna talk about a Kalman filter

34
00:02:39,360 --> 00:02:47,280
which we worked out that our posterior distribution of what

35
00:02:47,610 --> 00:02:51,170
the state what the state is given all the observations up to

36
00:02:51,360 --> 00:02:57,620
a certain time that was this. So this is from last time.

37
00:02:57,810 --> 00:03:01,300
So that given the observations Y one through YT,

38
00:03:01,510 --> 00:03:04,650
our posterior distribution of the current state ST

39
00:03:04,810 --> 00:03:06,150
was Gaussian

40
00:03:06,300 --> 00:03:09,560
would mean ST given T sigma T given T.

41
00:03:09,700 --> 00:03:13,200
So I said we use a Kalman filter to compute this thing,

42
00:03:13,380 --> 00:03:16,270
this ST given T, which is going to be our best guess

43
00:03:16,460 --> 00:03:19,850
for what the state is currently.

44
00:03:20,680 --> 00:03:31,160
And then we choose actions using our  estimate

45
00:03:31,400 --> 00:03:32,280
for what the state is,

46
00:03:32,430 --> 00:03:33,540
rather than using the true state

47
00:03:33,750 --> 00:03:37,210
because we don't know the true state anymore in this POMDP.

48
00:03:37,420 --> 00:03:42,310
So it turns out that this specific strategy

49
00:03:42,520 --> 00:03:48,180
actually allows you to choose optimal actions, allows you

50
00:03:48,340 --> 00:03:49,920
to choose actions as well as you

51
00:03:50,070 --> 00:03:50,780
possibly can given

52
00:03:50,950 --> 00:03:52,110
that this is a POMDP,

53
00:03:52,310 --> 00:03:53,610
and given there are these noisy observations.

54
00:03:53,810 --> 00:03:57,490
It turns out that in general finding optimal policies

55
00:03:57,660 --> 00:04:01,530
with POMDPs  finding optimal policies for these sorts

56
00:04:01,770 --> 00:04:04,220
of partially observable MDPs is an NP-hard problem.

57
00:04:04,400 --> 00:04:08,920
Just to be concrete about the formalism of the POMDP

58
00:04:08,920 --> 00:04:09,920
– I should just write it here a POMDP formally is a tuple

59
00:04:20,840 --> 00:04:32,670
like that where the changes

60
00:04:32,850 --> 00:04:37,110
are the set Y is the set of possible observations

61
00:04:46,170 --> 00:04:51,760
and this O subscript S are the observation distributions.

62
00:04:52,440 --> 00:05:00,340
at each step

63
00:05:00,610 --> 00:05:19,380
in the POMDP,

64
00:05:19,590 --> 00:05:21,430
if we're in some state ST,

65
00:05:21,630 --> 00:05:24,490
we observe some observation YT drawn

66
00:05:24,680 --> 00:05:26,940
from the observation distribution O subscript ST,

67
00:05:27,140 --> 00:05:29,520
that there's an index by what the current state is.

68
00:05:29,730 --> 00:05:34,190
And it turns out that computing the optimal policy in a POMDP

69
00:05:34,370 --> 00:05:35,170
is an NP-hard problem.

70
00:05:35,320 --> 00:05:38,330
For the specific case of linear dynamical systems

71
00:05:38,520 --> 00:05:40,250
with the Kalman filter model,

72
00:05:40,420 --> 00:05:41,940
we have this strategy

73
00:05:42,130 --> 00:05:45,140
of computing the optimal policy assuming full observability

74
00:05:45,300 --> 00:05:48,100
and then estimating the states from the observations,

75
00:05:48,340 --> 00:05:50,330
and then plugging the two together.

76
00:05:50,520 --> 00:05:52,390
That turns out to be optimal essentially

77
00:05:52,660 --> 00:05:55,390
for only that special case of a POMDP.

78
00:05:55,580 --> 00:05:57,440
In the more general case,

79
00:05:57,680 --> 00:06:01,020
that strategy of designing a controller

80
00:06:01,370 --> 00:06:03,180
assuming full observability and then

81
00:06:03,360 --> 00:06:05,610
just estimating the state and plugging the two together,

82
00:06:05,810 --> 00:06:07,740
for general POMDPs that

83
00:06:08,200 --> 00:06:11,660
same strategy is often a very reasonable strategy

84
00:06:11,870 --> 00:06:13,460
but is not always guaranteed to be optimal.

85
00:06:13,770 --> 00:06:16,580
Solving these problems in general, NP-hard.

86
00:06:16,730 --> 00:06:26,120
So what I want to do today is actually talk about

87
00:06:26,280 --> 00:06:28,930
a different class of reinforcement learning algorithms.

88
00:06:29,100 --> 00:06:31,040
These are called policy search algorithms.

89
00:06:31,230 --> 00:06:37,760
In particular, policy search algorithms

90
00:06:37,920 --> 00:06:41,060
can be applied equally well to MDPs,

91
00:06:41,210 --> 00:06:43,340
to fully observed Markov decision processes,

92
00:06:43,550 --> 00:06:48,260
or to these POMDPs, or to these partially observable MPDs.

93
00:06:48,440 --> 00:06:51,090
What I want to do now,

94
00:06:51,270 --> 00:06:52,850
I'll actually just describe policy

95
00:06:52,990 --> 00:06:56,010
search algorithms applied to MDPs,

96
00:06:56,210 --> 00:06:57,880
applied to the fully observable case.

97
00:06:58,060 --> 00:07:00,330
And in the end, I just briefly describe

98
00:07:00,540 --> 00:07:02,290
how you can take policy search algorithms

99
00:07:02,500 --> 00:07:03,580
and apply them to POMDPs.

100
00:07:03,740 --> 00:07:06,910
In the latter case,

101
00:07:07,050 --> 00:07:09,010
when you apply a policy search algorithm to a POMDP,

102
00:07:10,140 --> 00:07:14,670
it's going to be hard to guarantee that you get the globally optimal policy

103
00:07:14,950 --> 00:07:17,990
because solving POMDPs in general is NP-hard,

104
00:07:18,200 --> 00:07:20,910
but nonetheless policy search algorithms it turns out to be

105
00:07:21,110 --> 00:07:24,310
I think one of the most effective classes

106
00:07:24,530 --> 00:07:26,010
of reinforcement learning algorithms,

107
00:07:26,190 --> 00:07:27,780
as well both for MDPs and for POMDPs.

108
00:07:28,020 --> 00:07:35,730
So here's what we're going to do.

109
00:07:35,930 --> 00:07:42,640
In policy search, we're going to define of some set

110
00:07:42,860 --> 00:07:50,860
which I denote capital pi of policies,

111
00:07:51,110 --> 00:08:04,110
and our strategy is to search for a good policy

112
00:08:04,370 --> 00:08:07,630
lower pi into set capital pi.

113
00:08:07,630 --> 00:08:08,630
Just by analogy, I want to say –

114
00:08:13,490 --> 00:08:15,930
in the same way,

115
00:08:16,090 --> 00:08:18,330
back when we were talking about supervised learning,

116
00:08:18,550 --> 00:08:21,910
the way we defined the set capital pi of policies

117
00:08:22,090 --> 00:08:24,740
in the search for policy in this set capital

118
00:08:24,930 --> 00:08:29,350
pi is analogous to supervised learning

119
00:08:31,730 --> 00:08:39,400
where we defined a set script H of hypotheses and search

120
00:08:51,540 --> 00:08:52,630
in this policy script H.

121
00:08:52,630 --> 00:08:53,630
–	and would search for a good hypothesis

122
00:08:52,800 --> 00:09:01,560
Policy search is sometimes also called direct policy search.

123
00:09:03,290 --> 00:09:05,100
To contrast this with the source of algorithms

124
00:09:05,270 --> 00:09:05,820
we've been talking about

125
00:09:05,980 --> 00:09:08,670
so far,in all the algorithms we've been talking about so far,

126
00:09:08,840 --> 00:09:10,550
we would try to find V star.

127
00:09:10,740 --> 00:09:12,700
We would try to find the optimal value function.

128
00:09:12,910 --> 00:09:16,090
And then we'd use V star

129
00:09:16,090 --> 00:09:17,090
– we'd use the optimal value function to

130
00:09:17,780 --> 00:09:20,140
then try to compute or try to approximate pi star.

131
00:09:20,330 --> 00:09:22,750
So all the approaches we talked about previously

132
00:09:22,970 --> 00:09:25,570
are strategy for finding a good policy.

133
00:09:25,750 --> 00:09:27,380
Once we compute the value function,

134
00:09:27,580 --> 00:09:28,980
then we go from that to policy.

135
00:09:29,170 --> 00:09:31,630
In contrast, in policy search algorithms

136
00:09:31,800 --> 00:09:33,720
and something that's called direct policy search algorithms,

137
00:09:33,900 --> 00:09:37,730
the idea is that we're going to quote "directly" try

138
00:09:37,880 --> 00:09:41,070
to approximate a good policy without going through

139
00:09:41,170 --> 00:09:44,370
the intermediate stage of trying to find the value function.

140
00:09:44,550 --> 00:09:52,180
Let's see. And also as I develop policy search

141
00:09:52,420 --> 00:09:55,290
just one step that's sometimes slightly confusing.

142
00:09:55,500 --> 00:09:59,440
Making an analogy to supervised learning again,

143
00:09:59,630 --> 00:10:02,170
when we talked about logistic regression,

144
00:10:02,390 --> 00:10:07,600
I said we have input features X and some labels Y,

145
00:10:07,790 --> 00:10:09,950
and I sort of said let's approximate Y

146
00:10:10,130 --> 00:10:12,060
using the logistic function of the inputs X.

147
00:10:12,290 --> 00:10:14,440
And at least initially,

148
00:10:14,590 --> 00:10:16,850
the logistic function was sort of pulled out of the air.

149
00:10:17,020 --> 00:10:21,060
In the same way, as I define policy search algorithms,

150
00:10:21,310 --> 00:10:23,040
there'll sort of be a step where I say,

151
00:10:23,210 --> 00:10:25,640
"Well, let's try to compute the actions.

152
00:10:25,850 --> 00:10:27,790
Let's try to approximate what a good action

153
00:10:27,960 --> 00:10:30,550
is using a logistic function of the state."

154
00:10:30,760 --> 00:10:34,530
So again, I'll sort of pull a function out of the air.

155
00:10:34,710 --> 00:10:36,620
I'll say, "Let's just choose a function,

156
00:10:36,780 --> 00:10:39,180
and that'll be our choice of the policy cost,"

157
00:10:39,360 --> 00:10:42,570
and I'll say, "Let's take this input the state,

158
00:10:42,740 --> 00:10:44,300
and then we'll map it through logistic function,and

159
00:10:44,450 --> 00:10:46,900
then hopefully, we'll approximate what is a good function

160
00:10:46,900 --> 00:10:47,900
–excuse me, we'll approximate what is a good action

161
00:10:50,240 --> 00:10:52,970
using a logistic function of the state."

162
00:10:53,170 --> 00:10:54,670
So there's that sort of

163
00:10:54,880 --> 00:10:56,550
the function of the choice of policy cost

164
00:10:56,700 --> 00:11:00,400
that's again a little bit arbitrary,but it's arbitrary

165
00:11:00,570 --> 00:11:02,660
as it was when we were talking about supervised learning.

166
00:11:02,930 --> 00:11:11,850
So to develop our first policy search algorithm,

167
00:11:12,000 --> 00:11:13,900
I'm actually gonna need the new definition.

168
00:11:14,070 --> 00:12:07,840
So our first policy search algorithm,

169
00:12:08,060 --> 00:12:11,720
we'll actually need to work with stochastic policies.

170
00:12:11,980 --> 00:12:14,160
What I mean by stochastic policy

171
00:12:14,370 --> 00:12:16,440
is there's going to be a function

172
00:12:16,660 --> 00:12:20,670
that maps from the space of states across actions.They're

173
00:12:20,850 --> 00:12:25,740
real numbers where pi of S comma A will be interpreted

174
00:12:25,920 --> 00:12:29,830
as the probability of taking this action A in sum state S.

175
00:12:30,030 --> 00:12:32,790
And so we have to add sum over A

176
00:12:32,980 --> 00:12:48,070
In other words, for every state a stochastic policy

177
00:12:48,290 --> 00:12:52,370
specifies a probability distribution over the actions.

178
00:12:52,610 --> 00:13:02,660
So concretely, suppose you are executing some policy pi.

179
00:13:02,840 --> 00:13:04,460
Say I have some stochastic policy pi.

180
00:13:04,620 --> 00:13:06,490
I wanna execute the policy pi.

181
00:13:06,490 --> 00:13:07,490
What that means is that –

182
00:13:08,710 --> 00:13:11,320
in this example let's say I have three actions.

183
00:13:11,510 --> 00:13:14,200
What that means is that suppose I'm in some state S.

184
00:13:14,410 --> 00:13:18,920
I would then compute pi of S comma A1,

185
00:13:19,150 --> 00:13:26,690
pi of S comma A2, pi of S comma A3,

186
00:13:26,890 --> 00:13:28,620
if I have a three action MDP.

187
00:13:28,830 --> 00:13:33,010
These will be three numbers that sum up to one,

188
00:13:33,220 --> 00:13:37,430
and then my chance of taking action A1 will be equal to this.

189
00:13:37,610 --> 00:13:39,040
A1 will be equal to this.

190
00:13:39,220 --> 00:13:44,090
My chance of taking action A3 will be equal to this number.

191
00:13:44,360 --> 00:13:48,700
So that's what it means to execute a stochastic policy.

192
00:13:48,950 --> 00:13:52,010
So as a concrete example,

193
00:13:52,180 --> 00:13:53,900
just let me make this

194
00:13:54,060 --> 00:13:56,390
the concept of why you wanna use stochastic policy is

195
00:13:56,500 --> 00:13:57,610
maybe a little bit hard to understand.

196
00:13:57,780 --> 00:14:05,880
So let me just go ahead and give one specific example

197
00:14:06,060 --> 00:14:08,400
of what a stochastic policy may look like.

198
00:14:08,570 --> 00:14:12,560
For this example, I'm gonna use the inverted pendulum

199
00:14:12,710 --> 00:14:15,410
as my motivating example.

200
00:14:15,720 --> 00:14:17,740
It's that problem of balancing a pole.

201
00:14:20,040 --> 00:14:23,610
We have an inverted pendulum that swings freely,

202
00:14:23,780 --> 00:14:25,310
and you want to move the cart left and right

203
00:14:25,470 --> 00:14:25,980
to keep the pole vertical.

204
00:14:26,160 --> 00:14:30,330
Let's say my actions

205
00:14:30,530 --> 00:14:37,970
for today's example, I'm gonna use that angle

206
00:14:38,190 --> 00:14:41,930
to denote the angle of the pole phi.

207
00:14:42,140 --> 00:14:49,120
I have two actions where A1 is to accelerate left

208
00:14:49,320 --> 00:14:52,100
and A2 is to accelerate right.

209
00:14:55,100 --> 00:14:57,470
Actually, let me just write that the other way around.

210
00:14:57,640 --> 00:15:01,640
A1 is to accelerate right. A2 is to accelerate left.

211
00:15:01,800 --> 00:15:07,100
So let's see. Choose a reward function

212
00:15:07,270 --> 00:15:10,980
that penalizes the pole falling over whatever.And now

213
00:15:11,150 --> 00:15:13,840
let's come up with a stochastic policy for this problem.

214
00:15:14,090 --> 00:15:20,480
To come up with a class of stochastic policies really means

215
00:15:20,660 --> 00:15:22,440
coming up with some class of functions

216
00:15:22,630 --> 00:15:23,870
to approximate what action

217
00:15:24,070 --> 00:15:25,700
you want to take as a function of the state.

218
00:15:25,900 --> 00:15:28,650
So here's my somewhat arbitrary choice.

219
00:15:28,810 --> 00:15:29,610
I'm gonna say that

220
00:15:29,780 --> 00:15:34,830
the probability of action A1,so pi of S comma A1,

221
00:15:35,120 --> 00:15:40,680
I'm gonna write as okay?

222
00:15:40,860 --> 00:15:42,400
And I just chose the logistic function

223
00:15:42,560 --> 00:15:44,550
because it's a convenient function we've used a lot.

224
00:15:44,670 --> 00:15:45,520
So I'm gonna say that

225
00:15:45,680 --> 00:15:48,840
my policy is parameterized by a set of parameters theta,

226
00:15:49,010 --> 00:15:51,020
and for any given set of

227
00:15:51,200 --> 00:15:51,800
parameters theta,

228
00:15:51,980 --> 00:15:55,230
that gives me a stochastic policy.

229
00:15:55,380 --> 00:15:58,990
And if I'm executing that policy with parameters theta,

230
00:15:59,140 --> 00:16:00,770
that means that the chance of A right

231
00:16:00,910 --> 00:16:04,380
my choosing to a set of [inaudible] is given by this number.

232
00:16:04,580 --> 00:16:21,180
Because my chances of executing actions A1 or A2

233
00:16:21,320 --> 00:16:23,800
must sum to one, this gives me pi of S A2.

234
00:16:24,010 --> 00:16:26,680
So just [inaudible], this means that

235
00:16:26,840 --> 00:16:30,530
when I'm in sum state S, I'm going to compute  this number,

236
00:16:30,710 --> 00:16:33,840
compute one over one plus E to the minus state of transpose S.

237
00:16:33,990 --> 00:16:35,460
And then with this probability,

238
00:16:35,590 --> 00:16:38,490
I will execute the accelerate right action,

239
00:16:38,700 --> 00:16:41,670
and with one minus this probability,

240
00:16:41,840 --> 00:16:43,900
I'll execute the accelerate left action.

241
00:16:44,090 --> 00:16:47,240
And again, just to give you a sense of

242
00:16:47,390 --> 00:16:48,920
why this might be a reasonable thing to do,

243
00:16:48,920 --> 00:16:49,920
let's say my state vector is –

244
00:16:54,220 --> 00:17:02,560
this is [inaudible] state,

245
00:17:02,690 --> 00:17:04,950
and I added an extra one as an interceptor,

246
00:17:05,140 --> 00:17:07,190
just to give my logistic function an extra feature.

247
00:17:07,390 --> 00:17:13,640
If I choose my parameters and my policy to be say this,

248
00:17:19,820 --> 00:17:22,030
Then that means that at any state,

249
00:17:23,280 --> 00:17:32,890
the probability of my taking action A1 the probability of

250
00:17:33,160 --> 00:17:35,400
my taking the accelerate right actionis this

251
00:17:35,580 --> 00:17:37,180
one over one plus E to the minus state of

252
00:17:37,490 --> 00:17:38,490
transpose S,

253
00:17:38,850 --> 00:17:43,020
which taking the inner product of theta and S, this just

254
00:17:43,170 --> 00:17:50,140
gives you phi,equals one over one plus E to the minus phi.

255
00:17:50,340 --> 00:17:54,430
And so if I choose my parameters theta as follows,

256
00:17:54,640 --> 00:18:01,680
what that means is that just depending on

257
00:18:02,100 --> 00:18:04,630
the angle phi of my inverted pendulum,

258
00:18:04,860 --> 00:18:11,850
the chance of my accelerating to the right is

259
00:18:12,080 --> 00:18:16,790
just this function of the angle of my inverted pendulum.

260
00:18:16,990 --> 00:18:18,260
And so this means for example that

261
00:18:18,540 --> 00:18:22,200
if my inverted pendulum is leaning far over to the right,

262
00:18:22,390 --> 00:18:24,480
then I'm very likely to accelerate to the right

263
00:18:24,640 --> 00:18:25,760
to try to catch it.

264
00:18:26,080 --> 00:18:28,100
I hope the physics of this inverted pendulum thing

265
00:18:28,280 --> 00:18:28,780
make sense.

266
00:18:29,160 --> 00:18:30,910
If my pole's leaning over to the right,

267
00:18:31,090 --> 00:18:32,800
then I wanna accelerate to the right to  catch it.

268
00:18:32,970 --> 00:18:34,930
And conversely if phi is negative,

269
00:18:35,150 --> 00:18:36,380
it's leaning over to the left,

270
00:18:36,540 --> 00:18:38,140
and I'll accelerate to the left to try to catch it.

271
00:18:38,370 --> 00:18:40,100
So this is one example one example

272
00:18:40,350 --> 00:18:43,090
for one specific choice of parameters theta.

273
00:18:43,260 --> 00:18:46,240
Obviously, this isn't a great policy

274
00:18:46,430 --> 00:18:48,160
because it ignores the rest of the features.

275
00:18:48,380 --> 00:18:50,850
Maybe if the cart is further to the right,

276
00:18:51,400 --> 00:18:52,960
you want it to be less likely to accelerate to the  right,

277
00:18:53,150 --> 00:18:56,510
you can capture that  by changing one of these coefficients

278
00:18:56,680 --> 00:18:58,870
to take into account the actual position of the cart.

279
00:18:59,050 --> 00:19:01,160
And then depending on the velocity of the cart

280
00:19:01,320 --> 00:19:02,790
and the angle of velocity,

281
00:19:02,960 --> 00:19:04,590
you might want to change theta

282
00:19:04,740 --> 00:19:06,780
to take into account these other effects as well.

283
00:19:06,950 --> 00:19:09,450
Maybe if the pole's leaning far to the right,

284
00:19:09,600 --> 00:19:11,760
but is actually on its way to swinging back,

285
00:19:12,930 --> 00:19:14,760
i t's specified to the angle of velocity,

286
00:19:14,930 --> 00:19:16,770
then you might be less worried about

287
00:19:16,940 --> 00:19:18,220
having to accelerate hard to the right.

288
00:19:18,380 --> 00:19:20,380
And so these are the sorts of behavior

289
00:19:20,540 --> 00:19:22,300
you can get by varying the parameters theta.

290
00:19:22,500 --> 00:19:26,920
And so our goal is to tune the parameters theta

291
00:19:27,070 --> 00:19:30,380
our goal in policy search is to tune the parameters theta

292
00:19:30,570 --> 00:19:36,320
so that when we execute the policy pi subscript theta,

293
00:19:36,560 --> 00:19:39,140
the pole stays up as long as possible.

294
00:19:39,390 --> 00:19:45,030
In other words,our goal is to maximize as a function of theta

295
00:19:45,250 --> 00:20:02,590
our goal is to maximize the expected value of the payoff for

296
00:20:02,810 --> 00:20:09,190
when we execute the policy pi theta.

297
00:20:09,430 --> 00:20:12,520
We want to choose parameters theta to maximize that.

298
00:20:16,320 --> 00:20:20,750
Are there questions about the problem set up,

299
00:20:20,970 --> 00:20:22,060
and policy search

300
00:20:22,360 --> 00:20:23,870
and policy classes or anything? Yeah.

301
00:20:25,420 --> 00:20:27,690
Student:In a case where we have more than two actions,

302
00:20:27,850 --> 00:20:28,970
would we use a different theta for each of the distributions

303
00:20:29,130 --> 00:20:32,450
or still have the same parameters?

304
00:20:32,630 --> 00:20:33,630
Instructor (Andrew Ng):Oh, yeah.

305
00:20:33,760 --> 00:20:35,230
Right. So what if we have more than two actions.

306
00:20:35,370 --> 00:20:38,200
It turns out you can choose almost anything

307
00:20:38,370 --> 00:20:39,260
you want for the policy class,

308
00:20:39,450 --> 00:20:43,450
but you have say a fixed number of discrete actions,

309
00:20:43,660 --> 00:20:46,720
I would sometimes use like a softmax parameterization.

310
00:20:47,040 --> 00:20:51,470
Similar to softmax regression

311
00:20:51,660 --> 00:20:53,440
that we saw earlier in the class,

312
00:20:53,640 --> 00:20:56,880
you may say that [inaudible] out of space.

313
00:20:57,080 --> 00:21:11,160
You may have a set of parameters theta 1 through theta D

314
00:21:11,400 --> 00:21:13,940
if you have D actions and

315
00:21:14,180 --> 00:21:20,730
pi equals E to the theta I transpose S over

316
00:21:20,950 --> 00:21:26,030
so that would be an example

317
00:21:26,220 --> 00:21:28,290
of a softmax parameterization for multiple actions.

318
00:21:28,470 --> 00:21:31,560
It turns out that if you have continuous actions,

319
00:21:31,720 --> 00:21:35,150
you can actually make this be a density over the actions A

320
00:21:35,340 --> 00:21:37,880
and parameterized by other things as well.

321
00:21:38,090 --> 00:21:41,580
But the choice of policy class is somewhat up to you,

322
00:21:41,740 --> 00:21:45,010
in the same way that the choice of

323
00:21:45,170 --> 00:21:46,910
whether we chose to use a linear function

324
00:21:47,100 --> 00:21:50,510
or linear function with quadratic features or

325
00:21:50,680 --> 00:21:53,670
whatever in supervised learning that was sort of up to us.

326
00:21:53,860 --> 00:21:58,190
Anything else? Yeah.

327
00:21:58,380 --> 00:22:02,030
Student:[Inaudible] stochastic?

328
00:22:02,200 --> 00:22:02,900
Instructor (Andrew Ng):Yes.

329
00:22:03,410 --> 00:22:04,210
Student:So is it possible

330
00:22:04,400 --> 00:22:07,680
to [inaudible] a stochastic policy

331
00:22:07,930 --> 00:22:11,100
using numbers [inaudible]?

332
00:22:11,460 --> 00:22:13,090
Instructor (Andrew Ng):I see.

333
00:22:13,470 --> 00:22:16,500
Given that MDP has stochastic transition probabilities,

334
00:22:16,720 --> 00:22:20,380
is it possible to use [inaudible] policies and [inaudible]

335
00:22:20,550 --> 00:22:22,720
the stochasticity of the state transition probabilities.

336
00:22:22,930 --> 00:22:24,160
The answer is yes,

337
00:22:24,300 --> 00:22:27,700
but for the purposes of what I want to show  later,

338
00:22:27,890 --> 00:22:29,150
that won't be useful.

339
00:22:29,300 --> 00:22:30,550
But formally, it is possible.

340
00:22:30,550 --> 00:22:31,550
If you already have a fixed –

341
00:22:33,740 --> 00:22:37,600
if you have a fixed policy, then you'd be able to do that.

342
00:22:37,770 --> 00:22:41,170
Anything else? Yeah. No,

343
00:22:41,310 --> 00:22:43,390
I guess even a [inaudible] class of policy can do that,

344
00:22:43,550 --> 00:22:45,790
but for the derivation later,

345
00:22:45,960 --> 00:22:47,730
I actually need to keep it separate.

346
00:22:47,730 --> 00:22:48,730
Actually, could you just –

347
00:22:50,860 --> 00:22:52,940
I know the concept of policy search is

348
00:22:53,030 --> 00:22:54,040
sometimes a little confusing.

349
00:22:54,170 --> 00:22:56,450
Could you just raise your hand if this makes sense?

350
00:22:56,630 --> 00:23:02,770
Okay. Thanks. So let's talk about an algorithm.

351
00:23:02,960 --> 00:23:07,800
What I'm gonna talk about the first algorithm

352
00:23:07,990 --> 00:23:08,900
I'm going to present is

353
00:23:09,060 --> 00:23:11,650
sometimes called the reinforce algorithm.

354
00:23:11,840 --> 00:23:15,650
it turns out isn't exactly the reinforce algorithm ,

355
00:23:15,840 --> 00:23:19,560
as it was originally presented by the author Ron Williams,

356
00:23:19,720 --> 00:23:21,660
but it sort of captures its essence.

357
00:23:21,820 --> 00:23:25,500
Here's the idea.

358
00:23:25,660 --> 00:23:32,660
In the sequel in what I'm about to do,

359
00:23:32,820 --> 00:23:38,750
I'm going to assume that S0 is some fixed initial state.

360
00:23:38,960 --> 00:23:46,800
Or it turns out if S0 is drawn

361
00:23:47,010 --> 00:23:49,600
from some fixed initial state distribution

362
00:23:49,920 --> 00:23:51,070
then everything else will work out,

363
00:23:51,240 --> 00:23:53,330
but let's just say S0 is some fixed initial state.

364
00:23:54,250 --> 00:24:06,230
So my goal is to maximize this expected sum [inaudible].

365
00:24:06,420 --> 00:24:08,470
Given the policy and whatever else, drop that.

366
00:24:08,670 --> 00:24:12,660
So the random variables in this expectation

367
00:24:12,870 --> 00:24:15,670
is a sequence of states and actions: S0, A0,

368
00:24:15,870 --> 00:24:19,430
S1, A1, and so on, up to ST, AT are the random variables.

369
00:24:19,620 --> 00:24:23,300
So let me write out this expectation explicitly as a sum

370
00:24:23,510 --> 00:24:30,260
over all possible state and action sequences of that

371
00:24:30,530 --> 00:24:47,870
so that's what an expectation is.

372
00:24:48,060 --> 00:24:49,790
It's the probability of

373
00:24:49,790 --> 00:24:51,020
the random variables times that.

374
00:24:51,250 --> 00:24:54,860
Let me just expand out this probability.

375
00:24:55,030 --> 00:25:04,070
So the probability of seeing this exact sequence of states

376
00:25:04,230 --> 00:25:09,590
and actions is the probability of the MDP starting

377
00:25:09,740 --> 00:25:10,430
in that state.

378
00:25:10,570 --> 00:25:12,680
If this is a deterministic initial state,

379
00:25:12,900 --> 00:25:14,820
then all the probability mass would be on one  state.

380
00:25:14,950 --> 00:25:16,810
Otherwise, there's some distribution over initial states.

381
00:25:18,270 --> 00:25:24,750
Then times the probability that you chose action

382
00:25:24,960 --> 00:25:31,000
A0 from that state as zero, and then times  the probability

383
00:25:31,190 --> 00:25:36,830
that the MDP's transition probabilities  happen

384
00:25:37,030 --> 00:25:41,310
to transition you  to state S1 where you chose action A0 to

385
00:25:41,520 --> 00:25:51,380
state S0,times the probability that you chose that and so on

386
00:25:51,610 --> 00:26:11,500
The last term here is that, and then times that.

387
00:26:11,750 --> 00:26:13,290
So what I did was just

388
00:26:13,500 --> 00:26:17,110
take this probability of seeing this sequence of states

389
00:26:17,270 --> 00:26:19,620
and actions, and then just rotate explicity

390
00:26:19,930 --> 00:26:21,210
or expanded explicitly like this.

391
00:26:21,380 --> 00:26:23,890
It turns out later on

392
00:26:24,070 --> 00:26:27,670
I'm going to need to write this sum of rewards a lot,

393
00:26:27,830 --> 00:26:31,760
so I'm just gonna call this the payoff from now.

394
00:26:31,990 --> 00:26:33,650
So whenever later in this lecture

395
00:26:33,870 --> 00:26:35,330
I write the word payoff,

396
00:26:35,760 --> 00:26:47,240
I just mean this sum.  So our goal is to

397
00:26:47,670 --> 00:26:49,660
maximize the expected payoff,

398
00:26:49,800 --> 00:26:51,590
so our goal is to maximize this sum.

399
00:26:51,810 --> 00:26:53,910
Let me actually just skip ahead.

400
00:26:54,090 --> 00:26:55,840
I'm going to write down what the final answer is,

401
00:26:55,970 --> 00:26:58,530
and then I'll come back and justify the algorithm.

402
00:26:59,900 --> 00:28:13,930
So here's the algorithm.

403
00:28:14,130 --> 00:28:15,600
This is how we're going to

404
00:28:15,850 --> 00:28:17,750
update the parameters of the algorithm.

405
00:28:17,930 --> 00:28:20,720
We're going to sample a state action sequence.

406
00:28:20,890 --> 00:28:22,140
The way you do this is

407
00:28:22,310 --> 00:28:23,960
you just take your current stochastic policy,

408
00:28:24,180 --> 00:28:26,130
and you execute it in the MDP.

409
00:28:26,340 --> 00:28:28,420
So just go ahead and start from some initial state,

410
00:28:28,660 --> 00:28:30,370
take a stochastic action

411
00:28:30,530 --> 00:28:32,090
according to your current stochastic policy,

412
00:28:32,240 --> 00:28:34,320
see where the state transition probably takes you,

413
00:28:34,500 --> 00:28:36,610
and so you just do that for T times steps,

414
00:28:36,740 --> 00:28:38,010
and that's how you sample the state sequence.

415
00:28:38,180 --> 00:28:39,860
Then you compute the payoff,

416
00:28:40,060 --> 00:28:43,020
and then you perform this update.

417
00:28:45,280 --> 00:28:45,470
So

418
00:28:45,900 --> 00:28:48,320
let's go back and figure out what this algorithm is doing.

419
00:28:48,590 --> 00:28:52,070
Notice that this algorithm performs stochastic updates

420
00:28:52,250 --> 00:28:53,630
because on every step it

421
00:28:53,760 --> 00:28:55,360
updates data according to this thing

422
00:28:55,530 --> 00:28:56,450
on the right hand side.

423
00:28:56,630 --> 00:28:59,340
This thing on the right hand side depends very much on

424
00:28:59,510 --> 00:29:01,950
your payoff and on the state action sequence you saw.

425
00:29:02,180 --> 00:29:03,910
Your state action sequence is random,

426
00:29:04,120 --> 00:29:08,760
so what I want to do is figure out so on every step,

427
00:29:08,950 --> 00:29:11,790
I'll sort of take a step that's chosen randomly

428
00:29:12,000 --> 00:29:15,730
because it depends on this random state action sequence.

429
00:29:15,950 --> 00:29:18,740
So what I want to do is figure out on average

430
00:29:18,940 --> 00:29:20,920
how does it change the parameters theta.

431
00:29:21,140 --> 00:29:24,120
In particular, I want to know

432
00:29:24,320 --> 00:29:27,460
what is the expected value of the change to the parameters.

433
00:29:58,200 --> 00:29:59,120
So I want to know what is

434
00:29:59,300 --> 00:30:04,790
the expected value of this change to my parameters theta.

435
00:30:04,790 --> 00:30:05,790
Our goal is to maximize the  sum [inaudible] –

436
00:30:08,890 --> 00:30:10,520
our goal is to maximize the value of the payoff.

437
00:30:11,090 --> 00:30:16,560
So long as the updates on expectation are on average

438
00:30:16,750 --> 00:30:21,150
taking us uphill on the expected payoff, then we're happy.

439
00:30:21,350 --> 00:30:23,080
It turns out that

440
00:30:23,320 --> 00:30:28,630
this algorithm is a form of stochastic gradient ascent

441
00:30:28,630 --> 00:30:29,630
in which–remember when I talked about

442
00:30:33,140 --> 00:30:36,020
stochastic gradient descent for least squares regression,

443
00:30:36,280 --> 00:30:39,320
I said that you have some parameters maybe you're trying

444
00:30:39,500 --> 00:30:41,570
To minimize a quadratic function.Then

445
00:30:41,740 --> 00:30:47,900
you may have parameters that will wander around randomly

446
00:30:48,080 --> 00:30:50,620
until it gets close to

447
00:30:50,840 --> 00:30:53,760
the optimum of the [inaudible] quadratic surface.

448
00:30:53,970 --> 00:30:57,570
It turns out that the reinforce algorithm

449
00:30:57,710 --> 00:30:58,740
will be very much like that.

450
00:30:59,010 --> 00:31:01,640
It will be a stochastic gradient ascent algorithm in which

451
00:31:01,820 --> 00:31:02,560
on every step

452
00:31:02,700 --> 00:31:04,730
the step we take is a little bit random.

453
00:31:04,950 --> 00:31:06,740
It's determined by the random state action sequence,

454
00:31:06,950 --> 00:31:08,620
but on expectation this

455
00:31:08,790 --> 00:31:11,500
turns out to be essentially gradient ascent algorithm.

456
00:31:11,690 --> 00:31:13,300
And so we'll do something like this.

457
00:31:13,510 --> 00:31:14,570
It'll wander around randomly,

458
00:31:14,730 --> 00:31:16,530
but on average take you towards the optimum.

459
00:31:16,790 --> 00:31:19,390
So let me go ahead and prove that now.

460
00:31:19,590 --> 00:31:39,110
Let's see. What I'm going to do is

461
00:31:39,320 --> 00:31:43,890
I'm going to derive a gradient ascent

462
00:31:44,050 --> 00:31:47,950
update rule for maximizing the expected payoff.

463
00:31:48,170 --> 00:31:49,700
Then I'll hopefully show

464
00:31:49,900 --> 00:31:52,690
that by deriving a gradient ascent update rule,

465
00:31:52,850 --> 00:31:56,020
I'll end up with this thing on expectation.

466
00:31:56,190 --> 00:31:59,780
So before I do the derivation,

467
00:31:59,970 --> 00:32:04,450
let me just remind you of the chain rule  the product rule

468
00:32:04,720 --> 00:32:13,410
for differentiation in which if I have a product of functions,

469
00:32:13,640 --> 00:32:17,270
then the derivative of the product is given

470
00:32:17,470 --> 00:32:39,980
by taking of the derivativesof these things one at a time.

471
00:32:40,500 --> 00:32:42,470
So first I differentiate with respect to F prime,

472
00:32:42,650 --> 00:32:43,210
leaving the other two fixed.

473
00:32:43,410 --> 00:32:44,690
Then I differentiate with respect to G,

474
00:32:44,900 --> 00:32:46,340
leaving the other two fixed.

475
00:32:46,510 --> 00:32:47,960
Then I differentiate with respect to H,

476
00:32:48,140 --> 00:32:49,900
so I get H prime leaving the other two fixed.

477
00:32:50,080 --> 00:32:51,500
So that's the product rule for derivatives.

478
00:32:51,650 --> 00:33:05,650
If you refer back to this equation where earlier

479
00:33:05,820 --> 00:33:09,640
we wrote out that the expected payoff by this equation,

480
00:33:11,080 --> 00:33:13,930
this sum over all the states of the probability

481
00:33:14,250 --> 00:33:14,880
times the payoff.

482
00:33:15,080 --> 00:33:18,700
So what I'm going to do is  take the derivative

483
00:33:18,930 --> 00:33:22,180
of this expression with respect to the parameters theta

484
00:33:22,420 --> 00:33:25,690
because I want to do gradient ascent on this function.

485
00:33:25,920 --> 00:33:27,210
So I'm going to take the derivative of this function

486
00:33:27,410 --> 00:33:28,340
with respect to theta,

487
00:33:28,550 --> 00:33:29,840
and then try to go uphill on this function.

488
00:33:30,040 --> 00:33:32,450
So using the product rule,

489
00:33:32,630 --> 00:33:34,250
when I take the derivative of this function

490
00:33:34,460 --> 00:33:35,160
with respect to theta

491
00:33:35,450 --> 00:33:36,400
what I get is,

492
00:33:36,530 --> 00:33:38,490
we'll end up with the sum of terms right there.

493
00:33:38,670 --> 00:33:41,260
There are a lot of terms here that depend on theta,

494
00:33:41,480 --> 00:33:46,850
and so what I'll end up with is I'll end up having a sum

495
00:33:47,040 --> 00:33:48,870
having one term that corresponds to the derivative

496
00:33:49,050 --> 00:33:50,610
of this keeping everything else fixed,

497
00:33:50,780 --> 00:33:52,640
to have one term from the derivative

498
00:33:52,840 --> 00:33:54,020
of this keeping everything else fixed,

499
00:33:54,160 --> 00:33:56,090
and I'll have one term from the derivative

500
00:33:56,250 --> 00:33:58,160
of that last thing keeping everything else fixed.

501
00:33:58,370 --> 00:33:59,590
So just apply the product rule to this.

502
00:33:59,780 --> 00:34:11,230
Let's write that down. So I have that the derivative

503
00:34:11,380 --> 00:34:13,910
with respect to theta of the expected value of the payoff is

504
00:34:14,130 --> 00:34:17,400
it turns out I can actually do this

505
00:34:17,580 --> 00:34:21,330
entire derivation in exactly four steps,

506
00:34:21,530 --> 00:34:23,360
but each of the steps

507
00:34:23,520 --> 00:34:24,770
requires a huge amount of writing,

508
00:34:25,180 --> 00:34:29,120
so I'll just start writing

509
00:34:29,290 --> 00:34:31,980
and see how that goes, but this is a four step derivation.

510
00:34:38,460 --> 00:34:42,110
so there's the sum over the state action sequences

511
00:34:42,390 --> 00:34:43,370
as we saw before.

512
00:36:03,480 --> 00:36:03,880
Close the bracket,

513
00:36:04,100 --> 00:36:09,950
and then times the payoff. So that huge amount of writing,

514
00:36:10,150 --> 00:36:11,930
that was just taking my previous formula  and

515
00:36:12,200 --> 00:36:14,430
differentiating these terms that

516
00:36:14,680 --> 00:36:16,020
depend on theta one at a time.

517
00:36:16,190 --> 00:36:21,280
This was the term with the derivative of  pi of theta S0 A0.

518
00:36:21,460 --> 00:36:23,930
So there's the first derivative term.

519
00:36:24,110 --> 00:36:25,080
There's the second one.

520
00:36:25,270 --> 00:36:26,470
Then you have plus dot, dot,

521
00:36:26,650 --> 00:36:28,320
dot, like in terms of [inaudible].

522
00:36:28,550 --> 00:36:29,180
That's my last term.

523
00:36:31,310 --> 00:36:37,190
So that was step one of four.

524
00:36:37,560 --> 00:36:53,620
And so by algebra

525
00:37:00,120 --> 00:37:01,260
let me just write this downand convince us

526
00:37:01,510 --> 00:37:02,460
all that it's true.

527
00:38:06,960 --> 00:38:08,330
This is the second of four steps

528
00:38:08,680 --> 00:38:12,180
in which it just convinced itself that if I expand out

529
00:38:12,370 --> 00:38:15,200
take the sum and multiply it by that big product in front,

530
00:38:15,380 --> 00:38:17,220
then I get back that sum of terms I get.

531
00:38:17,410 --> 00:38:20,760
It's essentially for example, when I multiply out,

532
00:38:21,020 --> 00:38:23,840
this product on top of this ratio, of this first fraction,

533
00:38:24,100 --> 00:38:27,430
then pi subscript theta S0 A0,

534
00:38:27,610 --> 00:38:30,720
that would cancel out this pi subscript theta S0 A0

535
00:38:30,970 --> 00:38:33,370
and replace it with   the derivative with respect to

536
00:38:33,580 --> 00:38:34,730
theta of pi theta S0 A0.

537
00:38:34,940 --> 00:38:37,540
So [inaudible] algebra was the second.

538
00:38:37,810 --> 00:38:49,060
But that term on top is just what I worked out previously

539
00:38:49,320 --> 00:38:57,750
was the joint probability of the state action sequence,

540
00:38:58,120 --> 00:39:25,720
and now I have that times that times the payoff.

541
00:39:26,190 --> 00:39:33,910
And so by the definition of expectation,

542
00:39:34,160 --> 00:40:03,360
this is just equal to that thing times the payoff.

543
00:40:07,520 --> 00:40:11,560
So this thing inside the expectation, this is exactly the

544
00:40:11,990 --> 00:40:16,970
step that we were taking in the inner group of

545
00:40:17,210 --> 00:40:20,010
our reinforce algorithm, roughly the reinforce algorithm.

546
00:40:20,250 --> 00:40:22,020
This proves that

547
00:40:22,220 --> 00:40:26,370
the expected value of our change to theta  is exactly

548
00:40:26,610 --> 00:40:30,010
in the direction of the gradient of our expected payoff.

549
00:40:30,250 --> 00:40:31,870
That's how I started this whole derivation.

550
00:40:32,070 --> 00:40:33,890
I said let's look at our expected payoff

551
00:40:34,080 --> 00:40:36,220
and take the derivative of that with respect to theta.

552
00:40:36,450 --> 00:40:39,420
What we've proved is that on expectation,

553
00:40:39,630 --> 00:40:43,010
the step direction I'll take reinforce is exactly

554
00:40:43,210 --> 00:40:45,210
the gradient of the thing I'm trying to optimize.

555
00:40:45,470 --> 00:40:47,350
This shows that

556
00:40:47,520 --> 00:40:51,280
this algorithm is a stochastic gradient ascent algorithm.

557
00:40:51,450 --> 00:40:54,760
I wrote a lot.

558
00:40:54,950 --> 00:40:57,550
Why don't you take a minute to look at the equations

559
00:40:57,730 --> 00:40:59,300
and [inaudible] check if everything makes sense.

560
00:40:59,490 --> 00:41:00,260
I'll erase a couple of boards

561
00:41:00,410 --> 00:41:42,970
and then check if you have questions after that. Questions?

562
00:41:53,450 --> 00:41:56,440
Could you raise your hand if this makes sense? Great.

563
00:41:56,750 --> 00:42:07,120
Some of the comments we talked about

564
00:42:07,420 --> 00:42:10,000
those value function approximation approaches  where

565
00:42:10,220 --> 00:42:12,990
you approximate V star, then you go from V star to pi star.

566
00:42:13,210 --> 00:42:15,150
Then here was also policy search approaches,

567
00:42:15,500 --> 00:42:17,020
where you try to approximate the policy directly.

568
00:42:17,290 --> 00:42:20,000
So let's talk briefly about when either one

569
00:42:20,000 --> 00:42:21,930
may be preferable.

570
00:42:22,140 --> 00:42:24,920
It turns out that policy search algorithms

571
00:42:25,140 --> 00:42:26,520
are especially effective

572
00:42:26,730 --> 00:42:29,520
when you can choose a  simple policy class pi.

573
00:42:29,790 --> 00:42:33,230
So the question really is

574
00:42:33,460 --> 00:42:37,560
for your problem does there exist a simple  function

575
00:42:37,800 --> 00:42:40,160
like a linear function or a logistic function that

576
00:42:40,330 --> 00:42:41,840
mapsfrom features of the state

577
00:42:42,060 --> 00:42:44,460
to the action that works pretty well.

578
00:42:44,680 --> 00:42:47,290
So the problem with the inverted pendulum

579
00:42:47,480 --> 00:42:48,580
this is quite likely to be true.

580
00:42:48,770 --> 00:42:51,460
Going through all the different choices of parameters,

581
00:42:51,670 --> 00:42:52,380
you can say

582
00:42:52,550 --> 00:42:54,540
things like if the pole's leaning towards the right,

583
00:42:54,720 --> 00:42:56,690
then accelerate towards the right to try to catch it.

584
00:42:56,880 --> 00:42:59,990
Thanks to the inverted pendulum, this is probably true.

585
00:43:00,220 --> 00:43:02,260
For lots of what's called low level control tasks,

586
00:43:02,500 --> 00:43:04,360
things like driving a car,

587
00:43:04,590 --> 00:43:07,890
the low level reflexes of do you steer your car left

588
00:43:08,090 --> 00:43:10,030
to avoid another car, do you steer your car left

589
00:43:10,210 --> 00:43:12,930
to follow the car road, flying a helicopter,

590
00:43:13,130 --> 00:43:16,350
again very short time scale types of decisions

591
00:43:16,580 --> 00:43:19,670
I like to think of these as decisions like trained operator

592
00:43:19,860 --> 00:43:22,560
for like a trained driver or a trained pilot.

593
00:43:22,810 --> 00:43:24,920
It would almost be a reflex,

594
00:43:25,140 --> 00:43:26,620
these sorts of very quick instinctive things

595
00:43:27,050 --> 00:43:28,630
where you map very directly

596
00:43:28,780 --> 00:43:30,120
from the inputs, data, and action.

597
00:43:30,330 --> 00:43:31,840
These are problems for which

598
00:43:32,070 --> 00:43:34,010
you can probably choose a reasonable policy class

599
00:43:34,270 --> 00:43:35,610
like a logistic function or something,

600
00:43:35,810 --> 00:43:38,910
and it will often work well. In contrast,

601
00:43:39,220 --> 00:43:43,220
if you have problems that require long multistep reasoning,

602
00:43:43,470 --> 00:43:44,960
so things like a game of chess

603
00:43:45,240 --> 00:43:46,760
where you have to reason carefully

604
00:43:47,050 --> 00:43:48,720
about if I do this, then they'll do that,

605
00:43:48,920 --> 00:43:49,690
then they'll do this,

606
00:43:49,900 --> 00:43:51,500
then they'll do that.

607
00:43:52,000 --> 00:43:53,310
Those I think of as less instinctual,

608
00:43:53,510 --> 00:43:55,110
very high level decision making.

609
00:43:55,330 --> 00:44:00,760
For problems like that, I would sometimes use

610
00:44:00,960 --> 00:44:02,950
a value function approximation approaches instead.

611
00:44:03,170 --> 00:44:07,270
Let me say more about this later.

612
00:44:07,270 --> 00:44:08,270
The last thing I want to do is actually tell you about –

613
00:44:13,660 --> 00:44:20,910
I guess just as a side comment,

614
00:44:21,140 --> 00:44:24,480
it turns out also that if you have POMDP,

615
00:44:24,700 --> 00:44:25,920
if you have a partially observable MDP

616
00:44:26,240 --> 00:44:27,560
I don't want to say too much about this

617
00:44:27,780 --> 00:44:33,030
it turns out that if you only have an approximation

618
00:44:33,320 --> 00:44:40,810
let's call it S hat of the true state,

619
00:44:41,050 --> 00:44:44,820
and so this could be S hat

620
00:44:45,030 --> 00:44:48,900
equals S of T given T from Kalman filter then

621
00:44:49,130 --> 00:45:01,070
you can still use these sorts of policy search algorithms

622
00:45:01,310 --> 00:45:05,040
where you can say pi theta of S hat comma A

623
00:45:05,310 --> 00:45:15,270
There are various other ways you can use

624
00:45:15,500 --> 00:45:17,430
policy search algorithms for POMDPs,

625
00:45:17,650 --> 00:45:18,410
but this is one of them

626
00:45:18,600 --> 00:45:20,560
where if you only have estimates of the state,

627
00:45:20,740 --> 00:45:22,480
you can then choose a policy class

628
00:45:22,690 --> 00:45:24,350
that only looks at your estimate of the state

629
00:45:24,550 --> 00:45:25,230
to choose the action.

630
00:45:25,390 --> 00:45:28,370
By using the same way of estimating the states

631
00:45:28,580 --> 00:45:29,780
in both training and testing,

632
00:45:30,020 --> 00:45:31,730
this will usually do some

633
00:45:31,940 --> 00:45:35,550
so these sorts of policy search algorithms can be applied

634
00:45:35,750 --> 00:45:40,590
often reasonably effectively to POMDPs as well.

635
00:45:40,810 --> 00:45:50,010
There's one more algorithm I wanna talk about,

636
00:45:50,210 --> 00:45:51,870
but some final words on the reinforce algorithm.

637
00:45:52,080 --> 00:45:55,590
It turns out the reinforce algorithm often works well

638
00:45:55,800 --> 00:45:58,020
but is often extremely slow. So

639
00:45:58,200 --> 00:46:02,390
it [inaudible] works, but one thing to watch out for is

640
00:46:02,560 --> 00:46:04,790
that because you're taking these gradient ascent steps

641
00:46:04,990 --> 00:46:07,200
that are very noisy,you're sampling a state action sequence,

642
00:46:07,210 --> 00:46:08,000
and then you're sort of taking a gradient ascent step

643
00:46:08,290 --> 00:46:11,530
in essentially a sort of random direction

644
00:46:11,710 --> 00:46:13,330
that only on expectation is correct.

645
00:46:13,470 --> 00:46:16,630
The gradient ascent direction for reinforce

646
00:46:16,780 --> 00:46:19,480
can sometimes be a bit noisy, and so it's not that uncommon

647
00:46:19,650 --> 00:46:22,250
to need like a million iterations of gradient ascent,

648
00:46:22,410 --> 00:46:25,500
or ten million, or 100 million iterations of gradient ascent

649
00:46:25,680 --> 00:46:28,070
for reinforce [inaudible], so that's just something

650
00:46:28,210 --> 00:46:28,740
to watch out for.

651
00:46:28,740 --> 00:46:29,740
One consequence of that is in the reinforce algorithm –

652
00:46:40,630 --> 00:46:42,760
I shouldn't really call it reinforce.

653
00:46:42,910 --> 00:46:44,710
In what's essentially the reinforce algorithm,

654
00:46:44,880 --> 00:46:45,810
there's this step

655
00:46:45,970 --> 00:46:47,600
where you need to sample a state action sequence.

656
00:46:47,850 --> 00:46:52,590
So in principle you could do this on your own  robot.

657
00:46:52,780 --> 00:46:54,470
If there were a robot you were trying to control,

658
00:46:54,650 --> 00:46:56,910
you can actually physically initialize in some state,

659
00:46:57,090 --> 00:46:58,170
pick an action and so on,

660
00:46:58,320 --> 00:47:00,250
and go from there to sample a state action sequence.

661
00:47:00,410 --> 00:47:03,590
But if you need to do this ten million times,

662
00:47:03,760 --> 00:47:05,300
you probably don't want to [inaudible]

663
00:47:05,490 --> 00:47:06,470
your robot ten million times.

664
00:47:06,650 --> 00:47:09,120
I personally have seen many more

665
00:47:09,260 --> 00:47:11,440
applications of reinforce in simulation.

666
00:47:11,620 --> 00:47:15,270
You can easily run ten thousand simulations or

667
00:47:15,400 --> 00:47:17,870
ten million simulations of your robot in simulation maybe,

668
00:47:17,870 --> 00:47:18,870
but you might not want to do that –

669
00:47:20,560 --> 00:47:22,200
have your robot physically repeat

670
00:47:22,420 --> 00:47:23,860
some action ten million times. So

671
00:47:24,030 --> 00:47:27,220
I personally have seen many more applications of reinforce

672
00:47:27,390 --> 00:47:30,020
to learn using a simulator than

673
00:47:30,200 --> 00:47:32,600
to actually do this on a physical device.

674
00:47:32,780 --> 00:47:37,730
The last thing I wanted to do is

675
00:47:37,890 --> 00:47:38,760
tell you about one other algorithm,

676
00:47:38,910 --> 00:47:40,580
one final policy search algorithm. [Inaudible]

677
00:47:40,780 --> 00:47:42,300
the laptop display please.

678
00:47:52,520 --> 00:47:54,380
I t's a policy search algorithm called  Pegasus

679
00:47:54,610 --> 00:47:58,750
that's actually what we use on our autonomous helicopter

680
00:47:59,140 --> 00:48:00,880
flight things for many years.

681
00:48:01,080 --> 00:48:02,910
There are some other things we do now.

682
00:48:03,100 --> 00:48:06,090
So here's the idea.

683
00:48:06,260 --> 00:48:10,010
There's a reminder slide on RL formalism.

684
00:48:10,170 --> 00:48:11,270
There's nothing here that you don't know,

685
00:48:11,480 --> 00:48:15,430
but I just want to pictorially describe the RL formalism

686
00:48:15,590 --> 00:48:17,650
because I'll use that later. I'm gonna draw

687
00:48:17,840 --> 00:48:20,060
the reinforcement learning picture as follows.

688
00:48:20,230 --> 00:48:22,810
The initialized [inaudible] system,

689
00:48:22,960 --> 00:48:25,380
say a helicopter or whatever in sum state S0,

690
00:48:25,570 --> 00:48:28,510
you choose an action A0, and then you'll say

691
00:48:28,690 --> 00:48:31,030
helicopter dynamics takes you to some new state S1,

692
00:48:31,220 --> 00:48:33,800
you choose some other action A1, and so on.

693
00:48:33,990 --> 00:48:36,280
And then you have some reward function,

694
00:48:36,290 --> 00:48:38,810
which you reply to the sequence of states you summed out,

695
00:48:39,030 --> 00:48:40,270
and that's your total payoff.

696
00:48:40,450 --> 00:48:42,520
So this is just a picture

697
00:48:42,650 --> 00:48:45,920
I wanna use to summarize the RL problem.

698
00:48:46,110 --> 00:48:51,540
Our goal is to maximize the expected payoff, which is this,

699
00:48:51,700 --> 00:48:52,720
the expected sum of the rewards.

700
00:48:52,870 --> 00:48:54,630
And our goal is to learn the policy,

701
00:48:54,790 --> 00:48:56,300
which I denote by a green box.

702
00:48:56,450 --> 00:48:58,110
So our policy

703
00:48:58,320 --> 00:49:01,580
and I'll switch back to deterministic policies for now.

704
00:49:01,770 --> 00:49:04,460
So my deterministic policy will be

705
00:49:04,610 --> 00:49:07,800
some function mapping from the states to the actions.

706
00:49:07,990 --> 00:49:12,570
As a concrete example, you imagine that

707
00:49:12,790 --> 00:49:14,270
in the policy search setting,

708
00:49:14,430 --> 00:49:16,720
you may have a linear class of policies.

709
00:49:16,890 --> 00:49:19,780
So you may imagine that the action A

710
00:49:19,920 --> 00:49:23,300
will be say a linear function of the states, and your goal

711
00:49:23,480 --> 00:49:27,660
is to learn the parameters of the linear function.

712
00:49:27,840 --> 00:49:30,230
So imagine trying to do linear progression on policies,

713
00:49:30,410 --> 00:49:31,150
except you're trying to

714
00:49:31,280 --> 00:49:32,790
optimize the reinforcement learning objective.

715
00:49:32,970 --> 00:49:35,060
So just [inaudible] imagine that

716
00:49:35,250 --> 00:49:37,810
the action A is state of transpose S, and

717
00:49:38,010 --> 00:49:39,460
you go and policy search this

718
00:49:39,650 --> 00:49:41,440
to come up with good parameters theta

719
00:49:41,610 --> 00:49:43,590
so as to maximize the expected payoff.

720
00:49:43,790 --> 00:49:46,520
That would be one setting in which this picture  applies.

721
00:49:46,700 --> 00:49:49,300
There's the idea. Quite often

722
00:49:49,490 --> 00:49:53,050
we come up with a model or a simulator for the  MDP,

723
00:49:53,210 --> 00:49:57,710
and as before a model or a simulator is just a box that

724
00:49:57,840 --> 00:50:00,020
takes this input some state ST,

725
00:50:00,200 --> 00:50:02,290
takes this input some action AT,

726
00:50:02,590 --> 00:50:05,360
and then outputs some [inaudible] state ST plus one

727
00:50:05,560 --> 00:50:06,650
that you might want to take in the MDP.

728
00:50:06,860 --> 00:50:09,650
This ST plus one will be a random state.

729
00:50:09,840 --> 00:50:10,540
It will be drawn from

730
00:50:10,790 --> 00:50:13,050
the random state transition probabilities of MDP.

731
00:50:13,270 --> 00:50:15,730
This is important. Very important,

732
00:50:15,940 --> 00:50:19,180
ST plus one will be a random function ST and AT.

733
00:50:19,360 --> 00:50:21,450
In the simulator, this is [inaudible].

734
00:50:21,640 --> 00:50:24,070
So for example, for autonomous helicopter flight,

735
00:50:24,270 --> 00:50:28,880
you build a simulator using supervised learning,an algorithm

736
00:50:29,060 --> 00:50:31,160
like linear regression [inaudible] to linear regression,

737
00:50:31,370 --> 00:50:34,480
so we can get a nonlinear model of the dynamics of

738
00:50:34,720 --> 00:50:37,730
what ST plus one is as a random function of ST and AT.

739
00:50:37,940 --> 00:50:42,670
Now once you have a simulator,

740
00:50:42,890 --> 00:50:48,750
given any fixed policy you can quite straightforwardly

741
00:50:48,930 --> 00:50:51,080
evaluate any policy in a simulator.

742
00:50:51,300 --> 00:50:55,660
Concretely, our goal is to

743
00:50:55,860 --> 00:50:57,610
find the policy pi mapping from states to actions,

744
00:50:57,820 --> 00:51:01,520
so the goal is to find the green box like that.

745
00:51:01,740 --> 00:51:02,460
It works well.

746
00:51:02,630 --> 00:51:06,370
So if you have any one fixed policy pi,

747
00:51:06,590 --> 00:51:10,760
you can evaluate the policy pi just using the simulator

748
00:51:10,960 --> 00:51:15,320
via the picture shown at the bottom of the slide.

749
00:51:15,570 --> 00:51:17,020
So concretely, you can

750
00:51:17,170 --> 00:51:20,110
take your initial state S0, feed it into the policy pi,

751
00:51:20,320 --> 00:51:22,960
your policy pi will output some action A0,

752
00:51:23,160 --> 00:51:24,330
you plug it in the simulator,

753
00:51:24,490 --> 00:51:26,920
the simulator outputs a random state S1,

754
00:51:27,150 --> 00:51:29,560
you feed S1 into the policy and so on,

755
00:51:29,760 --> 00:51:33,100
and you get a sequence of states S0 through ST that

756
00:51:33,270 --> 00:51:34,810
your helicopter flies through in simulation.

757
00:51:35,010 --> 00:51:38,000
Then sum up the rewards, and this

758
00:51:38,160 --> 00:51:41,780
gives you an estimate of the expected payoff of the policy.

759
00:51:41,950 --> 00:51:44,110
This picture is just a fancy way

760
00:51:44,260 --> 00:51:46,940
of saying fly your helicopter in simulation

761
00:51:47,130 --> 00:51:48,500
and see how well it does, and measure

762
00:51:48,680 --> 00:51:51,040
the sum of rewards you get on average in the simulator.

763
00:51:51,250 --> 00:51:55,410
The picture I've drawn here assumes

764
00:51:55,590 --> 00:51:57,940
that you run your policy through the simulator just once.

765
00:51:58,130 --> 00:52:00,710
In general, you would run the policy through the simulator

766
00:52:00,930 --> 00:52:03,150
some number of times and then average to get an average

767
00:52:03,350 --> 00:52:06,230
over M simulations to get a better estimate

768
00:52:06,400 --> 00:52:13,090
of the policy's expected payoff. Now that

769
00:52:13,270 --> 00:52:17,140
I have a way given any one fixed policy,

770
00:52:17,340 --> 00:52:18,860
this gives me a way

771
00:52:19,040 --> 00:52:22,300
to evaluate the expected payoff of that policy.

772
00:52:22,500 --> 00:52:27,350
So one reasonably obvious thing you might  try to do

773
00:52:27,540 --> 00:52:30,470
is then just search for a policy, in other words

774
00:52:30,680 --> 00:52:32,790
search for parameters theta for your policy,

775
00:52:32,970 --> 00:52:36,700
that gives you high estimated payoff. Does that make sense?

776
00:52:36,880 --> 00:52:38,310
So my policy has some parameters theta,

777
00:52:38,500 --> 00:52:43,650
so my policy is my actions A are equal to theta transpose S

778
00:52:43,870 --> 00:52:45,210
say if there's a linear policy.

779
00:52:45,380 --> 00:52:48,770
For any fixed value of the parameters theta,

780
00:52:48,990 --> 00:52:52,530
I can evaluate I can get an estimate for how good

781
00:52:52,710 --> 00:52:53,650
the policy is using the simulator.

782
00:52:54,940 --> 00:52:58,770
One thing I might try to do is search for parameters theta

783
00:52:58,980 --> 00:52:59,890
to try to maximize my estimated payoff.

784
00:53:00,200 --> 00:53:04,950
It turns out you can sort of do that,

785
00:53:05,850 --> 00:53:09,550
but that idea as I've just stated is hard to get to work.

786
00:53:09,730 --> 00:53:14,200
Here's the reason. The simulator allows us to

787
00:53:14,390 --> 00:53:16,980
evaluate policy, so let's search for policy of high value.

788
00:53:17,190 --> 00:53:19,860
The difficulty is that the simulator is random,

789
00:53:20,080 --> 00:53:22,480
and so every time we evaluate a policy,

790
00:53:22,660 --> 00:53:24,940
we get back a very slightly different answer.

791
00:53:25,200 --> 00:53:30,260
So in the cartoon below, I want you to imagine

792
00:53:30,430 --> 00:53:32,970
that the horizontal axis is the space of policies.

793
00:53:33,180 --> 00:53:37,800
In other words, as I vary the parameters in  my policy,

794
00:53:38,030 --> 00:53:40,540
I get different points on the horizontal axis here.

795
00:53:40,740 --> 00:53:42,080
As I vary the parameters theta,

796
00:53:42,230 --> 00:53:43,110
I get different policies,

797
00:53:43,290 --> 00:53:45,500
and so I'm moving along the X axis,

798
00:53:45,700 --> 00:53:49,470
and my total payoff I'm gonna plot on the vertical axis,

799
00:53:49,680 --> 00:53:52,250
and the red line in this cartoon

800
00:53:52,440 --> 00:53:54,350
is the expected payoff of the different policies.

801
00:53:54,540 --> 00:53:57,640
My goal is to find

802
00:53:57,800 --> 00:54:00,120
the policy with the highest expected payoff.

803
00:54:00,340 --> 00:54:04,090
You could search for a policy with high expected payoff,

804
00:54:04,280 --> 00:54:06,410
but every time you evaluate a policy say

805
00:54:06,580 --> 00:54:07,750
I evaluate some policy,

806
00:54:07,910 --> 00:54:09,810
then I might get a point that just

807
00:54:10,030 --> 00:54:12,660
by chance looked a little bit better than it should have.

808
00:54:12,850 --> 00:54:14,570
If I evaluate a second policy

809
00:54:14,720 --> 00:54:16,410
and just by chance it looked a little bit worse.

810
00:54:16,410 --> 00:54:17,410
I evaluate a third policy,fourth,sometimes you look here–

811
00:54:22,220 --> 00:54:24,320
sometimes I might actually evaluate exactly

812
00:54:24,570 --> 00:54:25,580
the same policy twice

813
00:54:25,870 --> 00:54:28,280
and get back slightly different answers just because

814
00:54:28,540 --> 00:54:30,530
my simulator is random, so when I

815
00:54:30,700 --> 00:54:33,440
apply the same policy twice in simulation,

816
00:54:33,590 --> 00:54:35,270
I might get back slightly different answers.

817
00:54:35,480 --> 00:54:40,320
So as I evaluate more and more policies,

818
00:54:40,460 --> 00:54:41,530
these are the pictures I get.

819
00:54:41,710 --> 00:54:43,870
My goal is to try to optimize the red line.

820
00:54:44,090 --> 00:54:46,430
I hope you appreciate this is a hard problem,

821
00:54:47,120 --> 00:54:49,150
especially when all [inaudible] optimization algorithm

822
00:54:49,440 --> 00:54:51,250
gets to see are these black dots, and

823
00:54:51,500 --> 00:54:53,610
they don't have direct access to the red line.

824
00:54:53,800 --> 00:54:55,820
So when my input space is

825
00:54:56,000 --> 00:54:57,410
some fairly high dimensional space,

826
00:54:57,590 --> 00:54:58,860
if I have ten parameters,

827
00:54:59,100 --> 00:55:01,700
the horizontal axis would actually be a 10-D space,

828
00:55:01,920 --> 00:55:06,080
all I get are these noisy estimates of what the red line is.

829
00:55:06,300 --> 00:55:08,720
This is a very hard stochastic optimization problem.

830
00:55:08,950 --> 00:55:11,520
So it turns out there's one way

831
00:55:11,710 --> 00:55:13,260
to make this optimization problem much easier.

832
00:55:13,430 --> 00:55:18,020
Here's the idea. And the method is called Pegasus,

833
00:55:18,200 --> 00:55:21,800
which is an acronym for something. I'll tell you later.

834
00:55:21,990 --> 00:55:24,920
So the simulator repeatedly

835
00:55:25,120 --> 00:55:27,760
makes calls to a random number generator

836
00:55:27,960 --> 00:55:30,520
to generate random numbers RT,

837
00:55:30,750 --> 00:55:32,730
which are used to simulate the stochastic dynamics.

838
00:55:32,930 --> 00:55:35,330
What I mean by that is that the simulator

839
00:55:35,510 --> 00:55:37,320
takes this input of state and action,

840
00:55:37,490 --> 00:55:39,180
and it outputs the mixed state randomly,

841
00:55:39,350 --> 00:55:41,850
and if you peer into the simulator,

842
00:55:42,020 --> 00:55:43,820
if you open the box of the simulator and ask

843
00:55:44,000 --> 00:55:45,620
how is my simulator generating

844
00:55:45,800 --> 00:55:48,010
these random mixed states ST plus one,

845
00:55:48,180 --> 00:55:53,870
pretty much the only way to do so pretty much the only way

846
00:55:54,010 --> 00:55:56,390
to write a simulator with random outputs is

847
00:55:56,570 --> 00:55:59,710
we're gonna make calls to a random number generator,

848
00:55:59,910 --> 00:56:02,740
and get random numbers, these RTs,

849
00:56:02,980 --> 00:56:04,730
and then you have some function

850
00:56:04,870 --> 00:56:06,950
that takes this input S0, A0,

851
00:56:07,140 --> 00:56:11,670
and the results of your random number generator, it computes

852
00:56:11,870 --> 00:56:12,870
some mixed state

853
00:56:13,010 --> 00:56:14,620
as a function of the inputs and of the random

854
00:56:14,810 --> 00:56:16,430
number it got from the random number generator.

855
00:56:16,580 --> 00:56:18,880
This is pretty much the only way

856
00:56:19,040 --> 00:56:21,500
anyone implements any random code,

857
00:56:21,660 --> 00:56:23,920
any code that generates random outputs.

858
00:56:24,070 --> 00:56:25,630
You make a call to a random number generator,

859
00:56:25,800 --> 00:56:28,280
and you compute some function of the random number

860
00:56:28,440 --> 00:56:29,510
and of your other inputs.

861
00:56:29,670 --> 00:56:34,540
The reason that when you evaluate different policies

862
00:56:34,730 --> 00:56:36,630
you get different answers is because

863
00:56:36,820 --> 00:56:39,530
every time you rerun the simulator,

864
00:56:39,690 --> 00:56:41,530
you get a different sequence of random numbers

865
00:56:41,740 --> 00:56:43,000
from the random number generator,

866
00:56:43,190 --> 00:56:45,280
and so you get a different answer every time,

867
00:56:45,470 --> 00:56:47,180
even if you evaluate the same policy twice.

868
00:56:47,390 --> 00:56:51,190
So here's the idea. Let's say

869
00:56:51,380 --> 00:56:54,130
we fix in advance the sequence of random numbers,

870
00:56:54,330 --> 00:56:58,800
choose R1, R2, up to RT minus one.

871
00:56:58,990 --> 00:57:03,310
Fix the sequence of random numbers in advance,and

872
00:57:03,520 --> 00:57:07,100
we'll always use the same sequence of random numbers

873
00:57:07,290 --> 00:57:08,840
to evaluate different policies.

874
00:57:09,030 --> 00:57:14,190
Go into your code and fix R1, R2, through RT minus one.

875
00:57:14,370 --> 00:57:16,850
Choose them randomly once and then fix them forever.

876
00:57:17,030 --> 00:57:19,890
If you always use the same sequence of random numbers,

877
00:57:20,070 --> 00:57:22,620
then the system is no longer random,

878
00:57:22,810 --> 00:57:24,970
and if you evaluate the same policy twice,

879
00:57:25,160 --> 00:57:27,120
you get back exactly the same answer.

880
00:57:27,320 --> 00:57:32,300
And so these sequences of random numbers,[inaudible]

881
00:57:32,490 --> 00:57:34,020
call them scenarios, and Pegasus is

882
00:57:34,160 --> 00:57:35,650
an acronym for policy evaluation

883
00:57:35,950 --> 00:57:37,940
of gradient and search using scenarios.

884
00:57:38,180 --> 00:57:44,710
So when you do that, this is the picture you get.

885
00:57:44,950 --> 00:57:48,190
As before, the red line is your expected payoff,

886
00:57:48,390 --> 00:57:50,730
and by fixing the random numbers, you've defined

887
00:57:50,950 --> 00:57:52,770
some estimate of the expected payoff.

888
00:57:52,980 --> 00:57:57,140
And as you evaluate different policies, they're still

889
00:57:57,350 --> 00:57:59,820
only approximations to their true expected payoff,

890
00:58:00,000 --> 00:58:01,060
but at least now

891
00:58:01,210 --> 00:58:02,870
you have a deterministic function to optimize,

892
00:58:03,040 --> 00:58:05,510
and you can now apply your favorite algorithms,

893
00:58:05,700 --> 00:58:08,580
be it gradient ascent or some sort of local

894
00:58:08,760 --> 00:58:13,860
[inaudible] search to try to optimize the black curve.

895
00:58:14,140 --> 00:58:16,970
This gives you a much easier optimization problem,

896
00:58:17,180 --> 00:58:20,870
and you can optimize this to find hopefully a good policy.

897
00:58:21,140 --> 00:58:25,110
So this is the Pegasus policy search method.

898
00:58:25,330 --> 00:58:29,920
So when I started to talk about reinforcement learning,

899
00:58:30,140 --> 00:58:33,560
I showed that video of a helicopter flying upside down.

900
00:58:33,760 --> 00:58:35,720
That was actually done using exactly method,

901
00:58:35,950 --> 00:58:37,880
using exactly this policy search algorithm.

902
00:58:38,150 --> 00:58:43,330
This seems to scale well even to fairly large problems,

903
00:58:43,520 --> 00:58:45,630
even to fairly high dimensional state spaces.

904
00:58:45,830 --> 00:58:49,230
Typically Pegasus policy search algorithms

905
00:58:49,230 --> 00:58:50,230
have been using the optimization problem is still –

906
00:58:52,420 --> 00:58:55,290
is much easier than the stochastic version,

907
00:58:55,510 --> 00:58:57,740
but sometimes it's not entirely trivial,

908
00:58:57,950 --> 00:59:00,040
and so you have to apply this sort of method with

909
00:59:00,270 --> 00:59:03,460
maybe on the order of ten parameters or tens of parameters,

910
00:59:03,730 --> 00:59:04,890
so 30, 40 parameters,

911
00:59:05,090 --> 00:59:06,870
but not thousands of parameters,

912
00:59:07,090 --> 00:59:10,200
at least in these sorts of things with them.

913
00:59:10,400 --> 00:59:12,680
Student:So is that method different

914
00:59:12,870 --> 00:59:15,430
than just assuming that you know your simulator  exactly,

915
00:59:15,660 --> 00:59:17,410
just throwing away all the random numbers entirely?

916
00:59:17,610 --> 00:59:22,250
Instructor (Andrew Ng):So is this different

917
00:59:22,450 --> 00:59:24,520
from assuming that we have a deterministic simulator?

918
00:59:24,960 --> 00:59:29,570
The answer is no. In the way you do this,

919
00:59:29,790 --> 00:59:33,020
for the sake of simplicity I talked about

920
00:59:33,190 --> 00:59:34,810
one sequence of random numbers.

921
00:59:35,060 --> 00:59:38,470
What you do is so imagine that the random numbers

922
00:59:38,650 --> 00:59:41,690
are simulating different wind gusts against your helicopter.

923
00:59:41,910 --> 00:59:44,220
So what you want to do isn't really evaluate

924
00:59:44,460 --> 00:59:46,520
just against one pattern of wind gusts.

925
00:59:46,740 --> 00:59:47,760
What you want to do is

926
00:59:48,000 --> 00:59:51,290
sample some set of different patterns of wind gusts,

927
00:59:51,480 --> 00:59:53,320
and evaluate against all of them in average.

928
00:59:53,530 --> 00:59:55,720
So what you do is you actually sample

929
00:59:55,940 --> 00:59:58,930
say 100 some number I made up

930
00:59:59,140 --> 01:00:00,580
like 100 sequences of random numbers,

931
01:00:00,830 --> 01:00:03,210
and every time you want to evaluate a policy,

932
01:00:03,420 --> 01:00:06,720
you evaluate it against all 100 sequences

933
01:00:06,950 --> 01:00:08,300
of random numbers and then average.

934
01:00:08,500 --> 01:00:10,560
This is in exactly the same way

935
01:00:10,720 --> 01:00:12,040
that on this earlier picture

936
01:00:12,250 --> 01:00:15,740
you wouldn't necessarily evaluate the policy just once.

937
01:00:15,950 --> 01:00:17,950
You evaluate it maybe 100 times in simulation,

938
01:00:18,200 --> 01:00:18,900
and then average

939
01:00:19,070 --> 01:00:20,730
to get a better estimate of the expected reward.

940
01:00:20,910 --> 01:00:23,520
In the same way, you do that here

941
01:00:23,740 --> 01:00:26,740
but with 100 fixed sequences of random numbers.

942
01:00:26,950 --> 01:00:29,880
Does that make sense? Any other questions?

943
01:00:30,080 --> 01:00:34,540
Student:If we use 100 scenarios

944
01:00:34,770 --> 01:00:36,910
and get an estimate for the policy,

945
01:00:37,090 --> 01:00:41,570
[inaudible] 100 times [inaudible] random numbers

946
01:00:41,770 --> 01:00:47,130
[inaudible] won't you get similar ideas [inaudible]?

947
01:00:48,040 --> 01:00:48,860
Instructor (Andrew Ng):Yeah. I guess you're right.

948
01:00:49,250 --> 01:00:51,140
So the quality for a fixed policy,

949
01:00:51,410 --> 01:00:54,910
the quality of the approximation is equally

950
01:00:55,170 --> 01:00:56,200
good for both cases.

951
01:00:56,360 --> 01:00:59,290
The advantage of fixing the random numbers is that

952
01:00:59,480 --> 01:01:02,820
you end up with an optimization problem that's much easier.

953
01:01:03,040 --> 01:01:07,870
I have some search problem, and on the horizontal axis

954
01:01:08,060 --> 01:01:09,490
there's a space of control policies,

955
01:01:09,680 --> 01:01:12,150
and my goal is to find a control policy

956
01:01:12,400 --> 01:01:13,930
that maximizes the payoff.

957
01:01:14,110 --> 01:01:17,320
The problem with this earlier setting was

958
01:01:17,510 --> 01:01:21,240
that when I evaluate policies I get these noisy  estimates,

959
01:01:21,480 --> 01:01:25,130
and then it's just very hard to optimize the red curve

960
01:01:25,320 --> 01:01:26,700
if I have these points that are all over the place.

961
01:01:26,880 --> 01:01:28,130
And if I evaluate the same policy twice,

962
01:01:28,350 --> 01:01:29,580
I don't even get back the same answer.

963
01:01:29,780 --> 01:01:31,910
By fixing the random numbers,

964
01:01:32,120 --> 01:01:35,870
the algorithm still doesn't get to see the red curve,

965
01:01:36,060 --> 01:01:38,880
but at least it's now optimizing a deterministic function.

966
01:01:39,120 --> 01:01:41,440
That makes the optimization problem much easier.

967
01:01:41,650 --> 01:01:42,830
Does that make sense?

968
01:01:43,020 --> 01:01:49,580
Student:So every time you fix the random numbers,

969
01:01:49,750 --> 01:01:51,200
you get a nice curve to optimize.

970
01:01:51,380 --> 01:01:52,810
And then you change the random numbers

971
01:01:52,990 --> 01:01:54,270
to get a bunch of different curves that

972
01:01:54,540 --> 01:01:57,370
are easy to optimize. And then you smush them together?

973
01:01:57,630 --> 01:02:00,030
Instructor (Andrew Ng):Let's see. I have just

974
01:02:00,210 --> 01:02:02,500
one nice black curve that I'm trying to optimize.

975
01:02:02,700 --> 01:02:03,890
Student:For each scenario.

976
01:02:04,070 --> 01:02:04,820
Instructor (Andrew Ng):I see.

977
01:02:05,200 --> 01:02:06,550
So I'm gonna average over M scenarios,

978
01:02:06,770 --> 01:02:09,570
so I'm gonna average over 100 scenarios.

979
01:02:09,730 --> 01:02:10,570
So the black curve here is defined

980
01:02:10,790 --> 01:02:13,430
by averaging over a large set of scenarios.

981
01:02:13,630 --> 01:02:17,140
Does that make sense? So instead of only one

982
01:02:17,340 --> 01:02:19,760
if the averaging thing doesn't make sense, imagine that

983
01:02:19,950 --> 01:02:21,660
there's just one sequence of random numbers.

984
01:02:21,820 --> 01:02:22,820
That might be easier to think about.

985
01:02:22,990 --> 01:02:25,470
Fix one sequence of random numbers,

986
01:02:25,630 --> 01:02:27,300
and every time I evaluate another policy,

987
01:02:27,440 --> 01:02:30,300
I evaluate against the same sequence of random numbers,

988
01:02:30,460 --> 01:02:32,980
and that gives me a nice deterministic function to optimize.

989
01:02:33,180 --> 01:02:40,470
Any other questions? The advantage is really that

990
01:02:40,650 --> 01:02:42,470
one way to think about it is

991
01:02:42,650 --> 01:02:44,700
when I evaluate the same policy twice,

992
01:02:44,900 --> 01:02:47,040
at least I get back the same answer.

993
01:02:47,270 --> 01:02:49,700
This gives me a deterministic function

994
01:02:49,930 --> 01:02:52,460
mapping from parameters in my policy

995
01:02:52,680 --> 01:02:55,550
to my estimate of the expected payoff.

996
01:02:55,890 --> 01:02:58,870
That's just a function that I can try to

997
01:02:59,120 --> 01:03:00,480
optimize using the search algorithm.

998
01:03:00,710 --> 01:03:15,940
So we use this algorithm for inverted hovering,

999
01:03:16,170 --> 01:03:19,150
and again policy search algorithms tend to work well

1000
01:03:19,350 --> 01:03:21,880
when you can find a reasonably simple policy

1001
01:03:22,130 --> 01:03:24,470
mapping from the states to the actions.

1002
01:03:24,710 --> 01:03:29,000
This is sort of especially the low level control tasks,

1003
01:03:29,160 --> 01:03:31,590
which I think of as sort of reflexes almost.

1004
01:03:31,800 --> 01:03:35,750
Completely, if you want to solve a problem like Tetris

1005
01:03:35,940 --> 01:03:38,190
where you might plan ahead a few steps

1006
01:03:38,410 --> 01:03:40,770
about what's a nice configuration of the board,or something

1007
01:03:41,050 --> 01:03:44,280
like a game of chess, or problems of long path plannings

1008
01:03:44,490 --> 01:03:45,930
of go here, then go there, then go there,

1009
01:03:46,350 --> 01:03:47,800
then sometimes you might

1010
01:03:47,980 --> 01:03:49,710
apply a value function method instead.

1011
01:03:49,940 --> 01:03:52,660
But for tasks like helicopter flight,

1012
01:03:52,880 --> 01:03:56,470
for low level control tasks, f

1013
01:03:56,660 --> 01:03:59,340
or the reflexes of driving or controlling various robots,

1014
01:03:59,540 --> 01:04:04,020
policy search algorithms were easier

1015
01:04:04,240 --> 01:04:07,360
we can sometimes more easily approximate

1016
01:04:07,600 --> 01:04:09,270
the policy directly works very well.

1017
01:04:09,500 --> 01:04:13,700
Some [inaudible] the state of RL today.

1018
01:04:13,910 --> 01:04:17,090
RL algorithms are applied to a wide range of problems,

1019
01:04:17,290 --> 01:04:20,900
and the key is really sequential decision making.

1020
01:04:21,150 --> 01:04:22,960
The place where you think about applying

1021
01:04:23,170 --> 01:04:24,410
reinforcement learning algorithm is

1022
01:04:24,620 --> 01:04:26,360
when you need to make a decision, then another decision,

1023
01:04:26,550 --> 01:04:27,140
then another decision,

1024
01:04:27,330 --> 01:04:29,420
and some of your actions may

1025
01:04:29,580 --> 01:04:33,070
have long-term consequences.I think that is the heart

1026
01:04:33,300 --> 01:04:35,690
of RL's sequential decision making,

1027
01:04:35,900 --> 01:04:37,930
where you make multiple decisions, and

1028
01:04:38,110 --> 01:04:40,010
some of your actions may have long-term consequences.

1029
01:04:40,200 --> 01:04:42,600
I've shown you a bunch of robotics examples.

1030
01:04:42,870 --> 01:04:47,190
RL is also applied to thinks like medical decision making,

1031
01:04:47,400 --> 01:04:48,570
where you may have a patient

1032
01:04:48,770 --> 01:04:50,460
and you want to choose a sequence of treatments,

1033
01:04:50,730 --> 01:04:51,880
and you do this now for the patient,

1034
01:04:52,090 --> 01:04:53,710
and the patient may be in some other state,

1035
01:04:53,880 --> 01:04:55,340
and you choose to do that later, and so on.

1036
01:04:55,530 --> 01:04:58,490
It turns out there's a large community of people applying

1037
01:04:58,680 --> 01:04:59,870
these sorts of tools to queues.

1038
01:05:00,030 --> 01:05:02,160
So imagine you have a bank

1039
01:05:02,520 --> 01:05:04,040
where you have people lining up,

1040
01:05:04,210 --> 01:05:06,890
and after they go to one cashier, some of them have to

1041
01:05:07,080 --> 01:05:09,100
go to the manager to deal with something else.

1042
01:05:10,340 --> 01:05:11,040
You have a system of multiple people

1043
01:05:11,410 --> 01:05:13,300
standing in line in multiple queues,

1044
01:05:13,500 --> 01:05:15,020
and so how do you route people optimally

1045
01:05:15,210 --> 01:05:19,610
to minimize the waiting time. And not just people,

1046
01:05:19,890 --> 01:05:21,930
but objects in an assembly line and so on.

1047
01:05:22,110 --> 01:05:24,140
It turns out there's a surprisingly large community

1048
01:05:24,350 --> 01:05:25,540
working on optimizing queues.

1049
01:05:25,760 --> 01:05:28,630
I mentioned game playing a little bit already.

1050
01:05:28,840 --> 01:05:31,210
Things like financial decision making,

1051
01:05:31,420 --> 01:05:33,010
if you have a large amount of stock,

1052
01:05:33,230 --> 01:05:35,330
how do you sell off a large amount

1053
01:05:35,590 --> 01:05:38,210
how do you time the selling off of your stock

1054
01:05:38,440 --> 01:05:40,430
so as to not affect market prices adversely too much?

1055
01:05:40,640 --> 01:05:43,530
There are many operations research problems,

1056
01:05:43,730 --> 01:05:44,810
things like factory automation.

1057
01:05:45,000 --> 01:05:48,040
Can you design a factory to optimize throughput,

1058
01:05:48,220 --> 01:05:49,040
or minimize cost, or whatever.

1059
01:05:49,250 --> 01:05:51,750
These are all sorts of problems that people are applying

1060
01:05:51,940 --> 01:05:53,080
reinforcement learning algorithms to.

1061
01:05:53,300 --> 01:05:56,550
Let me just close with a few robotics examples

1062
01:05:56,760 --> 01:05:57,870
because they're always fun,

1063
01:05:58,040 --> 01:05:58,840
and we just have these videos.

1064
01:05:59,100 --> 01:06:03,650
This video was a work of Ziko Coulter and Peter Abiel,

1065
01:06:03,960 --> 01:06:05,300
which is a PhD student here.

1066
01:06:05,540 --> 01:06:09,710
They were working getting a robot dog

1067
01:06:09,940 --> 01:06:13,230
to climb over difficult rugged terrain.

1068
01:06:13,440 --> 01:06:15,600
Using a reinforcement learning algorithm,

1069
01:06:15,830 --> 01:06:20,230
they applied an approach  that's similar to

1070
01:06:20,430 --> 01:06:21,950
a value function approximation approach,

1071
01:06:22,150 --> 01:06:23,140
not quite but similar.

1072
01:06:23,320 --> 01:06:25,270
They allowed the robot dog

1073
01:06:25,530 --> 01:06:28,120
to sort of plan ahead multiple steps,

1074
01:06:28,310 --> 01:06:30,670
and carefully choose his footsteps

1075
01:06:30,870 --> 01:06:33,240
and traverse rugged terrain.

1076
01:06:33,450 --> 01:06:36,440
This is really state of the art in terms of

1077
01:06:36,650 --> 01:06:40,820
what can you get a robotic dog to do.

1078
01:06:41,030 --> 01:06:46,110
Here's another fun one. It turns out that

1079
01:06:46,380 --> 01:06:51,180
wheeled robots are very fuel-efficient. Cars and trucks

1080
01:06:51,380 --> 01:06:53,010
are the most fuel-efficient robots in the world almost.

1081
01:06:53,540 --> 01:06:56,920
Cars and trucks are very fuel-efficient,

1082
01:06:57,100 --> 01:06:58,790
but the bigger robots have the ability

1083
01:06:58,970 --> 01:07:00,490
to traverse more rugged terrain.

1084
01:07:00,720 --> 01:07:02,030
So this is a robot

1085
01:07:02,240 --> 01:07:03,860
this is actually a small scale mockup

1086
01:07:04,020 --> 01:07:06,020
of a larger vehicle built by Lockheed Martin,

1087
01:07:06,200 --> 01:07:09,240
but can you come up with a vehicle that

1088
01:07:09,470 --> 01:07:12,970
has wheels and has the fuel efficiency of wheeled robots,

1089
01:07:13,180 --> 01:07:16,540
but also has legs so it can traverse obstacles.Again,

1090
01:07:16,860 --> 01:07:22,200
using a reinforcement algorithm to design a controller

1091
01:07:22,390 --> 01:07:26,210
for this robot to make it traverse obstacles,

1092
01:07:26,400 --> 01:07:28,260
and somewhat complex gaits

1093
01:07:28,470 --> 01:07:31,040
that would be very hard to design by hand,

1094
01:07:31,280 --> 01:07:32,760
but by choosing a reward function,

1095
01:07:32,960 --> 01:07:35,320
tell the robot this is a plus one reward

1096
01:07:35,510 --> 01:07:37,600
that's top of the goal, and a few other things,

1097
01:07:37,810 --> 01:07:40,230
it learns these sorts of policies automatically.

1098
01:07:40,430 --> 01:07:47,770
Last couple fun ones

1099
01:07:48,030 --> 01:07:52,600
I'll show you a couple last helicopter videos.

1100
01:07:52,810 --> 01:07:59,700
This is the work of again PhD students here,

1101
01:07:59,930 --> 01:08:01,780
Peter Abiel and Adam Coates

1102
01:08:02,010 --> 01:08:14,160
who have been working on autonomous flight.

1103
01:08:27,160 --> 01:08:28,140
I'll just let you watch.

1104
01:09:49,600 --> 01:09:50,160
I'll just show you one more.

1105
01:09:50,470 --> 01:09:52,900
[Inaudible] do this with a real helicopter [inaudible]?

1106
01:09:53,290 --> 01:09:54,530
Not a full-size helicopter.

1107
01:09:54,710 --> 01:09:56,900
Only small radio control helicopters.

1108
01:09:57,110 --> 01:10:00,120
Student:[Inaudible].

1109
01:10:00,440 --> 01:10:03,100
Instructor (Andrew Ng):Full-size helicopters

1110
01:10:03,280 --> 01:10:03,930
are the wrong design for this.

1111
01:10:04,100 --> 01:10:05,230
You shouldn't do this on a full-size helicopter.

1112
01:10:05,390 --> 01:10:07,370
Many full-size helicopters would fall apart

1113
01:10:07,580 --> 01:10:08,320
if you tried to do this.

1114
01:10:08,490 --> 01:10:13,060
Let's see. There's one more.

1115
01:10:13,250 --> 01:10:19,270
Student:Are there any human [inaudible]?

1116
01:10:19,430 --> 01:10:19,790
Instructor (Andrew Ng):Yes,

1117
01:10:19,970 --> 01:10:21,090
there are very good human pilots that can.

1118
01:10:21,280 --> 01:10:30,030
This is just one more maneuver.

1119
01:10:44,240 --> 01:10:44,400
That was kind of fun.

1120
01:10:45,000 --> 01:10:47,440
So this is the work of Peter Abiel and Adam Coates.

1121
01:10:47,630 --> 01:10:52,850
So that was it.

1122
01:10:53,040 --> 01:10:55,220
That was all the technical material

1123
01:10:55,400 --> 01:10:57,410
I wanted to present in this class.

1124
01:10:57,600 --> 01:11:05,960
I guess you're all experts on machine learning now.

1125
01:11:06,150 --> 01:11:06,850
Congratulations.

1126
01:11:07,030 --> 01:11:12,610
And as I hope you've got the sense through this class

1127
01:11:12,820 --> 01:11:15,440
that this is one of the technologies that's really having

1128
01:11:15,660 --> 01:11:18,490
a huge impact on science in engineering and industry.

1129
01:11:18,690 --> 01:11:22,700
As I said in the first lecture, I think many people use

1130
01:11:22,900 --> 01:11:24,950
machine learning algorithms dozens of times a day

1131
01:11:25,140 --> 01:11:26,500
without even knowing about it.

1132
01:11:26,670 --> 01:11:31,550
Based on the projects you've done, I hope

1133
01:11:31,780 --> 01:11:36,260
that most of you will be able

1134
01:11:36,490 --> 01:11:39,490
to imagine yourself going out after this class

1135
01:11:39,680 --> 01:11:41,910
and applying these things to solve a variety of  problems.

1136
01:11:42,120 --> 01:11:45,290
Hopefully, some of you will also imagine yourselves

1137
01:11:45,500 --> 01:11:48,280
writing research papers after this class,

1138
01:11:48,480 --> 01:11:50,780
be it on a novel way to do machine learning,

1139
01:11:50,990 --> 01:11:53,610
or on some way of applying machine learning to a problem

1140
01:11:53,800 --> 01:11:56,290
that you care about. In fact,

1141
01:11:56,490 --> 01:11:58,220
looking at project milestones, I'm actually sure that

1142
01:11:58,430 --> 01:12:00,180
a large fraction of the projects in this class

1143
01:12:00,360 --> 01:12:01,610
will be publishable, so I think that's great.

1144
01:12:01,790 --> 01:12:10,200
I guess many of you will also go industry, make new products,

1145
01:12:10,370 --> 01:12:12,330
and make lots of money using learning algorithms.

1146
01:12:12,520 --> 01:12:14,330
Remember me if that happens.

1147
01:12:14,520 --> 01:12:17,940
One of the things I'm excited about

1148
01:12:18,140 --> 01:12:19,760
is through research or industry,

1149
01:12:19,960 --> 01:12:21,590
I'm actually completely sure that

1150
01:12:21,810 --> 01:12:23,590
the people in this class in the next few months

1151
01:12:23,810 --> 01:12:25,490
will apply machine learning algorithms to

1152
01:12:25,690 --> 01:12:27,780
lots of problems in industrial management,

1153
01:12:27,980 --> 01:12:28,810
and computer science,

1154
01:12:29,040 --> 01:12:30,770
things like optimizing computer architectures,

1155
01:12:31,070 --> 01:12:34,970
network security, robotics, computer vision,

1156
01:12:35,170 --> 01:12:37,600
to problems in computational biology,

1157
01:12:37,800 --> 01:12:41,900
to problems in aerospace, or understanding natural language,

1158
01:12:42,080 --> 01:12:43,640
and many more things like that.

1159
01:12:43,840 --> 01:12:49,320
So right now I have no idea what all of you are going to do

1160
01:12:49,470 --> 01:12:51,080
with the learning algorithms you learned about,

1161
01:12:51,260 --> 01:12:53,220
but every time as I wrap up this class,

1162
01:12:53,410 --> 01:12:56,920
I always feel very excited, and optimistic, and hopeful

1163
01:12:57,110 --> 01:12:59,660
about the sorts of amazing things you'll be able to do.

1164
01:12:59,810 --> 01:13:07,930
One final thing, I'll just give out this handout.

1165
01:13:08,130 --> 01:13:31,890
One final thing is that machine learning has grown

1166
01:13:32,090 --> 01:13:37,290
out of a larger literature on the AI  where this desire

1167
01:13:37,480 --> 01:13:39,890
to build systems that exhibit intelligent behavior

1168
01:13:40,110 --> 01:13:43,700
and machine learning is one of the tools of AI,

1169
01:13:43,890 --> 01:13:45,710
maybe one that's had a disproportionately large impact,

1170
01:13:45,910 --> 01:13:49,840
but there are many other ideas in AI that

1171
01:13:50,020 --> 01:13:52,990
I hope you go and continue to learn about.

1172
01:13:53,200 --> 01:13:54,880
Fortunately,

1173
01:13:55,090 --> 01:13:58,380
Stanford has one of the best and broadest sets of AI classes,

1174
01:13:58,580 --> 01:14:00,950
and I hope that you take advantage of some of these classes,

1175
01:14:01,190 --> 01:14:02,640
and go and learn more about AI,

1176
01:14:02,840 --> 01:14:05,980
and more about other fields which often apply learning

1177
01:14:06,160 --> 01:14:08,020
algorithms to problems in vision, problems in

1178
01:14:08,220 --> 01:14:10,220
natural language processing in robotics, and so on.

1179
01:14:10,430 --> 01:14:12,440
So the handout I just gave out

1180
01:14:12,650 --> 01:14:14,040
has a list of AI related courses.

1181
01:14:14,260 --> 01:14:16,460
Just running down very quickly, I guess,

1182
01:14:16,660 --> 01:14:18,880
CS221 is an overview that I teach.

1183
01:14:19,090 --> 01:14:21,210
There are a lot of robotics classes also:

1184
01:14:21,430 --> 01:14:26,530
223A, 225A, 225B many robotics class.

1185
01:14:26,740 --> 01:14:28,520
There are so many applications

1186
01:14:28,700 --> 01:14:31,060
of learning algorithms to robotics today.

1187
01:14:31,280 --> 01:14:34,810
222 and 227 are knowledge representation

1188
01:14:35,050 --> 01:14:35,870
and reasoning classes.

1189
01:14:36,050 --> 01:14:38,580
228 of all the classes on this list,

1190
01:14:38,770 --> 01:14:41,000
228, which Daphne Koller teaches,

1191
01:14:41,220 --> 01:14:42,800
is probably closest in spirit to 229.

1192
01:14:43,020 --> 01:14:45,550
This is one of the classes I highly recommend

1193
01:14:45,740 --> 01:14:47,010
to all of my PhD students as well.

1194
01:14:47,210 --> 01:14:51,870
Many other problems also touch on machine learning.

1195
01:14:52,050 --> 01:14:54,980
On the next page, courses on computer vision,

1196
01:14:55,160 --> 01:14:57,110
speech recognition, natural language processing,

1197
01:14:57,310 --> 01:15:00,900
various tools that aren't just machine learning,

1198
01:15:01,050 --> 01:15:02,610
but often involve machine learning in many ways.

1199
01:15:02,820 --> 01:15:04,490
Other aspects of AI,

1200
01:15:04,710 --> 01:15:07,940
multi-agent systems taught by [inaudible].

1201
01:15:08,180 --> 01:15:11,970
EE364A is convex optimization.

1202
01:15:12,160 --> 01:15:13,740
It's a class taught by Steve Boyd, and

1203
01:15:13,890 --> 01:15:16,100
convex optimization came up many times in this class.

1204
01:15:16,320 --> 01:15:18,040
If you want to become really good at it,

1205
01:15:18,200 --> 01:15:19,810
EE364 is a great class.

1206
01:15:20,020 --> 01:15:22,880
If you're interested in project courses,

1207
01:15:23,040 --> 01:15:25,150
I also teach a project class next quarter

1208
01:15:25,350 --> 01:15:26,860
where we spend the whole quarter

1209
01:15:27,060 --> 01:15:28,460
working on research projects.

1210
01:15:28,730 --> 01:15:34,200
So I hope you go and take some more of those classes.

1211
01:15:34,430 --> 01:15:44,620
In closing, let me just say this class has been really fun

1212
01:15:44,810 --> 01:15:47,770
to teach, and it's very satisfying to me personally

1213
01:15:47,970 --> 01:15:50,680
when we set these insanely difficult hallmarks,

1214
01:15:50,880 --> 01:15:53,000
and then we'd see a solution, and I'd be like,

1215
01:15:53,190 --> 01:15:54,610
"Oh my god. They actually figured that one out."

1216
01:15:54,780 --> 01:15:57,210
It's actually very satisfying when I see that.

1217
01:15:57,400 --> 01:16:01,000
Or looking at the milestones, I often go,

1218
01:16:01,240 --> 01:16:02,150
"Wow, that's really cool.

1219
01:16:02,330 --> 01:16:03,420
I bet this would be publishable."

1220
01:16:03,600 --> 01:16:06,850
So I hope you take what you've learned,

1221
01:16:07,050 --> 01:16:10,660
go forth, and do amazing things with learning algorithms.

1222
01:16:10,850 --> 01:16:13,200
I know this is a heavy workload class,

1223
01:16:13,410 --> 01:16:16,660
so thank you all very much for the hard work

1224
01:16:16,850 --> 01:16:17,660
you've put into this class,

1225
01:16:17,920 --> 01:16:19,590
and the hard work you've put into learning this material,

1226
01:16:19,790 --> 01:16:22,230
and thank you very much

1227
01:16:22,430 --> 01:16:23,750
for having been students in this class.

1228
01:16:23,940 --> 01:16:25,200
[End of Audio]

1229
01:16:28,500 --> 01:16:28,670
Duration: 78 minutes

