1
00:00:22,600 --> 00:00:24,660
So welcome back.

2
00:00:24,810 --> 00:00:27,570
And what I wanna do today is

3
00:00:27,690 --> 00:00:30,250
continue our discussion on support vector machines.

4
00:00:30,370 --> 00:00:31,260
And in particular,

5
00:00:31,390 --> 00:00:33,400
I wanna talk about the optimal margin classifier.

6
00:00:33,520 --> 00:00:35,520
Then I wanna take a brief digression

7
00:00:35,610 --> 00:00:37,450
and talk about primal and dual optimization

8
00:00:37,570 --> 00:00:39,020
problems, and in particular,

9
00:00:39,130 --> 00:00:40,970
what's called the KKT conditions.

10
00:00:41,080 --> 00:00:43,900
And then we'll derive the dual to

11
00:00:44,020 --> 00:00:46,100
the optimization problem that I had posed earlier.

12
00:00:46,210 --> 00:00:49,080
And that will lead us into a discussion of kernels,

13
00:00:49,180 --> 00:00:50,390
which I won't really

14
00:00:50,490 --> 00:00:52,850
which we just get to say couple words about,

15
00:00:52,960 --> 00:00:55,620
but which I'll do probably only in the next lecture.

16
00:00:55,720 --> 00:00:59,210
And as part of today's lecture,

17
00:00:59,290 --> 00:01:01,650
I'll spend some time talking about optimization problems.

18
00:01:01,740 --> 00:01:05,460
And in the little time I have today,

19
00:01:05,550 --> 00:01:08,280
I won't really be able to do this topic justice.

20
00:01:08,380 --> 00:01:10,660
I wanna talk about convex optimization

21
00:01:10,780 --> 00:01:12,490
and do that topic justice.

22
00:01:12,600 --> 00:01:15,030
And so at this week's discussion session,

23
00:01:15,140 --> 00:01:18,040
the TAs will have more time

24
00:01:18,130 --> 00:01:19,950
will teach a discussion session

25
00:01:20,050 --> 00:01:21,730
focus on convex optimization

26
00:01:21,850 --> 00:01:24,650
sort of very beautiful and useful theory.

27
00:01:24,750 --> 00:01:26,640
So you want to learn more about that,

28
00:01:26,750 --> 00:01:29,520
listen to this Friday's discussion session.

29
00:01:29,630 --> 00:01:35,590
Just to recap what we did in the previous lecture,

30
00:01:35,690 --> 00:01:41,030
as we were beginning on developing

31
00:01:41,130 --> 00:01:42,390
on support vector machines,

32
00:01:42,470 --> 00:01:46,200
I said that a hypothesis represented as

33
00:01:46,300 --> 00:01:50,210
H sub [inaudible] wb as g of w transpose

34
00:01:50,280 --> 00:02:05,340
[inaudible] x + b, where g will be +1 or -1,

35
00:02:05,440 --> 00:02:07,770
depending on whether z is greater than 0.

36
00:02:07,890 --> 00:02:11,190
And I said that in our development

37
00:02:11,280 --> 00:02:13,250
of support vector machines, we'll use

38
00:02:13,360 --> 00:02:16,400
we'll change the convention of letting y be +1,

39
00:02:16,480 --> 00:02:21,000
-1 to note the class labels. So last time,

40
00:02:21,080 --> 00:02:26,650
we also talked about the functional margin,

41
00:02:26,730 --> 00:02:28,830
which was this thing, gamma hat i.

42
00:02:28,930 --> 00:02:37,930
And so we had the intuition that

43
00:02:38,020 --> 00:02:41,560
the if functional margin is a large positive number,

44
00:02:41,640 --> 00:02:44,380
then that means that we are classifying

45
00:02:44,480 --> 00:02:46,430
a training example correctly and very confidently.

46
00:02:46,540 --> 00:02:49,250
So yi is +1.

47
00:02:49,340 --> 00:02:53,090
We would like w transpose xi + b to be very large.

48
00:02:53,170 --> 00:02:56,310
And it makes i if, excuse me, if yi is -1,

49
00:02:56,400 --> 00:02:58,910
then we'd w transpose xi + b to be

50
00:02:59,000 --> 00:03:00,180
a large negative number.

51
00:03:00,290 --> 00:03:01,950
So we'd sort of like functional margins to be large.

52
00:03:02,040 --> 00:03:04,730
We also said functional margin

53
00:03:04,850 --> 00:03:05,800
is a strange property

54
00:03:05,920 --> 00:03:08,630
that you can increase functional margin just by,

55
00:03:08,720 --> 00:03:10,870
say, taking your parameters, w and b,

56
00:03:10,960 --> 00:03:12,760
and multiplying them by 2.

57
00:03:12,880 --> 00:03:19,880
And then we also defined the geometric margin,

58
00:03:19,980 --> 00:03:34,760
which was that we just essentially,

59
00:03:34,870 --> 00:03:39,330
the functional margin divided by the normal w.

60
00:03:39,440 --> 00:03:42,210
And so the geometric margin had

61
00:03:42,320 --> 00:03:45,080
the interpretation as being

62
00:03:45,190 --> 00:03:47,260
I'll give you a few examples.

63
00:03:47,380 --> 00:03:50,920
The geometric margin, for example, is

64
00:03:51,030 --> 00:03:54,040
has the interpretation as a distance between

65
00:03:54,140 --> 00:03:55,760
a training example and a hyperplane.

66
00:03:55,870 --> 00:03:58,460
And it'll actually be a signed distance,

67
00:03:58,560 --> 00:04:00,150
so that this distance will be positive

68
00:04:00,260 --> 00:04:02,400
if you're classifying the example correctly.

69
00:04:02,510 --> 00:04:04,370
And if you misclassify the example,

70
00:04:04,490 --> 00:04:07,640
this distance it'll be the minus of the distance,

71
00:04:07,760 --> 00:04:09,720
reaching the point, reaching the training example.

72
00:04:09,850 --> 00:04:11,740
And you're separating hyperplane.

73
00:04:11,830 --> 00:04:14,040
Where you're separating hyperplane is defined

74
00:04:14,120 --> 00:04:18,260
by the equation w transpose x + b = 0.

75
00:04:18,370 --> 00:04:28,810
So oh, well, and I guess also defined

76
00:04:28,920 --> 00:04:34,790
these things as the functional margin,

77
00:04:34,880 --> 00:04:36,010
geometric margins,

78
00:04:36,010 --> 00:04:37,360
respect to training set

79
00:04:37,460 --> 00:04:39,430
I defined as the worst case

80
00:04:39,520 --> 00:04:40,420
or the minimum functional

81
00:04:40,420 --> 00:04:41,400
geometric margin.

82
00:04:41,520 --> 00:04:49,550
So in our development of

83
00:04:49,660 --> 00:04:51,720
the optimal margin classifier,

84
00:04:51,830 --> 00:04:55,350
our learning algorithm would choose parameters w

85
00:04:55,460 --> 00:04:57,710
and b so as to maximize the geometric margin.

86
00:04:57,820 --> 00:05:00,260
So our goal is to find the separating hyperplane that

87
00:05:00,350 --> 00:05:02,160
separates the positive and negative examples

88
00:05:02,290 --> 00:05:05,100
with as large a distance as possible between

89
00:05:05,210 --> 00:05:07,580
hyperplane and the positive and negative examples.

90
00:05:07,680 --> 00:05:12,880
And if you go to choose parameters w and b

91
00:05:12,960 --> 00:05:14,780
to maximize this, [inaudible] one copy

92
00:05:14,870 --> 00:05:17,060
of the geometric margin is that

93
00:05:17,160 --> 00:05:21,020
you can actually scale w and b arbitrarily.

94
00:05:21,120 --> 00:05:23,560
So you look at this definition for the geometric margin.

95
00:05:23,670 --> 00:05:29,180
I can choose to multiply my parameters w and b by 2

96
00:05:29,280 --> 00:05:30,650
or by 10 or any other constant.

97
00:05:30,760 --> 00:05:34,570
And it doesn't change my geometric margin.

98
00:05:34,670 --> 00:05:37,460
And one way of interpreting that

99
00:05:37,550 --> 00:05:40,590
is you're looking at just separating hyperplane.

100
00:05:40,680 --> 00:05:42,480
You look at this line you're separating by

101
00:05:42,600 --> 00:05:44,060
positive and negative training examples.

102
00:05:44,170 --> 00:05:46,270
If I scale w and b,

103
00:05:46,380 --> 00:05:49,190
that doesn't change the position of this plane,

104
00:05:49,290 --> 00:05:52,270
though because the equation wh + b = 0

105
00:05:52,360 --> 00:05:56,420
is the same as equation 2 w transpose x + 2b = 0.

106
00:05:56,520 --> 00:05:57,920
So it use the same straight line.

107
00:05:58,000 --> 00:06:00,010
And what that means is that

108
00:06:00,100 --> 00:06:04,000
I can actually choose whatever scaling for w and b

109
00:06:04,110 --> 00:06:05,720
is convenient for me.

110
00:06:05,830 --> 00:06:08,920
And in particular, we use in a minute,

111
00:06:09,020 --> 00:06:11,240
I can [inaudible] perfect constraint like that

112
00:06:11,360 --> 00:06:14,900
the normal w [inaudible] 1 because this means that

113
00:06:14,970 --> 00:06:17,070
you can find a solution to w and b.

114
00:06:17,180 --> 00:06:18,870
And then by rescaling the parameters,

115
00:06:18,960 --> 00:06:20,690
you can easily meet this condition,

116
00:06:20,800 --> 00:06:22,830
this rescaled w [inaudible] 1.

117
00:06:22,950 --> 00:06:24,850
And so I can add the condition like this

118
00:06:24,960 --> 00:06:26,710
and then essentially not change the problem.

119
00:06:26,830 --> 00:06:29,460
Or I can add other conditions.

120
00:06:29,560 --> 00:06:33,000
I can actually add a condition that

121
00:06:33,090 --> 00:06:37,000
excuse me, the absolute value of w1 = 1.

122
00:06:37,090 --> 00:06:38,380
I can have only one

123
00:06:38,480 --> 00:06:39,850
of these conditions right now [inaudible].

124
00:06:39,960 --> 00:06:41,720
And adding condition to the absolute value

125
00:06:41,800 --> 00:06:44,120
the first component of w must be to 1.

126
00:06:44,210 --> 00:06:46,480
And again, you can find the absolute solution

127
00:06:46,600 --> 00:06:48,730
and just rescale w and meet this condition.

128
00:06:48,850 --> 00:06:51,460
And it can have other,

129
00:06:51,570 --> 00:06:56,700
most esoteric conditions like that because again,

130
00:06:56,810 --> 00:06:58,420
this is a condition that

131
00:06:58,520 --> 00:07:00,610
you can solve for the optimal margin,

132
00:07:00,690 --> 00:07:03,950
and then just by scaling, you have w up and down.

133
00:07:04,040 --> 00:07:05,640
You can you can then ensure

134
00:07:05,740 --> 00:07:06,920
you meet this condition as well.

135
00:07:07,040 --> 00:07:09,500
So again, [inaudible]

136
00:07:09,600 --> 00:07:10,710
one of these conditions right now,

137
00:07:10,790 --> 00:07:11,750
not all of them.

138
00:07:11,850 --> 00:07:16,730
And so our ability to choose any scaling condition

139
00:07:16,840 --> 00:07:19,000
on w that's convenient to us will

140
00:07:19,120 --> 00:07:22,200
be useful again in a second. All right.

141
00:07:22,310 --> 00:07:26,200
So let's go ahead and break down the optimization problem.

142
00:07:26,320 --> 00:07:29,620
And again, my goal is to choose parameters w and b

143
00:07:29,730 --> 00:07:32,690
so as to maximize the geometric margin.

144
00:07:32,800 --> 00:07:35,840
Here's my first attempt at

145
00:07:35,920 --> 00:07:37,160
writing down the optimization problem.

146
00:07:37,300 --> 00:07:40,380
Actually wrote this one down right

147
00:07:40,490 --> 00:07:41,960
at the end of the previous lecture.

148
00:07:42,070 --> 00:07:43,590
Begin to solve the parameters

149
00:07:43,700 --> 00:07:46,930
gamma w and b such that

150
00:07:47,040 --> 00:07:58,710
that [inaudible] i for in training examples.

151
00:07:58,820 --> 00:08:04,270
Let's say I choose to add this normalization condition.

152
00:08:04,390 --> 00:08:08,010
So the norm condition that w

153
00:08:08,100 --> 00:08:11,890
the normal w is equal to 1 just makes

154
00:08:12,010 --> 00:08:14,310
the geometric and the functional margin the same.

155
00:08:14,420 --> 00:08:17,220
And so I'm saying I want to find a value

156
00:08:17,330 --> 00:08:20,800
I want to find a value for gamma as big as possible

157
00:08:20,900 --> 00:08:24,240
so that all of my training examples

158
00:08:24,320 --> 00:08:27,880
have functional margin greater than or equals gamma,

159
00:08:27,960 --> 00:08:32,700
and with the constraint that normal w equals 1,

160
00:08:32,800 --> 00:08:35,240
functional margin and geometric margin

161
00:08:35,330 --> 00:08:37,100
are the same. So it's the same.

162
00:08:37,200 --> 00:08:40,190
Find the value for gamma so that all the values

163
00:08:40,280 --> 00:08:42,400
all the geometric margins

164
00:08:42,520 --> 00:08:44,100
are greater or equal to gamma.

165
00:08:44,200 --> 00:08:48,290
So you solve this optimization problem,

166
00:08:48,420 --> 00:08:54,240
then you have derived the optimal margin classifier

167
00:08:54,360 --> 00:08:58,640
that there's not a very nice optimization problem

168
00:08:58,760 --> 00:09:01,770
because this is a nasty, nonconvex constraints.

169
00:09:01,870 --> 00:09:04,620
And [inaudible] is asking that you solve for

170
00:09:04,710 --> 00:09:09,100
parameters w that lie on the surface

171
00:09:09,170 --> 00:09:11,580
of a unisphere, lie on his [inaudible].

172
00:09:11,700 --> 00:09:14,100
It lies on a unicircle a unisphere.

173
00:09:14,230 --> 00:09:18,960
And so if we can come up with

174
00:09:19,080 --> 00:09:20,550
a convex optimization problem,

175
00:09:20,660 --> 00:09:22,350
then we'd be guaranteed that

176
00:09:22,460 --> 00:09:24,070
our [inaudible] descend to other local

177
00:09:24,160 --> 00:09:26,030
[inaudible] will not have local optimal.

178
00:09:26,140 --> 00:09:28,420
And it turns out this is an example

179
00:09:28,530 --> 00:09:29,910
of a nonconvex constraint.

180
00:09:29,980 --> 00:09:32,300
This is a nasty constraint that

181
00:09:32,450 --> 00:09:33,580
I would like to get rid of.

182
00:09:33,700 --> 00:09:42,210
So let's change the optimization problem

183
00:09:42,300 --> 00:09:52,460
one more time. Now, let me pose

184
00:09:52,580 --> 00:09:54,850
a slightly different optimization problem.

185
00:09:54,950 --> 00:09:59,670
Let me maximize the functional margin divided

186
00:09:59,760 --> 00:10:07,280
by the normal w subject to yi w transpose xi.

187
00:10:07,380 --> 00:10:14,490
So in other words, once you find a number,

188
00:10:14,550 --> 00:10:18,230
gamma hat, so that every one of

189
00:10:18,300 --> 00:10:21,160
my training examples has functional margin

190
00:10:21,260 --> 00:10:22,660
greater than the gamma hat,

191
00:10:22,770 --> 00:10:24,350
and my optimization objective is

192
00:10:24,440 --> 00:10:26,560
I want to maximize gamma hat divided

193
00:10:26,650 --> 00:10:27,950
by the normal w.

194
00:10:28,060 --> 00:10:31,260
And so I wanna maximize the function margin

195
00:10:31,310 --> 00:10:32,750
divided by the normal w.

196
00:10:32,850 --> 00:10:36,230
And we saw previously the function margin

197
00:10:36,350 --> 00:10:39,580
divided by the normal w is just a geometric margin,

198
00:10:39,680 --> 00:10:41,350
and so this is a different way of

199
00:10:41,460 --> 00:10:43,430
posing the same optimization problem.

200
00:10:43,540 --> 00:10:46,030
[Inaudible] confused, though.

201
00:10:46,130 --> 00:10:47,990
Are there questions about this?

202
00:10:48,110 --> 00:10:53,990
Student:[Inaudible] the second statement has

203
00:10:54,100 --> 00:10:56,870
to be made of the functional margin y divided by

204
00:10:56,980 --> 00:10:59,950
why don't you just have it the geometric margin?

205
00:11:00,060 --> 00:11:01,810
Why do you [inaudible]?

206
00:11:01,930 --> 00:11:04,990
Instructor (Andrew Ng):[Inaudible] say it again?

207
00:11:05,110 --> 00:11:05,970
Student:For the second statement,

208
00:11:06,090 --> 00:11:07,260
where we're saying the data of

209
00:11:07,350 --> 00:11:08,470
the functional margin is divided [inaudible].

210
00:11:08,590 --> 00:11:10,230
Instructor (Andrew Ng):Oh, I see, yes.

211
00:11:10,410 --> 00:11:12,150
Student:[Inaudible] is that [inaudible]?

212
00:11:12,270 --> 00:11:13,850
Instructor (Andrew Ng):So let's see,

213
00:11:13,960 --> 00:11:15,350
this is the function margin, right?

214
00:11:15,460 --> 00:11:16,720
This is not the geometric margin.

215
00:11:16,840 --> 00:11:17,710
Student:Yeah.

216
00:11:17,830 --> 00:11:18,990
Instructor (Andrew Ng):So oh,

217
00:11:19,110 --> 00:11:21,730
I want to divide by the normal w

218
00:11:21,840 --> 00:11:23,000
of my optimization objective.

219
00:11:23,100 --> 00:11:25,360
Student:I'm just wondering how come

220
00:11:25,470 --> 00:11:27,080
you end up dividing also under the second stage

221
00:11:27,180 --> 00:11:28,870
[inaudible] the functional margin.

222
00:11:28,980 --> 00:11:31,170
Why are you dividing there by the normal w?

223
00:11:31,270 --> 00:11:32,410
Instructor (Andrew Ng):Let's see.

224
00:11:32,520 --> 00:11:36,720
I'm not sure I get the question.

225
00:11:36,820 --> 00:11:39,040
Let me try saying this again. So here's my goal.

226
00:11:39,160 --> 00:11:42,460
My I want [inaudible]. So let's see,

227
00:11:42,570 --> 00:11:44,550
the parameters of this optimization problem where

228
00:11:44,660 --> 00:11:45,960
gamma hat w and b

229
00:11:46,060 --> 00:11:48,940
so the convex optimization software

230
00:11:48,940 --> 00:11:50,790
solves this problem

231
00:11:50,870 --> 00:11:53,120
for some set of parameters gamma w and b.

232
00:11:53,230 --> 00:11:57,170
And I'm imposing the constraint that whatever

233
00:11:57,250 --> 00:11:58,350
values it comes up with,

234
00:11:58,450 --> 00:12:02,790
yi x x5 + b must be greater than gamma hat.

235
00:12:02,910 --> 00:12:06,630
And so this means that the functional margin

236
00:12:06,750 --> 00:12:09,710
of every example had better be greater than

237
00:12:09,830 --> 00:12:11,100
equal to gamma hat.

238
00:12:11,190 --> 00:12:12,620
So there's a constraint to the function margin

239
00:12:12,730 --> 00:12:13,950
and a constraint to the gamma hat.

240
00:12:14,070 --> 00:12:16,030
But what I care about is not really

241
00:12:16,140 --> 00:12:17,670
maximizing the functional margin.

242
00:12:17,800 --> 00:12:19,650
What I really care about in other words,

243
00:12:19,760 --> 00:12:20,950
in optimization objective,

244
00:12:21,060 --> 00:12:24,520
is maximizing gamma hat divided by the normal w,

245
00:12:24,630 --> 00:12:26,600
which is the geometric margin.

246
00:12:26,710 --> 00:12:29,870
So in other words, my optimization [inaudible]

247
00:12:29,970 --> 00:12:32,520
is I want to maximize the function margin

248
00:12:32,630 --> 00:12:35,910
divided by the normal w. Subject to that,

249
00:12:36,070 --> 00:12:38,890
every example must have function margin

250
00:12:38,990 --> 00:12:40,510
and at least gamma hat.

251
00:12:40,610 --> 00:12:41,600
Does that make sense now?

252
00:12:41,710 --> 00:12:46,110
Student:[Inaudible] when you said that

253
00:12:46,200 --> 00:12:49,340
to maximize gamma or gamma hat,

254
00:12:49,440 --> 00:12:53,970
respect to gamma w and with respect to

255
00:12:54,090 --> 00:12:56,540
gamma hat so that [inaudible] gamma hat

256
00:12:56,650 --> 00:12:59,060
are no longer [inaudible]?

257
00:12:59,180 --> 00:13:03,400
Instructor (Andrew Ng):So this is the

258
00:13:03,480 --> 00:13:08,310
so it turns out so this is how I write down the

259
00:13:08,410 --> 00:13:11,030
this is how I write down an optimization problem

260
00:13:11,150 --> 00:13:13,700
in order to solve for the geometric margin.

261
00:13:13,790 --> 00:13:19,120
What is it so it turns out that the question of this

262
00:13:19,220 --> 00:13:21,280
is the gamma hat the function of w and b?

263
00:13:21,630 --> 00:13:22,910
And it turns out that

264
00:13:23,030 --> 00:13:24,710
in my previous mathematical definition, it was,

265
00:13:24,820 --> 00:13:27,100
but the way I'm going to pose this

266
00:13:27,210 --> 00:13:28,970
as an optimization problem

267
00:13:29,050 --> 00:13:31,950
is I'm going to ask the convex optimization solvers

268
00:13:32,020 --> 00:13:33,100
and this [inaudible] software

269
00:13:33,190 --> 00:13:35,560
unless you have software for

270
00:13:35,640 --> 00:13:37,310
solving convex optimization problems

271
00:13:37,370 --> 00:13:39,920
hen I'm going to pretend that

272
00:13:39,980 --> 00:13:41,090
these are independent variables

273
00:13:41,170 --> 00:13:43,420
and ask my convex optimization software

274
00:13:43,520 --> 00:13:46,150
to find me values for gamma, w, and b,

275
00:13:46,250 --> 00:13:48,450
to make this value as big as possible

276
00:13:48,550 --> 00:13:50,120
and subject to this constraint.

277
00:13:50,200 --> 00:13:53,330
And it'll turn out that when it does that,

278
00:13:53,420 --> 00:13:55,570
it will choose or obviously,

279
00:13:55,670 --> 00:13:57,610
it will choose for gamma to be as big as possible

280
00:13:57,720 --> 00:14:00,900
because optimization objective is this:

281
00:14:00,990 --> 00:14:02,810
You're trying to maximize gamma hat.

282
00:14:02,920 --> 00:14:06,260
So for x value of w and b, my software,

283
00:14:06,370 --> 00:14:08,210
which choose to make gamma hat as big as possible

284
00:14:08,320 --> 00:14:10,590
well, but how big can we make gamma hat?

285
00:14:10,690 --> 00:14:13,020
Well, it's limited by use constraints.

286
00:14:13,120 --> 00:14:14,510
It says that every training example

287
00:14:14,620 --> 00:14:17,650
must have function margin greater than

288
00:14:17,740 --> 00:14:21,280
equal to gamma hat. And so my

289
00:14:21,380 --> 00:14:24,460
the bigger you can make gamma hat will

290
00:14:24,540 --> 00:14:27,020
be the value of the smallest functional margin.

291
00:14:27,140 --> 00:14:29,440
And so when you solve this optimization problem,

292
00:14:29,560 --> 00:14:32,350
the value of gamma hat you get out will be,

293
00:14:32,450 --> 00:14:35,690
indeed, the minimum of the functional margins

294
00:14:35,770 --> 00:14:38,980
of your training set. Okay, so Justin?

295
00:14:39,010 --> 00:14:40,340
Student:Yeah, I was just wondering,

296
00:14:40,450 --> 00:14:42,340
I guess I'm a little confused because it's like, okay,

297
00:14:42,460 --> 00:14:44,660
you have two class of data. And you can say,

298
00:14:44,760 --> 00:14:46,620
"Okay, please draw me a line such that

299
00:14:46,730 --> 00:14:49,020
you maximize the distance between

300
00:14:49,120 --> 00:14:51,090
the smallest distance that [inaudible] between

301
00:14:51,170 --> 00:14:52,820
the line and the data points."

302
00:14:52,920 --> 00:14:54,950
And it seems like that's kind of what

303
00:14:55,030 --> 00:14:55,840
we're doing, but it's

304
00:14:55,930 --> 00:14:58,280
it seems like this is more complicated than that.

305
00:14:58,370 --> 00:15:00,730
And I guess I'm wondering what is the difference.

306
00:15:00,730 --> 00:15:01,730
Instructor (Andrew Ng):I see. So I mean, this is –

307
00:15:02,570 --> 00:15:04,790
the question is [inaudible]. Two class of data

308
00:15:04,860 --> 00:15:06,590
trying to find separate hyperplane.

309
00:15:06,670 --> 00:15:09,890
And this seems more complicated than

310
00:15:09,960 --> 00:15:10,920
trying to find a line [inaudible].

311
00:15:11,010 --> 00:15:14,590
So I'm just repeating the questions in case

312
00:15:14,720 --> 00:15:16,740
since I'm not sure how all the audio catches it.

313
00:15:16,840 --> 00:15:19,100
So the answer is this is

314
00:15:19,170 --> 00:15:20,250
actually exactly that problem.

315
00:15:20,330 --> 00:15:21,860
This is exactly that problem of

316
00:15:21,950 --> 00:15:25,060
given the two class of data,

317
00:15:25,140 --> 00:15:26,260
positive and negative examples,

318
00:15:26,350 --> 00:15:29,220
this is exactly the formalization of the problem

319
00:15:29,340 --> 00:15:32,970
where I go is to find a line that separates the two

320
00:15:33,050 --> 00:15:34,710
the positive and negative examples,

321
00:15:34,830 --> 00:15:37,700
maximizing the worst-case distance between

322
00:15:37,790 --> 00:15:39,940
the [inaudible] point and this line.

323
00:15:40,020 --> 00:15:42,360
Okay? Yeah, [Inaudible]?

324
00:15:42,470 --> 00:15:43,610
Student:So why do you care about

325
00:15:43,720 --> 00:15:44,520
the worst-case distance [inaudible]?

326
00:15:44,620 --> 00:15:46,420
Instructor (Andrew Ng):Yeah, let me for now,

327
00:15:46,500 --> 00:15:47,580
why do we care about the worst-case distance?

328
00:15:47,680 --> 00:15:50,150
For now, let's just say

329
00:15:50,270 --> 00:15:52,260
let's just care about the worst-case distance for now.

330
00:15:52,340 --> 00:15:54,110
We'll come back, and we'll fix that later. We'll

331
00:15:54,220 --> 00:15:57,140
that's a caring about the worst case is is just

332
00:15:57,230 --> 00:15:58,750
is just a nice way to

333
00:15:58,860 --> 00:15:59,900
formulate this optimization problem.

334
00:16:00,010 --> 00:16:01,950
I'll come back, and I'll change that later.

335
00:16:02,050 --> 00:16:04,920
Okay, raise your hand if this makes sense

336
00:16:05,010 --> 00:16:08,600
if this formulation makes sense? Okay, yeah, cool.

337
00:16:08,690 --> 00:16:13,600
Great. So let's see

338
00:16:13,710 --> 00:16:15,070
so this is just a different way of

339
00:16:15,180 --> 00:16:17,070
posing the same optimization problem.

340
00:16:17,180 --> 00:16:19,990
And on the one hand, I've got to get rid of this nasty,

341
00:16:20,070 --> 00:16:22,860
nonconvex constraint, while on the other hand,

342
00:16:22,950 --> 00:16:26,810
I've now added a nasty, nonconvex objective.

343
00:16:26,920 --> 00:16:29,100
In particular, this is not a convex function

344
00:16:29,210 --> 00:16:32,300
in parameters w. And so you can't

345
00:16:32,390 --> 00:16:35,010
you don't have the usual guarantees like

346
00:16:35,140 --> 00:16:37,830
if you [inaudible] global minimum.

347
00:16:37,940 --> 00:16:42,210
At least that does not follow immediately from this

348
00:16:42,300 --> 00:16:43,680
because this is nonconvex.

349
00:16:43,780 --> 00:16:55,250
So what I'm going to do is, earlier,

350
00:16:55,350 --> 00:16:58,880
I said that can pose any of a number of

351
00:16:58,960 --> 00:17:01,440
even fairly bizarre scaling constraints on w.

352
00:17:01,530 --> 00:17:04,370
So you can choose any scaling constraint like this,

353
00:17:04,460 --> 00:17:05,520
and things are still fine.

354
00:17:05,630 --> 00:17:09,520
And so here's the scaling I'm going to choose to add.

355
00:17:09,620 --> 00:17:16,090
Again, I'm gonna assume for the purposes

356
00:17:16,230 --> 00:17:17,970
of today's lecture, I'm gonna assume that

357
00:17:18,060 --> 00:17:19,370
these examples are linearly separable,

358
00:17:19,460 --> 00:17:20,810
that you can actually separate

359
00:17:20,910 --> 00:17:22,130
the positive and negative classes,

360
00:17:22,250 --> 00:17:24,540
and that we'll come back and fix this later as well.

361
00:17:24,650 --> 00:17:26,860
But here's the scaling constraint

362
00:17:26,950 --> 00:17:28,390
I want to impose on w.

363
00:17:28,490 --> 00:17:30,540
I want to impose a constraint that

364
00:17:30,650 --> 00:17:35,530
the functional margin is equal to 1.

365
00:17:35,640 --> 00:17:40,500
And another way of writing that is that

366
00:17:40,610 --> 00:17:45,100
I want to impose a constraint that min over i, yi

367
00:17:45,210 --> 00:17:54,260
that in the worst case, function y is over 1.

368
00:17:54,360 --> 00:17:57,020
And clearly, this is a scaling constraint

369
00:17:57,120 --> 00:18:00,020
because if you solve for w and b,

370
00:18:00,140 --> 00:18:04,160
and you find that your worst-case function margin

371
00:18:04,270 --> 00:18:06,020
is actually 10 or whatever,

372
00:18:06,120 --> 00:18:09,500
then by dividing through w and b by a factor of 10,

373
00:18:09,590 --> 00:18:11,730
I can get my functional margin to be over 1.

374
00:18:11,820 --> 00:18:15,900
So this is a scaling constraint [inaudible] would imply.

375
00:18:16,000 --> 00:18:19,890
And this is just more compactly written as follows.

376
00:18:19,980 --> 00:18:21,690
This is imposing a constraint that

377
00:18:21,790 --> 00:18:23,310
the functional margin be equal to 1.

378
00:18:23,420 --> 00:18:29,500
And so if we just take what I wrote down as No.2

379
00:18:29,610 --> 00:18:32,090
of our previous optimization problem

380
00:18:32,190 --> 00:18:33,480
and add the scaling constraint,

381
00:18:33,570 --> 00:18:37,100
we then get the following optimization problem:

382
00:18:37,210 --> 00:18:40,170
min over wb.

383
00:18:40,240 --> 00:18:59,700
I guess previously, we had a maximization

384
00:18:59,730 --> 00:19:03,220
over gamma hats divided by the normal w.

385
00:19:03,330 --> 00:19:06,650
So those maximize 1 over the normal w,

386
00:19:06,740 --> 00:19:08,370
but so that's the same as

387
00:19:08,490 --> 00:19:11,140
minimizing the normal w squared. It was great.

388
00:19:11,230 --> 00:19:13,860
Maximum normal w is min w normal w squared.

389
00:19:13,950 --> 00:19:15,560
And then these are our constraints.

390
00:19:15,640 --> 00:19:17,330
Since I've added the constraint,

391
00:19:17,420 --> 00:19:18,900
the functional margin is over 1.

392
00:19:19,020 --> 00:19:26,930
And this is actually my final

393
00:19:27,020 --> 00:19:31,290
well, final formulation

394
00:19:31,370 --> 00:19:33,370
of the optimal margin classifier problem,

395
00:19:33,440 --> 00:19:34,710
at least for now.

396
00:19:34,810 --> 00:19:41,830
So the picture to keep in mind for this, I guess,

397
00:19:41,940 --> 00:19:44,970
is that our optimization objective

398
00:19:45,060 --> 00:19:46,750
is once you minimize the normal w.

399
00:19:46,860 --> 00:19:50,650
And so our optimization objective is just the

400
00:19:50,740 --> 00:19:52,000
[inaudible] quadratic function.

401
00:19:52,130 --> 00:19:54,460
And [inaudible] those pictures [inaudible] can draw it.

402
00:19:54,570 --> 00:19:58,410
So it if [inaudible] is w1 and w2,

403
00:19:58,500 --> 00:19:59,770
and you want to minimize

404
00:19:59,860 --> 00:20:01,090
the quadratic function like this

405
00:20:01,190 --> 00:20:02,940
so quadratic function just has [inaudible]

406
00:20:03,030 --> 00:20:04,380
that look like this.

407
00:20:04,480 --> 00:20:07,890
And moreover, you have a number of

408
00:20:07,970 --> 00:20:09,400
linear constraints in your parameters,

409
00:20:09,510 --> 00:20:11,780
so you may have linear constraints that

410
00:20:11,890 --> 00:20:14,060
eliminates that half space or linear constraint eliminates

411
00:20:14,150 --> 00:20:15,340
that half space [inaudible].

412
00:20:15,450 --> 00:20:21,170
So there's that half space and so on.

413
00:20:21,270 --> 00:20:24,940
And so the picture is you have a quadratic function,

414
00:20:25,010 --> 00:20:30,740
and you're ruling out various half spaces where

415
00:20:30,820 --> 00:20:33,320
each of these linear constraints. And I hope

416
00:20:33,430 --> 00:20:35,270
if you can picture this in 3D,

417
00:20:35,380 --> 00:20:36,460
I guess [inaudible] kinda draw our own 3D,

418
00:20:36,570 --> 00:20:38,820
hope you can convince yourself that

419
00:20:38,930 --> 00:20:41,480
this is a convex problem that has no local optimum.

420
00:20:41,590 --> 00:20:45,000
But they be run great [inaudible] within

421
00:20:45,130 --> 00:20:48,530
this set of points that hasn't ruled out,

422
00:20:48,650 --> 00:20:50,600
then you convert to the global optimum.

423
00:20:50,700 --> 00:20:55,670
And so that's the convex optimization problem.

424
00:20:55,770 --> 00:20:58,470
The does this [inaudible] nice and [inaudible].

425
00:20:58,630 --> 00:21:03,390
Questions about this?

426
00:21:03,460 --> 00:21:18,550
Actually, just raise your hand if this makes sense.

427
00:21:18,650 --> 00:21:26,610
Okay, cool. So this gives you

428
00:21:26,710 --> 00:21:28,860
the optimal margin classifier algorithm.

429
00:21:28,940 --> 00:21:31,040
And it turns out that

430
00:21:31,110 --> 00:21:33,280
this is the convex optimization problem,

431
00:21:33,380 --> 00:21:36,130
so you can actually take this formulation of the problem

432
00:21:36,210 --> 00:21:39,590
and throw it at off-the-shelf software

433
00:21:39,690 --> 00:21:42,250
what's called a QP or quadratic program software.

434
00:21:42,330 --> 00:21:43,750
This [inaudible] optimization is called

435
00:21:43,860 --> 00:21:44,890
a quadratic program,

436
00:21:45,010 --> 00:21:47,050
where the quadratic convex objective function

437
00:21:47,130 --> 00:21:48,610
and [inaudible] constraints

438
00:21:48,710 --> 00:21:50,280
so you can actually download software

439
00:21:50,370 --> 00:21:52,020
to solve these optimization problems for you.

440
00:21:52,130 --> 00:21:54,990
Usually, as you wanna use the

441
00:21:55,100 --> 00:21:56,910
use [inaudible]

442
00:21:57,020 --> 00:21:58,670
because you have constraints like these,

443
00:21:58,770 --> 00:21:59,990
although you could actually modify

444
00:22:00,100 --> 00:22:01,470
[inaudible] work with this, too.

445
00:22:01,580 --> 00:22:05,340
So we could just declare success and say that

446
00:22:05,430 --> 00:22:07,180
we're done with this formulation of the problem.

447
00:22:07,290 --> 00:22:09,370
But what I'm going to do now

448
00:22:09,490 --> 00:22:12,300
is take a digression to talk about primal

449
00:22:12,410 --> 00:22:13,820
and duo optimization problems.

450
00:22:13,930 --> 00:22:16,530
And in particular, I'm going to

451
00:22:16,620 --> 00:22:18,840
later, I'm going to come back

452
00:22:18,930 --> 00:22:21,250
and derive yet another very different form

453
00:22:21,340 --> 00:22:22,910
of this optimization problem.

454
00:22:23,020 --> 00:22:26,130
And the reason we'll do that is

455
00:22:26,210 --> 00:22:27,840
because it turns out this optimization problem

456
00:22:27,940 --> 00:22:30,140
has certain properties that

457
00:22:30,240 --> 00:22:32,250
make it amenable to very efficient algorithms.

458
00:22:32,360 --> 00:22:35,560
And moreover, I'll be deriving what's called

459
00:22:35,680 --> 00:22:38,130
the duo formulation of this that allows us

460
00:22:38,220 --> 00:22:41,370
to apply the optimal margin classifier

461
00:22:41,480 --> 00:22:44,720
even in very high-dimensional feature spaces

462
00:22:44,810 --> 00:22:48,440
even in sometimes infinite dimensional feature spaces.

463
00:22:48,550 --> 00:22:53,690
So we can come back to that later. But let me know,

464
00:22:53,810 --> 00:22:57,850
since I'm talking about convex optimization.

465
00:22:57,950 --> 00:23:03,870
So how many here is

466
00:23:03,970 --> 00:23:07,340
how many of you, from, I don't know, calculus,

467
00:23:07,410 --> 00:23:10,990
remember the method of Lagrange multipliers

468
00:23:11,090 --> 00:23:14,480
for solving an optimization problem like minimum

469
00:23:14,580 --> 00:23:16,310
minimization, maximization problem subject

470
00:23:16,420 --> 00:23:17,980
to some constraint?

471
00:23:18,090 --> 00:23:19,710
How many of you remember the method

472
00:23:19,800 --> 00:23:21,200
of Lagrange multipliers for that?

473
00:23:21,300 --> 00:23:23,870
Oh, okay, cool. Some of you, yeah.

474
00:23:23,980 --> 00:23:26,090
So if you don't remember, don't worry. I

475
00:23:26,170 --> 00:23:28,720
I'll describe that briefly here as well,

476
00:23:28,830 --> 00:23:30,760
but what I'm really gonna do is talk about

477
00:23:30,860 --> 00:23:32,310
the generalization of this method

478
00:23:32,410 --> 00:23:34,040
of Lagrange multipliers that you may

479
00:23:34,150 --> 00:23:36,980
or may not have seen in some calculus classes.

480
00:23:37,090 --> 00:23:38,480
But if you haven't seen it before,

481
00:23:38,580 --> 00:23:39,670
don't worry about it.

482
00:23:39,770 --> 00:23:45,630
So the method of Lagrange multipliers is was

483
00:23:45,730 --> 00:23:50,080
well, suppose there's some function

484
00:23:50,160 --> 00:23:52,020
you want to minimize, or minimize f of w.

485
00:23:52,130 --> 00:23:57,070
We're subject to some set of constraints that

486
00:23:57,190 --> 00:24:01,730
each i of w must equal 0 for i = 1 [inaudible] l.

487
00:24:01,820 --> 00:24:03,980
And given this constraint,

488
00:24:04,090 --> 00:24:07,370
I'll actually usually write it in vectorial form in which

489
00:24:07,480 --> 00:24:12,910
I write h of w as this vector value function.

490
00:24:13,870 --> 00:24:20,680
So that is equal to 0, where 0 is the arrow on top.

491
00:24:20,800 --> 00:24:22,860
I used that to denote the vector of all 0s.

492
00:24:22,970 --> 00:24:29,050
So you want to solve this optimization problem.

493
00:24:29,070 --> 00:24:30,290
Some of you have seen method

494
00:24:30,380 --> 00:24:31,660
of Lagrange multipliers where

495
00:24:31,770 --> 00:24:35,840
you construct this [inaudible] Lagrangian,

496
00:24:35,920 --> 00:24:47,640
which is the original optimization objective

497
00:24:47,740 --> 00:24:50,240
plus some [inaudible] Lagrange multipliers

498
00:24:50,340 --> 00:24:51,520
the highest constraints.

499
00:24:51,630 --> 00:24:55,390
And these parameters they derive

500
00:24:55,490 --> 00:24:57,600
we call the Lagrange multipliers.

501
00:24:57,700 --> 00:25:03,640
And so the way you actually solve

502
00:25:03,730 --> 00:25:07,060
the optimization problem is you take

503
00:25:07,140 --> 00:25:09,170
the partial derivative of this with respect to

504
00:25:09,240 --> 00:25:12,090
the original parameters and set that to 0.

505
00:25:12,190 --> 00:25:15,900
So the partial derivative with respect to

506
00:25:15,960 --> 00:25:17,840
your Lagrange multipliers [inaudible],

507
00:25:17,950 --> 00:25:18,930
and set that to 0.

508
00:25:19,040 --> 00:25:21,930
And then the same as theorem through [inaudible],

509
00:25:22,030 --> 00:25:25,640
I guess [inaudible] Lagrange was that for w

510
00:25:25,740 --> 00:25:34,020
for some value w star to get a solution,

511
00:25:34,020 --> 00:25:35,020
it is necessary that –there exists beta star

512
00:25:49,640 --> 00:25:50,560
Student:Right.

513
00:25:50,680 --> 00:25:51,860
Instructor (Andrew Ng):The backwards e

514
00:25:51,960 --> 00:25:53,260
there exists.

515
00:25:53,350 --> 00:25:55,740
So there exists beta star such that

516
00:25:55,850 --> 00:26:20,870
those partial derivatives are equal to 0.

517
00:26:20,960 --> 00:26:26,700
So the method of Lagrange multipliers

518
00:26:26,810 --> 00:26:29,530
is to solve this problem, you construct a Lagrangian,

519
00:26:29,620 --> 00:26:31,770
take the derivative with respect to

520
00:26:31,880 --> 00:26:35,420
the original parameters b, the original parameters w,

521
00:26:35,520 --> 00:26:38,820
and with respect to the Lagrange multipliers beta.

522
00:26:38,970 --> 00:26:41,410
Set the partial derivatives equal to 0,

523
00:26:41,510 --> 00:26:43,320
and solve for our solutions.

524
00:26:43,430 --> 00:26:45,080
And then you check each of the solutions to see

525
00:26:45,190 --> 00:26:56,370
if it is indeed a minimum. Great. So great

526
00:26:56,470 --> 00:26:57,770
so what I'm going to do

527
00:26:57,860 --> 00:27:01,830
is actually write down the generalization of this.

528
00:27:01,920 --> 00:27:03,780
And if you haven't seen that before,

529
00:27:03,860 --> 00:27:06,060
don't worry about it. This is [inaudible].

530
00:27:06,160 --> 00:27:07,980
So what I'm going to do is actually write down

531
00:27:08,090 --> 00:27:09,280
the generalization of this

532
00:27:09,360 --> 00:27:12,440
to solve a slightly more difficult type

533
00:27:12,550 --> 00:27:14,130
of constraint optimization problem,

534
00:27:14,220 --> 00:27:19,040
which is suppose you want to minimize f of w subject

535
00:27:19,130 --> 00:27:24,830
to the constraint that gi of w, excuse me,

536
00:27:24,920 --> 00:27:34,580
is less than equal to 0, and that hi of w is equal to 0.

537
00:27:34,660 --> 00:27:40,810
And again, using my vector notation,

538
00:27:40,880 --> 00:27:43,820
I'll write this as g of w is equal to 0.

539
00:27:43,880 --> 00:27:47,580
And h of w is equal to 0.

540
00:27:47,660 --> 00:27:50,000
So in [inaudible]'s case,

541
00:27:50,080 --> 00:27:51,760
we now have inequality for constraint

542
00:27:51,850 --> 00:27:53,370
as well as equality constraint.

543
00:27:53,480 --> 00:28:04,980
I then have a Lagrangian, or it's actually still

544
00:28:05,480 --> 00:28:07,070
called say generalized Lagrangian,

545
00:28:07,160 --> 00:28:10,090
which is now a function

546
00:28:10,170 --> 00:28:12,610
of my original optimization for parameters w,

547
00:28:12,730 --> 00:28:15,470
as well as two sets of Lagrange multipliers,

548
00:28:15,560 --> 00:28:20,980
alpha and beta. And so this will be f of w.

549
00:28:21,070 --> 00:28:39,910
Now, here's a cool part.

550
00:28:40,010 --> 00:28:46,090
I'm going to define theta subscript p of w

551
00:28:46,180 --> 00:28:52,060
to be equal to max of alpha beta subject

552
00:28:52,120 --> 00:28:53,900
to the constraints that the alphas are,

553
00:28:54,000 --> 00:29:00,200
beta equal to 0 of the Lagrange.

554
00:29:00,280 --> 00:29:16,600
And so I want you to consider

555
00:29:16,710 --> 00:29:22,650
the optimization problem min over w

556
00:29:22,830 --> 00:29:26,440
of max over alpha beta, such that

557
00:29:26,530 --> 00:29:28,940
the alpha is a greater than 0 of the Lagrange.

558
00:29:29,030 --> 00:29:40,970
And that's just equal to min over w, theta p of w.

559
00:29:41,080 --> 00:29:46,830
And just to give us a name, the [inaudible]

560
00:29:46,920 --> 00:29:48,910
the subscript p here is a sense of primal problem.

561
00:29:49,030 --> 00:29:52,230
And that refers to this entire thing.

562
00:29:52,310 --> 00:29:59,150
This optimization problem that written down

563
00:29:59,230 --> 00:30:00,720
is called a primal problem.

564
00:30:00,820 --> 00:30:02,870
This means there's the original optimization problem

565
00:30:02,980 --> 00:30:04,050
in which [inaudible] solving.

566
00:30:04,160 --> 00:30:07,230
And later on, I'll derive in another version of this,

567
00:30:07,330 --> 00:30:09,590
but that's what p stands for. It's a

568
00:30:09,690 --> 00:30:10,740
this is a primal problem.

569
00:30:10,840 --> 00:30:14,360
Now, I want you to look at

570
00:30:14,480 --> 00:30:16,200
consider theta over p again.

571
00:30:16,290 --> 00:30:18,590
And in particular, I wanna consider the problem

572
00:30:18,700 --> 00:30:20,370
of what happens if you minimize w

573
00:30:20,470 --> 00:30:25,970
minimize as a function of w this quantity theta over p.

574
00:30:26,080 --> 00:30:36,510
So let's look at what theta p of w is.

575
00:30:36,600 --> 00:30:44,710
Notice that if gi of w is greater than 0,

576
00:30:44,820 --> 00:30:46,510
so let's pick the value of w.

577
00:30:46,590 --> 00:30:49,230
And let's ask what is the state of p of w?

578
00:30:49,320 --> 00:30:53,650
So if w violates one of your primal problems constraints,

579
00:30:53,740 --> 00:31:04,460
then state of p of w would be infinity. Why is that?

580
00:31:04,550 --> 00:31:07,450
[Inaudible] p [inaudible] second.

581
00:31:07,550 --> 00:31:10,840
Suppose I pick a value of w that

582
00:31:10,940 --> 00:31:12,240
violates one of these constraints.

583
00:31:12,330 --> 00:31:18,870
So gi of w is positive. Then well, theta p is this

584
00:31:18,970 --> 00:31:20,710
maximize this function of alpha

585
00:31:20,710 --> 00:31:21,730
and beta the Lagrange.

586
00:31:21,830 --> 00:31:25,930
So one of these gi of w's is this positive, then

587
00:31:26,010 --> 00:31:29,710
by setting the other responding alpha i to plus infinity,

588
00:31:29,830 --> 00:31:31,340
I can make this arbitrarily large.

589
00:31:31,450 --> 00:31:34,150
And so if w violates one of

590
00:31:34,240 --> 00:31:36,760
my primal problem's constraints in one of the gis,

591
00:31:36,870 --> 00:31:39,980
then max over alpha of this Lagrange

592
00:31:40,070 --> 00:31:42,360
will be plus infinity.

593
00:31:42,450 --> 00:31:45,590
There's some of and in the same way

594
00:31:45,690 --> 00:31:47,160
I guess in a similar way,

595
00:31:47,280 --> 00:31:52,870
if hi of w is not equal to 0,

596
00:31:53,120 --> 00:31:58,660
then theta p of w also

597
00:31:58,750 --> 00:32:01,430
be infinity for a very similar reason

598
00:32:01,520 --> 00:32:04,360
because if hi of w is not equal to 0

599
00:32:04,520 --> 00:32:07,160
for some value of i, then in my Lagrange,

600
00:32:07,270 --> 00:32:10,010
I had a beta i x hi theorem.

601
00:32:10,120 --> 00:32:12,960
And so by setting beta i to be plus infinity

602
00:32:13,060 --> 00:32:15,240
or minus infinity depending on the sign of hi,

603
00:32:15,330 --> 00:32:17,640
I can make this plus infinity as well.

604
00:32:17,740 --> 00:32:37,120
And otherwise, theta p of w is just equal to f of w.

605
00:32:37,210 --> 00:32:41,110
Turns out if I had a value of w that

606
00:32:41,200 --> 00:32:44,280
satisfies all of the gi and the hi constraints,

607
00:32:44,360 --> 00:32:47,400
then we maximize in terms of alpha and beta

608
00:32:47,470 --> 00:32:51,940
all the Lagrange multiply theorems will actually

609
00:32:52,030 --> 00:32:55,000
be obtained by setting all the Lagrange

610
00:32:55,080 --> 00:32:56,740
multiply terms to be 0,

611
00:32:56,840 --> 00:33:00,720
and so theta p just left with f of w.

612
00:33:00,820 --> 00:33:12,470
Thus, theta p of w is equal to f of w

613
00:33:12,560 --> 00:33:18,350
if constraints are satisfied [inaudible]

614
00:33:18,440 --> 00:33:23,600
the gi in hi constraints,

615
00:33:23,690 --> 00:33:27,150
and is equal to plus infinity otherwise.

616
00:33:27,270 --> 00:33:35,400
So the problem I wrote down that

617
00:33:35,400 --> 00:33:36,400
minimizes the function of w theta p of w –

618
00:33:37,690 --> 00:33:40,310
this is [inaudible] problem.

619
00:33:40,400 --> 00:33:48,160
That's just exactly the same problem

620
00:33:48,250 --> 00:33:49,970
as my original primal problem

621
00:33:50,080 --> 00:33:52,360
because if you choose a value of w that

622
00:33:52,460 --> 00:33:54,330
violates the constraints, you get infinity.

623
00:33:54,420 --> 00:33:57,280
And if you satisfy the constraints, you get f of w.

624
00:33:57,280 --> 00:33:58,280
So this is really just the same as –

625
00:33:59,970 --> 00:34:00,930
well, we'll say,

626
00:34:01,020 --> 00:34:03,220
"Satisfy the constraints, and minimize f of w."

627
00:34:03,330 --> 00:34:06,480
That's really what minimizing the state of p of w is.

628
00:34:06,560 --> 00:34:12,910
Raise your hand if this makes sense. Yeah, okay, cool.

629
00:34:13,000 --> 00:34:26,430
So all right. I hope no one's getting mad at me

630
00:34:26,480 --> 00:34:27,940
because I'm doing so much work,

631
00:34:28,010 --> 00:34:28,840
and when we come back,

632
00:34:28,920 --> 00:34:30,190
it'll be exactly the same thing we started with.

633
00:34:30,270 --> 00:34:31,890
So here's the cool part.

634
00:34:31,970 --> 00:34:35,050
Let me know if you find it in dual problem.

635
00:34:35,140 --> 00:34:46,100
To find theta d and d [inaudible] duo,

636
00:34:46,180 --> 00:34:49,300
and this is how the function of alpha and beta is.

637
00:34:49,380 --> 00:34:51,070
It's not the function of the Lagrange multipliers.

638
00:34:51,170 --> 00:34:53,360
It's not of w. To find this,

639
00:34:53,440 --> 00:34:57,120
we minimize over w of

640
00:34:57,120 --> 00:35:00,460
my generalized Lagrange.

641
00:35:00,540 --> 00:35:17,640
And my dual problem is this.

642
00:35:17,730 --> 00:35:26,980
So in other words, this is max over that.

643
00:35:27,040 --> 00:35:31,370
And so this is my dual optimization problem.

644
00:35:31,470 --> 00:35:33,360
To maximize over alpha and beta,

645
00:35:33,440 --> 00:35:35,430
theta d over alpha and beta.

646
00:35:35,520 --> 00:35:38,280
So this optimization problem, I guess,

647
00:35:38,360 --> 00:35:39,430
is my dual problem.

648
00:35:39,520 --> 00:35:42,430
I want you to compare this to

649
00:35:42,490 --> 00:35:44,890
our previous primal optimization problem.

650
00:35:44,960 --> 00:35:49,500
The only difference is that I took the max and min,

651
00:35:49,570 --> 00:35:51,590
and I switched the order around with the max and min.

652
00:35:51,670 --> 00:35:53,150
That's the difference in the primal

653
00:35:53,230 --> 00:35:54,770
and the dual optimization [inaudible].

654
00:35:54,860 --> 00:36:02,810
And it turns out that it's a it's sort of

655
00:36:02,910 --> 00:36:06,830
it's a fact it's true, generally, that d star

656
00:36:06,930 --> 00:36:08,980
is less than [inaudible] p star. In other words,

657
00:36:09,050 --> 00:36:10,960
I think I defined p star previously.

658
00:36:11,020 --> 00:36:13,180
P star was a value of the prime optimization problem.

659
00:36:13,270 --> 00:36:18,990
And in other words, that it's just generally true that

660
00:36:19,100 --> 00:36:23,330
the max of the min of something is less than

661
00:36:23,390 --> 00:36:27,550
equal to the min of the max of something.

662
00:36:27,620 --> 00:36:29,590
And this is a general fact.

663
00:36:29,680 --> 00:36:31,700
And just as a concrete example,

664
00:36:31,780 --> 00:36:38,790
the max over y in the set 01 x oh, excuse me,

665
00:36:38,870 --> 00:36:46,180
of the min of the set in 01 of indicator x = y

666
00:36:46,270 --> 00:36:51,060
this is [inaudible] equal to min.

667
00:36:51,160 --> 00:37:12,750
So this equality this inequality actually

668
00:37:12,920 --> 00:37:15,560
holds true for any function you might find in here.

669
00:37:15,650 --> 00:37:18,210
And this is one specific example where

670
00:37:18,300 --> 00:37:21,010
the min over xy excuse me,

671
00:37:21,110 --> 00:37:23,210
min over x of [inaudible] equals y

672
00:37:23,300 --> 00:37:28,130
this is always equal to 0 because whatever y is,

673
00:37:28,180 --> 00:37:30,080
you can choose x to be something different.

674
00:37:30,150 --> 00:37:31,590
So this is always 0,

675
00:37:31,680 --> 00:37:36,550
whereas if I exchange the order to min and max,

676
00:37:36,650 --> 00:37:39,050
then thing here is always equal to 1.

677
00:37:39,160 --> 00:37:41,220
So 0 [inaudible] to 1.

678
00:37:41,300 --> 00:37:43,890
And more generally, this min/max

679
00:37:43,950 --> 00:37:46,100
excuse me, this max/min,

680
00:37:46,160 --> 00:37:47,690
thus with the min/max holds true

681
00:37:47,780 --> 00:37:49,440
for any function you might put in there.

682
00:37:49,550 --> 00:37:53,410
But it turns out that sometimes

683
00:37:53,470 --> 00:37:54,860
under certain conditions,

684
00:37:54,950 --> 00:38:05,430
these two optimization problems have the same value.

685
00:38:05,540 --> 00:38:08,170
Sometimes under certain conditions, the primal

686
00:38:08,280 --> 00:38:11,260
and the dual problems have the same value.

687
00:38:11,400 --> 00:38:16,550
And so you might be able to solve the dual problem

688
00:38:16,660 --> 00:38:18,350
rather than the primal problem.

689
00:38:18,500 --> 00:38:22,340
And the reason to do that is that sometimes,

690
00:38:22,440 --> 00:38:25,020
which we'll see in the optimal margin classifier problem,

691
00:38:25,110 --> 00:38:26,070
the support vector machine problem,

692
00:38:26,460 --> 00:38:28,150
the dual problem turns out to be much easier than it

693
00:38:28,230 --> 00:38:29,490
often has many useful properties that

694
00:38:29,590 --> 00:38:34,860
will make user compared to the primal.

695
00:38:34,890 --> 00:38:48,550
So for the sake of

696
00:38:48,620 --> 00:39:02,500
so what I'm going to do now

697
00:39:02,570 --> 00:39:04,920
is write down formally the certain conditions under

698
00:39:05,020 --> 00:39:06,110
which that's true

699
00:39:06,180 --> 00:39:07,630
where the primal and the dual problems are equivalent.

700
00:39:07,720 --> 00:39:11,630
And so our strategy for working out

701
00:39:11,710 --> 00:39:12,920
the [inaudible] of support

702
00:39:13,010 --> 00:39:14,710
vector machine algorithm will be that

703
00:39:14,770 --> 00:39:17,020
we'll write down the primal optimization problem,

704
00:39:17,120 --> 00:39:19,700
which we did previously, and maximizing classifier.

705
00:39:19,780 --> 00:39:21,310
And then we'll derive

706
00:39:21,390 --> 00:39:23,500
the dual optimization problem for that.

707
00:39:23,560 --> 00:39:26,080
And then we'll solve the dual problem.

708
00:39:26,150 --> 00:39:27,920
And by modifying that a little bit,

709
00:39:27,980 --> 00:39:29,770
that's how we'll derive this support vector machine.

710
00:39:29,850 --> 00:39:32,740
But let me ask you for now, let me just first,

711
00:39:32,820 --> 00:39:34,290
for the sake of completeness,

712
00:39:34,410 --> 00:39:36,200
I just write down the conditions under which

713
00:39:36,280 --> 00:39:37,940
the primal and the duo optimization problems

714
00:39:38,030 --> 00:39:43,660
give you the same solutions. So let f be convex.

715
00:39:43,760 --> 00:39:47,990
If you're not sure what convex means,

716
00:39:48,090 --> 00:39:50,780
for the purposes of this class,

717
00:39:50,860 --> 00:39:53,180
you can take it to mean that the Hessian,

718
00:39:53,290 --> 00:39:55,400
h is positive. [Inaudible], so it just means

719
00:39:55,510 --> 00:39:57,480
it's a [inaudible] function like that.

720
00:39:57,580 --> 00:40:00,480
And once you learn more about optimization

721
00:40:00,560 --> 00:40:04,210
again, please come to this week's discussion session

722
00:40:04,310 --> 00:40:13,010
taught by the TAs. Then suppose hi

723
00:40:13,100 --> 00:40:14,780
the hi constraints is affine,

724
00:40:14,910 --> 00:40:18,820
and what that means is that hi of w

725
00:40:18,900 --> 00:40:21,630
equals alpha i transpose w plus vi.

726
00:40:21,740 --> 00:40:26,010
This actually means the same thing as linear.

727
00:40:26,150 --> 00:40:28,110
Without the term b here,

728
00:40:28,240 --> 00:40:30,220
we say that hi is linear where

729
00:40:30,280 --> 00:40:31,860
we have a constant interceptor as well.

730
00:40:31,950 --> 00:40:33,410
This is technically called

731
00:40:33,410 --> 00:40:34,700
affine other than linear.

732
00:40:34,800 --> 00:40:46,740
And let's suppose that gi's are strictly feasible.

733
00:40:46,840 --> 00:40:56,010
And what that means is that

734
00:40:56,120 --> 00:41:02,330
there is just a value of the w such that from i,

735
00:41:02,420 --> 00:41:05,900
gi of w is less than 0.

736
00:41:05,970 --> 00:41:08,110
Don't worry too much [inaudible].

737
00:41:08,230 --> 00:41:09,210
I'm writing these things down

738
00:41:09,310 --> 00:41:10,740
for the sake of completeness,

739
00:41:10,840 --> 00:41:11,740
but don't worry too much about

740
00:41:11,830 --> 00:41:13,300
all the technical details. Strictly feasible,

741
00:41:13,370 --> 00:41:15,920
which just means that there's a value of w such that

742
00:41:16,030 --> 00:41:18,090
all of these constraints are satisfy were stricter than

743
00:41:18,180 --> 00:41:19,290
the equality rather than

744
00:41:19,290 --> 00:41:20,400
what less than equal to.

745
00:41:20,510 --> 00:41:27,770
Under these conditions, there were exists w star,

746
00:41:27,900 --> 00:41:34,010
alpha star, beta star such that

747
00:41:34,110 --> 00:41:36,080
w star solves the primal problem.

748
00:41:36,140 --> 00:41:42,380
And alpha star and beta star,

749
00:41:42,480 --> 00:41:44,590
the Lagrange multipliers, solve the dual problem.

750
00:41:44,700 --> 00:41:55,530
And the value of the primal problem

751
00:41:55,590 --> 00:41:57,340
will be equal to the value of the dual problem

752
00:41:57,420 --> 00:41:59,760
will be equal to the value

753
00:41:59,860 --> 00:42:03,990
of your Lagrange multiplier excuse me,

754
00:42:04,060 --> 00:42:06,220
will be equal to the value of your generalized Lagrange,

755
00:42:06,310 --> 00:42:08,190
the value of that w star, alpha star, beta star.

756
00:42:08,260 --> 00:42:11,550
In other words, you can solve either the primal

757
00:42:11,630 --> 00:42:13,880
or the dual problem. You get the same solution.

758
00:42:13,970 --> 00:42:20,790
Further, your parameters will

759
00:42:20,880 --> 00:42:28,070
satisfy these conditions.

760
00:42:28,190 --> 00:42:30,850
Partial derivative perspective parameters would be 0.

761
00:42:30,930 --> 00:42:34,640
And actually, to keep this equation in mind,

762
00:42:34,730 --> 00:42:36,860
we'll actually use this in a second when

763
00:42:36,920 --> 00:42:38,540
we take the Lagrange, and we

764
00:42:38,620 --> 00:42:40,340
and our support vector machine problem,

765
00:42:40,460 --> 00:42:42,980
and take a derivative with respect to w to solve a

766
00:42:43,080 --> 00:42:45,690
to solve our to derive our dual problem.

767
00:42:45,790 --> 00:42:48,150
We'll actually perform this step ourselves in a second.

768
00:42:48,260 --> 00:42:53,290
Partial derivative with respect to

769
00:42:53,380 --> 00:42:55,780
the Lagrange multiplier beta is equal to 0.

770
00:42:55,900 --> 00:43:09,860
Turns out this will hold true, too.

771
00:43:09,950 --> 00:43:14,960
This is called the well

772
00:43:15,060 --> 00:43:31,300
this is called the KKT complementary condition.

773
00:43:31,350 --> 00:43:34,330
KKT stands for Karush-Kuhn-Tucker,

774
00:43:34,410 --> 00:43:35,950
which were the authors of this theorem.

775
00:43:36,040 --> 00:43:40,020
Well, and by tradition, usually this [inaudible]

776
00:43:40,110 --> 00:43:43,370
KKT conditions. But the other two are

777
00:43:43,490 --> 00:43:52,650
just so the [inaudible] is greater than 0,

778
00:43:52,730 --> 00:43:54,860
which we had previously and that

779
00:43:54,960 --> 00:43:57,580
your constraints are actually satisfied.

780
00:43:57,680 --> 00:44:27,750
So let's see. [Inaudible] All right.

781
00:44:27,820 --> 00:44:32,090
So let's take those and apply this to

782
00:44:32,160 --> 00:44:38,420
our optimal margin optimization problem that

783
00:44:38,490 --> 00:44:39,980
we had previously.

784
00:44:40,070 --> 00:44:44,250
I was gonna say one word about this, which is

785
00:44:44,330 --> 00:44:48,170
was gonna say one word about

786
00:44:48,280 --> 00:44:52,660
this KTT complementary condition is that

787
00:44:52,740 --> 00:44:54,250
a condition that is a

788
00:44:54,310 --> 00:44:56,810
at your solution, you must have that

789
00:44:56,900 --> 00:45:03,200
alpha star i times gi of w is equal to 0. So let's see.

790
00:45:03,290 --> 00:45:06,810
So the product of two numbers is equal to 0.

791
00:45:06,900 --> 00:45:10,530
That means that at least one of these things

792
00:45:10,620 --> 00:45:11,990
must be equal to 0.

793
00:45:12,100 --> 00:45:14,710
For the product of two things to be equal to 0, well,

794
00:45:14,790 --> 00:45:20,420
that's just saying either alpha or i or gi is equal to 0.

795
00:45:20,510 --> 00:45:23,110
So what that implies is that the

796
00:45:23,160 --> 00:45:41,570
just Karush-Kuhn-Tucker most people just say KKT,

797
00:45:41,680 --> 00:45:43,000
but we wanna show you the right

798
00:45:43,050 --> 00:45:46,250
spelling of their names.

799
00:45:46,340 --> 00:45:50,340
So KKT complementary condition implies that

800
00:45:50,440 --> 00:45:53,510
if alpha i is not 0,

801
00:45:53,600 --> 00:46:01,810
that necessarily implies that gi of w star

802
00:46:01,860 --> 00:46:25,080
is equal to 0. And usually, it turns out

803
00:46:25,110 --> 00:46:27,440
so all the KKT condition guarantees is that

804
00:46:27,490 --> 00:46:30,520
at least one of them is 0.

805
00:46:30,620 --> 00:46:32,590
It may actually be the case that

806
00:46:32,680 --> 00:46:35,910
both alpha and gi are both equal to 0. But in practice,

807
00:46:35,990 --> 00:46:37,830
when you solve this optimization problem,

808
00:46:37,940 --> 00:46:41,140
you find that to a large part, alpha i star

809
00:46:41,230 --> 00:46:45,120
is not equal to 0 if and only gi of w star 0, 0.

810
00:46:45,200 --> 00:46:47,720
This is not strictly true because it's possible that

811
00:46:47,810 --> 00:46:52,310
both of these may be 0. But in practice, when we

812
00:46:52,360 --> 00:46:53,800
because when we solve problems like these,

813
00:46:53,910 --> 00:46:55,080
you're, for the most part,

814
00:46:55,140 --> 00:46:57,510
usually exactly one of these will be non-0.

815
00:46:57,590 --> 00:47:01,120
And also, when this holds true,

816
00:47:01,200 --> 00:47:06,250
when gi of w star is equal to 0, we say that gi

817
00:47:06,350 --> 00:47:16,520
gi of w, I guess, is an active constraint

818
00:47:16,640 --> 00:47:20,360
because we call a constraint

819
00:47:20,720 --> 00:47:21,860
our constraint was a gi of w

820
00:47:21,930 --> 00:47:23,150
must be less than or equal to 0.

821
00:47:23,240 --> 00:47:25,090
And so it is equal to 0,

822
00:47:25,170 --> 00:47:28,010
then we say that that's a constraint that

823
00:47:28,080 --> 00:47:29,390
this is an active constraint.

824
00:47:29,480 --> 00:47:34,880
Once we talk about [inaudible],

825
00:47:34,960 --> 00:47:37,200
we come back and [inaudible]

826
00:47:37,240 --> 00:47:39,120
and just extend this idea a little bit more.

827
00:47:40,300 --> 00:47:43,930
[Inaudible] board.

828
00:47:44,020 --> 00:47:54,010
[Inaudible] turn to this board in a second, but

829
00:47:54,080 --> 00:48:09,200
so let's go back and work out one of the primal

830
00:48:09,290 --> 00:48:11,190
and the dual optimization problems for

831
00:48:11,560 --> 00:48:13,400
our optimal margin classifier for

832
00:48:13,490 --> 00:48:14,830
the optimization problem that

833
00:48:14,910 --> 00:48:16,210
we worked on just now.

834
00:48:16,300 --> 00:48:18,380
As a point of notation,

835
00:48:18,420 --> 00:48:22,050
in whatever I've been writing down so far

836
00:48:22,080 --> 00:48:23,740
in deriving the KKT conditions,

837
00:48:23,810 --> 00:48:31,810
when Lagrange multipliers were alpha i and beta i,

838
00:48:31,910 --> 00:48:34,650
it turns out that when applied as [inaudible] dm,

839
00:48:34,730 --> 00:48:38,080
turns out we only have

840
00:48:38,160 --> 00:48:41,860
one set of Lagrange multipliers alpha i. And also,

841
00:48:41,920 --> 00:48:45,790
as I was working out the KKT conditions,

842
00:48:45,890 --> 00:48:49,330
I used w to denote the parameters of

843
00:48:49,420 --> 00:48:51,470
my primal optimization problem.

844
00:48:51,550 --> 00:48:53,200
[Inaudible] I wanted to minimize f of w.

845
00:48:53,310 --> 00:48:55,350
In my very first optimization problem,

846
00:48:55,420 --> 00:48:56,830
I had that optimization problem

847
00:48:56,910 --> 00:48:58,540
[inaudible] finding the parameters w.

848
00:48:58,610 --> 00:49:00,960
In my svn problem,

849
00:49:01,040 --> 00:49:03,800
I'm actually gonna have two sets of parameters,

850
00:49:03,900 --> 00:49:05,530
w and b. So this is just a

851
00:49:05,620 --> 00:49:10,700
keep that sort of slight notation change in mind.

852
00:49:10,770 --> 00:49:17,280
So problem we worked out previously

853
00:49:17,340 --> 00:49:19,540
was we want to minimize the normal w squared

854
00:49:19,630 --> 00:49:22,060
and just add a half there by convention

855
00:49:22,140 --> 00:49:23,300
because it makes other work

856
00:49:23,380 --> 00:49:24,510
math work a little nicer.

857
00:49:24,620 --> 00:49:28,610
And subject to the constraint that

858
00:49:28,690 --> 00:49:35,230
yi x w [inaudible] xi + v must be = greater than 1.

859
00:49:35,310 --> 00:49:43,420
And so let me just take this constraint,

860
00:49:43,540 --> 00:49:47,150
and I'll rewrite it as a constraint. It's gi of w, b.

861
00:49:47,250 --> 00:49:50,820
Again, previously, I had gi of w,

862
00:49:50,920 --> 00:49:53,600
but now I have parameters w and b.

863
00:49:53,710 --> 00:50:03,620
So gi of w, b defined as 1.

864
00:50:03,730 --> 00:50:18,170
So let's look at the implications of this in terms of

865
00:50:18,280 --> 00:50:21,120
the KKT dual complementary condition again.

866
00:50:21,200 --> 00:50:27,930
So we have that alpha i is basically equal to 0.

867
00:50:28,030 --> 00:50:36,710
That necessarily implies that gi of w, b is equal to 0.

868
00:50:36,810 --> 00:50:39,140
In other words, this is an active constraint.

869
00:50:39,230 --> 00:50:48,480
And what does this mean? It means that

870
00:50:48,580 --> 00:50:54,900
it actually turns out gi of wb equal to 0 that is

871
00:50:54,990 --> 00:50:58,730
that means exactly that the training example xi,

872
00:50:58,820 --> 00:51:09,620
yi has functional margin equal to 1.

873
00:51:09,720 --> 00:51:13,910
Because this constraint was that

874
00:51:14,040 --> 00:51:17,630
the functional margin of every example

875
00:51:17,720 --> 00:51:19,420
has to be greater equal to 1.

876
00:51:19,530 --> 00:51:21,130
And so if this is an active constraint, it just

877
00:51:21,220 --> 00:51:22,740
inequality holds

878
00:51:22,840 --> 00:51:24,210
that equality.

879
00:51:24,310 --> 00:51:25,660
That means that my training example

880
00:51:25,710 --> 00:51:28,730
i must have functional margin equal to exactly 1.

881
00:51:28,770 --> 00:51:34,440
And so actually, yeah, right now,

882
00:51:34,540 --> 00:51:35,930
I'll do this on a different board, I guess.

883
00:51:36,020 --> 00:52:01,050
So in pictures, what that means is that,

884
00:52:01,150 --> 00:52:04,400
you have some training sets,

885
00:52:04,500 --> 00:52:12,160
and you'll have some separating hyperplane.

886
00:52:12,260 --> 00:52:17,730
And so the examples with functional margin

887
00:52:17,840 --> 00:52:22,740
equal to 1 will be exactly those which are

888
00:52:22,860 --> 00:52:27,100
so they're closest to my separating hyperplane.

889
00:52:27,200 --> 00:52:33,290
So that's my equation. [Inaudible] equal to 0.

890
00:52:33,380 --> 00:52:37,740
And so in this in this cartoon example that

891
00:52:37,830 --> 00:52:39,670
I've done, it'll be exactly

892
00:52:39,760 --> 00:52:49,770
these three examples that

893
00:52:49,900 --> 00:52:53,870
have functional margin equal to 1,

894
00:52:53,960 --> 00:52:56,960
and all of the other examples as being further away

895
00:52:57,010 --> 00:53:00,330
than these three will have functional margin that

896
00:53:00,390 --> 00:53:01,870
is strictly greater than 1.

897
00:53:01,970 --> 00:53:10,380
And the examples with functional margin equal to 1

898
00:53:10,450 --> 00:53:22,010
will usually correspond to the ones

899
00:53:22,110 --> 00:53:24,910
where the corresponding Lagrange multipliers

900
00:53:25,010 --> 00:53:26,580
also not equal to 0.

901
00:53:26,580 --> 00:53:27,710
And again, it may not hold true.

902
00:53:27,820 --> 00:53:30,940
It may be the case that gi and alpha i equal to 0.

903
00:53:31,010 --> 00:53:34,850
But usually, when gi's not is 0, alpha i will be non-0.

904
00:53:34,930 --> 00:53:37,620
And so the examples of functional margin

905
00:53:37,710 --> 00:53:38,920
equal to 1 will be the ones where

906
00:53:39,020 --> 00:53:40,880
alpha i is not equal to 0.

907
00:53:40,990 --> 00:53:48,400
One useful property of this is that

908
00:53:48,510 --> 00:53:50,670
as suggested by this picture and

909
00:53:50,770 --> 00:53:52,430
so true in general as well,

910
00:53:52,540 --> 00:53:55,150
it turns out that we find a solution to this

911
00:53:55,250 --> 00:53:56,850
to the optimization problem,

912
00:53:56,970 --> 00:53:59,170
you find that relatively few training examples

913
00:53:59,260 --> 00:54:00,830
have functional margin equal to 1.

914
00:54:00,930 --> 00:54:02,380
In this picture I've drawn,

915
00:54:02,460 --> 00:54:04,300
there are three examples

916
00:54:04,390 --> 00:54:05,680
with functional margin equal to 1.

917
00:54:05,800 --> 00:54:07,530
There are just few examples of this minimum

918
00:54:07,640 --> 00:54:09,930
possible distance to your separating hyperplane.

919
00:54:10,040 --> 00:54:13,170
And these are three

920
00:54:13,270 --> 00:54:15,950
these examples of functional margin equal to 1

921
00:54:16,060 --> 00:54:22,290
they are what we're going to call the support vectors.

922
00:54:22,410 --> 00:54:28,290
And this needs the name support vector machine.

923
00:54:28,370 --> 00:54:29,340
There'll be these three points

924
00:54:29,450 --> 00:54:31,140
with functional margin 1 that

925
00:54:31,290 --> 00:54:32,640
we're calling support vectors.

926
00:54:32,740 --> 00:54:38,020
And the fact that they're relatively

927
00:54:38,130 --> 00:54:40,240
few support vectors also means that usually,

928
00:54:40,330 --> 00:54:42,760
most of the alpha i's are equal to 0.

929
00:54:42,870 --> 00:54:55,200
So with alpha i equal to 0, for examples,

930
00:54:55,280 --> 00:55:01,920
though, not support vectors.

931
00:55:01,920 --> 00:55:03,990
Let's go ahead

932
00:55:04,070 --> 00:55:07,760
and work out the actual optimization problem.

933
00:55:20,720 --> 00:55:32,190
So we have a [inaudible] margin optimization problem.

934
00:55:32,300 --> 00:55:35,800
So there we go and write down the margin,

935
00:55:35,890 --> 00:55:41,500
and because we only have inequality constraints

936
00:55:41,580 --> 00:55:43,760
where we really have gi star constraints,

937
00:55:43,860 --> 00:55:45,650
no hi star constraint.

938
00:55:45,760 --> 00:55:47,750
We have inequality constraints

939
00:55:47,810 --> 00:55:49,400
and no equality constraints,

940
00:55:49,500 --> 00:55:53,960
I'll only have Lagrange multipliers of type alpha

941
00:55:54,040 --> 00:55:56,090
no betas in my generalized Lagrange.

942
00:55:56,190 --> 00:56:04,560
But my Lagrange will be one-half w squared minus.

943
00:56:04,660 --> 00:56:18,410
That's my Lagrange.

944
00:56:18,520 --> 00:56:29,020
And so let's work out what the dual problem is.

945
00:56:29,130 --> 00:56:30,350
And to do that,

946
00:56:30,440 --> 00:56:32,340
I need to figure out what theta d of alpha

947
00:56:32,420 --> 00:56:34,150
and I know again, beta's there

948
00:56:34,240 --> 00:56:37,920
so what theta d of alpha is min

949
00:56:37,990 --> 00:56:43,060
with respect to wb of lb alpha.

950
00:56:43,150 --> 00:56:46,990
So the dual problem is the maximize theta d

951
00:56:47,090 --> 00:56:48,110
as the function of alpha.

952
00:56:48,190 --> 00:56:50,050
So as to work out what theta d is,

953
00:56:50,150 --> 00:56:52,020
and then that'll give us our dual problem.

954
00:56:52,120 --> 00:56:56,000
So then to work out what this is,

955
00:56:56,100 --> 00:56:57,350
what do you need to do?

956
00:56:57,440 --> 00:56:59,050
We need to take a look at Lagrange

957
00:56:59,140 --> 00:57:02,500
and minimize it as a function of lv and b so

958
00:57:02,600 --> 00:57:03,510
and what is this?

959
00:57:03,590 --> 00:57:05,260
How do you minimize Lagrange?

960
00:57:05,370 --> 00:57:07,390
So in order to minimize the Lagrange

961
00:57:07,500 --> 00:57:10,410
as a function of w and b, we do the usual thing.

962
00:57:10,520 --> 00:57:12,290
We take the derivatives of w

963
00:57:12,370 --> 00:57:15,040
Lagrange with respect to w and b.

964
00:57:15,150 --> 00:57:16,290
And we set that to 0.

965
00:57:16,370 --> 00:57:18,030
That's how we minimize the Lagrange

966
00:57:18,110 --> 00:57:19,290
with respect to w and b.

967
00:57:19,410 --> 00:57:21,770
So take the derivative

968
00:57:21,770 --> 00:57:22,770
with respect to w of the Lagrange. And I want –

969
00:57:28,550 --> 00:57:29,810
I just write down the answer.

970
00:57:29,910 --> 00:57:31,640
You know how to do calculus like this.

971
00:57:31,750 --> 00:57:40,110
So I wanna minimize this function of w,

972
00:57:40,200 --> 00:57:42,200
so I take the derivative

973
00:57:42,200 --> 00:57:43,490
and set it to 0. And I get that.

974
00:57:43,570 --> 00:57:56,480
And then so this implies that w must be that.

975
00:57:56,600 --> 00:58:01,450
And so w, therefore,

976
00:58:01,550 --> 00:58:03,630
is actually a linear combination

977
00:58:03,710 --> 00:58:05,650
of your input feature vectors xi.

978
00:58:05,730 --> 00:58:08,670
This is sum of your various weights given

979
00:58:08,750 --> 00:58:10,880
by the alpha i's and times the xi's,

980
00:58:10,990 --> 00:58:12,550
which are your examples in your training set.

981
00:58:12,660 --> 00:58:14,160
And this will be useful later.

982
00:58:14,270 --> 00:58:17,810
The other equation we have is

983
00:58:17,910 --> 00:58:24,080
here, partial derivative of Lagrange

984
00:58:24,170 --> 00:58:25,670
with respect to p is equal to

985
00:58:25,780 --> 00:58:32,860
minus sum of i plus 1 to m [inaudible] for i.

986
00:58:32,930 --> 00:58:36,740
And so I'll just set that to equal to 0.

987
00:58:36,850 --> 00:58:38,980
And so these are my two constraints.

988
00:58:39,060 --> 00:58:47,810
And so [inaudible].

989
00:58:47,900 --> 00:58:51,060
So what I'm going to do is

990
00:58:51,170 --> 00:58:53,260
I'm actually going to take these two constraints,

991
00:58:53,350 --> 00:58:54,990
and well, I'm going to take

992
00:58:55,090 --> 00:58:56,590
whatever I thought to be the value for w.

993
00:58:56,670 --> 00:59:02,070
And I'm gonna take what

994
00:59:02,160 --> 00:59:03,570
I've worked out to be the value for w,

995
00:59:03,670 --> 00:59:07,670
and I'll plug it back in there to figure out

996
00:59:07,780 --> 00:59:10,010
what the Lagrange really

997
00:59:10,120 --> 00:59:12,370
is when I minimize with respect to w.

998
00:59:12,470 --> 00:59:16,200
[Inaudible] and I'll deal with b in a second.

999
00:59:16,250 --> 00:59:39,010
So let's see. So my Lagrange

1000
00:59:39,130 --> 00:59:43,100
is 1/2 w transpose w minus.

1001
00:59:43,170 --> 01:00:00,380
So this first term, w transpose w

1002
01:00:00,490 --> 01:00:07,700
this becomes sum y equals one to m,

1003
01:00:07,810 --> 01:00:12,020
alpha i, yi, xi transpose.

1004
01:00:12,120 --> 01:00:22,830
This is just putting in the value for w that

1005
01:00:22,900 --> 01:00:23,950
I worked out previously.

1006
01:00:24,030 --> 01:00:27,400
But since this is w transpose w

1007
01:00:27,510 --> 01:00:30,100
and so when they expand out of this quadratic function,

1008
01:00:30,210 --> 01:00:34,030
and when I plug in w over there as well,

1009
01:00:34,110 --> 01:01:23,970
I find that I have that. Oh, where I'm using these

1010
01:01:24,070 --> 01:01:26,270
angle brackets to denote inner product,

1011
01:01:26,370 --> 01:01:30,760
so this thing here, it just means the inner product,

1012
01:01:30,830 --> 01:01:33,130
xi transpose xj.

1013
01:01:33,240 --> 01:01:37,720
And the first and second terms

1014
01:01:37,800 --> 01:01:39,970
are actually the same except for the minus one half.

1015
01:01:40,070 --> 01:01:47,710
So to simplify to be equal to that.

1016
01:01:47,830 --> 01:02:20,520
So let me go ahead and call this w of alpha.

1017
01:02:20,620 --> 01:02:49,450
My dual problem is, therefore, the following.

1018
01:02:49,510 --> 01:02:52,950
I want to maximize w of alpha,

1019
01:02:53,050 --> 01:02:54,730
which is that [inaudible].

1020
01:02:54,810 --> 01:03:01,010
And I want to the

1021
01:03:01,100 --> 01:03:02,670
I realize the notation is somewhat unfortunate.

1022
01:03:02,770 --> 01:03:05,040
I'm using capital W of alpha to denote that

1023
01:03:05,130 --> 01:03:06,900
formula I wrote down earlier.

1024
01:03:06,960 --> 01:03:10,690
And then we also had our lowercase w.

1025
01:03:10,770 --> 01:03:12,970
The original [inaudible] is the primal problem.

1026
01:03:13,080 --> 01:03:15,040
Lowercase w transpose xi.

1027
01:03:15,130 --> 01:03:17,760
So uppercase and lowercase w

1028
01:03:17,860 --> 01:03:22,010
are totally different things, so unfortunately,

1029
01:03:22,120 --> 01:03:23,440
the notation is standard as well,

1030
01:03:23,550 --> 01:03:24,860
as far as I know, so.

1031
01:03:24,940 --> 01:03:30,170
So the dual problem is that

1032
01:03:30,260 --> 01:03:32,530
subject to the alpha [inaudible] related to 0,

1033
01:03:32,640 --> 01:03:40,210
and we also have that the sum of i,

1034
01:03:40,440 --> 01:03:43,170
yi, alpha i is related to 0.

1035
01:03:43,280 --> 01:03:47,410
That last constraint was the constraint

1036
01:03:47,470 --> 01:03:50,640
I got from this the sum of i

1037
01:03:50,710 --> 01:03:54,450
sum of i, yi alpha i equals to 0.

1038
01:03:54,560 --> 01:03:56,300
But that's where that [inaudible] came from.

1039
01:03:56,380 --> 01:04:04,170
Let me just I think in previous years that

1040
01:04:04,260 --> 01:04:05,950
I taught this, where

1041
01:04:06,060 --> 01:04:08,020
this constraint comes from is just

1042
01:04:08,070 --> 01:04:09,140
is slightly confusing.

1043
01:04:09,250 --> 01:04:11,350
So let me just take two minutes to say

1044
01:04:11,440 --> 01:04:12,710
what the real interpretation of that is.

1045
01:04:12,820 --> 01:04:13,990
And if you don't understand it,

1046
01:04:14,080 --> 01:04:17,020
it's not a big deal, I guess.

1047
01:04:17,110 --> 01:04:20,560
So when we took the partial derivative

1048
01:04:20,670 --> 01:04:22,350
of the Lagrange with respect to b,

1049
01:04:22,460 --> 01:04:25,240
we end up with this constraint that sum of i,

1050
01:04:25,330 --> 01:04:26,530
yi, alpha i must be equal to 0.

1051
01:04:26,630 --> 01:04:30,050
The interpretation of that, it turns out,

1052
01:04:30,160 --> 01:04:38,510
is that if sum of i, yi, alpha i is not equal to 0,

1053
01:04:38,610 --> 01:04:56,850
then theta d of wb is actually, excuse me.

1054
01:04:56,930 --> 01:05:03,450
Then theta d of alpha is equal to

1055
01:05:03,560 --> 01:05:09,520
minus infinity for minimizing.

1056
01:05:09,610 --> 01:05:16,450
So in other words, it turns out my Lagrange

1057
01:05:16,540 --> 01:05:19,220
is actually a linear function of my parameters b.

1058
01:05:19,330 --> 01:05:21,060
And so the interpretation of that

1059
01:05:21,170 --> 01:05:22,470
constraint we worked out previously was that

1060
01:05:22,570 --> 01:05:25,020
if sum of i or yi, alpha i is not equal to 0,

1061
01:05:25,110 --> 01:05:27,860
then theta d of alpha is equal to minus infinity.

1062
01:05:27,960 --> 01:05:32,580
And so if your goal is to maximize

1063
01:05:32,670 --> 01:05:37,700
as a function of alpha, theta d of alpha,

1064
01:05:37,760 --> 01:05:40,330
then you've gotta choose values of alpha

1065
01:05:40,420 --> 01:05:42,920
for which sum of yi alpha is equal to 0.

1066
01:05:43,000 --> 01:05:47,550
And then when sum of yi alpha is equal to 0,

1067
01:05:47,660 --> 01:06:02,580
then theta d of alpha is equal to w of alpha.

1068
01:06:02,680 --> 01:06:05,450
And so that's why we ended up

1069
01:06:05,550 --> 01:06:08,900
deciding to maximize w of alpha subject to that

1070
01:06:09,000 --> 01:06:10,590
sum of yi alpha is equal to 0.

1071
01:06:10,670 --> 01:06:16,300
Yeah, the unfortunately,

1072
01:06:16,420 --> 01:06:19,460
the fact of that d would be [inaudible] adds just

1073
01:06:19,550 --> 01:06:20,920
a little bit of extra notation

1074
01:06:21,020 --> 01:06:23,360
in our derivation of the dual. But by the way,

1075
01:06:23,460 --> 01:06:25,560
and [inaudible] all the action

1076
01:06:25,680 --> 01:06:27,620
of the optimization problem is with w

1077
01:06:27,700 --> 01:06:33,620
because b is just one parameter. So let's check.

1078
01:06:33,710 --> 01:06:46,460
Are there any questions about this? Okay, cool.

1079
01:06:46,550 --> 01:06:51,350
So what derived a dual optimization problem

1080
01:06:51,440 --> 01:06:53,210
and really, don't worry about this

1081
01:06:53,310 --> 01:06:55,340
if you're not quite sure where this was.

1082
01:06:55,410 --> 01:06:57,860
Just think of this as we worked out this constraint,

1083
01:06:57,940 --> 01:06:59,180
and we worked out,

1084
01:06:59,260 --> 01:07:00,750
and we took partial derivative with respect to b,

1085
01:07:00,840 --> 01:07:03,180
that this constraint has the [inaudible] and so

1086
01:07:03,270 --> 01:07:06,460
I just copied that over here. But so

1087
01:07:06,580 --> 01:07:08,900
worked out the dual of the optimization problem,

1088
01:07:08,950 --> 01:07:12,890
so our approach to finding

1089
01:07:12,980 --> 01:07:15,330
to deriving the optimal margin classifier

1090
01:07:15,420 --> 01:07:17,220
or support vector machine will be that

1091
01:07:17,310 --> 01:07:21,070
we'll solve along this dual optimization problem

1092
01:07:21,170 --> 01:07:22,850
for the parameters alpha star.

1093
01:07:22,940 --> 01:07:31,720
And then if you want, you can then

1094
01:07:31,820 --> 01:07:34,260
this is the equation that we worked out

1095
01:07:34,340 --> 01:07:35,340
on the previous board.

1096
01:07:35,420 --> 01:07:38,300
We said that w this [inaudible] alpha

1097
01:07:38,390 --> 01:07:45,270
w must be equal to that.

1098
01:07:45,340 --> 01:07:48,750
And so once you solve for alpha,

1099
01:07:48,850 --> 01:07:53,490
you can then go back and quickly derive w

1100
01:07:53,600 --> 01:07:55,060
in parameters to your primal problem.

1101
01:07:55,140 --> 01:07:56,370
And we worked this out earlier.

1102
01:07:56,450 --> 01:08:01,940
And moreover, once you solve alpha and w,

1103
01:08:02,030 --> 01:08:05,390
you can then focus back into your

1104
01:08:05,450 --> 01:08:06,690
once you solve for alpha and w,

1105
01:08:06,750 --> 01:08:10,180
it's really easy to solve for b,

1106
01:08:10,240 --> 01:08:12,140
so that b gives us the interpretation of

1107
01:08:12,240 --> 01:08:14,710
[inaudible] training set,

1108
01:08:14,790 --> 01:08:17,420
and you found the direction for w.

1109
01:08:17,500 --> 01:08:18,660
So you know where

1110
01:08:18,780 --> 01:08:20,560
your separating hyperplane's direction is.

1111
01:08:20,640 --> 01:08:22,930
You know it's got to be one of these things.

1112
01:08:23,030 --> 01:08:26,970
And you know the orientation

1113
01:08:27,030 --> 01:08:28,060
and separating hyperplane.

1114
01:08:28,150 --> 01:08:29,060
You just have to decide

1115
01:08:29,130 --> 01:08:31,520
where to place this hyperplane.

1116
01:08:31,550 --> 01:08:32,750
And that's what solving b is.

1117
01:08:32,840 --> 01:08:34,700
So once you solve for alpha and w,

1118
01:08:34,760 --> 01:08:36,100
it's really easy to solve b.

1119
01:08:36,180 --> 01:08:38,660
You can plug alpha and w back into

1120
01:08:38,750 --> 01:08:50,680
the primal optimization problem and solve for b.

1121
01:08:50,760 --> 01:09:03,520
And I just wrote it down

1122
01:09:03,600 --> 01:09:06,810
for the sake of completeness, but

1123
01:09:06,900 --> 01:09:32,040
and the intuition behind this formula is just that

1124
01:09:32,140 --> 01:09:38,790
find the worst positive [inaudible]

1125
01:09:38,840 --> 01:09:40,200
and the worst negative example.

1126
01:09:40,390 --> 01:09:43,170
Let's say this one and this one say [inaudible]

1127
01:09:43,260 --> 01:09:44,370
and [inaudible] the difference between them.

1128
01:09:44,450 --> 01:09:45,570
And that tells you where

1129
01:09:45,650 --> 01:09:47,320
you should set the threshold for where

1130
01:09:47,420 --> 01:09:49,380
to place the separating hyperplane.

1131
01:09:49,480 --> 01:09:56,290
And then that's the this is the optimal margin classifier.

1132
01:09:56,370 --> 01:09:58,020
This is also called a support vector machine.

1133
01:09:58,130 --> 01:10:00,630
If you do not use one y [inaudible],

1134
01:10:00,720 --> 01:10:01,990
it's called kernels.

1135
01:10:02,090 --> 01:10:03,090
And I'll say a few words about that.

1136
01:10:03,170 --> 01:10:06,050
But I hope the process is clear.

1137
01:10:06,130 --> 01:10:07,180
It's a dual problem.

1138
01:10:07,250 --> 01:10:08,510
We're going to solve the dual problem

1139
01:10:08,590 --> 01:10:09,730
for the alpha i's.

1140
01:10:09,820 --> 01:10:12,130
That gives us w, and that gives us b.

1141
01:10:12,250 --> 01:10:17,400
So there's just one more thing

1142
01:10:17,490 --> 01:10:21,120
I wanna point out as I lead into the next lecture,

1143
01:10:21,210 --> 01:10:24,280
which is that I'll just write this out again,

1144
01:10:24,360 --> 01:10:30,290
I guess which is that it turns out

1145
01:10:30,370 --> 01:10:32,720
we can take the entire algorithm,

1146
01:10:32,820 --> 01:10:34,980
and we can express the entire algorithm

1147
01:10:35,070 --> 01:10:36,810
in terms of inner products.

1148
01:10:36,890 --> 01:10:38,180
And here's what I mean by that.

1149
01:10:38,290 --> 01:10:41,510
So say that the parameters w

1150
01:10:41,580 --> 01:10:43,870
is the sum of your input examples.

1151
01:10:43,980 --> 01:10:47,250
And so we need to make a prediction.

1152
01:10:47,350 --> 01:10:51,060
Someone gives you a new value of x.

1153
01:10:51,150 --> 01:10:52,240
You want a value of the hypothesis

1154
01:10:52,330 --> 01:10:53,450
on the value of x.

1155
01:10:53,540 --> 01:10:57,390
That's given by g of w transpose x plus b,

1156
01:10:57,490 --> 01:11:00,390
or where g was this threshold function that

1157
01:11:00,480 --> 01:11:01,320
outputs minus 1 or plus 1.

1158
01:11:01,400 --> 01:11:05,280
And so you need to compute w transpose x plus b.

1159
01:11:05,370 --> 01:11:12,790
And that is equal to alpha i, yi.

1160
01:11:12,890 --> 01:11:21,290
And that can be expressed as

1161
01:11:21,390 --> 01:11:23,620
a sum of these inner products between

1162
01:11:23,720 --> 01:11:27,020
your training examples and this new value

1163
01:11:27,110 --> 01:11:28,270
of x [inaudible] value [inaudible].

1164
01:11:28,350 --> 01:11:35,240
And this will lead into our next lecture,

1165
01:11:35,330 --> 01:11:36,630
which is the idea of kernels.

1166
01:11:36,730 --> 01:11:40,850
And it turns out that

1167
01:11:40,970 --> 01:11:43,160
in the source of feature spaces where

1168
01:11:43,240 --> 01:11:44,440
used to support vector machines

1169
01:11:44,530 --> 01:11:47,250
it turns out that sometimes your training examples

1170
01:11:47,340 --> 01:11:48,890
may be very high-dimensional.

1171
01:11:48,980 --> 01:11:55,530
It may even be the case that

1172
01:11:55,610 --> 01:11:57,620
the features that you want to use

1173
01:11:57,670 --> 01:12:00,850
are infinite-dimensional feature vectors.

1174
01:12:00,910 --> 01:12:06,830
But despite this, it'll turn out that

1175
01:12:06,920 --> 01:12:11,750
there'll be an interesting representation that

1176
01:12:11,860 --> 01:12:13,830
you can use that will allow you

1177
01:12:13,910 --> 01:12:17,060
to compute inner products like these efficiently.

1178
01:12:17,160 --> 01:12:27,870
And this holds true only for certain feature spaces.

1179
01:12:27,980 --> 01:12:29,830
It doesn't hold true for arbitrary sets of features.

1180
01:12:29,940 --> 01:12:32,450
But we talk about the idea of kernels.

1181
01:12:32,560 --> 01:12:34,610
In the next lecture,

1182
01:12:34,690 --> 01:12:38,180
we'll see examples where even though

1183
01:12:38,260 --> 01:12:39,780
you have extremely high-dimensional feature vectors,

1184
01:12:39,890 --> 01:12:40,710
you can compute

1185
01:12:40,800 --> 01:12:43,630
you may never want to represent xi,

1186
01:12:43,730 --> 01:12:45,440
explicitly because xi is

1187
01:12:45,450 --> 01:12:46,380
an infinite-dimensional feature vector.

1188
01:12:46,480 --> 01:12:47,910
You cannot even store in computer memory.

1189
01:12:48,020 --> 01:12:49,640
But you will nonetheless be able to

1190
01:12:49,730 --> 01:12:51,100
compute inner products between different

1191
01:12:51,200 --> 01:12:53,260
[inaudible] feature vectors very efficiently.

1192
01:12:53,370 --> 01:12:55,710
And so you can for example,

1193
01:12:55,800 --> 01:12:56,780
you can make predictions by

1194
01:12:56,860 --> 01:12:58,250
making use of these inner products.

1195
01:12:58,330 --> 01:13:05,130
This is just xi transpose. You will compute

1196
01:13:05,240 --> 01:13:06,950
these inner products very efficiently and,

1197
01:13:07,020 --> 01:13:08,500
therefore, make predictions.

1198
01:13:08,580 --> 01:13:10,080
And this pointed also

1199
01:13:10,180 --> 01:13:12,780
the other reason we derive the dual

1200
01:13:12,860 --> 01:13:15,330
was because on this board,

1201
01:13:15,430 --> 01:13:17,480
when we worked out what w of alpha is,

1202
01:13:17,570 --> 01:13:20,290
w of alpha actually are the same property

1203
01:13:20,380 --> 01:13:23,840
w of alpha is again written

1204
01:13:23,950 --> 01:13:25,200
in terms of these inner products.

1205
01:13:25,290 --> 01:13:30,410
And so if you actually look at

1206
01:13:30,500 --> 01:13:32,400
the dual optimization problem and step

1207
01:13:32,500 --> 01:13:33,750
for all the steps of the algorithm,

1208
01:13:33,860 --> 01:13:36,040
you'll find that you actually do everything you want

1209
01:13:36,130 --> 01:13:37,500
learn the parameters of alpha.

1210
01:13:37,590 --> 01:13:39,530
So suppose you do an optimization problem,

1211
01:13:39,620 --> 01:13:40,660
go into parameters alpha,

1212
01:13:40,740 --> 01:13:42,150
and you do everything you want without

1213
01:13:42,260 --> 01:13:45,170
ever needing to represent xi directly.

1214
01:13:45,260 --> 01:13:46,720
And all you need to do

1215
01:13:46,800 --> 01:13:48,370
is represent this compute inner products

1216
01:13:48,480 --> 01:13:49,610
with your feature vectors like these.

1217
01:13:49,730 --> 01:13:55,270
Well, one last property of this algorithm

1218
01:13:55,380 --> 01:13:59,970
that's kinda nice is that I said previously that

1219
01:14:00,090 --> 01:14:03,600
the alpha i's are 0 only for the

1220
01:14:03,690 --> 01:14:06,870
are non-0 only for the support vectors,

1221
01:14:06,980 --> 01:14:08,300
only for the vectors

1222
01:14:08,300 --> 01:14:09,890
that function y [inaudible] 1.

1223
01:14:09,970 --> 01:14:12,650
And in practice, there are usually fairly few of them.

1224
01:14:12,730 --> 01:14:15,090
And so what this means is that

1225
01:14:15,180 --> 01:14:16,630
if you're representing w this way,

1226
01:14:16,720 --> 01:14:19,760
then w when represented

1227
01:14:19,870 --> 01:14:22,240
as a fairly small fraction of training examples

1228
01:14:22,340 --> 01:14:24,900
because mostly alpha i's is 0

1229
01:14:25,010 --> 01:14:28,150
and so when you're summing up the sum,

1230
01:14:28,220 --> 01:14:30,390
you need to compute inner products

1231
01:14:30,500 --> 01:14:31,610
only if the support vectors,

1232
01:14:31,680 --> 01:14:34,530
which is usually a small fraction of your training set.

1233
01:14:34,630 --> 01:14:36,030
So that's another nice [inaudible]

1234
01:14:36,130 --> 01:14:37,710
because [inaudible] alpha is 0.

1235
01:14:37,810 --> 01:14:42,270
And well, much of this will make much more sense

1236
01:14:42,380 --> 01:14:43,580
when we talk about kernels.

1237
01:14:43,670 --> 01:14:48,280
[Inaudible] quick questions before I close? Yeah.

1238
01:14:48,360 --> 01:14:50,120
Student:It seems that for anything

1239
01:14:50,220 --> 01:14:51,090
we've done the work,

1240
01:14:51,180 --> 01:14:52,410
the point file has to be really well behaved,

1241
01:14:52,490 --> 01:14:53,990
and if any of the points

1242
01:14:53,990 --> 01:14:55,740
are kinda on the wrong side

1243
01:14:55,850 --> 01:14:57,880
Instructor (Andrew Ng):No, oh, yeah, so again,

1244
01:14:57,990 --> 01:14:59,550
for today's lecture asks you that

1245
01:14:59,640 --> 01:15:01,190
the data is linearly separable

1246
01:15:01,300 --> 01:15:02,960
that you can actually get perfect [inaudible].

1247
01:15:03,050 --> 01:15:05,880
I'll fix this in the next lecture as well.

1248
01:15:05,980 --> 01:15:10,290
But excellent assumption. Yes?

1249
01:15:10,380 --> 01:15:11,680
Student:So can't we assume that [inaudible]

1250
01:15:11,770 --> 01:15:17,870
point [inaudible], so [inaudible] have [inaudible]?

1251
01:15:17,970 --> 01:15:19,240
Instructor (Andrew Ng):Yes, so unless I

1252
01:15:19,330 --> 01:15:21,130
says that there are ways to generalize this

1253
01:15:21,240 --> 01:15:22,520
in multiple classes that

1254
01:15:22,620 --> 01:15:23,800
I probably won't [inaudible]

1255
01:15:23,890 --> 01:15:25,950
but yeah, that's generalization [inaudible].

1256
01:15:26,050 --> 01:15:27,860
Okay. Let's close for today, then.

1257
01:15:27,970 --> 01:15:29,650
We'll talk about kernels in our next lecture.

