1
00:00:24,050 --> 00:00:25,530
好的  欢迎回来

2
00:00:25,830 --> 00:00:29,180
今天我要讲

3
00:00:29,480 --> 00:00:32,340
一些新的测试算法

4
00:00:32,530 --> 00:00:34,940
用来进行模型拟合  像logistic回归那样

5
00:00:35,180 --> 00:00:38,040
之后我们会讲到指数分布族

6
00:00:38,260 --> 00:00:40,240
与广义线性模型

7
00:00:40,440 --> 00:00:43,260
这是一组非常漂亮的概念

8
00:00:43,460 --> 00:00:44,500
将logistic回归和

9
00:00:44,500 --> 00:00:45,720
传统的最小二乘模型联系在一起

10
00:00:45,990 --> 00:00:47,530
我们之后将会看到

11
00:00:47,710 --> 00:00:49,580
希望我今天能讲到那里

12
00:00:49,760 --> 00:00:52,530
通过前面几课

13
00:00:52,710 --> 00:00:53,410
与今天这节课

14
00:00:53,410 --> 00:00:54,540
我们已经开始越来越多地用到

15
00:00:54,730 --> 00:00:56,550
大量的和概率论有关的知识

16
00:00:56,780 --> 00:01:01,850
所以如果你需要重新复习一下概率论的基础

17
00:01:02,030 --> 00:01:04,370
如果你不是很确定

18
00:01:04,570 --> 00:01:05,990
你是否掌握了这门课所需要的

19
00:01:06,180 --> 00:01:07,900
概率统计的背景知识

20
00:01:08,140 --> 00:01:10,210
那么这周助教主持的习题课

21
00:01:10,430 --> 00:01:14,100
会带你们重温

22
00:01:14,300 --> 00:01:16,380
并复习概率论

23
00:01:16,610 --> 00:01:18,880
在助教上的同一节习题课中

24
00:01:19,090 --> 00:01:20,770
我们会简单地讲一些

25
00:01:20,990 --> 00:01:23,280
MATLAB和Octave的符号的相关知识

26
00:01:24,470 --> 00:01:26,690
这两个工具你们会在作业中用到

27
00:01:26,910 --> 00:01:29,110
所以如果你们想对概率统计

28
00:01:29,360 --> 00:01:31,170
进行一下回顾

29
00:01:31,350 --> 00:01:32,950
或者你们想要学习一下MATLAB和Octave的简单的教程

30
00:01:33,240 --> 00:01:36,650
请来上下一次习题课

31
00:01:39,200 --> 00:01:43,240
好的 简要地回顾一下

32
00:01:43,500 --> 00:01:46,980
在上节课的末尾

33
00:01:47,180 --> 00:01:49,900
我讲到了logistic回归模型

34
00:01:50,110 --> 00:01:53,010
这是一个分类算法

35
00:01:53,210 --> 00:01:55,620
我们有

36
00:01:55,800 --> 00:02:02,270
P(y=1|x;θ)

37
00:02:02,790 --> 00:02:05,440
在这个模型下  它应该等于

38
00:02:05,750 --> 00:02:08,360
1/(1+e^(-θ^T x) )

39
00:02:09,120 --> 00:02:13,370
之后你可以

40
00:02:13,590 --> 00:02:17,200
将对数似然率

41
00:02:30,250 --> 00:02:31,700
写成这样

42
00:02:32,040 --> 00:02:34,930
对这个式子求导

43
00:02:35,130 --> 00:02:39,010
你可以应用梯度上升规则

44
00:02:39,220 --> 00:02:41,510
来进行极大似然估计

45
00:02:41,750 --> 00:02:43,370
从而找到logistic回归模型

46
00:02:43,730 --> 00:02:46,310
中的参数θ

47
00:02:46,660 --> 00:02:49,480
记得上一次我们使用的是

48
00:02:49,690 --> 00:02:52,920
批梯度上升规则  但是这一次我们使用的是

49
00:02:53,220 --> 00:02:56,710
随机梯度上升规则

50
00:02:56,920 --> 00:02:58,830
一次更新只使用一个训练样本

51
00:02:59,080 --> 00:03:01,750
更新规则是这样的

52
00:03:06,950 --> 00:03:09,720
所以上一次我用的是批梯度上升

53
00:03:09,980 --> 00:03:11,650
这次用的是随机梯度上升

54
00:03:12,030 --> 00:03:13,520
如果你们想要拟合出

55
00:03:13,770 --> 00:03:15,590
一个logistic回归模型

56
00:03:16,790 --> 00:03:19,090
这就意味着你需要找到θ值

57
00:03:19,370 --> 00:03:21,240
使得对数似然率最大

58
00:03:21,490 --> 00:03:23,630
无论是梯度上升或者随机梯度上升

59
00:03:23,870 --> 00:03:25,070
还是批梯度上升

60
00:03:25,250 --> 00:03:27,280
都是很好的可以使用的算法

61
00:03:27,540 --> 00:03:30,100
但是今天我要讲的

62
00:03:30,330 --> 00:03:33,090
是一种不同的用来进行模型拟合的算法

63
00:03:33,330 --> 00:03:35,580
例如可以对logistic回归模型进行拟合

64
00:03:35,810 --> 00:03:38,580
我认为这个算法

65
00:03:39,550 --> 00:03:40,810
通常情况下运行速度

66
00:03:40,810 --> 00:03:42,360
会比梯度上升算法快很多

67
00:03:42,570 --> 00:03:46,400
这个算法被称为牛顿方法

68
00:03:47,560 --> 00:03:49,880
当我们描述牛顿方法时

69
00:03:50,080 --> 00:03:53,130
我想请你们

70
00:03:56,120 --> 00:03:59,470
首先考虑一个不一样的问题

71
00:04:01,770 --> 00:04:07,420
比如说-你有一个函数:f(θ)

72
00:04:08,860 --> 00:04:12,060
比如说你们想找到θ值

73
00:04:12,310 --> 00:04:21,570
使得f(θ)等于0

74
00:04:22,500 --> 00:04:24,880
让我们先来考虑这个问题

75
00:04:25,110 --> 00:04:27,640
之后我们会将这个问题慢慢演化成一个算法

76
00:04:27,890 --> 00:04:29,520
可以对极大似然估计的模型进行拟合

77
00:04:29,740 --> 00:04:32,620
像logistic regression那样

78
00:04:32,890 --> 00:04:43,350
我们看 我想这样就可以了

79
00:04:51,650 --> 00:04:55,030
比如说这是我的函数f

80
00:04:55,530 --> 00:04:58,840
这是表示θ的横轴

81
00:04:59,080 --> 00:05:01,710
所以我们要找的这里的θ值

82
00:05:01,910 --> 00:05:05,340
使得f(θ)等于0

83
00:05:05,930 --> 00:05:07,760
这里是水平轴

84
00:05:07,960 --> 00:05:10,060
我们使用的算法是这样的

85
00:05:10,460 --> 00:05:17,160
我们用某些值初始化θ

86
00:05:18,750 --> 00:05:21,870
记为θ^((0) )

87
00:05:22,260 --> 00:05:24,430
之后牛顿方法会这样运行

88
00:05:24,660 --> 00:05:27,150
我们在这一点上对f求值

89
00:05:27,410 --> 00:05:31,750
之后我们计算

90
00:05:31,940 --> 00:05:33,720
这一点上的导数值  之后

91
00:05:33,920 --> 00:05:35,420
我们会对这一点上的f值

92
00:05:35,560 --> 00:05:37,780
进行线性逼近

93
00:05:38,060 --> 00:05:46,800
所以  我会在这一点上画出函数的切线

94
00:05:47,080 --> 00:05:53,340
希望你们能够明白

95
00:05:53,540 --> 00:05:56,620
这样的函数切线会更容易画一些

96
00:05:56,870 --> 00:05:59,120
我对θ^((0))对应的函数上的这一点

97
00:05:59,340 --> 00:06:01,760
做切线  之后

98
00:06:01,990 --> 00:06:03,860
我会延长这条切线

99
00:06:04,120 --> 00:06:06,430
直到它与水平轴相交

100
00:06:06,660 --> 00:06:08,630
我想知道这个值是多少

101
00:06:08,840 --> 00:06:11,090
所以我将其表示为θ^((1) )

102
00:06:11,300 --> 00:06:14,360
所以这样就完成了

103
00:06:14,600 --> 00:06:16,120
牛顿方法的一次迭代

104
00:06:16,290 --> 00:06:17,970
之后我会做相同的事情

105
00:06:18,140 --> 00:06:22,900
对于这个点  延长切线到这里

106
00:06:23,080 --> 00:06:25,830
所以这样就完成了算法的两次迭代

107
00:06:26,030 --> 00:06:28,120
之后继续

108
00:06:28,300 --> 00:06:30,900
我们得到了θ^((3) )  后面依此类推

109
00:06:31,480 --> 00:06:34,370
所以  让我们继续将这个算法写出来

110
00:06:34,640 --> 00:06:38,190
看看它到底做了些什么

111
00:06:38,450 --> 00:06:41,180
从θ^((0) )到θ^((1) )

112
00:06:41,390 --> 00:06:46,420
我们将这段长度

113
00:06:46,620 --> 00:06:48,770
记为Δ

114
00:06:49,020 --> 00:06:52,620
如果你记得

115
00:06:53,180 --> 00:06:55,240
导数的定义

116
00:06:55,470 --> 00:06:58,350
那么f在θ^((0) )处的导数

117
00:06:59,050 --> 00:07:01,980
换句话说  第一条直线的斜率

118
00:07:02,240 --> 00:07:04,520
根据斜率的定义  应该等于

119
00:07:04,790 --> 00:07:06,560
这段垂直长度

120
00:07:06,770 --> 00:07:08,640
除以这段水平长度

121
00:07:08,880 --> 00:07:11,730
所以函数在这一点的斜率

122
00:07:11,920 --> 00:07:13,600
被定义为这个三角形的

123
00:07:13,830 --> 00:07:16,190
高度和宽度之比

124
00:07:16,930 --> 00:07:20,640
所以它等于

125
00:07:23,030 --> 00:07:25,090
(f(θ^((0) )))/Δ

126
00:07:25,390 --> 00:07:31,300
这样可以推出:Δ等于

127
00:07:32,440 --> 00:07:38,990
(f(θ^((0) )))/(f^' (θ^((0) ))) 明白吗?

128
00:07:44,590 --> 00:07:48,580
所以θ^((1) )应该等于

129
00:07:48,800 --> 00:07:51,980
θ^((0) )减去Δ

130
00:07:52,230 --> 00:07:55,240
也就是减去

131
00:07:56,020 --> 00:07:59,800
(f(θ^((0) )))/(f^' (θ^((0) )))   好的

132
00:08:00,540 --> 00:08:02,220
一般地

133
00:08:02,440 --> 00:08:05,250
对于牛顿方法的一次迭代

134
00:08:05,450 --> 00:08:10,860
θ^((t+1) ) 应该等于

135
00:08:11,060 --> 00:08:14,380
θ^((t) )-(f(θ^((t) )))/(f^' (θ^((t) )))

136
00:08:15,360 --> 00:08:18,080
这就是牛顿方法的一次迭代

137
00:08:24,580 --> 00:08:26,950
现在  这个算法可以找到一个θ值

138
00:08:27,140 --> 00:08:29,890
使得f(θ)=0

139
00:08:30,090 --> 00:08:32,010
所以我们采用相同的想法

140
00:08:32,180 --> 00:08:35,210
来使得对数似然率最大化

141
00:08:35,430 --> 00:08:37,800
我们有一个函数:l(θ)

142
00:08:38,500 --> 00:08:40,500
我们想要使这个函数最大化

143
00:08:40,700 --> 00:08:42,320
怎样使一个函数最大化?

144
00:08:42,490 --> 00:08:44,400
你可以令导数值等于0

145
00:08:44,600 --> 00:08:47,160
所以我们需要找到一个θ

146
00:08:48,060 --> 00:08:51,710
使得l’(θ)=0

147
00:08:52,060 --> 00:08:53,690
所以为了使这个函数最大化

148
00:08:53,860 --> 00:08:55,520
我们需要找到一个点

149
00:08:55,750 --> 00:08:57,840
使导函数的值为0

150
00:08:58,030 --> 00:09:00,410
所以我们采用相同的方法

151
00:09:02,020 --> 00:09:06,280
所以θ^((t+1) )应该等于:

152
00:09:06,490 --> 00:09:18,880
θ^((t) )-(l^' (θ^((t) )))/(l^'' (θ^((t) )))

153
00:09:20,080 --> 00:09:25,950
好的

154
00:09:26,120 --> 00:09:28,200
因为要使这个函数最大化

155
00:09:28,470 --> 00:09:30,550
我们令f等于l’

156
00:09:30,780 --> 00:09:32,830
令f等于l的一阶导数

157
00:09:33,090 --> 00:09:35,030
之后我们需要找到θ

158
00:09:35,190 --> 00:09:37,640
使得l’(θ)=0

159
00:09:37,840 --> 00:09:39,970
因此一定可以得到一个局部最优值

160
00:09:48,530 --> 00:09:51,660
明白吗?有问题吗?

161
00:09:51,910 --> 00:09:54,810
是不是对于任何函数算法都是有效

162
00:09:58,980 --> 00:10:01,020
这个问题的答案

163
00:10:01,230 --> 00:10:02,640
是相当复杂的

164
00:10:02,910 --> 00:10:04,960
为了使算法有效

165
00:10:05,170 --> 00:10:06,920
需要令f满足某些条件

166
00:10:07,120 --> 00:10:08,850
这些条件相当复杂

167
00:10:09,070 --> 00:10:10,910
它们比我现在讲的东西

168
00:10:11,130 --> 00:10:12,630
要更加复杂 实际上

169
00:10:12,840 --> 00:10:14,780
对于logistic回归来说  这个算法效果很好

170
00:10:15,020 --> 00:10:17,000
并且对之后

171
00:10:17,230 --> 00:10:19,180
我要讲的广义线性模型来说效果也不错

172
00:10:19,460 --> 00:10:22,700
(应该在问θ的初值会不会对结果有影响)

173
00:10:23,450 --> 00:10:24,940
是的  通常情况下没有影响

174
00:10:25,180 --> 00:10:26,620
我在实现的时候

175
00:10:26,820 --> 00:10:29,100
通常会将θ^((0) )初始化为0

176
00:10:29,290 --> 00:10:31,160
或者将参数初始化为0向量

177
00:10:31,440 --> 00:10:34,040
通常情况下算法会工作得很好

178
00:10:34,230 --> 00:10:36,010
通常情况下你如何初始化θ

179
00:10:36,180 --> 00:10:38,740
并不是一个大问题

180
00:10:39,980 --> 00:10:47,180
【听不清】

181
00:10:48,730 --> 00:10:51,290
(应该在问和算法收敛有关的问题)

182
00:10:51,600 --> 00:10:55,690
我来讲一些东西

183
00:10:55,910 --> 00:10:58,720
也许能回答你的问题

184
00:10:58,880 --> 00:11:00,980
所有这些算法通常不会特别考虑收敛性问题

185
00:11:01,170 --> 00:11:03,230
所有的这些算法

186
00:11:03,390 --> 00:11:05,850
通常情况下都会收敛

187
00:11:06,020 --> 00:11:07,600
除非你选择了一个非常大的

188
00:11:07,600 --> 00:11:09,180
线性斜率或一些其它的原因

189
00:11:09,370 --> 00:11:11,440
但是这些算法的收敛速度

190
00:11:11,610 --> 00:11:14,180
会有很大的差别

191
00:11:15,110 --> 00:11:20,170
事实证明牛顿方法

192
00:11:20,350 --> 00:11:22,990
是一个收敛速度非常快的算法

193
00:11:23,160 --> 00:11:26,440
它的收敛速度用术语可以描述为

194
00:11:26,640 --> 00:11:28,840
二次收敛

195
00:11:29,040 --> 00:11:31,680
你可能不知道这个术语的含义

196
00:11:31,850 --> 00:11:34,140
非正式地讲

197
00:11:34,340 --> 00:11:36,850
这意味着  牛顿方法的每一次迭代

198
00:11:37,090 --> 00:11:39,070
都会使你正在逼近的

199
00:11:39,210 --> 00:11:41,580
解的有效数字的数目加倍

200
00:11:41,850 --> 00:11:46,300
不考虑常量因子

201
00:11:46,480 --> 00:11:52,270
假设在一次迭代中

202
00:11:52,720 --> 00:11:54,470
你的解距离最优解还有0.01

203
00:11:55,090 --> 00:11:58,910
所以你的误差是0.01 在一次迭代之后

204
00:11:59,100 --> 00:12:01,910
你的误差的量级会变为0.001

205
00:12:04,480 --> 00:12:06,610
又一次迭代之后

206
00:12:06,810 --> 00:12:12,890
你的误差的量级会变为0.0000001

207
00:12:13,990 --> 00:12:16,550
所以这样的性质被称为二次收敛

208
00:12:16,760 --> 00:12:18,420
因为实际上牛顿方法的每次迭代之后

209
00:12:18,790 --> 00:12:21,270
误差会变为之前的平方

210
00:12:21,410 --> 00:12:24,290
这样的性质只会

211
00:12:24,480 --> 00:12:26,780
当你的解距离最优值足够近时才会出现

212
00:12:26,930 --> 00:12:30,010
这是理论上正确的结果

213
00:12:30,200 --> 00:12:32,210
但是由于常量因子或者一些其它的因素

214
00:12:32,420 --> 00:12:34,370
可能会使得减缓

215
00:12:34,610 --> 00:12:36,490
算法的收敛速度

216
00:12:36,660 --> 00:12:38,870
但是事实上

217
00:12:39,110 --> 00:12:41,260
当我实现牛顿方法时

218
00:12:41,450 --> 00:12:43,780
对于logistic回归来说

219
00:12:43,970 --> 00:12:45,890
通常会在十几次迭代之后收敛

220
00:12:46,040 --> 00:12:47,540
对于那些规模合理的

221
00:12:47,720 --> 00:12:49,800
有几十或上百个特征的问题来说是这样的

222
00:12:50,050 --> 00:12:52,560
我还要讲一件事情

223
00:12:52,810 --> 00:12:55,860
我刚才写的

224
00:12:56,000 --> 00:12:58,560
实际上是牛顿方法的一个特例

225
00:12:58,760 --> 00:13:01,200
这里的θ是只有一行的数字

226
00:13:01,410 --> 00:13:04,450
一般化的牛顿方法中

227
00:13:04,610 --> 00:13:06,580
θ是一个向量

228
00:13:06,790 --> 00:13:08,270
而不是一个单行的数字

229
00:13:08,270 --> 00:13:09,610
一般化的算法是这样的

230
00:13:10,990 --> 00:13:15,060
θ^((t+1) )等于θ^((t) )加上

231
00:13:15,340 --> 00:13:18,170
二阶导数

232
00:13:18,360 --> 00:13:20,670
除以一阶导数-对不起  是一阶导数

233
00:13:20,890 --> 00:13:23,490
除以二阶导数

234
00:13:25,550 --> 00:13:28,410
对应的一般化方法是这样的

235
00:13:28,610 --> 00:13:32,190
这是你的目标函数的梯度

236
00:13:33,810 --> 00:13:39,580
这里的H是一个矩阵

237
00:13:39,890 --> 00:13:48,940
被称为Hessian矩阵

238
00:13:51,680 --> 00:14:00,550
H_ij表示一个二阶导数  等于-好的

239
00:14:06,500 --> 00:14:12,190
这有点类似于用一阶导数

240
00:14:12,370 --> 00:14:14,610
除以二阶导数  所以

241
00:14:14,780 --> 00:14:19,300
你用一个表示一阶导数的向量

242
00:14:19,480 --> 00:14:22,150
乘上一个表示二阶导数的矩阵的逆

243
00:14:22,360 --> 00:14:24,530
这个算法做的基本上是同样的事情

244
00:14:24,770 --> 00:14:27,250
只是扩展到了多维的情形

245
00:14:27,510 --> 00:14:31,080
所以对于logistic回归  使用-

246
00:14:31,360 --> 00:14:34,770
如果特征的数目以及训练样本的数量比较合理

247
00:14:34,950 --> 00:14:37,810
当我运行这个算法

248
00:14:38,080 --> 00:14:41,100
通常情况下你会看到算法收敛

249
00:14:41,310 --> 00:14:43,580
一般情况下

250
00:14:43,740 --> 00:14:45,550
会执行十几次迭代

251
00:14:45,750 --> 00:14:48,000
和梯度上升比起来

252
00:14:48,170 --> 00:14:49,620
和梯度上升比起来

253
00:14:49,790 --> 00:14:51,710
算法收敛所需要的迭代次数要少得多

254
00:14:51,930 --> 00:14:54,310
和梯度上升比起来

255
00:14:54,480 --> 00:14:56,120
比如说:批梯度上升

256
00:14:56,330 --> 00:14:58,410
牛顿方法的缺点是

257
00:14:58,650 --> 00:14:59,910
每一次迭代  你都需要

258
00:14:59,910 --> 00:15:01,860
重新计算一次Hession矩阵的逆

259
00:15:02,140 --> 00:15:04,660
Hessian矩阵是一个n*n的矩阵

260
00:15:04,880 --> 00:15:06,930
或者是一个(n+1)*(n+1)的矩阵

261
00:15:07,100 --> 00:15:09,450
n是特征的数量

262
00:15:09,650 --> 00:15:11,670
如果你要处理的问题中有大量的特征

263
00:15:11,890 --> 00:15:14,000
比如说有几千个特征

264
00:15:14,180 --> 00:15:16,600
那么H^(-1)的计算

265
00:15:16,740 --> 00:15:18,690
将会花费很大的代价

266
00:15:19,010 --> 00:15:21,180
但是对于规模较小 特征数量合理的问题

267
00:15:21,390 --> 00:15:23,140
这通常情况下会是一个很快的算法

268
00:15:23,350 --> 00:15:24,960
有问题吗?

269
00:15:25,220 --> 00:15:27,560
(学生认为黑板上的公式写错了)

270
00:15:30,860 --> 00:15:33,400
我们看看

271
00:15:33,780 --> 00:15:35,680
我想你是对的

272
00:15:35,930 --> 00:15:38,730
这里应该是一个减号

273
00:15:39,190 --> 00:15:44,890
好的  谢谢

274
00:15:45,220 --> 00:15:47,590
是的  应该是个减号

275
00:15:47,900 --> 00:15:49,910
谢谢

276
00:15:50,140 --> 00:15:52,640
我写了这个算法

277
00:15:52,940 --> 00:15:54,860
利用极大似然估计

278
00:15:55,080 --> 00:15:56,710
来求logistic回归的参数

279
00:15:56,910 --> 00:15:58,930
我写了这个算法用来最大化函数的值

280
00:15:59,180 --> 00:16:01,440
所以我会把这个问题留给你们

281
00:16:01,660 --> 00:16:03,880
如果我想用牛顿方法去求函数的最小值

282
00:16:04,070 --> 00:16:06,230
算法应该有何变化?

283
00:16:06,470 --> 00:16:08,900
好的  所以我把这个问题留给你们

284
00:16:09,220 --> 00:16:11,750
换句话说  这并不是最大化

285
00:16:11,940 --> 00:16:13,460
算法应该怎样变化

286
00:16:14,140 --> 00:16:16,310
如果你想用它进行最小化的话?

287
00:16:28,500 --> 00:16:34,510
实际上  答案是没有变化

288
00:16:34,960 --> 00:16:37,930
我会让你们自己想  到底为什么 好的

289
00:16:39,610 --> 00:16:42,380
好的  让我们来讨论广义线性模型

290
00:16:42,640 --> 00:16:49,930
让我们回顾一下

291
00:16:50,140 --> 00:16:53,680
我们到现在为止讲过的两个算法

292
00:16:53,900 --> 00:16:56,380
我们到现在为止讲了两个不同的算法

293
00:16:56,570 --> 00:16:58,480
用来对P(y|x;θ)

294
00:16:58,680 --> 00:17:00,930
进行建模

295
00:17:01,130 --> 00:17:04,970
其中的一个-R代表实数

296
00:17:05,240 --> 00:17:08,050
我们假设

297
00:17:08,360 --> 00:17:10,300
y满足高斯分布

298
00:17:10,560 --> 00:17:12,080
之后我们得到了

299
00:17:12,080 --> 00:17:14,680
基于最小二乘法的线性回归

300
00:17:14,950 --> 00:17:19,120
另一种情况下  我们看到

301
00:17:19,340 --> 00:17:22,730
对于一个分类问题

302
00:17:22,950 --> 00:17:27,920
y取0或1 在这种情况下

303
00:17:28,190 --> 00:17:30,680
最为自然的0-1之间的分布

304
00:17:31,110 --> 00:17:33,030
是伯努利分布

305
00:17:33,360 --> 00:17:35,610
伯努利分布可以对只取两个值的

306
00:17:35,790 --> 00:17:37,900
随机变量进行建模

307
00:17:38,120 --> 00:17:41,640
在这种情况下我们得到了logistic回归

308
00:17:45,140 --> 00:17:48,070
所以一些问题随之产生

309
00:17:48,300 --> 00:17:50,970
对于logistic回归

310
00:17:51,640 --> 00:17:53,730
我到底是如何得到的这个

311
00:17:53,960 --> 00:17:56,280
sigmoid函数?有很多其它的选择

312
00:17:56,510 --> 00:17:59,030
那么  到底是从哪里得到的

313
00:17:59,290 --> 00:18:01,010
这个函数?

314
00:18:01,240 --> 00:18:03,280
我可以引入其它的函数

315
00:18:03,500 --> 00:18:05,400
但是事实证明  sigmoid函数

316
00:18:05,650 --> 00:18:07,980
是一个可以引出logistic回归的

317
00:18:08,220 --> 00:18:10,570
最为自然的默认选择 我现在要做的

318
00:18:10,790 --> 00:18:13,340
是以这两个算法为例

319
00:18:13,610 --> 00:18:17,740
说明它们是

320
00:18:17,980 --> 00:18:20,540
一类更广泛的算法的特例

321
00:18:20,800 --> 00:18:23,750
这类算法被称为广义线性模型

322
00:18:24,020 --> 00:18:27,850
如果将它们看成这类算法的特例

323
00:18:28,010 --> 00:18:29,980
那么sigmoid函数的形式

324
00:18:30,200 --> 00:18:32,410
会很自然地得到

325
00:18:32,640 --> 00:18:34,920
让我找一根长点的粉笔

326
00:18:35,230 --> 00:18:39,250
我需要告诫你们

327
00:18:39,460 --> 00:18:42,030
广义线性模型中涉及到的概念有些复杂

328
00:18:42,290 --> 00:18:44,910
所以我今天要做的

329
00:18:45,070 --> 00:18:47,190
是尝试把要点给你们指出来

330
00:18:47,400 --> 00:18:49,250
让你们对整体有个大概的了解

331
00:18:49,440 --> 00:18:51,360
一些细节和推导过程

332
00:18:51,580 --> 00:18:53,290
我会留给你们自己看

333
00:18:53,480 --> 00:18:55,880
这些材料

334
00:18:56,110 --> 00:18:58,440
已经发布在网上了

335
00:19:04,330 --> 00:19:07,680
接下来考虑这两个分布

336
00:19:08,400 --> 00:19:11,240
伯努利分布和高斯分布

337
00:19:11,480 --> 00:19:16,440
假设我们有一组只能取0或1的数据

338
00:19:16,970 --> 00:19:18,820
我们希望可以用一个

339
00:19:18,820 --> 00:19:21,070
伯努利随机变量对其建模

340
00:19:21,900 --> 00:19:24,340
变量以Φ为参数

341
00:19:24,600 --> 00:19:28,380
所以对于伯努利分布

342
00:19:28,680 --> 00:19:31,030
P(y=1;Φ)

343
00:19:31,440 --> 00:19:33,270
等于Φ  好的

344
00:19:33,520 --> 00:19:35,940
所以伯努利分布中的参数Φ

345
00:19:36,150 --> 00:19:38,390
指定了y=1的概率

346
00:19:38,700 --> 00:19:41,980
现在  当你改变参数θ的时候  你实际上

347
00:19:42,230 --> 00:19:45,190
得到了不同的伯努利分布

348
00:19:45,410 --> 00:19:48,400
当你改变Φ值的时候

349
00:19:48,730 --> 00:19:51,460
你得到了关于y的不同的概率分布

350
00:19:51,640 --> 00:19:53,380
它们对于y=1这一事件有不同的概率

351
00:19:53,570 --> 00:19:56,070
我希望你们

352
00:19:56,310 --> 00:19:58,650
不要将它看成一个固定的概率分布

353
00:19:58,870 --> 00:20:01,020
而是将其看成

354
00:20:01,260 --> 00:20:03,870
当你改变Φ值时得到的一类概率分布

355
00:20:04,110 --> 00:20:05,950
同理  考虑高斯分布

356
00:20:09,110 --> 00:20:11,570
当你改变μ的时候

357
00:20:11,780 --> 00:20:15,530
你会得到不同的高斯分布

358
00:20:15,810 --> 00:20:17,810
所以再一次地将其视为一类

359
00:20:18,030 --> 00:20:20,500
或者一组概率分布

360
00:20:20,680 --> 00:20:23,330
我现在要让你们明白

361
00:20:26,410 --> 00:20:30,690
这些分布都是一类分布的特例

362
00:20:30,890 --> 00:20:33,270
这类分布被称为指数分布族

363
00:20:33,490 --> 00:20:36,060
特别地

364
00:20:36,270 --> 00:20:38,410
我们说一类概率分布

365
00:20:38,600 --> 00:20:40,480
比如说你改变Φ值得到的伯努利分布

366
00:20:40,690 --> 00:20:42,690
我们说

367
00:20:42,890 --> 00:20:45,160
某些概率分布术语指数分布族

368
00:20:45,390 --> 00:20:48,270
如果它能被写成这样的形式

369
00:20:49,620 --> 00:20:51,370
P(y;η)等于

370
00:20:51,540 --> 00:21:00,800
b(y)*exp(η^T T(y)-a(η))  好的

371
00:21:04,520 --> 00:21:08,510
让我先给这些项起个名字

372
00:21:08,760 --> 00:21:14,540
关于它们的含义我会多讲一些

373
00:21:14,720 --> 00:21:21,190
所以η被称为

374
00:21:21,370 --> 00:21:24,570
分布的自然参数

375
00:21:24,770 --> 00:21:33,840
T(y)被称为充分统计量

376
00:21:35,050 --> 00:21:39,760
通常情况下  我们经常见到的许多例子

377
00:21:39,940 --> 00:21:43,280
包括伯努利分布和高斯分布

378
00:21:43,870 --> 00:21:47,270
T(y)=y 所以对于大多数的课

379
00:21:47,480 --> 00:21:50,050
你们可以在心里认为T(y)=y

380
00:21:50,280 --> 00:21:52,760
虽然对于我们今天要讲的一些例子来说

381
00:21:53,020 --> 00:21:55,810
不一定会使用这种形式

382
00:21:56,490 --> 00:21:59,630
但是你可以心里认为T(y)=y

383
00:22:07,080 --> 00:22:11,780
选定一种函数的形式

384
00:22:12,000 --> 00:22:18,070
对于a  b和T

385
00:22:18,290 --> 00:22:21,430
我们固定这三个函数

386
00:22:21,640 --> 00:22:25,750
那么这个公式就定义了一个概率分布的集合

387
00:22:25,950 --> 00:22:30,550
它以η为参数

388
00:22:30,790 --> 00:22:33,130
定义了一类概率分布

389
00:22:33,330 --> 00:22:36,260
对于一组给定的a b与T

390
00:22:36,450 --> 00:22:38,500
对于这样一组固定的组合

391
00:22:38,700 --> 00:22:40,690
当我改变η时

392
00:22:40,870 --> 00:22:43,180
我会得到一组不同的概率分布

393
00:22:43,420 --> 00:22:46,150
我接下来要展示的是

394
00:22:46,420 --> 00:22:50,970
我接下来要展示

395
00:22:51,150 --> 00:22:53,450
伯努利分布和高斯分布

396
00:22:53,640 --> 00:22:56,070
都是指数分布族的特例

397
00:22:56,270 --> 00:22:58,980
这意味着  我可以选取

398
00:22:59,160 --> 00:23:01,700
特定形式的a b T

399
00:23:01,850 --> 00:23:04,120
使这个公式变成

400
00:23:04,300 --> 00:23:06,930
伯努利或高斯分布的形式

401
00:23:07,140 --> 00:23:09,340
当我改变η时

402
00:23:09,510 --> 00:23:11,750
我会得到均值不同的伯努利分布

403
00:23:11,990 --> 00:23:14,540
或者说当我改变η时

404
00:23:14,800 --> 00:23:17,290
我会得到均值不同的高斯分布

405
00:23:17,460 --> 00:23:20,020
对于给定的a b T

406
00:23:22,490 --> 00:23:25,220
对于你们那些

407
00:23:25,370 --> 00:23:28,710
理解充分统计量的人

408
00:23:28,880 --> 00:23:31,310
正规意义上讲  T(y)实际上

409
00:23:31,490 --> 00:23:35,540
是一个概率分布的

410
00:23:35,730 --> 00:23:37,610
充分统计量

411
00:23:37,830 --> 00:23:40,140
统计学的课上可能会讲到这些

412
00:23:40,290 --> 00:23:42,240
如果你们不知道什么是充分统计量

413
00:23:42,440 --> 00:23:44,170
不要担心

414
00:23:44,360 --> 00:23:46,820
了解我们今天的内容不需要知道它

415
00:23:49,650 --> 00:23:56,950
好的 最后补充一点

416
00:23:57,210 --> 00:24:02,910
很多时候  T(y)=y  在很多情况下

417
00:24:03,140 --> 00:24:05,550
η也仅仅是个实数

418
00:24:05,840 --> 00:24:07,320
所以很多情况下

419
00:24:07,530 --> 00:24:08,890
这个概率的参数

420
00:24:09,100 --> 00:24:11,340
仅仅是一个实数  所以η*T(y)

421
00:24:11,490 --> 00:24:13,280
是实数的乘积

422
00:24:13,510 --> 00:24:15,130
对于我们的前两个例子

423
00:24:15,310 --> 00:24:16,530
是这样的

424
00:24:16,730 --> 00:24:18,920
但是今天我要将的最后一个例子是一个例外

425
00:24:19,160 --> 00:24:23,310
现在我要向你们展示

426
00:24:23,500 --> 00:24:25,350
伯努利分布与高斯分布

427
00:24:25,630 --> 00:24:27,170
是指数分布族的特例

428
00:24:27,420 --> 00:24:28,870
我们先从伯努利分布开始

429
00:24:29,090 --> 00:24:30,440
以Φ为参数的伯努利分布

430
00:24:30,660 --> 00:24:32,960
我记得我已经写过了

431
00:24:33,190 --> 00:24:35,670
P(y=1;Φ)

432
00:24:35,920 --> 00:24:37,410
等于Φ

433
00:24:37,550 --> 00:24:39,530
所以参数Φ指定了

434
00:24:39,800 --> 00:24:42,630
y=1的概率

435
00:24:42,860 --> 00:24:46,490
所以我的目标是选取T a b

436
00:24:46,690 --> 00:24:49,660
或者选取a b T

437
00:24:49,890 --> 00:24:51,980
使得指数分布族的公式

438
00:24:52,220 --> 00:24:54,760
等同于伯努利分布的公式

439
00:25:02,620 --> 00:25:10,400
P(y;Φ)

440
00:25:10,750 --> 00:25:19,390
等于这个 你们已经见过

441
00:25:19,660 --> 00:25:23,480
相似的指数符号了

442
00:25:23,730 --> 00:25:25,880
当我们讲logistic回归的时候

443
00:25:26,090 --> 00:25:28,430
y=1的概率等于Φ

444
00:25:28,650 --> 00:25:31,280
y=0的概率等于1-Φ

445
00:25:31,480 --> 00:25:32,920
所以我们可以把这个公式统一写成

446
00:25:33,130 --> 00:25:35,410
Φ^y (1-Φ)^(1-y)

447
00:25:35,630 --> 00:25:42,100
我们将其对数作为指数

448
00:25:42,340 --> 00:25:44,290
先取对数  再作为指数

449
00:25:44,530 --> 00:25:46,890
二者抵消  所以结果没有变化

450
00:25:47,130 --> 00:25:54,740
它应该等于这个式子

451
00:26:31,290 --> 00:26:35,630
这一项是η  这一项是T(y)

452
00:26:37,160 --> 00:26:44,410
这一项是 –a(η)

453
00:26:45,720 --> 00:26:47,630
b(y)=1

454
00:26:47,910 --> 00:26:51,260
所以可以不考虑它

455
00:26:56,110 --> 00:26:59,570
花一点时间回顾一下

456
00:26:59,790 --> 00:27:01,660
确保你们弄明白了

457
00:27:01,850 --> 00:27:05,160
当你们回顾的时候我要擦一下黑板

458
00:27:43,120 --> 00:27:45,580
现在我们再来写一些东西

459
00:27:47,490 --> 00:27:49,210
从之前的黑板上抄过来

460
00:27:49,460 --> 00:27:53,150
η应该等于

461
00:27:53,420 --> 00:27:56,970
log Φ/(1-Φ)

462
00:27:57,190 --> 00:27:58,320
事实上  如果我希望对

463
00:27:58,320 --> 00:27:59,340
这个式子进行代数变换

464
00:27:59,580 --> 00:28:02,070
对于这个公式  如果你将其倒过来

465
00:28:02,300 --> 00:28:05,190
如果你对Φ求解-对不起  如果

466
00:28:05,490 --> 00:28:08,060
你对η求解

467
00:28:08,240 --> 00:28:10,280
Φ视为函数

468
00:28:10,530 --> 00:28:13,610
将这个公式倒过来  你会得到

469
00:28:13,870 --> 00:28:18,270
Φ=1/(1+e^(-η) )

470
00:28:18,480 --> 00:28:21,640
所以通过这个公式

471
00:28:21,910 --> 00:28:23,810
我们奇迹般地得到了logistic函数

472
00:28:24,110 --> 00:28:26,200
我们之后会更深入的讨论这个问题

473
00:28:26,440 --> 00:28:29,560
再一次从之前的黑板上抄过来

474
00:28:29,830 --> 00:28:32,690
a(η)等于

475
00:28:32,890 --> 00:28:39,980
-log(1-Φ)

476
00:28:40,320 --> 00:28:43,300
所以Φ和η互为彼此的函数

477
00:28:43,550 --> 00:28:45,420
η依赖于Φ

478
00:28:45,650 --> 00:28:47,430
同时Φ也依赖于η

479
00:28:47,670 --> 00:28:51,140
所以如果我将η的定义

480
00:28:51,380 --> 00:28:53,330
代入到这里-对不起  将Φ的定义

481
00:28:53,580 --> 00:28:55,840
代入到这里  那么会得到

482
00:28:56,070 --> 00:28:58,270
a(η)=log?(1+e^η)

483
00:28:58,560 --> 00:29:00,550
【听不清】

484
00:29:00,750 --> 00:29:02,270
这是通过一些很容易的代数变换得到的

485
00:29:02,270 --> 00:29:02,910
没什么值得注意的

486
00:29:03,170 --> 00:29:05,420
最后-对不起

487
00:29:08,240 --> 00:29:10,800
最后为了保持完整性

488
00:29:11,120 --> 00:29:13,210
T(y)=y

489
00:29:13,410 --> 00:29:17,410
b(y)=1  好的

490
00:29:18,610 --> 00:29:21,160
回顾一下我们做了些什么

491
00:29:21,400 --> 00:29:24,210
我们得到了一组函数

492
00:29:24,480 --> 00:29:27,440
a T b

493
00:29:27,670 --> 00:29:29,740
之后指数分布族的公式

494
00:29:30,050 --> 00:29:31,950
就变成了

495
00:29:32,220 --> 00:29:34,620
伯努利分布的公式

496
00:29:34,860 --> 00:29:37,300
或者说变成了伯努利分布的概率质量函数

497
00:29:37,510 --> 00:29:39,350
自然参数η

498
00:29:39,580 --> 00:29:41,810
与伯努利分布的原始参数之间

499
00:29:42,030 --> 00:29:45,050
存在特定的关系 有问题吗?

500
00:29:45,340 --> 00:29:47,940
(在问公式的推导过程)

501
00:29:48,610 --> 00:29:50,280
让我看看 哪一个?

502
00:29:50,490 --> 00:29:53,110
倒数第二个

503
00:29:54,540 --> 00:29:58,130
哦  结果没问题

504
00:29:58,410 --> 00:30:01,020
好的

505
00:30:01,510 --> 00:30:03,900
让我看看 是的

506
00:30:04,130 --> 00:30:06,240
如果你将这一项展开

507
00:30:06,500 --> 00:30:08,240
(1-y)*log(1-Φ)

508
00:30:08,420 --> 00:30:11,980
1乘以log(1-Φ)变成了这一项

509
00:30:12,650 --> 00:30:15,280
另外一项是-y* log(1-Φ)

510
00:30:15,580 --> 00:30:19,810
对数的相反数

511
00:30:20,110 --> 00:30:22,510
可以将负号转换为里面这一项的倒数

512
00:30:22,730 --> 00:30:26,460
所以-y* log(1-Φ)

513
00:30:26,720 --> 00:30:29,900
变成了

514
00:30:30,150 --> 00:30:32,990
y*log?( 1/(1-Φ))  明白了吗?

515
00:30:33,310 --> 00:30:34,980
明白了

516
00:30:35,290 --> 00:30:38,200
很好

517
00:30:38,690 --> 00:30:40,450
还有其他问题吗?什么问题?

518
00:30:41,030 --> 00:30:45,110
η是个标量吗?它的上面

519
00:30:45,300 --> 00:30:46,980
是的

520
00:30:47,200 --> 00:30:49,440
它的上面有个转秩符号


521
00:30:49,680 --> 00:30:50,950
它可以是一个矩阵  或者-

522
00:30:51,210 --> 00:30:52,380
是的

523
00:30:52,600 --> 00:30:54,170
在这个例子和下一个例子中

524
00:30:54,380 --> 00:30:56,330
η是一个标量

525
00:30:56,440 --> 00:30:58,750
在这块黑板上

526
00:30:58,950 --> 00:31:01,390
如果η是一个标量

527
00:31:01,560 --> 00:31:04,840
并且T(y)也是一个标量

528
00:31:05,110 --> 00:31:07,140
那么这一项就是一个实数乘以一个实数

529
00:31:07,320 --> 00:31:09,250
所以这将是一个一元矩阵的转秩

530
00:31:09,460 --> 00:31:11,380
乘以一个一维向量

531
00:31:11,570 --> 00:31:14,120
这一项就是一个实数乘以一个实数

532
00:31:14,300 --> 00:31:16,330
今天的课的最后

533
00:31:16,530 --> 00:31:18,240
我们会讲一个例子

534
00:31:18,420 --> 00:31:19,700
其中这两项都是向量

535
00:31:19,910 --> 00:31:21,460
但是对于很多分布而言

536
00:31:21,630 --> 00:31:23,490
这些都是标量

537
00:31:23,640 --> 00:31:27,340
【听不清】

538
00:31:27,560 --> 00:31:29,580
我的意思是 这里没有0的可能性

539
00:31:29,900 --> 00:31:33,820
或者0和1

540
00:31:40,740 --> 00:31:44,340
我看看  好的

541
00:31:44,610 --> 00:31:47,700
我们想象一下

542
00:31:47,980 --> 00:31:53,390
我们将输入域限制为

543
00:31:53,560 --> 00:31:55,650
y=0或1

544
00:31:57,050 --> 00:31:58,930
可以将其看成

545
00:31:59,560 --> 00:32:01,660
一个隐含的限制 我可以详细地写出来

546
00:32:01,920 --> 00:32:03,630
但是这是一个概率质量函数

547
00:32:03,860 --> 00:32:06,200
只处理y=1或y=0

548
00:32:06,430 --> 00:32:08,120
所以应该认为y=1或0

549
00:32:08,320 --> 00:32:11,090
我们将其看成隐含的限制

550
00:32:12,330 --> 00:32:16,610
很好 我们以伯努利分布为例

551
00:32:17,510 --> 00:32:22,630
将其写成了

552
00:32:22,850 --> 00:32:25,490
指数分布族的形式 让我快速地

553
00:32:25,700 --> 00:32:28,110
将高斯分布也写成这样的形式

554
00:32:28,370 --> 00:32:30,740
我不需要对高斯分布进行代数变换

555
00:32:30,890 --> 00:32:33,290
我仅仅把答案写出来

556
00:32:33,540 --> 00:32:36,600
对于一个以μ为均值

557
00:32:36,600 --> 00:32:39,940
以σ^2为方差的正态分布


558
00:32:40,110 --> 00:32:43,070
你们记得

559
00:32:43,350 --> 00:32:47,910
两节课之前  当我们

560
00:32:48,150 --> 00:32:51,060
推导极大似然性-对不起

561
00:32:51,220 --> 00:32:53,680
不对  是在上一课

562
00:32:53,910 --> 00:32:56,360
当我们利用极大似然估计

563
00:32:56,570 --> 00:32:59,570
推导最小二乘的参数时

564
00:32:59,810 --> 00:33:02,200
我们发现

565
00:33:02,470 --> 00:33:04,480
参数σ^2并不重要

566
00:33:04,690 --> 00:33:06,690
当我们推导最小二乘回归

567
00:33:06,850 --> 00:33:08,510
的概率模型时  我们说

568
00:33:08,660 --> 00:33:10,140
不管σ^2是什么

569
00:33:10,310 --> 00:33:12,310
我们都会得到同样的参数

570
00:33:12,460 --> 00:33:15,320
出于今天上课的目的

571
00:33:15,500 --> 00:33:18,030
我们不考虑

572
00:33:18,210 --> 00:33:19,610
σ^2

573
00:33:19,800 --> 00:33:23,070
我仅仅将σ^2设为1

574
00:33:23,250 --> 00:33:25,520
好的  所以我们不要管它

575
00:33:25,640 --> 00:33:28,220
讲义上关于这点讲的比较多

576
00:33:28,340 --> 00:33:31,230
但是在课堂上

577
00:33:31,450 --> 00:33:34,040
出于简洁的考虑

578
00:33:34,220 --> 00:33:35,940
我们仅仅将σ^2设成1

579
00:33:36,200 --> 00:33:38,070
σ^2 实际上

580
00:33:38,310 --> 00:33:41,820
仅仅是变量y的比例因子

581
00:33:44,010 --> 00:33:46,870
这种情况下高斯密度函数是这样的

582
00:33:47,080 --> 00:33:56,420
接下来

583
00:33:59,510 --> 00:34:02,710
经过几步代数推导  我这里就不写了

584
00:34:02,980 --> 00:34:06,130
详细过程写在讲义上

585
00:34:06,360 --> 00:34:10,080
你们可以自行下载

586
00:34:10,240 --> 00:34:12,800
等于

587
00:34:13,140 --> 00:34:17,560
1/√2π  exp?(-1/2 y^2 )

588
00:34:17,730 --> 00:34:22,050
exp?(μy- 1/2 μ^2)

589
00:34:22,380 --> 00:34:24,870
好的  我省略了代数推导

590
00:34:25,170 --> 00:34:34,400
所以  这是b(y)

591
00:34:34,620 --> 00:34:40,450
μ=η  T(y)=y

592
00:34:40,650 --> 00:34:46,910
a(η)等于

593
00:34:47,180 --> 00:34:54,200
负1/2  实际上

594
00:34:54,470 --> 00:34:59,840
我认为应该是正1/2 这样对吗?

595
00:35:00,130 --> 00:35:02,410
对不起

596
00:35:02,620 --> 00:35:04,830
这里是正号  好的

597
00:35:05,070 --> 00:35:07,650
如果你减去-1/2 μ^2

598
00:35:07,910 --> 00:35:10,370
由于μ=η

599
00:35:10,620 --> 00:35:13,450
所以这里是-1/2 η^2  好的

600
00:35:13,800 --> 00:35:17,980
所以这样的一组a b t

601
00:35:18,200 --> 00:35:22,380
就将高斯密度函数

602
00:35:22,570 --> 00:35:25,630
表示成了指数分布族的形式

603
00:35:25,850 --> 00:35:28,580
在这个例子中

604
00:35:28,790 --> 00:35:31,240
μ和η间的关系是

605
00:35:31,560 --> 00:35:33,440
μ=η

606
00:35:33,600 --> 00:35:35,300
所以高斯分布的均值

607
00:35:35,490 --> 00:35:37,430
恰好等于

608
00:35:37,660 --> 00:35:39,280
指数分布族的自然参数

609
00:35:39,460 --> 00:35:40,980
应该是减去1/2

610
00:35:41,210 --> 00:35:43,200
这里是减去1/2?

611
00:35:43,390 --> 00:35:45,120
【听不清】

612
00:35:45,340 --> 00:35:46,950
好的  谢谢

613
00:35:47,290 --> 00:35:50,150
我猜这里应该是正号了

614
00:35:50,420 --> 00:35:52,530
这样对吗?好的  你是对的

615
00:35:52,740 --> 00:35:55,470
谢谢  好的

616
00:36:02,700 --> 00:36:06,640
实际上

617
00:36:06,780 --> 00:36:08,880
如果你们上过本科生的统计课

618
00:36:09,070 --> 00:36:11,970
实际上书上的很多分布

619
00:36:12,160 --> 00:36:14,440
不是全部  但是其中的大部分

620
00:36:14,630 --> 00:36:17,050
都可以写成指数分布族的形式

621
00:36:17,230 --> 00:36:18,440
你们已经看到了高斯分布

622
00:36:18,440 --> 00:36:19,470
或者说正态分布是这样的

623
00:36:19,660 --> 00:36:21,430
实际上多元正态分布

624
00:36:21,650 --> 00:36:23,740
随机变量是一种

625
00:36:23,960 --> 00:36:25,890
一般化的高斯随机变量

626
00:36:26,090 --> 00:36:28,090
变量是一个多维向量

627
00:36:28,230 --> 00:36:30,240
多元正态分布

628
00:36:30,380 --> 00:36:32,420
也属于指数分布族

629
00:36:32,470 --> 00:36:34,710
你们看到  伯努利分布属于指数分布族

630
00:36:34,880 --> 00:36:37,190
实际上多项式分布也属于指数分布族

631
00:36:37,410 --> 00:36:39,590
好的 所以伯努利分布用来对

632
00:36:39,730 --> 00:36:40,900
0 1问题进行建模

633
00:36:41,130 --> 00:36:43,360
就像掷硬币那样  只能有两个结果

634
00:36:43,470 --> 00:36:44,840
而多项式分布用来对可能

635
00:36:44,840 --> 00:36:46,330
有K个结果的事件进行建模

636
00:36:46,540 --> 00:36:48,920
它同样属于指数分布族

637
00:36:49,090 --> 00:36:51,720
你们也许听说过泊松分布

638
00:36:51,940 --> 00:36:53,610
泊松分布通常

639
00:36:53,660 --> 00:36:55,750
用来对计数的过程进行建模

640
00:36:55,940 --> 00:36:58,370
例如一个样本中

641
00:36:58,560 --> 00:37:00,280
放射性衰变的数目

642
00:37:00,510 --> 00:37:02,630
或者你的网站的访客数量

643
00:37:02,860 --> 00:37:04,890
或者是商店的顾客数量

644
00:37:05,040 --> 00:37:06,630
泊松分布

645
00:37:06,880 --> 00:37:08,550
同样属于指数分布族

646
00:37:08,720 --> 00:37:11,070
伽马分布和指数分布也属于指数分布族

647
00:37:11,240 --> 00:37:13,260
如果你们听说过的话

648
00:37:13,450 --> 00:37:15,660
伽马分布和指数分布

649
00:37:15,810 --> 00:37:17,820
考虑的是正数的分布

650
00:37:17,930 --> 00:37:20,180
所以它们经常被用来对间隔进行建模

651
00:37:20,410 --> 00:37:22,520
比如说你在公车站等车

652
00:37:22,670 --> 00:37:24,110
你可能会问

653
00:37:24,290 --> 00:37:26,340
"下一辆车可能什么时候到?"

654
00:37:26,500 --> 00:37:28,400
"我的车到达还要多少时间?"

655
00:37:28,620 --> 00:37:31,140
通常情况下你用伽马分布

656
00:37:31,340 --> 00:37:34,040
或指数分布族

657
00:37:34,120 --> 00:37:36,310
哦  对不起  是指数分布进行建模

658
00:37:36,470 --> 00:37:38,650
它们同样属于指数分布族

659
00:37:38,770 --> 00:37:40,290
还有更多的分布

660
00:37:40,460 --> 00:37:42,610
例如:β分布和Dirichlet分布

661
00:37:42,800 --> 00:37:44,950
它们是用来对小数进行建模的

662
00:37:45,120 --> 00:37:47,220
是对概率分布进行建模的

663
00:37:47,330 --> 00:37:48,940
概率分布

664
00:37:49,060 --> 00:37:52,270
还有一些分布例如:Wishart分布

665
00:37:52,390 --> 00:37:54,620
是协方差矩阵的分布

666
00:37:54,730 --> 00:37:57,110
事实上  所有这些分布

667
00:37:57,230 --> 00:37:59,120
都可以写成指数分布族的形式

668
00:37:59,200 --> 00:38:05,660
作业中

669
00:38:05,790 --> 00:38:08,520
有的题目会要求你选择这些概率

670
00:38:08,760 --> 00:38:10,720
分布中的一个  将其写成

671
00:38:10,900 --> 00:38:12,920
指数分布族的形式

672
00:38:13,080 --> 00:38:15,280
并且推导出它的广义线性模型  好的

673
00:38:15,420 --> 00:38:17,260
接下来是本节课的下一个主题:

674
00:38:29,800 --> 00:38:32,710
选定了指数分布族

675
00:38:32,890 --> 00:38:35,330
应该怎样用它

676
00:38:35,500 --> 00:38:37,950
来推导出一个广义线性模型?

677
00:38:38,130 --> 00:38:43,110
广义线性模型

678
00:38:43,350 --> 00:38:49,320
通常被简写为GLM

679
00:38:49,440 --> 00:38:51,500
我接下来要写出三个假设

680
00:38:51,620 --> 00:38:53,820
你可以将它们看成假设

681
00:38:54,050 --> 00:38:56,270
或者也可以将它们看成是设计决策

682
00:38:56,480 --> 00:38:58,600
这可以使我

683
00:38:58,830 --> 00:39:01,460
生成广义线性模型

684
00:39:01,630 --> 00:39:04,010
第一个假设是-我会假设

685
00:39:04,180 --> 00:39:09,000
给定输入x和参数θ

686
00:39:09,140 --> 00:39:12,720
我会假设变量y

687
00:39:12,940 --> 00:39:16,750
输出y  或者说我正在试图预测的响应变量y

688
00:39:16,940 --> 00:39:22,070
属于指数分布族

689
00:39:23,960 --> 00:39:26,220
以η作为自然参数

690
00:39:26,460 --> 00:39:29,060
这意味着

691
00:39:29,210 --> 00:39:32,130
我们可以选取一些函数  a b T

692
00:39:32,400 --> 00:39:35,710
使得y在给定x  以θ为参数

693
00:39:36,180 --> 00:39:39,030
下的条件概率分布

694
00:39:39,230 --> 00:39:42,520
属于指数分布族  并以η为参数

695
00:39:42,740 --> 00:39:44,170
这里

696
00:39:44,370 --> 00:39:47,030
η可能以某种方式依赖于x

697
00:39:47,160 --> 00:39:49,540
例如  如果你尝试预测

698
00:39:49,690 --> 00:39:52,010
如果你尝试预测

699
00:39:52,180 --> 00:39:54,420
你的网站会有多少访客

700
00:39:54,650 --> 00:39:56,620
你可以选择对人数

701
00:39:56,810 --> 00:39:59,060
或者对你网站的点击量进行建模

702
00:39:59,200 --> 00:40:01,390
通过泊松分布

703
00:40:01,530 --> 00:40:03,790
因为泊松分布天生适合对计数问题进行建模

704
00:40:04,000 --> 00:40:06,590
所以你可以将这里的指数分布族

705
00:40:06,770 --> 00:40:08,640
指定为泊松分布

706
00:40:14,170 --> 00:40:21,630
假设给定x  我们的目标是

707
00:40:24,210 --> 00:40:28,970
输出E[y|x]

708
00:40:29,210 --> 00:40:32,630
所以给定一些网站样本中的特征

709
00:40:32,760 --> 00:40:35,840
我会得到一组特征

710
00:40:36,090 --> 00:40:38,280
例如是否占有一定的比率

711
00:40:38,430 --> 00:40:40,210
是否售出商品

712
00:40:40,330 --> 00:40:42,170
有多少人连接到你的网站上

713
00:40:42,310 --> 00:40:44,680
或一些其它的特征 我会假设

714
00:40:44,810 --> 00:40:46,860
我们的学习问题的目标是估计

715
00:40:46,980 --> 00:40:49,870
在某一天访问我们网站

716
00:40:50,040 --> 00:40:51,760
的人数的期望

717
00:40:52,000 --> 00:40:54,470
换句话说

718
00:40:54,650 --> 00:40:58,950
我应该让h(x)等于  对不起

719
00:40:59,080 --> 00:41:01,980
这里应该是T(y)

720
00:41:02,320 --> 00:41:07,930
我的目标是

721
00:41:08,150 --> 00:41:10,720
让我的学习算法的假设输出

722
00:41:10,910 --> 00:41:12,780
E[T(y)|x]

723
00:41:12,960 --> 00:41:15,200
再强调一次  很多例子中

724
00:41:15,360 --> 00:41:17,190
T(y)=y

725
00:41:17,380 --> 00:41:19,070
所以对于大多数例子

726
00:41:19,280 --> 00:41:21,400
我们的目标是让我们的学习算法

727
00:41:21,670 --> 00:41:23,730
输出E[y|x]

728
00:41:23,910 --> 00:41:26,900
因为T(y)通常情况下等于通常情况下等于y 什么?

729
00:41:30,160 --> 00:41:32,550
听不清

730
00:41:33,060 --> 00:41:35,710
是的  是一样的

731
00:41:36,390 --> 00:41:39,120
T(y)就是充分统计量 同样的T(y)

732
00:41:39,580 --> 00:41:43,100
我写的最后一条

733
00:41:43,310 --> 00:41:46,410
这些是假设  最后一条

734
00:41:46,550 --> 00:41:48,700
你们可以认为它是一条设计决策

735
00:41:48,940 --> 00:41:52,090
这里我假设y|x;θ的概率分布

736
00:41:52,290 --> 00:41:55,350
属于指数分布族

737
00:41:55,570 --> 00:41:57,330
以η为参数

738
00:41:57,530 --> 00:41:59,890
所以网站某天的访客数量

739
00:42:00,040 --> 00:42:02,560
将会满足以某些η为参数的泊松分布

740
00:42:02,760 --> 00:42:05,490
最后我需要做的决策是

741
00:42:05,630 --> 00:42:08,210
我的输入特征

742
00:42:08,510 --> 00:42:11,930
与参数η之间有何关系?

743
00:42:12,100 --> 00:42:14,760
其中我们的概率分布以η为参数

744
00:42:15,000 --> 00:42:17,710
最后一步

745
00:42:17,890 --> 00:42:21,150
我要做一个假设  或者一个设计决策

746
00:42:22,790 --> 00:42:25,410
我要假设

747
00:42:25,560 --> 00:42:27,820
η和输入特征之间的关系是线性的

748
00:42:27,990 --> 00:42:30,170
特别的  它们之间的关系由θ确定

749
00:42:30,330 --> 00:42:32,530
等于θ^T x

750
00:42:32,730 --> 00:42:35,080
我之所以做出这个设计决策

751
00:42:35,240 --> 00:42:37,370
是因为它会帮我们导出

752
00:42:37,530 --> 00:42:39,790
广义线性模型

753
00:42:39,980 --> 00:42:42,150
并且得到非常漂亮的算法

754
00:42:42,290 --> 00:42:46,510
用来拟合模型  例如:泊松回归模型

755
00:42:46,820 --> 00:42:49,810
或者对gama分布

756
00:42:50,090 --> 00:42:53,540
或者指数分布的输出结果进行回归

757
00:42:53,770 --> 00:43:00,310
让我们看一个例子

758
00:43:17,380 --> 00:43:20,970
η=θ^T x

759
00:43:21,200 --> 00:43:23,750
仅当η是实数时才是有意义的

760
00:43:23,980 --> 00:43:27,260
对于更为一般化的情形

761
00:43:27,450 --> 00:43:30,930
你需要令η_i=θ_i^T x

762
00:43:35,310 --> 00:43:37,250
如果η是一个向量  而不是一个实数

763
00:43:37,450 --> 00:43:39,930
但是大多数例子中

764
00:43:40,140 --> 00:43:43,380
η是一个实数

765
00:43:43,750 --> 00:43:56,120
所以让我们看看伯努利分布的例子

766
00:44:00,600 --> 00:44:03,500
你们可以看到

767
00:44:03,740 --> 00:44:07,000
y|x;θ

768
00:44:07,250 --> 00:44:10,540
属于指数分布族

769
00:44:10,780 --> 00:44:13,220
以η为自然参数

770
00:44:13,440 --> 00:44:16,450
对于伯努利分布

771
00:44:16,700 --> 00:44:20,040
我要选取特定形式的a b T

772
00:44:20,220 --> 00:44:23,530
似的这些指数分布族

773
00:44:23,760 --> 00:44:25,670
变成伯努利分布的形式

774
00:44:25,940 --> 00:44:29,310
这个例子刚才我们已经见过了

775
00:44:30,070 --> 00:44:33,080
就是我们刚才看的第一个例子

776
00:44:33,390 --> 00:44:38,010
所以对于任何给定的x和θ

777
00:44:41,740 --> 00:44:45,800
我的假设

778
00:44:46,180 --> 00:44:52,350
我的学习算法会进行一次预测

779
00:44:53,320 --> 00:44:56,830
会输出h_θ (x)

780
00:44:57,130 --> 00:45:03,150
它等于

781
00:45:03,490 --> 00:45:06,850
根据假设2

782
00:45:07,050 --> 00:45:09,050
我们的学习算法

783
00:45:09,300 --> 00:45:11,490
会输出E[y|x;θ]

784
00:45:11,750 --> 00:45:14,090
y的值

785
00:45:14,480 --> 00:45:17,960
只能取0或1

786
00:45:18,210 --> 00:45:21,410
所以y的期望值就等于

787
00:45:21,630 --> 00:45:24,930
y=1的概率

788
00:45:25,160 --> 00:45:28,540
所以伯努利随机变量的期望值

789
00:45:28,760 --> 00:45:30,790
恰好等于值为1的概率

790
00:45:36,540 --> 00:45:40,100
所以y=1的概率

791
00:45:40,450 --> 00:45:44,400
恰好等于Φ

792
00:45:45,060 --> 00:45:47,870
因为它是我们的伯努利分布的参数

793
00:45:48,050 --> 00:45:50,390
根据定义  Φ是伯努利分布下

794
00:45:50,620 --> 00:45:53,090
随机变量取值为1的概率

795
00:45:57,370 --> 00:45:59,330
根据我们之前的结论

796
00:45:59,660 --> 00:46:02,000
Φ=1/(1+e^(-η) )

797
00:46:02,260 --> 00:46:04,620
在之前的黑板上

798
00:46:04,850 --> 00:46:06,980
我们推出过这个结论 关系是-

799
00:46:07,170 --> 00:46:09,120
当我们把伯努利分布

800
00:46:09,310 --> 00:46:12,150
写成指数分布族的形式时

801
00:46:12,310 --> 00:46:14,650
我们得到了Φ与η的关系

802
00:46:14,880 --> 00:46:17,290
这就是最终的关系式

803
00:46:17,490 --> 00:46:19,970
所以我们最后得到了

804
00:46:20,270 --> 00:46:22,230
y的期望值

805
00:46:22,430 --> 00:46:24,260
与η的关系

806
00:46:24,420 --> 00:46:27,730
最后  由于我们的设计决策

807
00:46:27,970 --> 00:46:31,030
η和θ是线性相关的

808
00:46:31,210 --> 00:46:35,580
所以最后它等于:

809
00:46:36,750 --> 00:46:40,690
1/(1+e^(-θ^T x) )

810
00:46:42,590 --> 00:46:47,240
以上就是我得到

811
00:46:47,440 --> 00:46:52,580
logistic回归算法的过程

812
00:46:52,800 --> 00:46:55,110
此时你有一个变量y  也可以叫目标变量

813
00:46:55,300 --> 00:46:57,940
或者响应变量

814
00:46:58,160 --> 00:47:00,480
只会取两个值  然后你选择

815
00:47:00,640 --> 00:47:02,860
用伯努利分布对其进行建模

816
00:47:07,820 --> 00:47:09,820
你们明白了吗?

817
00:47:10,010 --> 00:47:12,740
明白请举手

818
00:47:12,980 --> 00:47:15,010
很好

819
00:47:15,220 --> 00:47:18,140
我希望你们能够明白这样做的便利性

820
00:47:18,300 --> 00:47:20,720
或者说这样做的强大之处 我做的唯一决策

821
00:47:20,900 --> 00:47:23,860
对于y  比如说

822
00:47:24,030 --> 00:47:26,340
我有一个新的机器学习问题

823
00:47:26,490 --> 00:47:28,800
并且我尝试预测一个变量的值y

824
00:47:28,940 --> 00:47:31,970
y恰好只能取两个值

825
00:47:32,280 --> 00:47:34,530
我需要做的唯一决策是:

826
00:47:34,720 --> 00:47:36,300
我选择使用伯努利分布

827
00:47:36,490 --> 00:47:38,470
我的意思是  我需要假设

828
00:47:38,640 --> 00:47:41,140
给定x和θ  我需要假设y

829
00:47:41,390 --> 00:47:43,020
服从伯努利分布

830
00:47:43,250 --> 00:47:45,080
这是我需要做的唯一决策

831
00:47:45,240 --> 00:47:46,900
之后所有的一切工作都是自动进行的

832
00:47:47,110 --> 00:47:50,600
在假设y|x;θ

833
00:47:50,810 --> 00:47:53,640
服从伯努利分布之后

834
00:47:54,720 --> 00:47:56,490
同理  你可以选择

835
00:47:56,650 --> 00:47:58,700
不同的分布  你可以选择

836
00:47:58,890 --> 00:47:59,740
令y服从泊松分布

837
00:47:59,740 --> 00:48:01,020
或伽马分布或其它任何分布

838
00:48:01,090 --> 00:48:03,050
并在之后遵从相似的过程

839
00:48:03,210 --> 00:48:05,400
并得到一个不同的模型和不同的学习算法

840
00:48:05,610 --> 00:48:07,680
得到一个不同的广义线性模型

841
00:48:07,850 --> 00:48:10,620
不管你面临任何问题

842
00:48:11,590 --> 00:48:16,780
这个符号  函数g

843
00:48:17,010 --> 00:48:22,620
g(η)将自然参数η

844
00:48:22,790 --> 00:48:26,970
与y的期望值联系起来

845
00:48:27,150 --> 00:48:32,740
在这个例子中  它等于:

846
00:48:32,830 --> 00:48:35,050
1/(1+e^(-η) )   这个函数被称为

847
00:48:35,150 --> 00:48:42,610
正则响应函数

848
00:48:42,870 --> 00:48:46,660
而g^(-1)被称为正则关联函数

849
00:48:53,770 --> 00:48:56,230
这里没什么东西好讲

850
00:48:56,380 --> 00:48:59,230
我不会过多使用这两个术语 只是提一下

851
00:48:59,390 --> 00:49:01,680
确保当你们听到有人在谈论

852
00:49:01,810 --> 00:49:04,040
广义线性模型时  如果他们谈到

853
00:49:04,190 --> 00:49:06,390
正则响应函数

854
00:49:06,560 --> 00:49:07,630
正则关联函数时

855
00:49:07,630 --> 00:49:09,520
你们能够知道这两个概念是什么意思

856
00:49:09,670 --> 00:49:12,880
实际上  很多技术人员都是反着用的

857
00:49:13,050 --> 00:49:15,180
这是g^(-1)  这是g

858
00:49:15,450 --> 00:49:17,590
但是事实证明  这样的符号表示法

859
00:49:17,820 --> 00:49:20,010
更能和其他的机器学习算法保持一致

860
00:49:20,220 --> 00:49:23,400
所以我会使用这样的符号表示

861
00:49:23,590 --> 00:49:25,790
但是我可能不会在课上过多地使用

862
00:49:25,970 --> 00:49:28,120
正则响应函数和

863
00:49:28,320 --> 00:49:30,620
正则关联函数这两个属于  所以-

864
00:49:30,800 --> 00:49:33,170
我不知道 我并不太擅长

865
00:49:33,300 --> 00:49:35,590
记住很多事情的名字 我仅仅是把它写出来

866
00:49:35,780 --> 00:49:38,200
以防你们在别处看到它们

867
00:49:44,110 --> 00:49:52,850
好的 为了节省时间

868
00:49:53,070 --> 00:49:56,070
我要跳过高斯分布的例子

869
00:49:56,340 --> 00:49:58,510
正如我说过的  我选择了伯努利分布

870
00:49:58,660 --> 00:50:00,930
y服从伯努利分布

871
00:50:01,180 --> 00:50:03,260
最终我得到了logistic回归

872
00:50:03,410 --> 00:50:05,630
你们可以选择高斯分布进行同样的工作

873
00:50:05,830 --> 00:50:08,300
并且得到一般的最小二乘模型

874
00:50:08,480 --> 00:50:11,540
高斯分布的问题是

875
00:50:11,760 --> 00:50:14,750
它是如此的简单  当你第一次看到它时

876
00:50:14,920 --> 00:50:16,860
它的性质和伯努利分布比起来

877
00:50:16,990 --> 00:50:17,830
有时会令人困惑

878
00:50:17,830 --> 00:50:18,950
因为它看起来是如此的简单

879
00:50:19,160 --> 00:50:20,920
看起来它有必要变得更复杂一些

880
00:50:21,130 --> 00:50:23,850
所以让我们跳过这个例子

881
00:50:24,070 --> 00:50:25,090
你们课后自己阅读

882
00:50:25,090 --> 00:50:26,630
将以上关于高斯分布的例子

883
00:50:26,750 --> 00:50:29,130
我接下来

884
00:50:29,370 --> 00:50:31,490
要讲一个更为复杂的例子 有问题吗?

885
00:50:31,690 --> 00:50:33,150
【听不清】

886
00:50:33,340 --> 00:50:35,410
好的

887
00:50:35,640 --> 00:50:37,900
应该怎样选择θ?

888
00:50:38,050 --> 00:50:41,740
我们快结束的时候会来讨论

889
00:50:41,930 --> 00:50:44,730
这里你有一个logistic回归模型

890
00:50:44,900 --> 00:50:47,000
这是一个概率模型

891
00:50:47,250 --> 00:50:50,730
假设在给定x时y的概率

892
00:50:50,960 --> 00:50:53,060
是以一种特定的形式给出的

893
00:50:53,230 --> 00:50:56,170
所以你要做的是

894
00:50:56,380 --> 00:50:58,590
写出你的训练集合的对数似然性

895
00:50:58,700 --> 00:51:01,210
之后找到θ使得

896
00:51:01,360 --> 00:51:03,320
参数的对数似然性最大

897
00:51:03,620 --> 00:51:05,770
明白了吗?

898
00:51:05,950 --> 00:51:08,250
在今天的课快结束时

899
00:51:08,490 --> 00:51:10,020
我会再讲到

900
00:51:10,160 --> 00:51:12,430
但是对于logistic回归

901
00:51:12,580 --> 00:51:14,590
选择θ的方式就是极大似然性

902
00:51:14,700 --> 00:51:16,400
正如我们上一课讲到的那样

903
00:51:16,520 --> 00:51:19,120
使用牛顿方法或者梯度上升

904
00:51:19,250 --> 00:51:23,080
在下个例子中

905
00:51:23,270 --> 00:51:25,350
我会再做一次

906
00:51:25,530 --> 00:51:28,500
在今天课的末尾处

907
00:51:28,670 --> 00:51:33,000
我希望在这节课剩下的

908
00:51:33,140 --> 00:51:36,140
我不知道  大概19分钟里

909
00:51:36,220 --> 00:51:39,140
讲一个更加复杂-

910
00:51:39,310 --> 00:51:42,300
甚至有可能是最复杂的

911
00:51:42,430 --> 00:51:44,480
广义线性模型的例子

912
00:51:44,650 --> 00:51:47,030
我希望讲这个例子  因为

913
00:51:47,180 --> 00:51:50,560
它比书上的很多其他例子

914
00:51:50,790 --> 00:51:53,780
更加具有技巧性

915
00:51:53,980 --> 00:51:58,740
所以我会

916
00:51:58,900 --> 00:52:01,840
跳过一些推导过程

917
00:52:02,020 --> 00:52:03,310
让你们对整体过程有一个了解

918
00:52:03,310 --> 00:52:04,260
如果有一些步骤我跳过了

919
00:52:04,420 --> 00:52:06,610
或者省略了  希望你们在课后

920
00:52:06,780 --> 00:52:09,870
能够认真地阅读讲义

921
00:52:10,120 --> 00:52:14,730
我要讲的例子是多项式分布

922
00:52:17,010 --> 00:52:26,300
多项式分布式在k可能取值上的分布

923
00:52:26,890 --> 00:52:30,510
想象一下

924
00:52:30,640 --> 00:52:33,140
你们现在在处理一个机器学习问题

925
00:52:33,320 --> 00:52:36,070
其中你们需要预测的y值可以取k个值

926
00:52:36,220 --> 00:52:38,100
而不仅仅是2个值

927
00:52:38,240 --> 00:52:40,080
很显然  我们之前提过这样的例子

928
00:52:40,190 --> 00:52:42,750
如果你想设计一个学习算法

929
00:52:42,920 --> 00:52:45,180
自动地将发送给你的邮件

930
00:52:45,340 --> 00:52:47,210
保存在正确的邮件目录下  如果你有十几个

931
00:52:47,400 --> 00:52:49,900
邮件目录  你希望你的算法可以

932
00:52:50,160 --> 00:52:52,020
将邮件分入这些目录中

933
00:52:52,200 --> 00:52:54,800
或者预测病人生病

934
00:52:55,050 --> 00:52:57,570
或者没有生病

935
00:52:57,770 --> 00:52:59,760
此时这个问题是一个二元分类问题

936
00:52:59,950 --> 00:53:01,930
如果你认为病人可能

937
00:53:02,120 --> 00:53:04,090
患k种并  并且你希望

938
00:53:04,260 --> 00:53:06,020
设计一个学习算法来帮你确定

939
00:53:06,170 --> 00:53:07,850
病人到底得了哪种病

940
00:53:08,000 --> 00:53:09,690
有很多多类的分类问题

941
00:53:09,820 --> 00:53:11,590
其中种类数要超过两类

942
00:53:11,790 --> 00:53:13,660
此时你需要用多项式分布进行建模

943
00:53:13,790 --> 00:53:18,720
对于logistic回归

944
00:53:18,880 --> 00:53:21,560
我有这样的一张图

945
00:53:21,640 --> 00:53:23,450
对于这个训练集合  你需要

946
00:53:23,540 --> 00:53:26,220
决定一条边界将两类数据分开

947
00:53:26,340 --> 00:53:28,350
对于我们现在讨论的问题

948
00:53:28,480 --> 00:53:30,680
我们再加入一类数据

949
00:53:30,800 --> 00:53:33,860
所以这时我们有了三类数据

950
00:53:34,010 --> 00:53:37,290
学习算法将以某种方式学习

951
00:53:37,490 --> 00:53:39,570
将三类或更多类的数据分开

952
00:53:39,720 --> 00:53:41,940
而不仅仅是两类

953
00:53:44,090 --> 00:53:46,550
所以让我们将多项式分布

954
00:53:47,050 --> 00:53:49,930
写成指数分布族的形式

955
00:53:50,200 --> 00:53:55,710
多项式分布的参数是Φ_1  Φ_2

956
00:53:58,150 --> 00:54:00,860
一直到Φ_k

957
00:54:01,570 --> 00:54:04,720
我要改变函数的定义-

958
00:54:04,890 --> 00:54:07,260
P(y=i)=Φ_i

959
00:54:07,590 --> 00:54:10,700
因为有k个可能的结果

960
00:54:11,040 --> 00:54:13,010
如果我用这样的方式

961
00:54:13,010 --> 00:54:15,430
对多项式分布进行参数化

962
00:54:15,560 --> 00:54:17,780
那么这些参数实际上是冗余的

963
00:54:18,000 --> 00:54:20,730
因为如果这些参数表示的是概率

964
00:54:20,920 --> 00:54:23,370
那么这些参数之和应该等于1

965
00:54:23,590 --> 00:54:25,980
因此

966
00:54:26,500 --> 00:54:29,020
我可以导出最后一个参数Φ_k

967
00:54:29,130 --> 00:54:33,440
Φ_k=1-(Φ_1+?+Φ_(k-1))

968
00:54:33,900 --> 00:54:36,980
所以这样的参数化方法

969
00:54:37,140 --> 00:54:40,020
对于多项式分布来说是冗余的

970
00:54:40,150 --> 00:54:42,320
结果被过度参数化了

971
00:54:44,130 --> 00:54:49,800
由于这个推导公式的存在

972
00:54:49,980 --> 00:54:52,750
我会将多项式分布的参数设为

973
00:54:52,890 --> 00:54:56,880
Φ_1  Φ_2…Φ_(k-1)

974
00:54:57,280 --> 00:55:00,490
我不将Φ_k视为参数

975
00:55:00,710 --> 00:55:03,350
所以

976
00:55:03,520 --> 00:55:05,720
我只定义了k-1个参数

977
00:55:05,790 --> 00:55:07,750
用来对多项式分布进行参数化

978
00:55:07,900 --> 00:55:11,400
有的时候我的推导中也会出现Φ_k

979
00:55:11,620 --> 00:55:14,620
你们应该将Φ_k

980
00:55:14,720 --> 00:55:16,720
视为这个式子的简略形式

981
00:55:16,860 --> 00:55:20,630
1减去剩下的这些参数  明白吗?

982
00:55:37,340 --> 00:55:39,960
实际上  多项式分布

983
00:55:40,110 --> 00:55:42,300
是T(y)不等于y的

984
00:55:42,500 --> 00:55:44,730
几个例子之一

985
00:55:45,190 --> 00:55:49,780
这个例子中  y会取k个可能的值

986
00:55:50,020 --> 00:55:54,270
T(y)会如下定义:

987
00:55:54,670 --> 00:55:57,970
T(1)是一个向量  其中有一个1

988
00:55:58,230 --> 00:56:01,000
其他位置都是0

989
00:56:02,200 --> 00:56:08,160
T(2)是这样的 依此类推

990
00:56:08,390 --> 00:56:10,740
这些向量

991
00:56:10,910 --> 00:56:14,070
都是k-1维的向量

992
00:56:14,280 --> 00:56:19,340
所以T(k-1)是这样的

993
00:56:19,690 --> 00:56:24,530
T(k)

994
00:56:24,730 --> 00:56:28,960
是一个0向量

995
00:56:29,680 --> 00:56:32,350
这是我定义T(y)的方式

996
00:56:32,580 --> 00:56:36,650
定义T(y)是为了将多项式分布写成

997
00:56:36,890 --> 00:56:40,900
指数分布族的形式

998
00:56:41,090 --> 00:56:44,060
这些向量都是k-1维的

999
00:56:44,300 --> 00:56:48,780
这里可以引出一个

1000
00:56:49,060 --> 00:56:51,910
有用的符号  称之为

1001
00:56:52,050 --> 00:56:54,870
指示函数

1002
00:56:55,010 --> 00:56:59,060
我会写一个1  之后是一个大括号

1003
00:56:59,210 --> 00:57:01,140
如果括号内是一个真命题

1004
00:57:01,240 --> 00:57:04,300
那么这个命题的指示函数值为1

1005
00:57:04,480 --> 00:57:06,620
我写一个1  之后在里面写一个假命题

1006
00:57:06,750 --> 00:57:09,770
那么指示函数的值

1007
00:57:10,060 --> 00:57:12,510
为0

1008
00:57:12,650 --> 00:57:16,070
例如  如果我写1

1009
00:57:16,250 --> 00:57:18,490
很显然这个命题是假命题

1010
00:57:18,720 --> 00:57:20,980
所以它应该等于0

1011
00:57:21,050 --> 00:57:23,890
1

1012
00:57:24,090 --> 00:57:27,170
我在里面写了个真命题

1013
00:57:27,260 --> 00:57:29,660
所以函数值

1014
00:57:29,760 --> 00:57:32,020
等于1 所以指示函数

1015
00:57:32,160 --> 00:57:34,640
是一个非常有用的符号  用来指示

1016
00:57:34,750 --> 00:57:37,850
括号中命题的真假

1017
00:57:39,050 --> 00:57:46,460
让我们写在这里

1018
00:57:46,680 --> 00:57:49,500
把这些连在一起

1019
00:57:49,810 --> 00:57:57,770
这里开一块地方

1020
00:57:57,980 --> 00:58:06,440
T(y)是一个向量 y取k值中的一个

1021
00:58:06,580 --> 00:58:10,320
所以T(y)是这k个向量中的一个

1022
00:58:10,460 --> 00:58:13,820
如果我用T(y)_i表示

1023
00:58:13,970 --> 00:58:16,840
向量T(y)的第i个元素

1024
00:58:17,130 --> 00:58:21,650
那么向量T(y)的第i个元素

1025
00:58:21,950 --> 00:58:27,240
就等于1

1026
00:58:27,360 --> 00:58:32,320
让我擦几块黑板

1027
00:58:32,500 --> 00:58:34,870
看一会儿这些符号

1028
00:58:35,060 --> 00:58:38,070
保证你明白

1029
00:58:38,200 --> 00:58:41,060
所有的符号以及为什么这个公式是正确的

1030
00:59:10,420 --> 00:59:12,670
好的 如果看明白这个公式

1031
00:59:12,790 --> 00:59:15,170
请举一下手

1032
00:59:15,380 --> 00:59:18,380
大多数人  不是全部  好的

1033
00:59:18,570 --> 00:59:20,790
作为一个例子

1034
00:59:21,010 --> 00:59:29,760
假设y=1

1035
00:59:30,490 --> 00:59:35,910
假设y=1

1036
00:59:36,100 --> 00:59:38,930
所以T(y)等于这个向量

1037
00:59:39,140 --> 00:59:43,630
因此这个向量的第一个元素

1038
00:59:43,840 --> 00:59:46,990
是1

1039
00:59:47,200 --> 00:59:50,160
其他的元素是0

1040
00:59:50,520 --> 00:59:53,360
所以 对不起  我们再来一遍

1041
00:59:53,540 --> 00:59:56,020
假设我想看一下

1042
00:59:56,160 --> 00:59:57,990
向量T(y)的第i个元素

1043
00:59:58,150 --> 01:00:00,800
我想知道它是0还是1  对吗?

1044
01:00:00,910 --> 01:00:02,870
它应该等于1

1045
01:00:03,020 --> 01:00:05,390
向量T(y)的第i个元素等于1

1046
01:00:05,520 --> 01:00:08,750
当且仅当y=i

1047
01:00:08,940 --> 01:00:11,190
例如  如果y=1

1048
01:00:11,430 --> 01:00:13,790
所以只有向量的第一个元素会是1

1049
01:00:13,970 --> 01:00:15,940
如果y=2

1050
01:00:16,080 --> 01:00:17,990
那么只有向量的第2个元素会是1

1051
01:00:18,150 --> 01:00:20,190
依此类推

1052
01:00:20,300 --> 01:00:22,680
所以向量的第i个元素

1053
01:00:22,810 --> 01:00:25,010
是否为1

1054
01:00:25,180 --> 01:00:29,450
取决于y是否等于i

1055
01:00:32,010 --> 01:00:35,100
好的 如果你们仍然不是

1056
01:00:35,100 --> 01:00:38,190
很明白为什么它是正确的

1057
01:00:38,450 --> 01:00:40,800
回家好好想一想

1058
01:00:40,960 --> 01:00:43,290
回去看一下讲义

1059
01:00:43,460 --> 01:00:46,140
也许会有帮助

1060
01:00:46,280 --> 01:00:49,320
现在至少先记住我说的话

1061
01:00:49,530 --> 01:00:54,030
让我们继续

1062
01:00:54,190 --> 01:00:57,060
将多项式分布写成指数分布族的形式

1063
01:00:57,180 --> 01:01:07,020
所以P(y)等于Φ_1^1

1064
01:01:07,220 --> 01:01:10,660
乘以Φ_2^1

1065
01:01:10,920 --> 01:01:17,730
一直乘到Φ_k^1

1066
01:01:17,810 --> 01:01:21,090
再次强调  Φ_k不是参数

1067
01:01:21,240 --> 01:01:25,110
Φ_k是

1068
01:01:25,230 --> 01:01:27,700
1-(Φ_1+Φ_2+?+Φ_(k-1)) 的简写形式

1069
01:01:27,810 --> 01:01:33,230
将左边的公式代入

1070
01:01:33,390 --> 01:01:36,800
可以将其写成

1071
01:01:36,890 --> 01:01:41,490
Φ_1^(T(y)_1 ) Φ_2^(T(y)_2 )…Φ_(k-1)^(T(y)_(k-1)

1072
01:01:41,630 --> 01:01:49,430
Φ_k^(1-∑_(j=1)^(k-1)?〖T(y)_j 〗)

1073
01:01:49,640 --> 01:02:01,790
这里应该是j

1074
01:02:06,100 --> 01:02:12,080
实际上

1075
01:02:12,200 --> 01:02:15,880
还需要几步代数推导  我没有时间列出来了

1076
01:02:16,060 --> 01:02:20,760
实际上  你可以将其简化成这样的形式

1077
01:02:20,880 --> 01:02:24,180
其中

1078
01:02:31,490 --> 01:02:42,190
η是一个向量

1079
01:02:53,640 --> 01:03:00,780
是一个k-1维的向量  好的

1080
01:03:03,100 --> 01:03:07,720
推出这些需要几步代数推导

1081
01:03:08,980 --> 01:03:13,270
你们可以回去自己证  我这里就不写了

1082
01:03:13,360 --> 01:03:17,250
结合之前对于T(y)的定义

1083
01:03:17,870 --> 01:03:21,850
通过按这种方式选取η a b

1084
01:03:22,050 --> 01:03:23,940
我将概率分布

1085
01:03:24,030 --> 01:03:25,880
从多项式分布的形式

1086
01:03:26,040 --> 01:03:28,860
转化成了指数分布族的形式

1087
01:03:29,060 --> 01:03:35,650
实际上  我们看看

1088
01:03:36,980 --> 01:03:40,760
我们将η

1089
01:03:40,900 --> 01:03:43,830
表示成了Φ的函数

1090
01:03:43,920 --> 01:03:46,980
之后我们反过来将Φ写成η的函数

1091
01:03:47,120 --> 01:03:50,170
实际上你也可以这样做

1092
01:03:50,350 --> 01:03:53,410
这里将η定义成了

1093
01:03:53,530 --> 01:03:56,300
多项式分布的参数Φ的函数

1094
01:03:56,670 --> 01:03:58,680
所以你可以

1095
01:03:58,810 --> 01:04:01,280
将η和Φ之间的关系倒过来

1096
01:04:01,380 --> 01:04:05,070
并将Φ写成η的函数

1097
01:04:05,180 --> 01:04:09,420
实际上  你最终可以得到Φ_i等于

1098
01:04:10,080 --> 01:04:17,660
对不起 所以你可以得到Φ_i等于

1099
01:04:17,780 --> 01:04:26,970
这个式子

1100
01:04:33,080 --> 01:04:37,830
你可以推出这个式子-

1101
01:04:38,000 --> 01:04:41,210
这里将η定义为Φ的函数

1102
01:04:41,420 --> 01:04:43,540
求解这个式子

1103
01:04:43,790 --> 01:04:45,870
最后得到了这个结果

1104
01:04:46,040 --> 01:04:47,660
其中需要进行一些数学推导

1105
01:04:47,840 --> 01:04:49,320
我已经省略掉了

1106
01:04:49,510 --> 01:04:52,100
最后应用我们的假设

1107
01:04:52,230 --> 01:04:55,740
η是输入变量x的线性函数

1108
01:04:55,850 --> 01:04:59,510
因此  Φ_i等于

1109
01:04:59,610 --> 01:05:02,060
e^(θ_i^T x)/(1+∑_(j=1)^(k-1)?e^(θ_j^T x) )

1110
01:05:02,160 --> 01:05:09,990
e^(θ_i^T x)/(1+∑_(j=1)

1111
01:05:10,210 --> 01:05:18,550
^(k-1)?e^(θ_j^T x) )

1112
01:05:18,900 --> 01:05:25,750
这里应用到了这个事实:η_i=θ_i^T x

1113
01:05:26,820 --> 01:05:29,490
这是之前我们的

1114
01:05:29,620 --> 01:05:31,590
广义线性模型的设计决策

1115
01:05:43,710 --> 01:05:46,250
我们基本快讲完了

1116
01:05:46,520 --> 01:05:52,150
所以我们的学习算法h_θ (x)

1117
01:05:52,320 --> 01:05:56,720
我认为它等于

1118
01:05:56,890 --> 01:06:01,400
E[T(y)|x;θ]

1119
01:06:02,640 --> 01:06:07,810
T(y)是一个指示函数的向量

1120
01:06:07,960 --> 01:06:11,390
T(1)=I

1121
01:06:11,590 --> 01:06:17,000
一直到y=k-1

1122
01:06:19,770 --> 01:06:21,480
好的 所以我希望我的学习

1123
01:06:21,480 --> 01:06:22,900
算法可以输出这个式子

1124
01:06:23,100 --> 01:06:26,590
输出这个

1125
01:06:26,750 --> 01:06:28,990
指示函数的向量的期望值

1126
01:06:31,670 --> 01:06:38,220
1的期望值

1127
01:06:38,370 --> 01:06:42,650
就是y=1的概率

1128
01:06:42,780 --> 01:06:45,090
也就是Φ_1

1129
01:06:45,250 --> 01:06:47,730
这是一个随机变量  当y=1时

1130
01:06:47,850 --> 01:06:50,300
值为1  否则值为0

1131
01:06:50,450 --> 01:06:52,760
所以1的

1132
01:06:52,880 --> 01:06:54,910
期望值恰好等于

1133
01:06:55,050 --> 01:06:57,280
y=1的概率

1134
01:06:57,410 --> 01:06:59,740
也就是Φ_1

1135
01:06:59,910 --> 01:07:03,970
因此  根据之前的结论

1136
01:07:04,070 --> 01:07:09,130
应该等于这个向量

1137
01:07:09,260 --> 01:07:18,020
好的

1138
01:07:46,210 --> 01:07:49,540
所以我的学习算法会输出

1139
01:07:49,700 --> 01:07:51,990
y=1 2

1140
01:07:52,100 --> 01:07:54,600
一直到y=k-1的概率

1141
01:07:54,730 --> 01:07:57,350
这些概率

1142
01:07:57,460 --> 01:08:00,760
通过这些函数参数化

1143
01:08:21,540 --> 01:08:25,390
我们给这个算法起个名字

1144
01:08:25,550 --> 01:08:33,530
这个算法被称之为softmax回归

1145
01:08:35,090 --> 01:08:38,390
它被普遍认为是logistic回归

1146
01:08:38,590 --> 01:08:41,430
的推广  logistic回归

1147
01:08:41,540 --> 01:08:43,680
只处理两种类别 它被普遍认为是

1148
01:08:43,800 --> 01:08:46,060
logistic回归的推广

1149
01:08:46,180 --> 01:08:49,510
可以处理k类而不仅仅是2类

1150
01:08:49,600 --> 01:08:55,040
你要做的事情非常具体

1151
01:08:55,170 --> 01:08:57,500
你有一个机器学习问题

1152
01:08:57,610 --> 01:08:59,730
并且你想使用softmax回归来解决它

1153
01:08:59,810 --> 01:09:01,760
通常情况下

1154
01:09:01,860 --> 01:09:03,710
使用这些推导得出最后的预测

1155
01:09:03,830 --> 01:09:05,730
我认为你们的问题是

1156
01:09:05,820 --> 01:09:07,300
怎样拟合出这些参数

1157
01:09:07,410 --> 01:09:10,320
假设你们有一个机器学习问题

1158
01:09:10,410 --> 01:09:13,220
y可能属于k类中的1类

1159
01:09:13,330 --> 01:09:15,390
你可能会坐在那里说

1160
01:09:15,510 --> 01:09:17,360
"好的  对于给定的任何x和theta  "

1161
01:09:17,470 --> 01:09:19,430
"我要用多项式分布"

1162
01:09:19,580 --> 01:09:21,880
"对y进行建模" 所以你选择了多项式分布

1163
01:09:21,950 --> 01:09:24,470
作为指数分布

1164
01:09:24,580 --> 01:09:26,640
之后你不需要任何工作

1165
01:09:26,740 --> 01:09:28,950
我写的每一步

1166
01:09:29,040 --> 01:09:30,460
在你选择了多项式分布

1167
01:09:30,560 --> 01:09:32,640
作为指数分布后

1168
01:09:32,750 --> 01:09:35,460
都会自动地得到

1169
01:09:35,540 --> 01:09:37,700
你之后的工作就是要处理训练集合

1170
01:09:37,780 --> 01:09:44,550
(x^((1) )  y^((1) ) )  …  (x^((m) )  y^((m) )))

1171
01:09:44,700 --> 01:09:47,780
所以你得到了一个训练集合

1172
01:09:47,920 --> 01:09:49,810
训练集合中的每个y

1173
01:09:49,880 --> 01:09:52,290
可能有k个取值

1174
01:09:52,360 --> 01:09:56,390
之后  你需要通过极大似然估计

1175
01:09:56,560 --> 01:09:58,810
找到参数θ

1176
01:09:58,940 --> 01:10:01,130
所以你写出参数的似然性

1177
01:10:01,230 --> 01:10:03,610
并且使似然性最大化

1178
01:10:03,690 --> 01:10:05,790
似然性是什么?

1179
01:10:05,910 --> 01:10:08,360
通常情况下  似然性是训练集合中

1180
01:10:08,470 --> 01:10:12,260
每个样本概率的乘积

1181
01:10:12,350 --> 01:10:18,580
这是似然性  和我们之前的形式相同

1182
01:10:18,640 --> 01:10:25,440
它是你训练样本的-

1183
01:10:25,530 --> 01:10:31,370
让我现在写出来

1184
01:10:31,470 --> 01:10:34,640
∏_(i=1)^m?〖Φ_1^1

1185
01:10:34,780 --> 01:10:44,350
…Φ_k^1〗

1186
01:10:47,470 --> 01:10:51,230
这里  例如  Φ_1

1187
01:10:51,370 --> 01:10:54,470
依赖于参数θ

1188
01:10:54,570 --> 01:10:58,000
Φ_1=e^(θ_1^T x)/(1+∑_je^(θ_j^T x) )

1189
01:10:58,090 --> 01:11:03,780
好的  这个公式是这样的

1190
01:11:03,920 --> 01:11:10,490
Φ_1是这个公式的简略形式

1191
01:11:11,100 --> 01:11:14,470
其他的Φ也一样

1192
01:11:14,610 --> 01:11:17,950
一直到Φ_k  其中Φ_k

1193
01:11:18,050 --> 01:11:20,490
等于1减去所有这些之和 好的

1194
01:11:20,580 --> 01:11:24,240
这个公式的外表


1195
01:11:24,320 --> 01:11:26,790
比它的实质要复杂地多

1196
01:11:26,920 --> 01:11:29,210
你真正需要做的是

1197
01:11:29,270 --> 01:11:31,690
取对数

1198
01:11:31,780 --> 01:11:34,810
对θ求导

1199
01:11:34,930 --> 01:11:37,480
并且应用梯度上升之类的方法使似然性最大化

1200
01:11:37,660 --> 01:11:41,500
θ的一行是什么意思?

1201
01:11:41,610 --> 01:11:43,310
它不是一个向量吗?

1202
01:11:43,430 --> 01:11:45,060
它现在看起来像是二维的

1203
01:11:45,200 --> 01:11:47,340
是的

1204
01:11:47,440 --> 01:11:50,140
从θ_1到θ_(k-1)

1205
01:11:50,290 --> 01:11:54,860
我们认为每个θ

1206
01:11:55,000 --> 01:11:58,150
都是一个n+1维的向量

1207
01:11:58,540 --> 01:12:01,280
如果x是n+1维的

1208
01:12:01,440 --> 01:12:05,770
那么我认为你将有一组

1209
01:12:05,900 --> 01:12:08,520
由k-1个向量组成参数

1210
01:12:08,620 --> 01:12:12,110
每个向量都是一个n+1维的数组-你可以将这些

1211
01:12:12,240 --> 01:12:13,720
向量组合在一起成为一个矩阵

1212
01:12:13,820 --> 01:12:15,870
但是我没有这样做

1213
01:12:15,960 --> 01:12:18,480
只是使用k-1个参数向量

1214
01:12:19,440 --> 01:12:21,100
那么这些是和什么相符合的

1215
01:12:21,260 --> 01:12:23,130
[听不清]

1216
01:12:23,260 --> 01:12:27,660
我们没有时间了  课下再回答你的问题

1217
01:12:27,820 --> 01:12:29,760
很难用相同的方式回答

1218
01:12:29,910 --> 01:12:32,250
对于logistic回归

1219
01:12:32,340 --> 01:12:34,180
θ对应着什么

1220
01:12:34,310 --> 01:12:36,250
你可以这样回答-

1221
01:12:36,270 --> 01:12:38,730
是的

1222
01:12:40,020 --> 01:12:41,670
这是一种特点

1223
01:12:41,930 --> 01:12:43,300
是的

1224
01:12:43,390 --> 01:12:45,490
差不多相似的解释  是的

1225
01:12:45,600 --> 01:12:47,990
很好 我觉得已经晚了

1226
01:12:48,090 --> 01:12:49,580
为什么不

1227
01:12:49,650 --> 01:12:51,350
先下课

1228
01:12:51,500 --> 01:12:53,700
之后如果你们有问题可以再来找我

