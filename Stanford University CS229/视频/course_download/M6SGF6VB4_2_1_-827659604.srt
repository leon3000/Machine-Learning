1
00:00:23,790 --> 00:00:24,970
好的

2
00:00:24,970 --> 00:00:27,920
让我们开始今天的内容

3
00:00:31,200 --> 00:00:33,450
欢迎来到第二讲

4
00:00:34,310 --> 00:00:37,760
今天我想讲的有:线性回归

5
00:00:38,080 --> 00:00:41,350
梯度下降与正规方程组

6
00:00:42,270 --> 00:00:45,390
今天的讲义我已经发到网上了

7
00:00:45,620 --> 00:00:48,060
所以今天讲的数学知识

8
00:00:48,210 --> 00:00:50,430
我会讲得非常快

9
00:00:50,580 --> 00:00:53,070
所以如果你想慢慢地研究一下

10
00:00:53,240 --> 00:00:55,050
每一个公式的细节

11
00:00:55,580 --> 00:00:56,840
你可以到课程主页上

12
00:00:57,040 --> 00:01:00,820
下载详细的描述

13
00:01:01,080 --> 00:01:02,910
我今天所讲的数学

14
00:01:03,040 --> 00:01:04,750
和技术内容的讲义

15
00:01:05,980 --> 00:01:08,480
今天  我们要深入研究

16
00:01:08,650 --> 00:01:10,070
一些线性代数的知识

17
00:01:10,820 --> 00:01:12,690
所以如果你想重新

18
00:01:12,890 --> 00:01:14,490
补习一下线性代数的话

19
00:01:15,610 --> 00:01:17,880
那么这周助教主持的讨论课上

20
00:01:18,050 --> 00:01:20,570
将会回顾这些知识

21
00:01:20,860 --> 00:01:23,820
所以今天关于线性代数的知识

22
00:01:23,990 --> 00:01:26,480
我会讲得非常快

23
00:01:26,650 --> 00:01:28,710
所以如果你想看

24
00:01:28,910 --> 00:01:31,530
我今天讲的结论的证明过程

25
00:01:31,700 --> 00:01:33,070
或者想看一些

26
00:01:33,220 --> 00:01:34,350
更详细的知识

27
00:01:34,720 --> 00:01:36,580
你可以来上这周的讨论课

28
00:01:41,230 --> 00:01:43,930
开始之前我想先让你们看一段有趣的视频

29
00:01:44,790 --> 00:01:48,010
记得上一讲

30
00:01:48,180 --> 00:01:49,780
我提到了监督学习

31
00:01:50,020 --> 00:01:51,760
在监督学习问题中

32
00:01:51,940 --> 00:01:54,790
我会告诉算法

33
00:01:54,960 --> 00:01:58,490
每个样本的

34
00:01:59,010 --> 00:02:02,460
正确答案是什么

35
00:02:02,810 --> 00:02:03,980
并且希望学习后的算法

36
00:02:04,140 --> 00:02:05,250
对于新的输入也能告诉我大致相同的答案

37
00:02:05,410 --> 00:02:08,120
我在第一节课中举的是

38
00:02:08,310 --> 00:02:10,450
房价预测的例子

39
00:02:10,610 --> 00:02:11,950
你可能有一组训练样本

40
00:02:12,490 --> 00:02:13,650
我们会告诉算法

41
00:02:13,820 --> 00:02:16,020
对于每一个集合中的样本

42
00:02:16,190 --> 00:02:17,670
什么是它"正确"的房价

43
00:02:18,250 --> 00:02:19,640
之后你希望这个算法能够学习

44
00:02:19,800 --> 00:02:22,530
房屋大小和房屋价格之间的关系

45
00:02:22,760 --> 00:02:26,100
并且能够产生出更多"正确"答案

46
00:02:26,830 --> 00:02:30,230
现在我们来看一段视频

47
00:02:30,780 --> 00:02:31,500
谢谢

48
00:02:31,660 --> 00:02:32,660
我向你展示的这段视频

49
00:02:33,410 --> 00:02:34,930
介绍的是Dean Pomerleau

50
00:02:35,240 --> 00:02:37,280
在卡内基梅隆大学进行的工作

51
00:02:38,580 --> 00:02:40,140
他利用监督学习

52
00:02:40,310 --> 00:02:41,680
让一辆汽车可以自动行驶

53
00:02:42,810 --> 00:02:45,030
这项工作是在一辆被称为

54
00:02:45,220 --> 00:02:46,370
Alvin的汽车上进行的

55
00:02:46,540 --> 00:02:48,950
这项工作已经是15年前的事了

56
00:02:49,920 --> 00:02:55,910
我认为它是一个关于监督学习

57
00:02:56,220 --> 00:02:58,730
或其他算法可以完成的工作的

58
00:02:58,880 --> 00:03:00,400
一个非常优雅的例子

59
00:03:01,180 --> 00:03:02,560
在视频中

60
00:03:02,720 --> 00:03:04,540
你会听到Dean Pomerleau的介绍

61
00:03:04,710 --> 00:03:06,620
他把他的算法称为神经网络算法

62
00:03:06,870 --> 00:03:08,170
我一会儿会再说一下这个算法

63
00:03:08,520 --> 00:03:11,050
它的核心算法

64
00:03:11,220 --> 00:03:12,820
被称为梯度下降

65
00:03:12,970 --> 00:03:15,310
是我们今天要讲的知识之一

66
00:03:15,520 --> 00:03:19,620
让我们来看视频

67
00:04:25,590 --> 00:04:26,600
I:两点说明

68
00:04:26,780 --> 00:04:28,740
这之所以是监督学习

69
00:04:28,940 --> 00:04:30,890
是因为它是从人类司机那里学习到的知识

70
00:04:31,210 --> 00:04:33,110
人类司机会教给它:

71
00:04:33,310 --> 00:04:34,930
在这一段路

72
00:04:35,110 --> 00:04:36,160
我会朝这个方向行驶；

73
00:04:36,350 --> 00:04:38,330
在这段路上  我会想那个方向行驶

74
00:04:38,670 --> 00:04:40,120
所以人类司机为汽车提供了

75
00:04:40,300 --> 00:04:43,140
一系列正确的行驶方向

76
00:04:43,500 --> 00:04:46,130
之后产生更多"正确"的行驶方向

77
00:04:47,000 --> 00:04:49,740
以确保汽车始终行驶在路上

78
00:04:49,970 --> 00:04:51,370
就是汽车的任务了

79
00:04:52,380 --> 00:04:54,820
在这里的检测器上

80
00:04:55,060 --> 00:04:56,940
我要告诉你

81
00:04:57,110 --> 00:04:58,310
这些显示是什么意思

82
00:04:58,500 --> 00:04:59,970
左上角

83
00:05:00,140 --> 00:05:01,580
我的光标指的这里

84
00:05:01,880 --> 00:05:04,290
这条横线表示

85
00:05:04,450 --> 00:05:06,050
人类司机的行驶方向

86
00:05:06,390 --> 00:05:10,040
这个白色的条状区域

87
00:05:10,300 --> 00:05:13,680
表示人类司机通过

88
00:05:13,890 --> 00:05:16,740
转动方向盘所选取的行驶方向

89
00:05:16,930 --> 00:05:18,770
现在白色区域显示人类司机

90
00:05:18,940 --> 00:05:21,290
正在往偏左一些的方向行驶

91
00:05:21,680 --> 00:05:25,810
光标指的第二条线

92
00:05:26,020 --> 00:05:29,210
是学习算法的输出

93
00:05:29,400 --> 00:05:30,340
表示了学习算法

94
00:05:30,510 --> 00:05:32,380
现在认为正确的行驶方向

95
00:05:32,640 --> 00:05:34,840
现在你们看到的是学习算法

96
00:05:35,030 --> 00:05:36,740
刚开始训练时的状态

97
00:05:36,920 --> 00:05:39,180
所以它根本不知道向那里行驶

98
00:05:39,380 --> 00:05:40,510
所以它的输出

99
00:05:40,690 --> 00:05:42,520
是遍布于

100
00:05:42,690 --> 00:05:44,210
整个方向上的污痕

101
00:05:44,420 --> 00:05:46,620
当算法采集了更多的样本

102
00:05:46,910 --> 00:05:47,950
并且学习了一段时间之后

103
00:05:48,110 --> 00:05:49,930
你们会看到

104
00:05:50,110 --> 00:05:51,920
它对于如何选取行驶方向将会更加自信

105
00:05:52,290 --> 00:05:53,720
让我们继续来看视频

106
00:05:55,060 --> 00:05:59,290
(视频播放)

107
00:08:39,830 --> 00:08:41,820
好的  谁能想到驾驶

108
00:08:41,980 --> 00:08:43,290
是一件如此引人入胜的事?

109
00:08:44,330 --> 00:08:46,370
请帮忙切换回黑板

110
00:08:47,560 --> 00:08:51,170
这项工作是15年之前开始的

111
00:08:51,490 --> 00:08:53,060
从那以后自动驾驶方面的研究

112
00:08:53,250 --> 00:08:54,360
由已经开展很长时间了

113
00:08:54,520 --> 00:08:55,870
你们中的很多人一定

114
00:08:56,020 --> 00:08:57,060
听说过DARPA大挑战

115
00:08:57,210 --> 00:08:59,560
我的一个同事  Sebastian Thrun

116
00:08:59,710 --> 00:09:02,110
曾经赢得过这个比赛

117
00:09:02,270 --> 00:09:03,700
他可以让一辆汽车自动穿过沙漠

118
00:09:03,860 --> 00:09:05,940
所以我认为Alvin

119
00:09:06,080 --> 00:09:07,320
在那时是绝对神奇的工作

120
00:09:07,480 --> 00:09:11,060
从那以后  自动驾驶方面的研究

121
00:09:11,200 --> 00:09:12,740
就开始蓬勃发展了

122
00:09:14,260 --> 00:09:17,980
刚才你们又看到了

123
00:09:18,170 --> 00:09:19,970
一个监督学习的例子

124
00:09:20,200 --> 00:09:21,990
实际上它是一个更为特殊的

125
00:09:22,210 --> 00:09:24,570
被称之为回归问题的例子

126
00:09:25,100 --> 00:09:27,340
由于汽车尝试预测表示行驶方向的

127
00:09:27,510 --> 00:09:29,740
连续变量的值

128
00:09:29,930 --> 00:09:33,860
所以我们称它为回归问题

129
00:09:35,690 --> 00:09:37,730
我今天要开始讲我们的

130
00:09:37,930 --> 00:09:40,570
第一个监督学习算法

131
00:09:40,880 --> 00:09:42,980
它同样可以归约于一个回归任务

132
00:09:44,860 --> 00:09:48,190
对于今天的课上

133
00:09:48,350 --> 00:09:50,660
需要用到的例子

134
00:09:50,940 --> 00:09:53,270
你们需要回到之前的

135
00:09:53,460 --> 00:09:55,580
那个关于房价预测的例子

136
00:09:55,800 --> 00:10:05,870
这是一组由

137
00:10:06,040 --> 00:10:08,340
Dan Ramage助教

138
00:10:08,540 --> 00:10:11,490
收集的Portland Oregon的房价数据

139
00:10:27,480 --> 00:10:29,890
这里是一组

140
00:10:30,090 --> 00:10:32,650
不同大小的房子

141
00:10:33,590 --> 00:10:42,580
这里是它们的要价  单位是1000美元

142
00:10:44,050 --> 00:10:46,330
200  000美元

143
00:10:53,790 --> 00:11:00,580
我们可以将这些数据点画出来  平方英尺

144
00:11:01,780 --> 00:11:08,240
最优价格  你的数据可能是这样的

145
00:11:08,610 --> 00:11:11,010
问题是  给你这样一组数据

146
00:11:11,190 --> 00:11:13,260
或者说给你这样一个训练数据的集合

147
00:11:13,730 --> 00:11:14,720
你能否学会预测

148
00:11:14,890 --> 00:11:15,950
房屋大小和

149
00:11:16,110 --> 00:11:17,940
房价之间的关系?

150
00:11:19,800 --> 00:11:21,080
我之后会回来

151
00:11:21,270 --> 00:11:23,370
并稍微改动一下任务

152
00:11:23,630 --> 00:11:27,250
但是让我们先引入一些符号

153
00:11:27,510 --> 00:11:29,230
再剩下的课里

154
00:11:29,400 --> 00:11:30,800
我一直都会使用这些符号

155
00:11:32,810 --> 00:11:34,230
我要引入的

156
00:11:34,970 --> 00:11:38,390
第一个符号是m

157
00:11:38,980 --> 00:11:41,080
表示训练样本的数目

158
00:11:41,320 --> 00:11:42,510
也就是这些数据的行数

159
00:11:42,690 --> 00:11:43,870
或者说是我们有的这些样本、

160
00:11:44,040 --> 00:11:45,380
房子、价格的数目

161
00:11:45,840 --> 00:11:47,340
实际上在这个数据集中

162
00:11:47,540 --> 00:11:51,930
我们一共有

163
00:11:52,110 --> 00:11:53,830
47个训练样本

164
00:11:54,030 --> 00:11:55,580
虽然我只写了5个

165
00:12:02,000 --> 00:12:04,340
好的  那么在这之后

166
00:12:05,190 --> 00:12:11,730
我就会一直使用m

167
00:12:12,500 --> 00:12:14,920
表示训练样本的数目

168
00:12:17,230 --> 00:12:21,670
我要用x

169
00:12:22,310 --> 00:12:27,600
表示输入变量

170
00:12:33,110 --> 00:12:36,230
通常也可以称为特征(feature)

171
00:12:36,490 --> 00:12:38,670
在这个例子中  x表示

172
00:12:38,870 --> 00:12:41,220
他们在看的房子大小

173
00:12:42,110 --> 00:12:52,250
我要用y来表示输出变量

174
00:12:52,440 --> 00:13:01,100
有时也称为目标变量

175
00:13:02,140 --> 00:13:06,730
用(x  y)

176
00:13:07,480 --> 00:13:10,650
表示一个样本

177
00:13:13,400 --> 00:13:14,300
换句话说

178
00:13:14,460 --> 00:13:16,550
我画的这个数据表中

179
00:13:16,880 --> 00:13:19,520
的一行表示一个训练样本

180
00:13:19,860 --> 00:13:24,850
第i个训练样本

181
00:13:30,690 --> 00:13:33,150
或者说表中的第i行

182
00:13:33,380 --> 00:13:40,670
我会将其表示为:XI Y I.

183
00:13:41,470 --> 00:13:45,590
这个符号中i表示

184
00:13:45,810 --> 00:13:49,030
上标而不是指数

185
00:13:49,190 --> 00:13:51,600
所以这不是表示x的i次方或者y的i次方

186
00:13:51,970 --> 00:13:53,490
这个符号中

187
00:13:54,190 --> 00:13:57,590
用括号括起来的上标i

188
00:13:57,770 --> 00:14:01,920
表示训练样本列表中的第i行

189
00:14:03,460 --> 00:14:08,680
在监督学习中

190
00:14:08,880 --> 00:14:12,410
我们会这样做

191
00:14:13,140 --> 00:14:16,570
我们先找到一个训练集合

192
00:14:19,490 --> 00:14:21,980
然后将我们的由m个

193
00:14:22,150 --> 00:14:24,230
训练样本组成的训练集合

194
00:14:24,400 --> 00:14:27,470
这里是47个训练样本  提供给学习算法

195
00:14:35,000 --> 00:14:38,180
之后我们的算法

196
00:14:38,370 --> 00:14:42,030
会生成一个输出函数

197
00:14:42,220 --> 00:14:43,730
由于惯例和一些历史原因

198
00:14:44,460 --> 00:14:48,530
我们用h来表示这个函数

199
00:14:48,850 --> 00:14:51,680
这个函数称之为假设

200
00:14:53,530 --> 00:14:54,710
不要过多地思考

201
00:14:54,880 --> 00:14:56,750
这个词的内在含义

202
00:14:57,420 --> 00:15:00,220
它只是出于历史原因而被使用的一个术语

203
00:15:03,100 --> 00:15:06,910
这个假设的任务  就是接收输入

204
00:15:09,120 --> 00:15:11,640
比如说一个用

205
00:15:12,500 --> 00:15:14,580
平方英尺表示的新的面积

206
00:15:16,410 --> 00:15:18,420
并且输出

207
00:15:19,470 --> 00:15:26,960
对于房价的估计

208
00:15:27,410 --> 00:15:33,730
所以假设h将输入x映射到输出y

209
00:15:37,620 --> 00:15:40,100
为了设计学习算法

210
00:15:40,310 --> 00:15:41,560
我们第一步要做的

211
00:15:41,740 --> 00:15:44,890
决定应该怎样表示假设

212
00:15:45,320 --> 00:15:47,330
对于这节课

213
00:15:47,530 --> 00:15:49,300
对于我们的第一个学习算法来说

214
00:15:49,580 --> 00:15:51,110
我要对假设

215
00:15:51,280 --> 00:15:53,060
进行线性表示

216
00:15:53,230 --> 00:15:55,310
所以这里我会把假设表示成:

217
00:15:55,560 --> 00:16:01,260
H of X equals theta zero

218
00:16:01,260 --> 00:16:04,010
plus theta 1X,

219
00:16:04,520 --> 00:16:07,800
这里x是一个输入特征

220
00:16:08,090 --> 00:16:11,440
也就是我们要考虑的房子大小

221
00:16:12,320 --> 00:16:16,400
通常情况下  回到这里

222
00:16:18,260 --> 00:16:21,880
通常情况下许多回归问题

223
00:16:22,060 --> 00:16:23,830
都需要多个输入特征

224
00:16:24,010 --> 00:16:26,920
例如  如果除了知道

225
00:16:27,100 --> 00:16:28,520
房子的大小之外

226
00:16:28,830 --> 00:16:35,050
比如说  我们还知道

227
00:16:35,230 --> 00:16:36,580
房子的卧室数量

228
00:16:43,270 --> 00:16:46,000
那么

229
00:16:53,940 --> 00:16:58,010
我们的训练集合就可以有第二个特征

230
00:16:58,210 --> 00:17:00,250
表示房子的卧室数量

231
00:17:00,500 --> 00:17:03,320
那么你们就可以用  比如说

232
00:17:03,480 --> 00:17:08,040
来表示房子的大小和平方公尺

233
00:17:08,520 --> 00:17:11,760
用表示卧室的数量

234
00:17:15,000 --> 00:17:19,670
然后我就可以将假设写成这样:

235
00:17:19,840 --> 00:17:26,200
H of X, as theta rho plus

236
00:17:28,060 --> 00:17:32,900
theta 1X1 plus theta 2X2.

237
00:17:33,500 --> 00:17:36,390
好的  有时我需要

238
00:17:36,560 --> 00:17:38,160
让假设h对的依赖

239
00:17:38,330 --> 00:17:39,930
显示地

240
00:17:40,080 --> 00:17:41,560
表示出来

241
00:17:42,000 --> 00:17:43,040
我有时会将它写成:

242
00:17:43,040 --> 00:17:45,700
H subscript theta of X.

243
00:17:46,140 --> 00:17:50,120
这就是我的假设

244
00:17:50,290 --> 00:17:54,670
对于特征x预测的花费

245
00:17:55,330 --> 00:17:57,100
所以给定房子的特征x

246
00:17:57,490 --> 00:17:59,680
一个明确的面积大小和一个明确的卧室数目

247
00:17:59,910 --> 00:18:01,800
这就是我的假设

248
00:18:01,960 --> 00:18:04,530
预测出的开销

249
00:18:10,420 --> 00:18:12,490
最后一个符号

250
00:18:13,760 --> 00:18:23,720
为了便于将整个公式

251
00:18:23,880 --> 00:18:25,280
写得更为简洁

252
00:18:25,480 --> 00:18:27,410
我可以定义

253
00:18:27,610 --> 00:18:34,420
这样整个公式就可以

254
00:18:34,610 --> 00:18:40,520
写成对于i从1到2

255
00:18:40,970 --> 00:18:44,070
对不起  是从0到2  对进行求和

256
00:18:44,390 --> 00:18:47,350
如果你将也作为像x一样的向量

257
00:18:47,610 --> 00:18:51,010
那么可以写成.

258
00:18:52,660 --> 00:18:56,300
最后最后一个符号是

259
00:18:57,270 --> 00:19:07,220
我希望用n来表示

260
00:19:07,370 --> 00:19:09,330
学习问题中特征的数目

261
00:19:09,490 --> 00:19:10,410
所以这里

262
00:19:10,560 --> 00:19:19,420
就变成了i从0到n

263
00:19:19,780 --> 00:19:21,930
这个例子中你有2个特征

264
00:19:22,110 --> 00:19:23,990
因此n=2

265
00:19:26,880 --> 00:19:30,230
我意识到有很多的符号

266
00:19:30,520 --> 00:19:34,950
当我今天或者未来几周

267
00:19:35,170 --> 00:19:36,830
上课的时候

268
00:19:37,020 --> 00:19:39,790
如果有一天你看着我

269
00:19:40,030 --> 00:19:41,230
写了一个符号  然后想

270
00:19:41,410 --> 00:19:43,220
怎么又是那个n?

271
00:19:43,390 --> 00:19:44,880
或者怎么又是那个x?

272
00:19:45,060 --> 00:19:45,980
或其他的问题

273
00:19:46,130 --> 00:19:47,200
请举手  我会回答你

274
00:19:47,360 --> 00:19:49,120
真的有很多的符号

275
00:19:49,260 --> 00:19:53,300
我们很可能得适应几天

276
00:19:53,870 --> 00:19:55,390
使得我们以后的符号表示更加标准

277
00:19:55,570 --> 00:19:56,880
使得我们对学习算法

278
00:19:57,070 --> 00:19:58,380
的描述更容易

279
00:19:58,560 --> 00:19:59,430
再强调一遍

280
00:19:59,590 --> 00:20:00,730
如果你看到我写的一些

281
00:20:00,940 --> 00:20:02,490
符号中有你不认识的

282
00:20:02,660 --> 00:20:03,830
有可能同样有人

283
00:20:04,010 --> 00:20:05,080
忘记它的含义了

284
00:20:05,270 --> 00:20:06,480
所以如果你还不了解

285
00:20:06,670 --> 00:20:08,850
某些符号的含义的话

286
00:20:09,650 --> 00:20:12,220
请举手?有问题吗?

287
00:20:20,800 --> 00:20:24,540
I:能再说一遍吗?

288
00:20:26,580 --> 00:20:28,370
I:好的

289
00:20:34,830 --> 00:20:36,110
我后面会讲到

290
00:20:36,290 --> 00:20:39,700
这里的或者被称为参数

291
00:20:46,460 --> 00:20:47,260
被称为我们的

292
00:20:47,400 --> 00:20:49,670
学习算法的参数

293
00:20:49,830 --> 00:20:51,640
都是实数

294
00:20:52,310 --> 00:20:54,680
利用训练集合选择或学习

295
00:20:55,370 --> 00:20:58,910
得到合适的参数值

296
00:20:59,090 --> 00:21:01,060
是学习算法的任务

297
00:21:01,790 --> 00:21:03,110
好的  还有其他问题吗?

298
00:21:32,820 --> 00:21:34,520
好问题

299
00:21:35,030 --> 00:21:36,430
你问的问题是

300
00:21:36,600 --> 00:21:38,430
典型的假设或者

301
00:21:38,590 --> 00:21:41,560
能不能是其它变量的参数

302
00:21:41,770 --> 00:21:43,070
答案是肯定的

303
00:21:43,390 --> 00:21:45,760
现在  对于我们要讨论的

304
00:21:45,980 --> 00:21:48,430
第一个学习算法

305
00:21:48,620 --> 00:21:50,400
我们仅仅会讨论线性假设类

306
00:21:50,950 --> 00:21:53,140
晚些的时候

307
00:21:53,430 --> 00:21:54,200
我们会讨论

308
00:21:54,370 --> 00:21:55,660
复杂得多的假设类

309
00:21:56,980 --> 00:21:57,770
晚些时候

310
00:21:57,930 --> 00:22:00,480
我同样会讨论更高阶的函数

311
00:22:03,420 --> 00:22:09,330
好的  继续讨论学习问题

312
00:22:10,060 --> 00:22:12,140
我们怎样选取参数

313
00:22:12,780 --> 00:22:15,300
来让我们的假设h对

314
00:22:15,460 --> 00:22:17,700
所有房屋做出准确的预测呢?

315
00:22:18,240 --> 00:22:21,810
一种合理的情形似乎是

316
00:22:22,010 --> 00:22:23,860
我们有一个训练集合

317
00:22:24,160 --> 00:22:26,980
基于这个训练集合

318
00:22:27,290 --> 00:22:29,690
我们的假设会做出一些

319
00:22:30,270 --> 00:22:31,590
对于训练集合中

320
00:22:31,760 --> 00:22:36,590
房子的房价的预测

321
00:22:37,220 --> 00:22:38,900
一件可以做的事是尝试

322
00:22:39,090 --> 00:22:42,740
让学习算法的预测在

323
00:22:43,470 --> 00:22:44,970
训练数据上尽可能准确

324
00:22:45,190 --> 00:22:47,750
给定一些特征  x

325
00:22:47,940 --> 00:22:49,640
以及一些正确的价格  y

326
00:22:49,930 --> 00:22:53,400
我们希望取使得

327
00:22:53,600 --> 00:22:54,990
算法预测和实际价格的

328
00:22:55,160 --> 00:22:56,480
平方差尽可能的小

329
00:22:59,400 --> 00:23:01,430
为了选择参数

330
00:23:01,700 --> 00:23:02,760
以使得参数取时

331
00:23:02,950 --> 00:23:05,370
能够使得预测价格

332
00:23:05,960 --> 00:23:07,890
和实际价格之差的平方最小

333
00:23:10,030 --> 00:23:12,870
让我们把这些写上

334
00:23:13,100 --> 00:23:14,770
我们有m个训练样本

335
00:23:14,980 --> 00:23:17,120
所以对于我们的m个样本

336
00:23:17,280 --> 00:23:19,130
i从1取到m

337
00:23:19,810 --> 00:23:22,830
对应地加上

338
00:23:23,070 --> 00:23:24,360
第i个房子的预测价格

339
00:23:24,580 --> 00:23:27,140
减去目标变量

340
00:23:27,360 --> 00:23:29,140
也就是第i个

341
00:23:29,340 --> 00:23:30,850
训练样本的实际价格

342
00:23:32,550 --> 00:23:33,770
习惯上

343
00:23:33,990 --> 00:23:35,110
我们会在这里

344
00:23:35,520 --> 00:23:37,230
乘上一个1/2

345
00:23:37,410 --> 00:23:38,730
而不是直接对平方差求和

346
00:23:39,060 --> 00:23:43,550
这会简化我们之后的数学运算

347
00:23:44,500 --> 00:23:47,620
好的  让我们继续

348
00:23:47,790 --> 00:23:50,630
对于我们的m个样本  同样定义

349
00:23:50,840 --> 00:23:54,270
乘上1/2

350
00:23:54,450 --> 00:23:56,630
i从1取到m

351
00:23:56,890 --> 00:24:03,400
我们的假设产生的

352
00:24:03,760 --> 00:24:05,290
预测值减去实际值

353
00:24:05,870 --> 00:24:13,420
我们要做的

354
00:24:13,660 --> 00:24:16,990
是要使一个关于的函数的值最小化

355
00:24:17,390 --> 00:24:19,170
也就是这个

356
00:24:20,700 --> 00:24:22,410
对于那些上过线性代数

357
00:24:22,600 --> 00:24:24,630
或者基础统计课程的同学

358
00:24:25,250 --> 00:24:28,850
你们中的一些人可能看过

359
00:24:29,060 --> 00:24:31,170
诸如最小平方回归

360
00:24:31,360 --> 00:24:35,990
或者最小二乘之类的知识

361
00:24:37,600 --> 00:24:39,320
可能你们中很多人还没有看过这个

362
00:24:39,530 --> 00:24:41,380
可能有些已经看过了

363
00:24:41,720 --> 00:24:43,160
但是不管

364
00:24:43,330 --> 00:24:44,280
你之前看没看过

365
00:24:44,450 --> 00:24:45,310
我们都要继续讲

366
00:24:45,600 --> 00:24:47,830
对于那些之前看过的人

367
00:24:48,090 --> 00:24:49,290
我想说  我们会逐渐地讲到

368
00:24:49,450 --> 00:24:52,130
这个算法是一类

369
00:24:52,320 --> 00:24:54,280
范围更广的算法的一个特例

370
00:24:54,870 --> 00:24:55,810
但是让我们继续

371
00:24:55,980 --> 00:24:57,420
我们会逐渐讲到那里

372
00:24:59,490 --> 00:25:06,990
我接下来要讲几个

373
00:25:07,150 --> 00:25:08,650
不同的算法

374
00:25:08,850 --> 00:25:11,500
来选取使得最小

375
00:25:12,650 --> 00:25:13,810
我要讲的第一个算法

376
00:25:13,970 --> 00:25:15,550
是一个搜索算法

377
00:25:16,120 --> 00:25:17,310
基本的想法是

378
00:25:17,490 --> 00:25:25,060
我们先给参数向量

379
00:25:25,240 --> 00:25:26,810
一个初始值

380
00:25:30,830 --> 00:25:33,390
比如可以将参数向量

381
00:25:33,730 --> 00:25:35,480
初始化为一个0向量

382
00:25:36,640 --> 00:25:39,490
抱歉  这里改一下

383
00:25:40,120 --> 00:25:42,670
我在0上加了个箭头表示

384
00:25:42,850 --> 00:25:45,110
这是一个所有位置都是0的向量

385
00:25:45,670 --> 00:25:48,500
然后我要不断地

386
00:25:51,650 --> 00:25:54,160
改变参数向量

387
00:25:54,540 --> 00:26:00,660
使得不断减小

388
00:26:00,990 --> 00:26:03,040
直到我们满怀希望地

389
00:26:03,240 --> 00:26:05,980
找到了一个使得取到了最小值

390
00:26:07,260 --> 00:26:11,550
请帮我打开笔记本  把大屏幕放下来

391
00:26:12,580 --> 00:26:19,190
让我继续给你们展示一段动画

392
00:26:19,560 --> 00:26:23,170
来演示第一个使最小化的算法

393
00:26:23,660 --> 00:26:25,700
我们称之为梯度下降

394
00:26:26,580 --> 00:26:30,170
算法的思想是这样的

395
00:26:30,460 --> 00:26:35,090
你们看到  这里显示了一个图形和坐标轴

396
00:26:35,280 --> 00:26:38,630
水平方向的坐标轴表示和

397
00:26:38,820 --> 00:26:40,810
这里通常是的最小值

398
00:26:41,020 --> 00:26:44,020
用图形的高度来表示

399
00:26:44,700 --> 00:26:47,160
所以图形的表面表示的是的值

400
00:26:47,510 --> 00:26:49,040
函数的坐标轴

401
00:26:49,300 --> 00:26:50,850
或者函数的

402
00:26:51,030 --> 00:26:52,470
输入时参数和

403
00:26:52,660 --> 00:26:53,950
写在下面

404
00:26:54,160 --> 00:26:56,280
这就是梯度下降算法

405
00:26:56,840 --> 00:26:58,820
我要选取一个初始点

406
00:26:59,030 --> 00:27:00,530
它可能是0向量

407
00:27:00,700 --> 00:27:01,930
也可能是一个随机生成的点

408
00:27:02,150 --> 00:27:03,330
让我们从这个

409
00:27:03,530 --> 00:27:07,250
十字星号表示的点开始

410
00:27:09,070 --> 00:27:11,060
现在我要你们想象

411
00:27:11,260 --> 00:27:15,270
你们看到图形是一个三维地表

412
00:27:15,490 --> 00:27:17,550
想象一下你们都在一个有很多小山的公园里

413
00:27:17,740 --> 00:27:19,970
这个图形描绘的就是

414
00:27:20,140 --> 00:27:21,430
公园里的一座小山的形状

415
00:27:23,270 --> 00:27:24,510
想象你正站在

416
00:27:24,680 --> 00:27:28,030
十字星号标识的这一点上

417
00:27:28,810 --> 00:27:32,990
想象一下你正站在山上

418
00:27:33,630 --> 00:27:36,600
360度环视一周

419
00:27:36,930 --> 00:27:38,460
然后问你自己

420
00:27:38,610 --> 00:27:39,810
如果我希望只迈一小步

421
00:27:40,040 --> 00:27:42,120
那么往哪个方向走能使我下山最快?

422
00:27:42,500 --> 00:27:44,090
好的  想象这就是一座山

423
00:27:44,240 --> 00:27:45,390
你站在这里  环视一周

424
00:27:45,550 --> 00:27:46,690
然后问

425
00:27:46,840 --> 00:27:48,060
"如果我只迈一小步

426
00:27:48,270 --> 00:27:50,110
那么往哪个方向走

427
00:27:50,290 --> 00:27:52,320
能使我下山的速度最快?"

428
00:27:52,980 --> 00:27:55,040
梯度下降算法就是这样工作的

429
00:27:55,220 --> 00:27:56,620
我向下降最快的

430
00:27:57,740 --> 00:27:59,300
方向走一步

431
00:27:59,460 --> 00:28:01,320
事实上这个方向正是梯度的方向

432
00:28:01,770 --> 00:28:02,910
之后你走了一小步

433
00:28:03,070 --> 00:28:05,220
站在了一个新的地方

434
00:28:06,510 --> 00:28:07,710
之后继续

435
00:28:07,880 --> 00:28:09,370
你站在山上的

436
00:28:09,560 --> 00:28:10,900
一个新的位置

437
00:28:11,110 --> 00:28:15,320
360度环视一周  然后问

438
00:28:15,810 --> 00:28:17,190
"往哪个方向走

439
00:28:17,370 --> 00:28:19,350
能使我下山最快?"

440
00:28:19,520 --> 00:28:21,100
我们想尽可能快地下山

441
00:28:21,310 --> 00:28:23,780
因为我们要找到最小值

442
00:28:25,490 --> 00:28:26,600
你可以再走一步  好的

443
00:28:26,770 --> 00:28:29,590
然后你就一直走

444
00:28:29,790 --> 00:28:33,560
直到你到达了这个

445
00:28:33,760 --> 00:28:36,000
函数的一个局部最小值

446
00:28:38,710 --> 00:28:40,660
梯度下降的一个性质

447
00:28:41,720 --> 00:28:43,500
是它一定会结束  在这个例子中

448
00:28:43,670 --> 00:28:45,500
我们最终停在了

449
00:28:46,030 --> 00:28:49,020
左下角的这个点上

450
00:28:49,490 --> 00:28:52,860
让我们尝试从不同的位置

451
00:28:53,040 --> 00:28:54,280
开始再进行一遍梯度下降

452
00:28:54,550 --> 00:28:57,430
这里是我开始进行梯度下降的地方

453
00:28:57,910 --> 00:28:59,440
让我们回到这里

454
00:28:59,620 --> 00:29:01,680
换一个位置稍微不同的起始点

455
00:29:01,890 --> 00:29:05,790
一个稍微偏右上的一个点

456
00:29:06,860 --> 00:29:08,470
所以如果你从这个点

457
00:29:08,650 --> 00:29:10,060
开始梯度下降

458
00:29:10,240 --> 00:29:12,810
并且再一次找一个最陡峭的方向前进的话

459
00:29:13,790 --> 00:29:15,380
你的第一步是这样的

460
00:29:15,650 --> 00:29:19,720
如果你继续的话  你会发现

461
00:29:20,240 --> 00:29:22,020
换了一个位置稍微不同的起始点之后

462
00:29:22,220 --> 00:29:22,980
你会找到一个

463
00:29:23,130 --> 00:29:24,720
完全不同的局部最优值

464
00:29:25,270 --> 00:29:28,130
好的  这是一个梯度下降的性质

465
00:29:28,310 --> 00:29:29,490
一会儿我们在回来研究它

466
00:29:29,670 --> 00:29:30,720
这次我们注意到了

467
00:29:30,880 --> 00:29:33,160
梯度下降的结果有时会

468
00:29:33,350 --> 00:29:36,390
依赖于参数初始值

469
00:29:36,570 --> 00:29:37,990
这里是的初始值

470
00:29:39,060 --> 00:29:40,730
请切换回黑板

471
00:29:41,590 --> 00:29:43,660
我们继续研究梯度

472
00:29:43,840 --> 00:29:45,260
下降算法的数学部分

473
00:29:45,440 --> 00:29:46,780
之后我们会回来重新来看

474
00:29:47,260 --> 00:29:48,950
局部最优的问题

475
00:30:05,130 --> 00:30:08,170
这里是梯度下降算法

476
00:30:09,180 --> 00:30:11,030
我们会不停地重复向

477
00:30:11,240 --> 00:30:13,030
最陡的方向下降的步骤

478
00:30:13,210 --> 00:30:15,120
事实上你可以把这个步骤

479
00:30:15,300 --> 00:30:17,870
写成这样的形式

480
00:30:18,050 --> 00:30:26,280
我们会把的值更新

481
00:30:26,470 --> 00:30:30,690
为减去对的偏微分

482
00:30:31,030 --> 00:30:33,030
好的  这样我们就更新了

483
00:30:33,260 --> 00:30:35,750
第i个参数的值

484
00:30:36,450 --> 00:30:37,780
这就是我们在每一次

485
00:30:37,960 --> 00:30:39,890
迭代中对更新的方式

486
00:30:40,500 --> 00:30:43,160
注意一下

487
00:30:43,660 --> 00:30:45,720
我用了这个赋值符号

488
00:30:46,400 --> 00:30:50,090
表示把左边变量的值

489
00:30:50,260 --> 00:30:52,480
设成右边变量的值

490
00:30:52,670 --> 00:30:55,710
好的  如果我写:

491
00:30:56,190 --> 00:30:57,330
那么我想表达的是

492
00:30:57,520 --> 00:30:58,960
这是一个计算机程序的一部分

493
00:30:59,150 --> 00:31:00,470
或者说这是一个算法的一部分

494
00:31:00,740 --> 00:31:02,050
这里我们要用

495
00:31:02,290 --> 00:31:03,700
右边的变量b的值

496
00:31:03,900 --> 00:31:06,470
覆盖掉左边的变量a的值

497
00:31:06,860 --> 00:31:09,700
对比一下  如果我写

498
00:31:10,400 --> 00:31:14,070
这实际上是一个真值断言

499
00:31:14,240 --> 00:31:15,860
我想表达的是a的值

500
00:31:16,070 --> 00:31:17,510
等于b的值

501
00:31:17,770 --> 00:31:19,600
而这里是一个可以覆盖掉

502
00:31:19,780 --> 00:31:21,480
变量a值的计算机操作

503
00:31:21,950 --> 00:31:23,230
如果我写

504
00:31:23,430 --> 00:31:26,020
那么我是在断言a和b的值相等

505
00:31:28,710 --> 00:31:32,160
让我们来看一下这个算法的含义

506
00:31:32,680 --> 00:31:37,890
好吧  我们还是继续讲吧

507
00:31:38,130 --> 00:31:39,760
我们继续将这个算法

508
00:31:39,960 --> 00:31:41,450
应用到我们的问题中

509
00:31:49,010 --> 00:31:52,010
为了弄明白梯度下降

510
00:31:54,100 --> 00:31:55,220
让我们先将它应用

511
00:31:55,390 --> 00:31:56,720
到我们的问题中

512
00:31:56,890 --> 00:31:59,400
由于这是我们第一次

513
00:31:59,630 --> 00:32:01,130
涉及有些数学化的内容

514
00:32:01,350 --> 00:32:04,060
所以和之后的课程比起来

515
00:32:04,250 --> 00:32:06,870
这次我会慢慢地仔细讲推导过程

516
00:32:07,050 --> 00:32:08,730
和之后的课比起来

517
00:32:08,930 --> 00:32:12,630
这些步骤我们会讲得更加详细

518
00:32:12,870 --> 00:32:14,050
让我们来搞清楚

519
00:32:14,220 --> 00:32:15,880
梯度下降的规则

520
00:32:17,580 --> 00:32:20,870
我先演示一下

521
00:32:21,130 --> 00:32:23,010
只有一组训练样本的情形

522
00:32:23,690 --> 00:32:25,750
在这种情形下

523
00:32:25,990 --> 00:32:27,880
我需要求

524
00:32:28,050 --> 00:32:30,350
对的偏微分

525
00:32:33,680 --> 00:32:35,740
如果我们只有

526
00:32:36,420 --> 00:32:38,250
一组训练数据  那么将会是

527
00:32:38,430 --> 00:32:44,330
1/2  下标  x  减去y  上标

528
00:32:44,540 --> 00:32:46,380
所以如果你只有一组

529
00:32:46,600 --> 00:32:49,180
由(x  y)组成的训练样本

530
00:32:49,360 --> 00:32:52,070
那么将会是这样的

531
00:32:52,420 --> 00:32:55,670
现在你要对1/2乘上

532
00:32:55,900 --> 00:32:57,580
一个式子的平方求导

533
00:32:57,770 --> 00:32:59,470
把2挪下来

534
00:32:59,640 --> 00:33:01,390
所以得到了2  乘以1/2

535
00:33:01,650 --> 00:33:04,360
乘以

536
00:33:05,130 --> 00:33:08,710
然后根据链式求导法则

537
00:33:09,130 --> 00:33:11,720
我们还要对

538
00:33:11,920 --> 00:33:14,060
平方式中的这项求导

539
00:33:19,710 --> 00:33:22,990
好的  2和1/2抵消

540
00:33:23,420 --> 00:33:31,400
然后留下了这些项  乘上

541
00:33:32,830 --> 00:33:35,770
点点点  加上

542
00:33:40,840 --> 00:33:44,620
好的  如果你深入研究

543
00:33:46,790 --> 00:33:48,750
这个求和的式子

544
00:33:48,950 --> 00:33:51,610
我们现在对参数求偏导数

545
00:33:52,730 --> 00:33:56,110
但是这个式子里的所有项  除了一项之外

546
00:33:56,320 --> 00:33:57,730
都不依赖于

547
00:33:57,910 --> 00:33:59,430
在这个式子中

548
00:33:59,660 --> 00:34:01,550
唯一的依赖

549
00:34:01,970 --> 00:34:05,570
于的一项是

550
00:34:06,460 --> 00:34:08,240
所以我们只

551
00:34:08,430 --> 00:34:11,150
需对求偏导数

552
00:34:11,340 --> 00:34:13,250
对这项中的求偏导数

553
00:34:13,500 --> 00:34:19,690
所以你得到了

554
00:34:19,690 --> 00:34:23,810
这些乘以

555
00:34:24,640 --> 00:34:27,530
好的  所以我们得到了学习规则

556
00:34:30,690 --> 00:34:34,080
可以把更新为

557
00:34:34,280 --> 00:34:45,980
减去乘上这个

558
00:34:46,530 --> 00:34:54,830
好的  这个希腊字母

559
00:34:54,990 --> 00:34:56,980
是一个算法的参数

560
00:34:57,290 --> 00:34:58,670
称之为学习速度

561
00:34:59,020 --> 00:35:01,740
它控制了

562
00:35:01,950 --> 00:35:04,120
你的步子迈的有多大

563
00:35:04,330 --> 00:35:05,850
你站在山上

564
00:35:06,070 --> 00:35:09,360
决定朝着一个方向迈一步

565
00:35:09,570 --> 00:35:12,110
所以这个参数控制了你

566
00:35:12,370 --> 00:35:15,540
在朝着最陡峭的方向下降的时候

567
00:35:15,800 --> 00:35:17,710
有迈的步子有多大

568
00:35:18,450 --> 00:35:21,970
如果你-这个参数

569
00:35:22,150 --> 00:35:24,200
通常都是手动设置的

570
00:35:24,700 --> 00:35:27,210
如果的值设的过小

571
00:35:27,380 --> 00:35:28,470
你的向着最陡峭方向下降的算法

572
00:35:28,630 --> 00:35:29,750
将会每次迈很小的一步

573
00:35:29,920 --> 00:35:31,310
这样它会花很长时间去收敛

574
00:35:31,470 --> 00:35:34,080
如果的值设的过大

575
00:35:34,240 --> 00:35:36,480
你的算法可能会越过最小值

576
00:35:36,790 --> 00:35:38,960
因为你的步子迈的太大了

577
00:35:41,920 --> 00:35:45,040
什么?

578
00:35:47,090 --> 00:35:48,340
I:能再说一遍吗?

579
00:35:48,620 --> 00:35:51,290
S:是不是什么地方少了个1/2?

580
00:35:53,510 --> 00:35:55,120
I:少了个1/2吗?

581
00:35:55,300 --> 00:35:56,440
S:我知道了 (注:缺词  听译)

582
00:35:56,600 --> 00:35:58,260
I:谢谢

583
00:35:58,430 --> 00:35:59,990
我确实可能会犯很多错误

584
00:36:01,940 --> 00:36:04,920
还有问题吗?

585
00:36:23,170 --> 00:36:30,540
好的  让我们用这个性质

586
00:36:30,710 --> 00:36:31,790
生成一个算法

587
00:36:31,960 --> 00:36:33,320
这里  我用只有一个

588
00:36:33,510 --> 00:36:35,270
训练样本的算法

589
00:36:36,290 --> 00:36:38,490
派生出更为通用的m个训练样本的算法

590
00:36:38,700 --> 00:36:40,390
这是梯度下降算法就变成了这样的形式

591
00:36:41,290 --> 00:36:43,990
我们会不断重复

592
00:36:44,160 --> 00:36:48,720
这一步骤直到算法收敛

593
00:36:49,210 --> 00:36:52,770
好的  将更新为

594
00:36:53,420 --> 00:36:57,180
我将要写出m个样本

595
00:36:57,350 --> 00:36:59,760
而不是1个样本的正确的公式

596
00:37:01,100 --> 00:37:02,150
更新

597
00:37:02,350 --> 00:37:04,000
减去乘以i

598
00:37:04,150 --> 00:37:06,240
从1到m的和

599
00:37:28,920 --> 00:37:33,020
好的  我不会证明它

600
00:37:33,230 --> 00:37:36,610
但是你们可以

601
00:37:36,800 --> 00:37:38,260
回去自己验证一下

602
00:37:38,670 --> 00:37:43,710
是不是能够得到这个求和式

603
00:37:43,890 --> 00:37:46,010
这实际上是对求偏导的结果

604
00:37:46,210 --> 00:37:48,860
的定义为使用m个

605
00:37:49,020 --> 00:37:51,950
训练样本下的定义

606
00:37:53,600 --> 00:37:56,390
好的  我下面要展示

607
00:37:56,560 --> 00:37:57,910
请切换回电脑的显示

608
00:37:58,090 --> 00:37:59,390
我要向你们展示

609
00:37:59,550 --> 00:38:01,250
算法执行的情形

610
00:38:03,920 --> 00:38:06,380
实际上对于

611
00:38:06,550 --> 00:38:08,410
线性规约问题

612
00:38:08,590 --> 00:38:09,700
或者说对于这种通常的平方函数

613
00:38:09,870 --> 00:38:11,160
就像我们今天看到的那样

614
00:38:11,590 --> 00:38:14,500
函数实际上并不想我给你们展示的

615
00:38:14,700 --> 00:38:16,120
这个图形那样复杂

616
00:38:16,290 --> 00:38:17,830
复杂到会有多个局部最优值

617
00:38:18,210 --> 00:38:19,240
实际上

618
00:38:19,410 --> 00:38:21,540
可以证明这种通常的平方函数  -

619
00:38:21,720 --> 00:38:23,870
仅仅是一个

620
00:38:23,870 --> 00:38:25,590
二次函数

621
00:38:26,190 --> 00:38:28,260
所以我们会得到一个漂亮的碗状形体

622
00:38:28,520 --> 00:38:31,100
像你这里看到的一样

623
00:38:31,420 --> 00:38:33,510
它只有一个全局最小值

624
00:38:33,690 --> 00:38:35,050
没有其他的局部最优值

625
00:38:35,890 --> 00:38:37,870
当你执行梯度下降算法的时候

626
00:38:38,100 --> 00:38:40,580
这里是函数J的等高线

627
00:38:40,820 --> 00:38:43,700
所以这个碗状形体的

628
00:38:43,880 --> 00:38:45,530
等高线将会是椭圆形

629
00:38:45,870 --> 00:38:50,820
如果你用这个算法进行梯度下降

630
00:38:51,020 --> 00:38:52,010
这将是你所看到的

631
00:38:52,200 --> 00:38:53,680
我们看一下  先初始化这些参数

632
00:38:53,980 --> 00:38:55,620
比如说  随机选取这个

633
00:38:55,810 --> 00:38:57,440
用十字星号标识的位置

634
00:38:57,620 --> 00:38:59,560
那个右上角的十字星号

635
00:38:59,840 --> 00:39:01,670
梯度下降算法经过第一次迭代之后

636
00:39:01,880 --> 00:39:06,330
由于改变了参数的空间

637
00:39:08,260 --> 00:39:10,540
所以一次梯度下降之后会得到这个结果

638
00:39:11,020 --> 00:39:12,850
两次  三次

639
00:39:12,850 --> 00:39:15,150
四次  五次  等等

640
00:39:15,580 --> 00:39:18,340
你看  它最后很容易地

641
00:39:18,530 --> 00:39:20,980
快速收敛到了的全局最小值

642
00:39:21,680 --> 00:39:26,460
好的  这是一个这类线性假设的

643
00:39:26,870 --> 00:39:28,980
规约所具有的一个性质

644
00:39:29,160 --> 00:39:31,730
函数没有其他的局部最优值

645
00:39:32,790 --> 00:39:34,340
有问题吗?

646
00:39:34,550 --> 00:39:36,490
S:每次的值会变吗?

647
00:39:36,670 --> 00:39:38,190
因为算法的步子一次比一次小

648
00:39:39,880 --> 00:39:41,390
I:实际上

649
00:39:42,500 --> 00:39:45,730
这和的取值无关

650
00:39:45,890 --> 00:39:47,790
这是梯度下降

651
00:39:48,000 --> 00:39:50,340
的一个性质

652
00:39:50,510 --> 00:39:52,150
当你接近局部最小值的时候

653
00:39:52,550 --> 00:39:54,050
步子确实会越来越小

654
00:39:54,220 --> 00:39:55,730
最终直到收敛

655
00:39:55,900 --> 00:40:00,920
原因是  你在更新的时候

656
00:40:01,290 --> 00:40:04,400
你会减去乘以梯度

657
00:40:04,990 --> 00:40:07,080
当你达到局部最小值的时候

658
00:40:07,350 --> 00:40:09,530
梯度也会减为0

659
00:40:09,970 --> 00:40:12,260
当你接近局部最小值的时候

660
00:40:12,610 --> 00:40:14,790
在局部最小值处  梯度是0

661
00:40:15,010 --> 00:40:16,290
所以当你接近局部最小值的时候

662
00:40:16,480 --> 00:40:18,440
梯度也会越来越小

663
00:40:18,950 --> 00:40:21,820
所以当你逐渐接近局部最小值的时候

664
00:40:22,010 --> 00:40:24,820
梯度下降的每一步都会自动变得越来越小

665
00:40:25,500 --> 00:40:27,300
明白吗?

666
00:40:30,050 --> 00:40:32,430
这里是同样的点状图

667
00:40:33,330 --> 00:40:36,130
实际上是房价数据的点状图

668
00:40:36,590 --> 00:40:39,530
这里  把参数初始化

669
00:40:39,750 --> 00:40:41,950
为一个0向量

670
00:40:42,150 --> 00:40:45,020
所以底部的这条蓝线表示了

671
00:40:45,260 --> 00:40:48,840
初始参数下的假设

672
00:40:49,030 --> 00:40:52,070
初始的都是0

673
00:40:52,290 --> 00:40:53,460
所以假设预测出的

674
00:40:53,650 --> 00:40:56,410
所有价格都是0

675
00:40:56,860 --> 00:40:59,350
梯度下降算法的一次迭代之后

676
00:40:59,570 --> 00:41:01,360
你得到了这条蓝线

677
00:41:01,770 --> 00:41:03,430
两次迭代之后

678
00:41:03,730 --> 00:41:06,330
三次  四次  五次  几次迭代之后

679
00:41:06,730 --> 00:41:08,880
对不起  算法收敛了

680
00:41:09,100 --> 00:41:13,500
你现在得到了这组数据的最小二乘拟合

681
00:41:14,900 --> 00:41:21,800
好的  让我们切换回黑板

682
00:41:22,340 --> 00:41:27,060
有问题吗?什么?

683
00:41:38,500 --> 00:41:39,400
I:是的

684
00:41:39,550 --> 00:41:40,870
S:收敛是否意味着

685
00:41:41,040 --> 00:41:44,280
值相同或者大致相同?

686
00:41:44,520 --> 00:41:45,500
I:好的

687
00:41:45,670 --> 00:41:49,880
这有点类似于一个问题

688
00:41:50,050 --> 00:41:51,160
关于你该如何检测收敛

689
00:41:51,330 --> 00:41:54,030
有几种不同的方式来检测收敛

690
00:41:54,250 --> 00:41:56,540
一种是检验两次迭代

691
00:41:56,730 --> 00:41:58,180
看两次迭代中是否改变了很多

692
00:41:58,370 --> 00:42:00,600
如果在两次迭代中没怎么改变

693
00:42:00,820 --> 00:42:02,650
你或许就可以说算法有可能收敛了

694
00:42:03,060 --> 00:42:05,800
更常用的方法

695
00:42:05,980 --> 00:42:07,740
是检验的值

696
00:42:07,960 --> 00:42:11,620
如果-或者你试图最小化的量 

697
00:42:11,790 --> 00:42:13,700
不再发生很大的改变

698
00:42:13,880 --> 00:42:16,060
你也许就可以认为它收敛了

699
00:42:16,300 --> 00:42:18,550
这些方法都有点类似于

700
00:42:18,730 --> 00:42:20,250
标准的启发式或者经验做法

701
00:42:20,460 --> 00:42:23,190
经常被用来检测梯度下降是否收敛

702
00:42:23,480 --> 00:42:25,960
什么?

703
00:42:50,310 --> 00:42:53,250
我明白了 你的问题是

704
00:42:53,440 --> 00:42:57,530
梯度下降算法怎样环视一周

705
00:42:57,710 --> 00:42:59,300
并且选取下降最陡峭的方向

706
00:43:00,910 --> 00:43:02,640
实际上

707
00:43:02,820 --> 00:43:04,270
我不太确定我能否回答你的第二个问题

708
00:43:04,420 --> 00:43:07,670
但是实际上  当你站在山上

709
00:43:07,940 --> 00:43:11,830
而且你-实际上当你计算

710
00:43:12,000 --> 00:43:13,200
这个函数的梯度的时候

711
00:43:13,380 --> 00:43:14,740
或者说当你计算这个函数的偏导数的时候

712
00:43:15,110 --> 00:43:16,560
实际上你计算得到的方向

713
00:43:16,740 --> 00:43:18,610
就是下降最陡峭的方向

714
00:43:19,270 --> 00:43:20,780
我想顺便指出

715
00:43:20,950 --> 00:43:21,810
你永远不会想

716
00:43:21,980 --> 00:43:22,800
向着相反的方向走

717
00:43:22,970 --> 00:43:24,360
因为相反的方向实际上

718
00:43:24,520 --> 00:43:26,500
是上升最陡峭的方向  对吧

719
00:43:27,660 --> 00:43:28,730
所以  实际上

720
00:43:28,950 --> 00:43:32,420
如果你有兴趣的话也许

721
00:43:32,590 --> 00:43:34,820
助教们给你多讲一些这一部分的内容

722
00:43:35,090 --> 00:43:35,680
实际上

723
00:43:35,680 --> 00:43:37,090
当你对这个函数求导的时候

724
00:43:37,330 --> 00:43:39,850
求导的结果已经告诉了你

725
00:43:40,040 --> 00:43:41,830
下降最陡峭的方向

726
00:43:42,720 --> 00:43:44,540
所以你并没有

727
00:43:44,740 --> 00:43:47,650
真正地360度环视一周

728
00:43:47,880 --> 00:43:49,690
你只是计算出了函数的偏导数

729
00:43:49,870 --> 00:43:51,850
而那恰恰就是下降最陡峭的方向

730
00:43:54,550 --> 00:43:55,770
也许助教在周五的时候

731
00:43:55,990 --> 00:43:58,430
会多讲一些这方面的内容

732
00:44:02,670 --> 00:44:25,140
好的  让我们继续来给这个算法

733
00:44:25,290 --> 00:44:28,160
起一个专门的名字

734
00:44:28,410 --> 00:44:29,950
这个算法实际上被称为

735
00:44:30,130 --> 00:44:33,690
批梯度下降算法

736
00:44:38,710 --> 00:44:41,410
batch这个词并不是一个好的词

737
00:44:41,640 --> 00:44:43,350
它是指梯度下降算法的

738
00:44:43,540 --> 00:44:45,480
每一次迭代都需要

739
00:44:45,700 --> 00:44:48,330
遍历整个训练集合

740
00:44:48,500 --> 00:44:50,460
因为你需要基于你的

741
00:44:50,670 --> 00:44:52,580
m个训练样本进行求和

742
00:44:52,780 --> 00:44:59,240
梯度下降算法工作的很好  我经常会使用

743
00:44:59,500 --> 00:45:02,870
实际上你有时会遇到

744
00:45:03,050 --> 00:45:04,590
非常非常大的训练集合

745
00:45:04,770 --> 00:45:06,220
想象一下你的训练集合

746
00:45:06,410 --> 00:45:09,100
不仅仅包括47幢Portland  Oregon的房屋

747
00:45:09,330 --> 00:45:10,060
如果你有  比如说

748
00:45:10,230 --> 00:45:11,640
美国普查数据库或一些类似的东西

749
00:45:11,840 --> 00:45:13,460
在类似于美国普查数据这样的数据库中

750
00:45:13,660 --> 00:45:15,460
你经常会需要处理

751
00:45:15,650 --> 00:45:16,900
几十万甚至几亿的训练样本

752
00:45:19,170 --> 00:45:22,610
如果m是一个几百万的数

753
00:45:23,160 --> 00:45:24,750
并且你要执行一个批梯度下降算法

754
00:45:24,930 --> 00:45:26,570
这意味着每次梯度下降

755
00:45:26,750 --> 00:45:28,520
你都需要对j从1

756
00:45:28,750 --> 00:45:31,610
到一百万进行一个求和

757
00:45:31,820 --> 00:45:33,940
你的程序就需要检测

758
00:45:34,160 --> 00:45:35,720
数量如此之多的训练样本

759
00:45:35,940 --> 00:45:38,240
甚至你还没有能够

760
00:45:38,440 --> 00:45:40,210
完成值下降的第一步

761
00:45:41,130 --> 00:45:43,430
所以如果你有一个

762
00:45:44,310 --> 00:45:45,770
很大的训练集合

763
00:45:46,080 --> 00:45:49,420
那么你应该使用另外一个

764
00:45:50,850 --> 00:45:53,820
称之为随机梯度下降的算法

765
00:46:00,490 --> 00:46:02,630
有时我也称它为增量梯度下降

766
00:46:03,170 --> 00:46:05,400
这个算法如下所示

767
00:46:07,170 --> 00:46:09,860
再一次地  它会重复

768
00:46:13,470 --> 00:46:15,710
直到算法收敛 j从1迭代到m

769
00:46:22,060 --> 00:46:23,770
每一次迭代内部会用类似于

770
00:46:23,940 --> 00:46:25,490
梯度下降的方式更新参数的值

771
00:46:25,660 --> 00:46:29,820
最后算法结束的时候仅仅会使用j个样本

772
00:46:55,710 --> 00:46:57,780
像往常一样

773
00:46:57,970 --> 00:47:00,130
你需要更新所有的参数

774
00:47:00,350 --> 00:47:03,370
对所有i进行更新

775
00:47:05,470 --> 00:47:08,100
对参数向量的

776
00:47:08,290 --> 00:47:09,310
所有第i个位置

777
00:47:09,480 --> 00:47:11,280
按这个方式进行更新

778
00:47:13,360 --> 00:47:15,500
这个算法的好处是

779
00:47:17,140 --> 00:47:20,380
为了开始学习

780
00:47:20,560 --> 00:47:22,580
为了开始修改参数

781
00:47:23,370 --> 00:47:25,480
你仅仅需要查看第一个训练样本

782
00:47:25,670 --> 00:47:27,220
你应该查看你的第一训练样本

783
00:47:27,410 --> 00:47:30,390
并且利用第一个

784
00:47:30,560 --> 00:47:32,510
训练样本进行更新

785
00:47:32,720 --> 00:47:35,250
之后你需要使用第二个训练样本

786
00:47:35,450 --> 00:47:36,700
执行下一次更新

787
00:47:36,880 --> 00:47:38,330
这样  你调整参数的速度会快得多

788
00:47:38,490 --> 00:47:41,480
因为你不需要在调整之前

789
00:47:41,710 --> 00:47:45,740
遍历所有的

790
00:47:45,930 --> 00:47:48,020
美国普查数据库中的数据

791
00:47:48,920 --> 00:47:53,770
我们看一看  对于大规模的数据集

792
00:47:54,100 --> 00:47:57,060
随机梯度下降算法通常会快得多

793
00:47:57,590 --> 00:48:01,050
但是随机梯度下降算法

794
00:48:01,280 --> 00:48:04,460
不会精确地收敛到

795
00:48:04,630 --> 00:48:06,310
全局的最小值

796
00:48:06,590 --> 00:48:12,040
所以如果你的函数图形的等高线是这样的

797
00:48:12,830 --> 00:48:14,770
那么当你执行随机梯度下降算法时

798
00:48:14,970 --> 00:48:16,710
你的位置有点类似于像这样徘徊

799
00:48:17,360 --> 00:48:19,660
有可能偶尔你会向高处走

800
00:48:20,010 --> 00:48:22,500
但是你的参数总体趋向于

801
00:48:22,690 --> 00:48:25,650
向着全局最小值附近徘徊

802
00:48:25,850 --> 00:48:27,390
但是可能会在全局

803
00:48:27,570 --> 00:48:29,170
最小值附近一直徘徊

804
00:48:29,630 --> 00:48:33,350
通常得到的参数值

805
00:48:33,930 --> 00:48:38,540
能够很接近全局最小值

806
00:48:38,900 --> 00:48:41,060
这已经足够了 实际上

807
00:48:41,250 --> 00:48:43,980
这个算法通常比批梯度下降算法快得多

808
00:48:44,150 --> 00:48:46,400
尤其是当你有一个大规模训练集合的时候

809
00:48:54,190 --> 00:48:57,370
好的  我要擦几块黑板

810
00:48:57,580 --> 00:48:58,330
当我擦黑板的时候

811
00:48:58,490 --> 00:48:59,820
你们可以看一下这些等式

812
00:48:59,990 --> 00:49:01,560
当我擦完黑板的时候

813
00:49:01,750 --> 00:49:03,050
我会来看看你们有什么问题

814
00:49:43,290 --> 00:49:45,790
好的  你们关于我讲的这些有什么问题吗?

815
00:49:46,570 --> 00:49:50,690
S:你仅仅重新安排了

816
00:49:50,860 --> 00:49:54,390
计算的顺序吗?

817
00:49:54,580 --> 00:49:56,800
你是不是用第一个训练样本

818
00:49:57,000 --> 00:49:59,950
然后更新所有的  之后开始下一步

819
00:50:00,170 --> 00:50:03,680
开始用第二个训练样本

820
00:50:03,890 --> 00:50:05,500
开始更新所有的  之后再开始下一步?

821
00:50:06,060 --> 00:50:08,050
你是这样算的吗?

822
00:50:08,880 --> 00:50:10,140
I:我们看一下

823
00:50:10,310 --> 00:50:11,850
我开始看第一个训练样本

824
00:50:12,070 --> 00:50:13,430
然后走一步

825
00:50:13,610 --> 00:50:17,260
之后使用新的

826
00:50:17,430 --> 00:50:20,190
修改过的参数数组

827
00:50:20,400 --> 00:50:22,560
进行第二次梯度下降更新操作

828
00:50:22,740 --> 00:50:24,670
之后继续

829
00:50:25,340 --> 00:50:26,970
明白吗?

830
00:50:27,730 --> 00:50:29,440
S:所以每次更新所有时

831
00:50:29,610 --> 00:50:31,100
你只是用了

832
00:50:31,250 --> 00:50:32,170
I:一个训练样本

833
00:50:32,320 --> 00:50:33,650
S:一个训练样本

834
00:50:39,910 --> 00:50:41,240
I:我们看一下

835
00:50:44,260 --> 00:50:47,680
我相信这个理论能很好地支持

836
00:50:48,340 --> 00:50:50,990
这个理论能够支持它

837
00:50:51,220 --> 00:50:53,830
我忘记那个定理是怎么说的了

838
00:50:59,500 --> 00:51:06,540
很好 到现在为止

839
00:51:06,780 --> 00:51:09,880
我已经介绍一个迭代算法

840
00:51:10,480 --> 00:51:14,630
用来求的最小值

841
00:51:16,130 --> 00:51:18,810
实际上对于这类特定的

842
00:51:19,000 --> 00:51:21,620
最小二乘回归问题

843
00:51:21,830 --> 00:51:23,320
或者普通的最小二乘问题

844
00:51:23,710 --> 00:51:24,720
实际上存在着

845
00:51:24,910 --> 00:51:27,120
其它的方法使取到最小值

846
00:51:27,390 --> 00:51:30,780
这种方法可以给出

847
00:51:30,970 --> 00:51:32,250
参数向量的解析表达式

848
00:51:32,430 --> 00:51:34,490
这样为了求参数的值就不需要进行迭代了

849
00:51:35,080 --> 00:51:37,640
我知道你们中的一部分人也许

850
00:51:37,850 --> 00:51:39,670
已经看过我一会儿要讲的东西

851
00:51:39,840 --> 00:51:43,330
比如在本科生的线性代数课上

852
00:51:43,650 --> 00:51:50,250
会需要大量的推导

853
00:51:50,650 --> 00:51:51,360
并且需要

854
00:51:51,360 --> 00:51:52,770
写很多的代数公式

855
00:51:53,370 --> 00:51:56,300
我想教你们一种方法

856
00:51:56,560 --> 00:51:59,780
只要几行公式就可以

857
00:52:00,380 --> 00:52:02,530
推出的解析表达式

858
00:52:03,470 --> 00:52:04,290
为了使用这种方法

859
00:52:04,520 --> 00:52:05,870
我需要引入一种新的

860
00:52:06,090 --> 00:52:08,550
用于矩阵求导的符号  实际上

861
00:52:08,780 --> 00:52:13,550
我将要定义的这个符号

862
00:52:13,780 --> 00:52:16,400
在我的个人工作中非常有用

863
00:52:16,600 --> 00:52:17,760
我一直

864
00:52:17,760 --> 00:52:19,310
都在用它

865
00:52:19,580 --> 00:52:21,410
使用这个符号

866
00:52:21,640 --> 00:52:23,340
你可以对矩阵求导

867
00:52:23,660 --> 00:52:26,940
所以你用几行公式就可以解决

868
00:52:27,120 --> 00:52:28,570
求最小值的问题  而不用

869
00:52:28,760 --> 00:52:31,140
进行长达几页的矩阵运算和求导运算

870
00:52:31,950 --> 00:52:34,060
所以接下来我们先定义这个新的符号

871
00:52:34,270 --> 00:52:36,400
之后我们会继续开始研究最小化的问题

872
00:52:37,240 --> 00:52:40,270
给定一个函数J

873
00:52:41,320 --> 00:52:44,020
由于J是一个关于参数数组的函数

874
00:52:44,550 --> 00:52:47,200
好的  我要定义J的

875
00:52:47,410 --> 00:52:49,880
梯度关于的导数

876
00:52:51,140 --> 00:53:02,400
它自己也是一个向量 好的

877
00:53:03,080 --> 00:53:05,500
这是一个

878
00:53:05,760 --> 00:53:07,350
n+1维的向量

879
00:53:07,590 --> 00:53:10,680
是一个n+1维的向量

880
00:53:10,890 --> 00:53:12,650
下标从0到n

881
00:53:12,940 --> 00:53:13,970
所以我可以将导数

882
00:53:14,150 --> 00:53:16,130
定义成这边的形式

883
00:53:17,010 --> 00:53:20,400
好的  我们可以重新

884
00:53:24,330 --> 00:53:26,480
把我们的梯度下降算法写成下面这样

885
00:53:26,710 --> 00:53:28,440
这是批梯度下降算法的形式

886
00:53:28,880 --> 00:53:30,330
我们把梯度下降写成

887
00:53:31,010 --> 00:53:32,950
是对于参数向量的更新

888
00:53:33,250 --> 00:53:34,830
注意这里现在没有下标

889
00:53:35,090 --> 00:53:37,420
I更新成原来的参数向量

890
00:53:37,610 --> 00:53:44,230
减去乘以梯度

891
00:53:45,090 --> 00:53:50,020
好的  在这个等式中所有的这些量

892
00:53:50,310 --> 00:53:56,300
还有这个梯度向量

893
00:53:56,910 --> 00:53:58,850
所有这些都是n+1维的数组

894
00:54:01,480 --> 00:54:04,150
我已经把黑板用乱了  是吗?

895
00:54:21,240 --> 00:54:23,290
更为概括一些

896
00:54:24,850 --> 00:54:28,240
如果你有一个函数f

897
00:54:28,450 --> 00:54:39,110
从矩阵空间映射到  比如说

898
00:54:39,290 --> 00:54:41,770
从m*n的矩阵空间

899
00:54:41,960 --> 00:54:43,180
映射到实数空间

900
00:54:43,370 --> 00:54:46,220
如果说你有一个函数  f(A)

901
00:54:47,380 --> 00:54:51,430
A是一个m*n的矩阵

902
00:54:52,360 --> 00:54:54,720
这个函数是从矩阵

903
00:54:54,880 --> 00:54:55,670
映射到实数的

904
00:54:55,840 --> 00:54:57,710
它以矩阵作为输入

905
00:54:58,010 --> 00:55:00,830
让我们定义f

906
00:55:01,020 --> 00:55:03,790
关于矩阵A的导数

907
00:55:04,570 --> 00:55:06,230
现在我要考虑f关于

908
00:55:06,410 --> 00:55:08,510
它的输入矩阵的梯度

909
00:55:08,780 --> 00:55:12,240
我要将它也定义成一个矩阵

910
00:55:35,880 --> 00:55:38,200
好的  所以f相对于A的导数

911
00:55:38,370 --> 00:55:41,210
本身也是一个矩阵

912
00:55:41,600 --> 00:55:44,770
矩阵中包含了f关于A的

913
00:55:44,940 --> 00:55:47,720
每个元素的偏导数

914
00:55:58,910 --> 00:56:04,560
另外一个定义是  如果A是一个方阵

915
00:56:04,810 --> 00:56:09,350
即如果A是一个n*n的矩阵

916
00:56:09,600 --> 00:56:11,310
其行数等于列数

917
00:56:11,840 --> 00:56:15,360
我就可以将A的迹

918
00:56:15,540 --> 00:56:17,600
定义为A的对角元素之和

919
00:56:18,770 --> 00:56:25,270
就是基于i对求和

920
00:56:26,390 --> 00:56:28,110
对于你们那些之前没有见过

921
00:56:28,300 --> 00:56:30,020
这类运算符的人来说

922
00:56:30,200 --> 00:56:32,570
你们可以认为矩阵A的迹

923
00:56:32,800 --> 00:56:35,270
是一个应用在矩阵A上的运算符

924
00:56:35,650 --> 00:56:36,920
但是通常会

925
00:56:37,160 --> 00:56:38,260
把括号省略掉

926
00:56:38,450 --> 00:56:40,250
所以我通常都这样表示矩阵A的迹

927
00:56:40,440 --> 00:56:42,660
它表示A的对角元素之和

928
00:56:44,420 --> 00:56:50,350
这里我讲列举一些关于

929
00:56:50,520 --> 00:56:52,280
迹运算符和导数的一些结论

930
00:56:52,490 --> 00:56:54,140
我只会把它们写出来而不会证明

931
00:56:54,310 --> 00:56:55,800
助教会在讨论课上

932
00:56:55,990 --> 00:56:57,580
为你们证明

933
00:56:57,770 --> 00:57:00,550
或者你们可以

934
00:57:00,730 --> 00:57:02,840
回去自己证明

935
00:57:03,030 --> 00:57:09,760
可以证明  给定两个矩阵A和B

936
00:57:09,960 --> 00:57:11,880
A*B的迹

937
00:57:12,040 --> 00:57:16,660
等于B*A的迹

938
00:57:16,860 --> 00:57:18,180
好的  我不会证明这个式子

939
00:57:18,370 --> 00:57:19,600
你们应该可以回去自己证明

940
00:57:19,780 --> 00:57:22,520
不是很难

941
00:57:24,160 --> 00:57:29,040
类似地  对于三个矩阵乘积的迹

942
00:57:29,260 --> 00:57:31,060
你可以将末尾的矩阵

943
00:57:31,260 --> 00:57:34,100
循环地移到前面

944
00:57:34,290 --> 00:57:35,620
所以  ABC的迹

945
00:57:35,900 --> 00:57:38,260
等于CAB的迹

946
00:57:39,050 --> 00:57:40,290
把末尾的C

947
00:57:40,460 --> 00:57:41,770
移到前面

948
00:57:41,960 --> 00:57:46,060
它同样等于BCA的迹

949
00:57:46,260 --> 00:57:48,770
把末尾的B移到前面

950
00:57:49,330 --> 00:58:03,450
好的 假设你有一个函数f(A)

951
00:58:03,630 --> 00:58:05,660
将它定义成AB的迹

952
00:58:06,660 --> 00:58:09,440
好的  迹是一个实数

953
00:58:09,640 --> 00:58:11,900
所以AB的迹是一个以矩阵A为输入

954
00:58:12,080 --> 00:58:14,990
以实数为输出的函数

955
00:58:16,300 --> 00:58:20,380
所以这个函数关于

956
00:58:20,550 --> 00:58:30,440
A的导数将会等于

957
00:58:30,630 --> 00:58:31,770
这个结论

958
00:58:31,960 --> 00:58:33,770
你可以自己证明

959
00:58:33,940 --> 00:58:35,550
回顾一下之前关于迹

960
00:58:35,740 --> 00:58:37,600
和矩阵求导的定义

961
00:58:37,800 --> 00:58:39,900
我不打算证明它  你们应该可以证出来

962
00:58:40,710 --> 00:58:42,790
下面是几个简单的结论

963
00:58:43,060 --> 00:58:46,590
A的迹等于的迹

964
00:58:46,800 --> 00:58:49,490
这是因为迹仅仅是对角元素的和

965
00:58:50,010 --> 00:58:51,350
对一个矩阵进行转秩不会

966
00:58:51,540 --> 00:58:53,080
改变它的对角线上的元素

967
00:58:53,520 --> 00:58:57,580
如果a是一个实数

968
00:58:57,810 --> 00:59:02,050
那么一个实数的迹还是它本身

969
00:59:02,290 --> 00:59:06,120
可以将实数想象成一个1*1的矩阵

970
00:59:06,270 --> 00:59:07,790
一个1*1的矩阵的迹

971
00:59:07,990 --> 00:59:10,200
就是这个实数

972
00:59:15,820 --> 00:59:20,370
最后是一个巧妙的结论

973
00:59:28,910 --> 00:59:30,250
对关于A求导  等于

974
00:59:30,400 --> 00:59:32,830
我同样不会证明这个结论

975
00:59:44,280 --> 00:59:47,360
这需要某些代数证明  你们可以自己证

976
00:59:48,710 --> 01:00:02,680
好的  所以我会再次用到这5个

977
01:00:02,870 --> 01:00:08,600
和迹与矩阵求导有关的关键结论

978
01:00:11,950 --> 01:00:17,040
10分钟 好的

979
01:00:17,220 --> 01:00:19,350
利用我刚才提到的这些结论

980
01:00:19,520 --> 01:00:23,780
让我们尝试对于


981
01:00:23,960 --> 01:00:26,030
最小化进行快速推导

982
01:00:26,210 --> 01:00:29,490
求出的解析表达式

983
01:00:29,690 --> 01:00:32,100
这样我们就不需要使用迭代算法了

984
01:00:33,370 --> 01:00:39,600
开始推导 我们定义矩阵X

985
01:00:39,840 --> 01:00:42,140
这个矩阵被称之为设计矩阵

986
01:00:43,090 --> 01:00:44,690
将它定义成包含了训练集中

987
01:00:44,920 --> 01:00:46,870
所有输入的矩阵

988
01:00:47,310 --> 01:00:51,440
所以是我的对应着第1个

989
01:00:51,640 --> 01:00:53,600
训练样本特征矩阵的输入矩阵

990
01:00:53,960 --> 01:00:58,260
我要将设为

991
01:00:58,460 --> 01:01:01,250
X矩阵的第1行

992
01:01:05,550 --> 01:01:08,400
将我的第2个训练样本

993
01:01:08,640 --> 01:01:10,090
设为矩阵的第2行

994
01:01:10,250 --> 01:01:11,310
以此类推

995
01:01:11,440 --> 01:01:13,470
由于我有m个训练样本

996
01:01:14,120 --> 01:01:18,040
所以这就是我的设计矩阵X

997
01:01:18,540 --> 01:01:21,350
好的  矩阵X像这样定义

998
01:01:22,110 --> 01:01:26,220
让我们用这个矩阵X

999
01:01:26,430 --> 01:01:28,710
乘上我的向量矩阵

1000
01:01:28,960 --> 01:01:31,350
接下来会用两三步来推导

1001
01:01:31,540 --> 01:01:35,470
所以X乘以-还记得如何

1002
01:01:35,690 --> 01:01:37,720
进行矩阵和向量的乘法吧

1003
01:01:37,920 --> 01:01:39,700
用这个向量

1004
01:01:39,880 --> 01:01:41,420
乘上矩阵的每一行

1005
01:01:42,260 --> 01:01:44,670
所以将会是

1006
01:01:46,420 --> 01:01:49,650
点点点

1007
01:01:49,860 --> 01:01:56,700
这显然就是

1008
01:01:56,940 --> 01:02:10,320
你对于m个

1009
01:02:10,660 --> 01:02:12,680
训练样本的假设

1010
01:02:16,270 --> 01:02:21,130
同样地我们将向量y定义为

1011
01:02:21,340 --> 01:02:26,990
所有训练集合中的数据的目标值  从到

1012
01:02:27,520 --> 01:02:29,880
好的  所以y是一个m维的向量

1013
01:02:48,760 --> 01:02:56,770
所以包含了

1014
01:02:56,970 --> 01:02:58,420
前面的黑板上写的公式

1015
01:02:58,600 --> 01:03:18,260
将会是这样的

1016
01:03:18,470 --> 01:03:19,390
现在是一个向量

1017
01:03:19,560 --> 01:03:21,020
这是一个对应着m个

1018
01:03:21,180 --> 01:03:22,520
训练样本的m维向量

1019
01:03:23,030 --> 01:03:24,440
所以我要用这个

1020
01:03:24,610 --> 01:03:27,250
向量和自己做内积

1021
01:03:27,630 --> 01:03:33,300
好的  回忆一下

1022
01:03:33,330 --> 01:03:38,060
如果z是一个向量

1023
01:03:38,260 --> 01:03:39,840
那么应该是基于

1024
01:03:39,990 --> 01:03:41,940
i对求和的结果

1025
01:03:42,130 --> 01:03:44,620
所以如果你用

1026
01:03:48,400 --> 01:03:51,250
向量和自己做内积

1027
01:03:51,860 --> 01:03:59,660
那么就会得到i从1到m求和

1028
01:03:59,820 --> 01:04:06,480
减去y  平方

1029
01:04:06,870 --> 01:04:08,810
好的  这正是向量

1030
01:04:09,070 --> 01:04:11,520
所有元素的平方之和

1031
01:04:12,410 --> 01:04:17,730
不要忘记在前面加上1/2

1032
01:04:19,470 --> 01:04:22,960
这是我们之前对的定义

1033
01:04:23,720 --> 01:04:28,180
好的 什么?

1034
01:04:33,710 --> 01:04:36,690
好的  我今天向你们灌了很多符号

1035
01:04:36,880 --> 01:04:40,630
m是训练样本数

1036
01:04:40,780 --> 01:04:43,040
训练样本数从1到m

1037
01:04:43,860 --> 01:04:46,250
特征矩阵从0到n

1038
01:04:46,440 --> 01:04:49,420
明白吗?

1039
01:04:49,720 --> 01:04:52,970
这是从1到m的加和

1040
01:04:53,670 --> 01:04:59,600
等于j从0到n

1041
01:04:59,800 --> 01:05:05,470
对进行加和

1042
01:05:06,540 --> 01:05:08,430
明白吗?

1043
01:05:08,730 --> 01:05:12,720
特征向量的下标从0到n

1044
01:05:12,910 --> 01:05:15,690
其中  而训练样本的

1045
01:05:15,870 --> 01:05:17,960
下标从1到m

1046
01:05:22,060 --> 01:05:23,670
让我再擦一些黑板

1047
01:05:23,840 --> 01:05:25,430
你们再看一下

1048
01:05:26,310 --> 01:06:03,950
确保都弄明白了 好的  什么?

1049
01:06:09,040 --> 01:06:12,570
哦  是的  谢谢你 你是这个意思把?

1050
01:06:13,460 --> 01:06:14,980
好的  谢谢你

1051
01:06:15,130 --> 01:06:18,580
很好  第i个训练样本 还有其他的问题吗?

1052
01:06:24,900 --> 01:06:28,610
好的 我们的推导

1053
01:06:28,790 --> 01:06:30,870
就要完成了

1054
01:06:31,520 --> 01:06:34,840
我们希望求得  使得取最小值

1055
01:06:35,020 --> 01:06:38,800
通过矩阵和向量符号

1056
01:06:38,980 --> 01:06:41,710
我们将用一种非常简洁的形式表示了出来

1057
01:06:42,940 --> 01:06:44,480
为了求得

1058
01:06:44,480 --> 01:06:46,010
使得取最小值

1059
01:06:47,690 --> 01:06:50,040
我们需要将关于的导数

1060
01:06:50,270 --> 01:06:52,350
设成0向量

1061
01:06:54,640 --> 01:06:58,240
然后根据这个式子解出

1062
01:06:59,220 --> 01:07:07,640
好的  所以这一项

1063
01:07:07,810 --> 01:07:18,570
关于的导数等于

1064
01:07:21,180 --> 01:07:23,490
我应该提过  很多步骤我会很快地过掉

1065
01:07:23,670 --> 01:07:25,340
不会去证明它们

1066
01:07:25,520 --> 01:07:27,080
很显然1/2乘以一个式子的导数

1067
01:07:27,250 --> 01:07:29,130
等于对这个式子求导再乘以1/2

1068
01:07:29,300 --> 01:07:30,560
然后我交换了1/2

1069
01:07:30,770 --> 01:07:32,290
和求导运算符的位置

1070
01:07:32,440 --> 01:07:33,540
这些结论是正确的

1071
01:07:33,690 --> 01:07:35,530
但是你们回去之后

1072
01:07:35,680 --> 01:07:37,420
应该好好看看讲义

1073
01:07:37,570 --> 01:07:39,100
确保你们理解为什么

1074
01:07:39,260 --> 01:07:41,040
每个步骤是正确的

1075
01:07:41,190 --> 01:07:42,720
我会很快地过掉这些步骤

1076
01:07:42,960 --> 01:07:45,040
你们可以回去之后参照着

1077
01:07:45,200 --> 01:07:47,460
讲义一步一步仔细看

1078
01:07:47,650 --> 01:07:50,950
好的  所以它等于

1079
01:07:51,180 --> 01:07:53,070
我要展开这个二次函数

1080
01:07:53,410 --> 01:08:09,450
所以它将会是这样的  好的

1081
01:08:11,290 --> 01:08:13,490
这有点类似于将一个两项相乘的

1082
01:08:13,670 --> 01:08:15,100
二次函数通过逐项相乘的方式展开

1083
01:08:15,280 --> 01:08:17,840
再说一次  如果你不是很确定我是

1084
01:08:18,010 --> 01:08:21,450
如何推导的话可以回去慢慢看

1085
01:08:21,820 --> 01:08:26,330
这个矩阵和矩阵相乘的部分

1086
01:08:26,560 --> 01:08:29,010
对  这一项

1087
01:08:29,220 --> 01:08:32,410
这是一个实数

1088
01:08:32,650 --> 01:08:35,670
一个实数的迹是它本身

1089
01:08:36,690 --> 01:08:41,910
哦  谢谢你  Dan 很好

1090
01:08:42,460 --> 01:08:44,570
括号里的式子是

1091
01:08:44,750 --> 01:08:46,670
是一个实数

1092
01:08:47,360 --> 01:08:49,400
一个实数的迹

1093
01:08:49,590 --> 01:08:50,840
还是它本身

1094
01:08:51,040 --> 01:08:52,070
所以你可以在前面

1095
01:08:52,240 --> 01:08:54,160
加上一个求迹的运算符  而不会有任何影响

1096
01:08:56,130 --> 01:09:01,710
所以它等于1/2乘以

1097
01:09:01,870 --> 01:09:05,920
这一项的迹对求导

1098
01:09:06,100 --> 01:09:08,560
根据前面第二个排列的性质

1099
01:09:08,730 --> 01:09:11,300
你可以把末尾的移到前面

1100
01:09:11,800 --> 01:09:12,960
这里将是

1101
01:09:13,150 --> 01:09:17,130
减去迹

1102
01:09:18,510 --> 01:09:27,340
对于的

1103
01:09:27,500 --> 01:09:30,480
导数

1104
01:09:30,700 --> 01:09:33,760
我要把这项移到

1105
01:09:34,880 --> 01:09:38,580
哦  对不起

1106
01:09:39,070 --> 01:09:42,550
实际上  这是个实数

1107
01:09:43,520 --> 01:09:46,350
并且实数的转秩就是它本身

1108
01:09:46,970 --> 01:09:48,440
所以  对一个实数取转秩

1109
01:09:48,630 --> 01:09:50,390
不会有任何影响

1110
01:09:50,580 --> 01:09:52,640
所以让我们继续  并在这里取转秩

1111
01:09:53,170 --> 01:09:55,020
一个实数的转秩

1112
01:09:55,190 --> 01:09:57,170
还是同样的实数

1113
01:09:57,340 --> 01:09:59,230
所以可以

1114
01:10:00,120 --> 01:10:01,340
减去转秩的迹

1115
01:10:01,540 --> 01:10:07,820
这里是  减去

1116
01:10:10,310 --> 01:10:13,260
好的  最后一项

1117
01:10:13,450 --> 01:10:15,050
并不依赖于

1118
01:10:15,260 --> 01:10:17,300
所以当我对最后一项相对于求导时

1119
01:10:17,480 --> 01:10:20,140
结果是0 所以可以扔掉最后一项

1120
01:10:22,760 --> 01:10:31,630
最后

1121
01:10:32,030 --> 01:10:39,030
对的迹关于求导

1122
01:10:39,570 --> 01:10:42,680
我要用到我之前写的一个

1123
01:10:42,860 --> 01:10:44,580
没有经过证明的结论

1124
01:10:44,950 --> 01:10:47,860
我令这一部分为A

1125
01:10:48,510 --> 01:10:50,700
这里加上一个单位矩阵

1126
01:10:50,880 --> 01:10:54,390
所以这里可以看成

1127
01:10:54,740 --> 01:10:56,740
使用之前写过的结论

1128
01:10:56,920 --> 01:10:58,440
你可以在讲义上找到

1129
01:10:58,700 --> 01:11:01,030
它还在之前的某块黑板上

1130
01:11:01,220 --> 01:11:08,420
这个式子等于

1131
01:11:19,280 --> 01:11:26,060
这是C  A  B

1132
01:11:26,220 --> 01:11:28,190
B是单位矩阵

1133
01:11:28,640 --> 01:11:33,080
可以忽略  加上

1134
01:11:33,250 --> 01:11:38,820
这里是

1135
01:11:38,990 --> 01:11:41,210
A  	再一次乘上一个

1136
01:11:41,370 --> 01:11:43,490
可以忽略的单位矩阵

1137
01:11:43,690 --> 01:11:46,320
由于是对称的

1138
01:11:49,490 --> 01:11:52,130
所以等于C

1139
01:11:52,320 --> 01:11:58,510
类似的

1140
01:12:01,230 --> 01:12:11,360
对的迹关于求导

1141
01:12:12,090 --> 01:12:14,380
这相当于对BA的迹

1142
01:12:14,860 --> 01:12:17,880
相对于A求导

1143
01:12:18,350 --> 01:12:21,780
最后的结果就是

1144
01:12:21,950 --> 01:12:25,760
这相当于

1145
01:12:26,340 --> 01:12:46,900
这又是基于之前提到的一个结论

1146
01:12:47,090 --> 01:12:50,090
所以如果把这些结果代入

1147
01:12:50,260 --> 01:12:56,280
我们会因此发现  导数-哇

1148
01:13:06,790 --> 01:13:08,920
这块黑板太糟了

1149
01:13:09,100 --> 01:13:10,810
所以如果你将这些结果

1150
01:13:12,910 --> 01:13:13,750
代入

1151
01:13:13,750 --> 01:13:15,870
对J的求导

1152
01:13:16,020 --> 01:13:22,980
你会发现关于的导数等于:

1153
01:13:33,620 --> 01:13:37,120
最后结果就是

1154
01:13:40,250 --> 01:13:45,260
好的  我们令这个式子等于0向量

1155
01:13:45,690 --> 01:13:52,880
最终得到了这个式子

1156
01:14:03,030 --> 01:14:04,910
它被称为正规方程组

1157
01:14:05,890 --> 01:14:10,820
我们现在可以给出这个

1158
01:14:11,040 --> 01:14:20,360
关于的方程组解的解析表达式了

1159
01:14:20,530 --> 01:14:22,530
最后结果是

1160
01:14:23,310 --> 01:14:25,170
这给了我们另外一种方式来求解

1161
01:14:25,360 --> 01:14:29,910
最小二乘拟合问题  使得我们不再需要

1162
01:14:30,090 --> 01:14:32,400
使用类似于梯度下降这样的迭代算法

1163
01:14:32,680 --> 01:14:34,460
好的  通过使用矩阵向量的符号  我认为

1164
01:14:36,500 --> 01:14:41,060
我不太确定  我认为我们只用了

1165
01:14:41,900 --> 01:14:44,180
大概十分钟时间就整个事情说清楚了

1166
01:14:44,340 --> 01:14:45,640
但是如果我们使用大量的

1167
01:14:45,810 --> 01:14:47,160
代数公式来表达的话绝对办不到

1168
01:14:47,320 --> 01:14:50,870
好的  你们中的很多人看起来神志恍惚

1169
01:14:52,610 --> 01:14:55,380
但是这是我们学习的第一课

1170
01:14:55,540 --> 01:14:57,320
你们不觉得兴奋吗?

1171
01:14:57,530 --> 01:14:59,310
在今天下课之前

1172
01:15:00,590 --> 01:15:01,970
还有问题吗?

1173
01:15:02,210 --> 01:15:04,630
能再说一遍吗?

1174
01:15:08,870 --> 01:15:10,110
什么逆?

1175
01:15:12,610 --> 01:15:14,180
伪逆

1176
01:15:14,340 --> 01:15:15,380
伪逆?

1177
01:15:15,540 --> 01:15:16,770
伪逆

1178
01:15:16,930 --> 01:15:18,340
好的  实际上对于有些情况

1179
01:15:18,500 --> 01:15:20,190
如果是不可逆的

1180
01:15:20,390 --> 01:15:22,190
那么确实可以用伪逆最小化的方法

1181
01:15:22,490 --> 01:15:23,700
来解决这个问题

1182
01:15:23,880 --> 01:15:25,620
但是实际上如果不可逆

1183
01:15:26,040 --> 01:15:28,360
这通常说明你提供的特征是有依赖性的

1184
01:15:28,550 --> 01:15:31,260
例如你可能在你的训练集中

1185
01:15:31,510 --> 01:15:34,150
提供了两个同样的特征

1186
01:15:34,330 --> 01:15:36,370
事实上如果矩阵是不可逆的

1187
01:15:36,720 --> 01:15:38,180
那么可以通过伪逆的方法

1188
01:15:38,370 --> 01:15:39,690
来达到最小值

1189
01:15:39,880 --> 01:15:41,900
如果你没听懂我说的话  也不要担心

1190
01:15:42,090 --> 01:15:44,220
这通常情况下都不会成为一个问题

1191
01:15:44,980 --> 01:15:46,910
还有其他问题吗?

1192
01:15:47,280 --> 01:15:53,310
让我把它放下来

1193
01:15:53,490 --> 01:15:54,720
下课了  今天就到这里吧

1194
01:15:54,920 --> 01:15:57,480
如果你们还有其他问题  我会课后为你们解答

