1
00:00:22,650 --> 00:00:30,030
好的  欢迎回来

2
00:00:30,180 --> 00:00:34,750
我今天想

3
00:00:34,890 --> 00:00:37,470
开始新的一章

4
00:00:37,580 --> 00:00:39,120
我们今天要讲的是学习理论

5
00:00:39,240 --> 00:00:44,240
在之前  我记得是八讲

6
00:00:44,370 --> 00:00:46,830
你们已经学习了许多学习算法

7
00:00:46,920 --> 00:00:50,930
使得你们现在已经了解到了

8
00:00:51,010 --> 00:00:53,420
一些人工智能领域中

9
00:00:53,530 --> 00:00:57,340
最强大的机器学习的工具

10
00:00:57,800 --> 00:01:00,310
你们现在已经能够理论联系实际

11
00:01:00,420 --> 00:01:02,560
将那些强大的学习算法

12
00:01:02,640 --> 00:01:03,810
应用到各种问题上

13
00:01:03,930 --> 00:01:06,080
实际上  我希望你们可以

14
00:01:06,170 --> 00:01:07,740
马上开始着手于你们的项目

15
00:01:07,830 --> 00:01:12,160
你们可能记得

16
00:01:12,270 --> 00:01:14,280
我想是在第一讲的时候

17
00:01:14,370 --> 00:01:16,240
我做了一个类比

18
00:01:16,340 --> 00:01:18,640
如果你们都要努力成为一个木匠

19
00:01:18,740 --> 00:01:19,820
所以可以想象一下

20
00:01:19,930 --> 00:01:22,760
你们将要去木匠学校学习成为一名木匠

21
00:01:22,860 --> 00:01:26,060
其中你需要做的一件事就是

22
00:01:26,170 --> 00:01:28,000
要掌握一系列工具的使用技巧

23
00:01:28,110 --> 00:01:29,500
如果你们要成为一个木匠

24
00:01:29,600 --> 00:01:31,010
你们不可能每次干活时都要拿起

25
00:01:31,090 --> 00:01:32,320
一个工具箱之后在里面随意地挑选

26
00:01:32,420 --> 00:01:37,470
所以当你们的工作是切木头的时候

27
00:01:37,560 --> 00:01:38,540
你们需要知道你们要用到粗尺锯

28
00:01:38,620 --> 00:01:40,040
或者是线锯  或者是栓孔锯

29
00:01:40,160 --> 00:01:41,760
这说明要想成为一名好木匠

30
00:01:41,880 --> 00:01:44,740
熟练地掌握各种工具是至关重要的

31
00:01:44,840 --> 00:01:48,510
我接下来几讲的内容就是

32
00:01:48,650 --> 00:01:51,220
帮助你们熟练掌握

33
00:01:51,330 --> 00:01:53,310
这些你们已经学会的机器学习的工具

34
00:01:53,430 --> 00:01:55,020
明白吗?

35
00:01:55,120 --> 00:01:58,190
具体地  在下面的三讲中

36
00:01:58,300 --> 00:02:01,560
我将要深入地讲解

37
00:02:01,670 --> 00:02:03,790
不同的机器学习算法的属性

38
00:02:03,990 --> 00:02:05,200
这样可以让你们了解到

39
00:02:05,280 --> 00:02:07,020
每个算法都适用于怎样的情景

40
00:02:07,160 --> 00:02:09,150
实际上在应用机器学习算法的时候

41
00:02:09,260 --> 00:02:11,540
有一种情景是经常会遇到的

42
00:02:11,660 --> 00:02:15,940
可能有一天你们会去公司里做研究

43
00:02:16,050 --> 00:02:17,630
你们可能会用到

44
00:02:17,730 --> 00:02:19,280
曾经学过的某个机器学习算法

45
00:02:19,360 --> 00:02:20,660
你可能会用到logistic回归

46
00:02:20,770 --> 00:02:21,870
或者SVM

47
00:02:21,940 --> 00:02:22,840
或者朴素贝叶斯或者一些其他的算法

48
00:02:22,940 --> 00:02:25,280
可能因为一些奇怪的原因

49
00:02:25,390 --> 00:02:27,620
它们并不像你们想象的效果那么好

50
00:02:27,700 --> 00:02:29,650
或者可能它并不是

51
00:02:29,770 --> 00:02:36,250
向你们想象的那样工作

52
00:02:36,360 --> 00:02:38,800
对我来说

53
00:02:38,940 --> 00:02:41,020
真正能够将那些真正理解机器学习的人

54
00:02:41,130 --> 00:02:44,160
和那些仅仅读过课本

55
00:02:44,270 --> 00:02:46,270
仅仅了解数学公式的人区分开来的

56
00:02:46,380 --> 00:02:47,700
是你们是否能够学会接下来要学的内容

57
00:02:47,800 --> 00:02:49,290
如果你们选择

58
00:02:49,370 --> 00:02:51,410
使用SVM

59
00:02:51,520 --> 00:02:53,110
并且结果并不像你预期的那样

60
00:02:54,960 --> 00:02:55,510
你们知道

61
00:02:55,630 --> 00:02:57,140
下面应该怎样

62
00:02:57,260 --> 00:02:58,870
修改算法吗?

63
00:02:58,990 --> 00:03:00,310
对我来说

64
00:03:00,430 --> 00:03:02,080
这一点真正能够将

65
00:03:02,190 --> 00:03:03,480
真正理解机器学习的人

66
00:03:03,590 --> 00:03:05,460
区别于那些只会纸上谈兵的人

67
00:03:05,570 --> 00:03:07,030
他们也许只能看懂表面

68
00:03:07,170 --> 00:03:08,480
明白吗?

69
00:03:08,620 --> 00:03:10,830
所以今天我要讲的主要是

70
00:03:10,940 --> 00:03:14,370
关于学习理论的内容

71
00:03:14,480 --> 00:03:16,880
我们要开始讲一些

72
00:03:16,980 --> 00:03:17,870
和机器学习有关的理论方面的内容

73
00:03:17,980 --> 00:03:20,970
下一讲  这周的晚些时候

74
00:03:21,070 --> 00:03:24,070
我们将会关注算法对问题的拟合

75
00:03:24,200 --> 00:03:27,550
学习理论会帮助我们

76
00:03:27,670 --> 00:03:29,020
更深入地理解这些内容

77
00:03:29,120 --> 00:03:31,500
下下次课  我们会关注于一些

78
00:03:31,800 --> 00:03:35,090
应用机器学习算法的具体建议

79
00:03:35,230 --> 00:03:38,800
明白吗?

80
00:03:38,910 --> 00:03:42,160
在我开始之前还有问题吗?

81
00:03:42,260 --> 00:03:49,730
好的

82
00:03:49,840 --> 00:03:53,540
我们要讲的第一个知识点

83
00:03:53,660 --> 00:03:56,560
你们可能已经在

84
00:03:56,670 --> 00:03:57,560
第一次作业中见过了

85
00:03:58,010 --> 00:03:59,740
我在之前的课上也或多或少地提到过

86
00:03:59,850 --> 00:04:03,380
这个知识点就是偏差-方差权衡

87
00:04:03,510 --> 00:04:08,020
以我们的第一个学习算法--

88
00:04:08,130 --> 00:04:09,760
最小二乘法为例

89
00:04:09,920 --> 00:04:13,120
如果你用一条直线拟合这组数据

90
00:04:13,220 --> 00:04:14,720
显然这不会是一个很好的模型

91
00:04:14,810 --> 00:04:21,990
如果你这样做的话

92
00:04:22,130 --> 00:04:25,100
我们将这种现象称之为欠拟合

93
00:04:25,490 --> 00:04:26,530
或者说我们认为

94
00:04:26,630 --> 00:04:28,450
这样的算法的偏差很高

95
00:04:28,580 --> 00:04:30,410
因为它不能很好地拟合

96
00:04:30,540 --> 00:04:33,170
这组明显具有二次函数特征的数据

97
00:04:33,290 --> 00:04:41,680
你可以认为很高的偏差意味着一个事实

98
00:04:41,790 --> 00:04:45,100
即使你有成千上万的

99
00:04:45,210 --> 00:04:47,450
无穷多的训练数据

100
00:04:47,590 --> 00:04:49,410
算法仍然不能拟合出二次函数--

101
00:04:49,520 --> 00:04:53,850
或者说

102
00:04:53,960 --> 00:04:56,050
数据的二次结构

103
00:04:56,170 --> 00:04:57,100
所以我们认为

104
00:04:57,210 --> 00:04:59,210
这样的算法偏差很高

105
00:04:59,330 --> 00:05:02,060
然后我们再来看

106
00:05:02,180 --> 00:05:03,290
一个相反的问题

107
00:05:03,400 --> 00:05:06,450
这是同样的数据集

108
00:05:06,560 --> 00:05:16,120
如果你们用一个四次多项式函数

109
00:05:16,230 --> 00:05:20,300
拟合这组数据

110
00:05:20,410 --> 00:05:21,370
那么你会得到--

111
00:05:21,480 --> 00:05:23,140
你的曲线会精确地通过这五个点

112
00:05:23,250 --> 00:05:24,420
但是很明显

113
00:05:24,540 --> 00:05:28,280
对于这组数据的结构来说

114
00:05:28,380 --> 00:05:29,860
这仍然不是一个好模型

115
00:05:29,980 --> 00:05:35,430
我们说这个算法具有一个问题--

116
00:05:35,550 --> 00:05:38,690
对不起  这样的问题称之为过拟合

117
00:05:38,800 --> 00:05:45,990
或者也可以说这个算法的方差很高

118
00:05:46,100 --> 00:05:46,820
明白吗?

119
00:05:46,940 --> 00:05:49,870
对于过拟合和高方差的直观理解是

120
00:05:49,980 --> 00:05:53,590
算法拟合出了数据中的一些奇怪的规律

121
00:05:53,710 --> 00:05:57,020
或者说一些怪异的属性

122
00:05:57,130 --> 00:05:59,090
这组数据可能表示房屋的价格

123
00:05:59,180 --> 00:06:01,360
或者其他的数据

124
00:06:01,470 --> 00:06:02,170
通常情况下

125
00:06:02,280 --> 00:06:03,280
我们能够在这两种方案之间

126
00:06:03,400 --> 00:06:09,890
找到折衷的方案

127
00:06:10,010 --> 00:06:11,560
我们可以用一个二次函数

128
00:06:11,640 --> 00:06:12,760
虽然不能精确地匹配每一个训练样本

129
00:06:12,860 --> 00:06:15,250
但是相对于可能产生欠拟合的简单模型

130
00:06:15,370 --> 00:06:17,350
这样的模型

131
00:06:17,460 --> 00:06:19,360
能够匹配数据集合的一般规律

132
00:06:19,470 --> 00:06:23,390
你们可能对于分类问题

133
00:06:23,500 --> 00:06:25,210
也能画出同样的图

134
00:06:25,360 --> 00:06:35,970
比如说这是我的训练集合

135
00:06:36,120 --> 00:06:38,730
这些是正样本和负样本

136
00:06:38,890 --> 00:06:47,480
你可以用一个次数很高的多项式函数

137
00:06:47,600 --> 00:06:49,170
来进行logistic回归

138
00:06:49,330 --> 00:07:02,000
对于x的预测等于这个sigmoid函数的值

139
00:07:02,130 --> 00:07:03,190
sigmoid函数作用

140
00:07:03,290 --> 00:07:04,220
在一个十次多项式函数上

141
00:07:04,370 --> 00:07:05,210
如果你这样做的话

142
00:07:05,340 --> 00:07:13,860
你可能会得到这样的一个决策边界

143
00:07:14,000 --> 00:07:15,520
这样确实能够精确地将正负样本区分开

144
00:07:15,640 --> 00:07:17,750
但是这明显又是一个过拟合的例子

145
00:07:17,850 --> 00:07:23,610
相反

146
00:07:23,720 --> 00:07:25,320
如果你用线性模型进行logistic回归

147
00:07:25,440 --> 00:07:26,440
而不考虑任何二次特征

148
00:07:26,570 --> 00:07:28,520
那么你会得到这样的决策边界

149
00:07:29,440 --> 00:07:31,610
这显然是欠拟合 好的

150
00:07:31,700 --> 00:07:35,770
我接下来要做的

151
00:07:35,880 --> 00:07:38,870
是帮助你们更深入地理解过

152
00:07:38,980 --> 00:07:41,100
拟合与欠拟合

153
00:07:41,240 --> 00:07:42,880
或者高偏差与高方差

154
00:07:42,970 --> 00:07:47,230
我会提出一个更为正式的机器学习模型

155
00:07:47,340 --> 00:07:49,450
并尝试证明这一对问题--

156
00:07:49,590 --> 00:07:52,820
或者说这两个问题

157
00:07:52,940 --> 00:07:56,210
何时会出现

158
00:07:56,470 --> 00:08:03,550
我首先要说的是

159
00:08:03,660 --> 00:08:04,420
线性分类

160
00:08:04,550 --> 00:08:06,870
h_? (x)=?^T x

161
00:08:06,980 --> 00:08:24,570
对吗?

162
00:08:24,650 --> 00:08:26,030
这是线性分类器

163
00:08:26,110 --> 00:08:27,930
这里我要用z

164
00:08:28,040 --> 00:08:29,440
对不起

165
00:08:29,520 --> 00:08:30,740
我要用g(z)表示z是否大于等于0

166
00:08:30,850 --> 00:08:41,190
很抱歉

167
00:08:41,310 --> 00:08:43,800
我们现在又要改变符号了

168
00:08:43,900 --> 00:08:46,400
在讲SVM的时候

169
00:08:46,500 --> 00:08:48,020
我们说y取-1或1

170
00:08:48,110 --> 00:08:50,310
但是在学习理论的课程部分

171
00:08:50,420 --> 00:08:52,750
我们再次让y取0或1

172
00:08:52,990 --> 00:08:54,300
这样会方便一些

173
00:08:54,430 --> 00:08:56,230
所以我们又变回了最早的符号表示

174
00:08:56,350 --> 00:09:03,620
你可以将这个模型看成是logistic回归

175
00:09:03,690 --> 00:09:05,180
这和logistic回归

176
00:09:05,260 --> 00:09:06,560
非常相似

177
00:09:06,660 --> 00:09:07,660
除了我们现在要

178
00:09:07,740 --> 00:09:09,910
强制logistic回归算法

179
00:09:10,020 --> 00:09:14,170
输出0或1作为类标签

180
00:09:14,280 --> 00:09:14,980
明白吗?

181
00:09:15,060 --> 00:09:16,920
你可以认为这个分类器

182
00:09:17,030 --> 00:09:18,960
是按某种概率分布选取0或1作为类标签

183
00:09:19,080 --> 00:09:25,800
像往常一样

184
00:09:25,890 --> 00:09:28,480
比如说我们现在有一个m个训练样本

185
00:09:35,430 --> 00:09:35,930
构成的训练集合

186
00:09:36,040 --> 00:09:38,460
我这里表示有m个样本

187
00:09:38,550 --> 00:09:39,920
i等于1到m

188
00:09:40,030 --> 00:09:42,990
我这里假设

189
00:09:43,100 --> 00:09:45,650
样本(x^((i) )  y^((i) ))之间

190
00:09:45,760 --> 00:09:47,590
是独立同分布的

191
00:09:47,720 --> 00:09:50,880
服从于某个分布D

192
00:09:51,000 --> 00:09:51,760
明白吗?

193
00:09:51,830 --> 00:09:53,740
iid表示独立同分布--

194
00:09:53,830 --> 00:09:57,390
如果你们将房屋销售问题

195
00:09:57,500 --> 00:09:58,390
作为一个分类问题看待

196
00:09:58,500 --> 00:10:01,250
例如根据房屋的特征

197
00:10:01,360 --> 00:10:03,470
判断其是否会在未来六个月内被卖掉

198
00:10:03,560 --> 00:10:06,830
这个概率分布就是

199
00:10:06,940 --> 00:10:10,190
它们是否会被卖掉的先验概率分布

200
00:10:10,350 --> 00:10:10,940
明白吗?

201
00:10:11,050 --> 00:10:12,330
我需要假设

202
00:10:12,410 --> 00:10:13,750
训练样本都是独立同分布的

203
00:10:13,860 --> 00:10:16,650
均由同一个概率分布生成

204
00:10:16,760 --> 00:10:19,410
对于垃圾邮件分类也是一样的

205
00:10:19,520 --> 00:10:21,730
如果你尝试建立一个垃圾邮件分类器

206
00:10:21,840 --> 00:10:25,560
那么它们是否

207
00:10:25,670 --> 00:10:30,590
为垃圾邮件是由同一个概率分布决定的

208
00:10:30,720 --> 00:10:35,060
具体地  为了理解偏差方差现象

209
00:10:35,160 --> 00:10:38,170
我将会使用一个简化的机器学习模型

210
00:10:38,280 --> 00:10:39,760
logistic回归

211
00:10:39,900 --> 00:10:46,630
通过最大化

212
00:10:46,710 --> 00:10:49,390
对数似然性拟合出参数

213
00:10:49,460 --> 00:10:53,900
但是为了更为深入地理解机器学习算法

214
00:10:54,010 --> 00:10:54,790
我要假设一个

215
00:10:54,870 --> 00:10:56,170
机器学习的简化版的模型

216
00:10:56,280 --> 00:10:57,880
让我将它写下来

217
00:10:57,960 --> 00:11:04,220
我会定义训练误差

218
00:11:04,330 --> 00:11:10,850
这是h_? (x)的训练误差

219
00:11:10,850 --> 00:11:14,110
我用ε ?表示

220
00:11:14,220 --> 00:11:17,170
如果我希望训练误差显式地

221
00:11:17,280 --> 00:11:20,680
依赖于某个训练集合

222
00:11:20,790 --> 00:11:21,750
我这里会加上一个下标s

223
00:11:21,860 --> 00:11:24,840
表示所依赖的训练集合

224
00:11:24,970 --> 00:11:26,260
我会将它定义成这样的形式

225
00:11:26,380 --> 00:11:42,810
我希望这个公式的含义足够明确

226
00:11:42,900 --> 00:11:45,720
被假设错误分类的训练样本数之和

227
00:11:45,830 --> 00:11:49,440
当这一项除以m时

228
00:11:49,550 --> 00:11:53,900
它表示的就是被你的假设错误

229
00:11:54,010 --> 00:11:56,800
分类训练样本所占的比例

230
00:11:56,910 --> 00:12:00,380
这被定义为训练误差

231
00:12:00,490 --> 00:12:02,630
训练误差也被称为风险

232
00:12:02,740 --> 00:12:06,810
我要将的机器学习的简化模型

233
00:12:06,970 --> 00:12:12,960
被称之为经验风险最小化(ERM)

234
00:12:13,100 --> 00:12:15,880
具体地  我会假设我的学习算法

235
00:12:16,030 --> 00:12:19,690
工作的方式是选择参数值

236
00:12:19,840 --> 00:12:22,750
使我的训练误差最小化

237
00:12:22,900 --> 00:12:36,240
明白吗? 我们之后的证明

238
00:12:36,320 --> 00:12:40,780
都会依赖于这个假设

239
00:12:40,870 --> 00:12:42,980
实际上你可以认为

240
00:12:43,060 --> 00:12:46,570
这是最为基本的学习算法

241
00:12:46,650 --> 00:12:48,850
目的是使你的训练误差最小化

242
00:12:48,960 --> 00:12:50,840
事实证明logistic回归和SVM

243
00:12:50,930 --> 00:12:53,890
可以被视为这个算法的一种近似

244
00:12:53,960 --> 00:12:57,590
实际上如果你真的想

245
00:12:57,710 --> 00:13:00,590
求解这个非凸性优化问题

246
00:13:00,700 --> 00:13:03,090
这实际上是一个NP难的问题

247
00:13:03,220 --> 00:13:04,920
logistic回归与SVM实际上可以看成

248
00:13:05,050 --> 00:13:06,420
这个非凸性优化问题

249
00:13:06,540 --> 00:13:11,480
的一种凸性近似

250
00:13:11,600 --> 00:13:14,900
可以认为类似于

251
00:13:15,010 --> 00:13:16,610
logistic回归的算法

252
00:13:16,730 --> 00:13:19,370
也是这样工作的

253
00:13:19,500 --> 00:13:23,070
让我重新定义ERM

254
00:13:23,180 --> 00:13:25,270
将其定义成

255
00:13:25,390 --> 00:13:33,600
一种等价的形式

256
00:13:33,710 --> 00:13:38,600
对于我今天想证明的结论

257
00:13:38,710 --> 00:13:43,780
相对于将我们的算法目标

258
00:13:43,860 --> 00:13:48,320
看成选取一组参数

259
00:13:48,440 --> 00:13:51,420
不如将其看成选取一个函数

260
00:13:51,560 --> 00:13:53,910
我们来看看这是什么意思

261
00:13:54,020 --> 00:13:58,100
让我定义假设类  H

262
00:13:58,210 --> 00:14:03,980
将其定义由所有假设构成的集合--

263
00:14:04,100 --> 00:14:06,610
换句话说

264
00:14:06,760 --> 00:14:08,110
将其定义成

265
00:14:08,220 --> 00:14:09,450
所有的线性分类器构成的集合

266
00:14:09,560 --> 00:14:12,260
你的学习算法

267
00:14:12,370 --> 00:14:20,280
将会从中选取一个假设作为目标

268
00:14:20,390 --> 00:14:24,070
明白么?所以h_?是一个特定的线性分类器

269
00:14:24,180 --> 00:14:30,820
h_?  这些函数之一--

270
00:14:30,930 --> 00:14:32,690
是一个函数

271
00:14:32,810 --> 00:14:37,510
从输入域X

272
00:14:37,610 --> 00:14:42,240
映射到类

273
00:14:42,300 --> 00:14:43,350
其中的每个成员都是一个函数

274
00:14:43,450 --> 00:14:44,170
当你改变参数的时候

275
00:14:44,260 --> 00:14:45,450
你实际上就是在选择不同的函数

276
00:14:45,550 --> 00:14:47,110
所以我们定义的这个假设类H

277
00:14:47,200 --> 00:14:48,570
就是一个假设的集合

278
00:14:48,680 --> 00:14:50,690
logistic回归可以从中

279
00:14:50,780 --> 00:14:53,120
选取一个假设作为结果  明白吗?

280
00:14:53,200 --> 00:14:54,440
所以这是一个线性分类器的集合

281
00:14:54,570 --> 00:14:58,690
所以我接下来要对ERM进行定义

282
00:14:58,820 --> 00:15:01,920
或重新定义

283
00:15:02,020 --> 00:15:06,140
相对于原来将其定义成对参数的选取

284
00:15:06,240 --> 00:15:08,200
我们现在将其定义成

285
00:15:08,310 --> 00:15:14,740
从假设类H中选取一个函数

286
00:15:14,850 --> 00:15:22,940
使得训练误差最小

287
00:15:23,050 --> 00:15:24,560
明白吗?

288
00:15:24,670 --> 00:15:32,150
如果你们明白了

289
00:15:32,260 --> 00:15:33,510
这两种定义之间的等价性

290
00:15:33,620 --> 00:15:35,050
请举下手

291
00:15:35,150 --> 00:15:37,620
很好 谢谢

292
00:15:37,740 --> 00:15:44,230
处于接下来的课程内容的考虑

293
00:15:44,310 --> 00:15:46,880
我们将算法看成是从假设类中选取函数

294
00:15:46,990 --> 00:15:50,120
因为在更为一般的情况下

295
00:15:50,220 --> 00:15:53,560
H可能表示其他类的函数

296
00:15:53,670 --> 00:15:56,460
例如可能是一类由神经网络表示的函数

297
00:15:56,590 --> 00:16:01,980
或者是算法所需要

298
00:16:02,090 --> 00:16:03,130
从中选取的其他的一类函数

299
00:16:03,250 --> 00:16:07,900
这些情况下

300
00:16:08,010 --> 00:16:09,680
我们仍然会使用这种ERM的定义

301
00:16:09,750 --> 00:16:10,640
明白吗?

302
00:16:10,720 --> 00:16:16,770
我们接下来要做的是尝试理解:

303
00:16:16,880 --> 00:16:20,620
ERM是一个合理的算法

304
00:16:20,730 --> 00:16:21,420
Alex?

305
00:16:21,540 --> 00:16:27,090
S:假设函数是否仍然用g(?^T x)定义?

306
00:16:27,200 --> 00:16:28,970
或者说这是一个更一般的定义?

307
00:16:29,090 --> 00:16:31,030
I:让我看看

308
00:16:31,150 --> 00:16:37,070
你的问题是说h_?

309
00:16:37,190 --> 00:16:41,450
是否仍然用g(?^T x)定义

310
00:16:41,560 --> 00:16:42,290
或者说这里表示一个一般的定义?

311
00:16:42,370 --> 00:16:43,810
【听不清】

312
00:16:43,890 --> 00:16:49,040
I:哦  为了回答这个问题

313
00:16:49,110 --> 00:16:50,860
首先  这个框架是通用的

314
00:16:50,940 --> 00:16:52,440
对于这一讲

315
00:16:52,560 --> 00:16:54,900
你将h_?看成是

316
00:16:55,010 --> 00:16:58,470
logistic回归中

317
00:16:58,580 --> 00:16:59,710
使用到的线性分类器也许是有用的

318
00:16:59,820 --> 00:17:01,590
但是要说明的是

319
00:17:01,670 --> 00:17:06,250
所有黑板上的这些东西

320
00:17:06,360 --> 00:17:08,500
都是通用的

321
00:17:08,600 --> 00:17:10,530
H可以是任意函数构成的集合

322
00:17:10,610 --> 00:17:13,330
只要能从输入域映射到

323
00:17:13,420 --> 00:17:14,780
类标签集合

324
00:17:14,890 --> 00:17:16,420
此外你也可以给予

325
00:17:16,540 --> 00:17:20,120
任意的假设类执行ERM

326
00:17:20,230 --> 00:17:22,270
对于今天的课来说

327
00:17:22,380 --> 00:17:25,360
我讲解的内容

328
00:17:25,470 --> 00:17:26,970
仅限于二元分类

329
00:17:27,080 --> 00:17:28,240
但是实际上

330
00:17:28,340 --> 00:17:30,090
我今天讲的所有东西

331
00:17:30,190 --> 00:17:31,710
都可以被推广到其他情况 还有问题吗?

332
00:17:31,820 --> 00:17:32,960
S:没问题了

333
00:17:33,060 --> 00:17:34,850
I:很好

334
00:17:34,950 --> 00:17:37,680
我们想要知道ERM是否是一

335
00:17:37,800 --> 00:17:39,140
个合理的算法

336
00:17:39,240 --> 00:17:42,000
我们需要怎样证明呢?

337
00:17:44,280 --> 00:17:47,410
很明显  我们并不会太关心训练误差

338
00:17:47,520 --> 00:17:49,450
我们实际关心的并不是

339
00:17:49,720 --> 00:17:51,180
我们对于训练集合的预测有多么准确

340
00:17:51,300 --> 00:17:52,390
至少这不是我们的终极目的

341
00:17:52,480 --> 00:17:53,910
我们的终极目的是

342
00:17:54,040 --> 00:18:03,090
对于那些我们之前

343
00:18:03,180 --> 00:18:04,230
没有见过的样本的预测效果

344
00:18:04,300 --> 00:18:05,420
例如:它对于一个之前没有见过

345
00:18:05,530 --> 00:18:08,460
的房屋的价格的预测

346
00:18:08,530 --> 00:18:09,370
以及该房屋

347
00:18:09,460 --> 00:18:11,470
是否能够被卖掉的预测是否准确

348
00:18:11,590 --> 00:18:14,440
我们真正关心的是一般误差

349
00:18:14,550 --> 00:18:16,930
我这里用?(h)来表示

350
00:18:17,050 --> 00:18:19,140
它被定义为:

351
00:18:19,290 --> 00:18:25,050
对于取样得到的一个新的

352
00:18:25,220 --> 00:18:31,620
由某个分布D生成的样本(x  y)

353
00:18:31,710 --> 00:18:43,920
假设对该样本错误分类的概率

354
00:18:44,040 --> 00:18:48,350
处于一种符号上的使用习惯

355
00:18:48,470 --> 00:18:49,490
通常情况下--

356
00:18:49,570 --> 00:18:52,970
如果我将"() ?"放在了某个符号上

357
00:18:53,080 --> 00:18:54,050
这通常意味着--

358
00:18:54,190 --> 00:18:55,130
并不是绝对的--

359
00:18:55,210 --> 00:18:57,210
它通常表示我们尝试

360
00:18:57,310 --> 00:18:58,660
对某个量进行的估计

361
00:18:58,770 --> 00:19:02,100
例如  ? ?表示--

362
00:19:02,210 --> 00:19:04,110
我们尝试着--

363
00:19:04,190 --> 00:19:05,340
我们尝试用训练误差

364
00:19:05,430 --> 00:19:08,500
来近似表示一般误差

365
00:19:08,630 --> 00:19:11,790
所以根据符号使用上的习惯

366
00:19:11,910 --> 00:19:15,250
如果一个概念的上面有"() ?"

367
00:19:15,360 --> 00:19:17,870
那么这表示该概念是

368
00:19:17,940 --> 00:19:20,190
对另外一个量的估计

369
00:19:20,270 --> 00:19:21,420
h ?是一个学习算法输出的假设

370
00:19:21,480 --> 00:19:22,620
表示对于真实函数y=h(x)的估计

371
00:19:22,710 --> 00:19:26,450
让我们证明一些结论

372
00:19:26,540 --> 00:19:30,870
来表明ERM是一个合理的算法

373
00:19:30,990 --> 00:19:32,570
因为它能够带来较小的一般误差

374
00:19:32,680 --> 00:19:33,910
这是我们真正关心的

375
00:19:33,990 --> 00:19:49,320
为了证明我们的第一个学习理论的结论

376
00:19:49,440 --> 00:19:51,170
我先介绍两条引理

377
00:19:51,290 --> 00:19:54,460
第一个是联合界引理

378
00:19:54,600 --> 00:20:01,770
表述如下

379
00:20:01,880 --> 00:20:10,910
令A_1 到A_k表示k个事件

380
00:20:11,020 --> 00:20:13,590
这里我用了事件这个概念

381
00:20:13,670 --> 00:20:15,800
这表示它是一种概率事件

382
00:20:15,870 --> 00:20:17,290
或者发生  或者不发生

383
00:20:17,370 --> 00:20:20,600
这些事件不一定是独立的

384
00:20:20,720 --> 00:20:31,520
A_1

385
00:20:31,640 --> 00:20:34,040
到A_k的事件之间

386
00:20:34,140 --> 00:20:35,310
可能是有关联的

387
00:20:35,410 --> 00:20:36,210
他们之间可以独立也可以不独立

388
00:20:36,290 --> 00:20:37,120
我们关于这一点没有假设

389
00:20:37,200 --> 00:20:49,030
P(A_1∪A_2∪…∪A_k)应该等于

390
00:20:49,140 --> 00:20:51,750
这个"∪"符号

391
00:20:51,860 --> 00:20:52,950
是一个集合符号

392
00:20:53,030 --> 00:20:55,920
用在概率意义上表示"或"

393
00:20:56,030 --> 00:20:57,430
所以这些事件中


394
00:20:57,540 --> 00:21:00,310
至少有一个发生的概率

395
00:21:00,420 --> 00:21:03,110
也就是P(A_1∪A_2∪…∪A_k)

396
00:21:03,230 --> 00:21:06,880
应该小于等于A_1+A_2+?+A_k

397
00:21:06,990 --> 00:21:13,920
明白吗?

398
00:21:14,030 --> 00:21:19,920
这个公式背后的直观意义是


399
00:21:20,050 --> 00:21:22,510
我不知道你们之前是否用过

400
00:21:22,620 --> 00:21:25,750
文氏图来表示概率

401
00:21:25,840 --> 00:21:26,580
如果你们没有的话

402
00:21:26,660 --> 00:21:28,430
那么我接下来要画的东西

403
00:21:28,510 --> 00:21:29,340
看起来会有点难懂

404
00:21:29,440 --> 00:21:30,500
但是不用太在意

405
00:21:30,590 --> 00:21:31,510
如果你之前没有见过

406
00:21:31,610 --> 00:21:33,040
这种表示方法的话就忽略掉吧

407
00:21:33,160 --> 00:21:33,960
如果你们之前见过的话

408
00:21:34,040 --> 00:21:40,050
这样的表示方式是很好的

409
00:21:40,130 --> 00:21:43,320
所以P(A_1∪A_2∪A_3 )≤

410
00:21:43,400 --> 00:21:48,290
P(A_1 )

411
00:21:48,370 --> 00:21:52,540
+P(A_2 )+P(A_3)

412
00:21:52,620 --> 00:21:54,960
这意味着这三部分的并的面积

413
00:21:55,040 --> 00:21:56,990
要小于等于这三部分面积之和

414
00:21:57,090 --> 00:21:59,240
这个结论很显然

415
00:21:59,320 --> 00:22:02,300
实际上取决于你们

416
00:22:02,370 --> 00:22:03,880
如何定义概率论的公理

417
00:22:03,960 --> 00:22:07,880
这个经常作为概率论的公理出现

418
00:22:07,950 --> 00:22:09,540
所以我不打算证明它

419
00:22:09,610 --> 00:22:13,680
它经常被写成一个公理

420
00:22:13,770 --> 00:22:16,880
通常被称为求和定律--

421
00:22:16,960 --> 00:22:18,550
有时会这样叫

422
00:22:18,630 --> 00:22:27,640
但是在学习理论中

423
00:22:27,720 --> 00:22:29,260
我们称之为联合界--

424
00:22:29,330 --> 00:22:30,240
我这样叫它

425
00:22:30,320 --> 00:22:34,880
我要写的另外一个引理

426
00:22:34,950 --> 00:22:35,810
称之为Hoeffding不等式

427
00:22:35,920 --> 00:22:41,710
我不会证明它

428
00:22:41,820 --> 00:22:42,690
我仅仅将它写下来--

429
00:22:42,750 --> 00:22:52,020
z_1  z_2 z_

430
00:22:52,090 --> 00:22:58,470
m 为m个独立同分布的随机变量

431
00:22:58,570 --> 00:23:10,280
它们服从均值为φ的伯努利分布

432
00:23:10,440 --> 00:23:20,460
这意味着P(z_i=1)=φ

433
00:23:20,540 --> 00:23:21,890
假设你有m个独立同分布的随机变量

434
00:23:21,970 --> 00:23:24,070
你希望估计它们的均值

435
00:23:24,150 --> 00:23:25,460
所以让我定义φ ?

436
00:23:25,550 --> 00:23:27,630
根据符号习惯

437
00:23:27,720 --> 00:23:28,480
这个符号表示

438
00:23:28,550 --> 00:23:32,870
我们希望对某个值进行估计

439
00:23:32,960 --> 00:23:35,910
所以我们将φ ?定义成

440
00:23:35,990 --> 00:23:38,020
这m个随机变量的均值

441
00:23:38,100 --> 00:23:39,660
明白吗?

442
00:23:39,730 --> 00:23:42,100
所以这个式子

443
00:23:42,180 --> 00:23:43,870
表示我们希望用这样的平均式

444
00:23:43,940 --> 00:23:45,490
去估计这些随机变量的均值

445
00:23:45,580 --> 00:23:53,510
给定γ

446
00:23:53,620 --> 00:24:04,990
Hoeffding不等式说的是

447
00:24:05,070 --> 00:24:16,860
你估计的φ值

448
00:24:17,020 --> 00:24:20,060
和真实的φ值之间的差异

449
00:24:20,170 --> 00:24:22,630
大于γ的概率

450
00:24:22,740 --> 00:24:23,980
不会超过由这个式子表示的上界

451
00:24:24,060 --> 00:24:24,810
明白吗?

452
00:24:24,890 --> 00:24:26,960
用图来表示


453
00:24:27,080 --> 00:24:35,050
这个引理

454
00:24:35,160 --> 00:24:36,000
Hoeffding不等式

455
00:24:36,100 --> 00:24:36,960
是一个事实

456
00:24:37,040 --> 00:24:37,850
它是一个真命题

457
00:24:37,930 --> 00:24:40,390
让我画一张图来解释一下

458
00:24:40,470 --> 00:24:42,330
这个公式背后的直观含义

459
00:24:42,440 --> 00:24:45,430
让我看看

460
00:24:45,540 --> 00:24:47,190
比如说

461
00:24:47,280 --> 00:24:48,440
这条线表示实数域内的0到1的值

462
00:24:48,550 --> 00:24:51,860
φ是你的伯努利分布的均值

463
00:24:51,930 --> 00:24:56,730
你可能记得

464
00:24:56,800 --> 00:24:59,700
本科生的概率统计课上

465
00:24:59,810 --> 00:25:02,250
曾经讲过

466
00:25:02,330 --> 00:25:03,420
中心极限定律告诉我们

467
00:25:03,500 --> 00:25:04,880
如果你对这些变量取平均

468
00:25:04,990 --> 00:25:05,870
那么平均随机变量趋向于服从高斯分布

469
00:25:05,970 --> 00:25:08,300
如果你投掷m次硬币

470
00:25:08,410 --> 00:25:09,860
每次正面的概率都是φ

471
00:25:09,940 --> 00:25:11,960
那么我们会观察到m个伯努利随机变量

472
00:25:12,080 --> 00:25:13,720
对这些变量取平均

473
00:25:13,800 --> 00:25:18,390
那么平均随机变量的分布

474
00:25:18,480 --> 00:25:28,040
大概会是一个高斯分布

475
00:25:28,150 --> 00:25:29,450
明白吗?

476
00:25:29,540 --> 00:25:31,850
实际上如果你之前没有见过这个结论

477
00:25:31,960 --> 00:25:33,330
那么它实际上说的是

478
00:25:33,430 --> 00:25:34,660
φ ?的积累分布函数会

479
00:25:34,730 --> 00:25:35,750
收敛到一个高斯函数

480
00:25:36,330 --> 00:25:38,110
严格来说φ ?只能

481
00:25:38,190 --> 00:25:39,630
取一些离散的值

482
00:25:39,710 --> 00:25:42,310
因为他们是基于m的一个比率

483
00:25:42,400 --> 00:25:43,750
但是这张图只是帮助你们理解

484
00:25:43,840 --> 00:25:45,500
它大概收敛

485
00:25:45,500 --> 00:25:47,160
到高斯分布

486
00:25:47,280 --> 00:25:51,240
Hoeffding不等式说的是

487
00:25:51,350 --> 00:25:52,630
如果你给定了一个γ

488
00:25:52,760 --> 00:25:58,460
这里是一个长度为γ的区间

489
00:25:58,580 --> 00:25:59,740
这里是另一个

490
00:25:59,860 --> 00:26:02,680
那么它说的是

491
00:26:02,790 --> 00:26:07,840
φ ?距离真实值

492
00:26:07,930 --> 00:26:10,060
超过一个γ的概率

493
00:26:10,160 --> 00:26:16,610
也就是这些概率质量--

494
00:26:16,690 --> 00:26:24,100
这两个尾部的概率质量最多为这个值

495
00:26:24,220 --> 00:26:25,850
明白吗?

496
00:26:25,930 --> 00:26:27,130
这就是Hoeffding不等式--

497
00:26:27,490 --> 00:26:29,050
如果你看不清这里的话--

498
00:26:29,160 --> 00:26:29,950
它实际上就是

499
00:26:30,050 --> 00:26:31,760
不等式右侧的那个式子表示的界

500
00:26:31,870 --> 00:26:34,410
所以这个不等式给出了一个在

501
00:26:34,520 --> 00:26:36,840
估计伯努利随机变量均值时

502
00:26:36,950 --> 00:26:37,720
犯错误的概率的上界

503
00:26:37,810 --> 00:26:45,000
关于这个上界的一个很酷的结论是--

504
00:26:45,090 --> 00:26:45,780
这个上界背后的一个有趣的性质是

505
00:26:45,860 --> 00:26:51,930
它会随着m的增长指数下降

506
00:26:52,020 --> 00:26:53,500
对于一个给定的γ

507
00:26:53,610 --> 00:26:57,230
当你增加训练集合的规模时

508
00:26:57,310 --> 00:26:58,310
即:你掷出更多次的硬币

509
00:26:58,390 --> 00:27:00,340
那么高斯函数的凸性会收缩

510
00:27:00,450 --> 00:27:03,470
高斯函数的值会

511
00:27:03,570 --> 00:27:04,820
随着m指数的增长的倒数收缩

512
00:27:04,900 --> 00:27:08,760
这会导致尾部的概率密度会

513
00:27:08,840 --> 00:27:11,000
随着m的增长指数收缩

514
00:27:11,070 --> 00:27:13,980
稍后会看到

515
00:27:14,080 --> 00:27:15,690
这一点很重要

516
00:27:15,770 --> 00:27:17,260
什么问题?

517
00:27:17,350 --> 00:27:20,780
S:这些来自于中心极限定律吗?

518
00:27:20,870 --> 00:27:22,110
I:不是的

519
00:27:22,190 --> 00:27:23,460
它是由一个不同的--

520
00:27:23,560 --> 00:27:26,400
中心极限定律--

521
00:27:26,520 --> 00:27:28,570
中心极限定律有很多版本

522
00:27:28,680 --> 00:27:30,450
但是我所熟悉的版本基本上

523
00:27:30,560 --> 00:27:32,180
都是渐进意义上成立的结论

524
00:27:32,290 --> 00:27:34,490
但是Hoeffding不等式

525
00:27:34,620 --> 00:27:38,330
对于任意m值都成立

526
00:27:38,410 --> 00:27:39,220
即使m=2

527
00:27:39,290 --> 00:27:40,490
这个式子也是成立的

528
00:27:40,560 --> 00:27:41,440
但是当m比较小时

529
00:27:41,540 --> 00:27:43,490
中心极限定律将不会成立

530
00:27:43,600 --> 00:27:45,910
但是这个结论对于任意m值都是成立的

531
00:27:45,990 --> 00:27:46,800
明白吗?

532
00:27:46,880 --> 00:27:49,230
我画的这张图

533
00:27:49,340 --> 00:27:50,930
只是用来解释其直观意义

534
00:27:51,040 --> 00:27:53,040
但是结论总是成立的

535
00:27:53,140 --> 00:27:55,040
这和中心极限定律无关

536
00:27:55,150 --> 00:28:03,500
好的

537
00:28:03,610 --> 00:28:11,200
接下来让我们来看ERM

538
00:28:11,310 --> 00:28:23,810
我要以logistic回归为例

539
00:28:23,920 --> 00:28:31,990
来介绍ERM的性质

540
00:28:32,100 --> 00:28:34,810
具体地

541
00:28:34,890 --> 00:28:36,000
我要考虑的是

542
00:28:36,090 --> 00:28:38,200
有限假设类的情形

543
00:28:38,300 --> 00:28:50,460
令H为一个包含k个假设的假设类

544
00:28:50,620 --> 00:28:58,380
好的 这k个函数

545
00:28:58,500 --> 00:29:00,010
每一个都是一个

546
00:29:00,120 --> 00:29:01,050
从输入映射到输出的函数

547
00:29:01,170 --> 00:29:02,110
并没有参数

548
00:29:02,220 --> 00:29:07,860
ERM要做的是

549
00:29:07,980 --> 00:29:10,070
对于给定的训练集合

550
00:29:10,180 --> 00:29:16,460
它会从这k个函数中选取一个

551
00:29:16,580 --> 00:29:20,170
使得训练误差最小

552
00:29:20,270 --> 00:29:21,240
明白吗?

553
00:29:21,370 --> 00:29:23,150
需要注意的是logistic回归用到的是

554
00:29:23,260 --> 00:29:26,090
无限大的--

555
00:29:26,200 --> 00:29:27,740
连续无限大的假设类H

556
00:29:27,830 --> 00:29:31,910
但是对于我们的第一个学习定理

557
00:29:31,990 --> 00:29:34,680
我们仅仅先考虑有限假设类的情形

558
00:29:34,760 --> 00:29:36,470
之后我们会将其推广到

559
00:29:36,580 --> 00:29:39,730
无限假设类的情形

560
00:29:39,850 --> 00:29:54,820
所以ERM会选择具有

561
00:29:54,920 --> 00:29:56,790
最小训练误差的假设

562
00:29:56,870 --> 00:30:01,620
我们要证明一般误差

563
00:30:01,720 --> 00:30:05,270
和最小误差h ?之间的差值是有上界的

564
00:30:05,350 --> 00:30:06,790
好的  换句话说

565
00:30:06,870 --> 00:30:07,970
我要证明的是

566
00:30:08,050 --> 00:30:08,980
如果训练误差很小

567
00:30:09,060 --> 00:30:10,370
那么一般误差也不会太大

568
00:30:10,470 --> 00:30:11,430
证明策略是这样的

569
00:30:11,520 --> 00:30:22,770
首先  我要证明训练误差是

570
00:30:22,840 --> 00:30:27,240
一个对一般误差的很好的近似

571
00:30:27,310 --> 00:30:34,920
之后我要证明

572
00:30:35,020 --> 00:30:42,800
ERM输出的假设的一般误差

573
00:30:42,900 --> 00:30:44,770
存在上界

574
00:30:44,860 --> 00:30:50,660
我意识到这节课上

575
00:30:50,750 --> 00:30:55,140
我们的符号系统似乎有些复杂

576
00:30:55,210 --> 00:30:57,760
我们引入了太多的符号

577
00:30:57,840 --> 00:31:00,910
所以如果在课上

578
00:31:01,020 --> 00:31:02,000
你们觉得对于某些符号

579
00:31:02,070 --> 00:31:03,420
你们记不清楚它的含义

580
00:31:03,520 --> 00:31:04,510
请举手问我

581
00:31:04,580 --> 00:31:05,790
你可以问这些符号都是什么意思

582
00:31:05,870 --> 00:31:07,670
或者什么是一般误差

583
00:31:07,760 --> 00:31:09,040
或者一些其他的问题

584
00:31:09,120 --> 00:31:12,850
如果你们不明白符号的含义

585
00:31:12,930 --> 00:31:13,950
请举手问我

586
00:31:14,030 --> 00:31:20,010
好的  让我们来证明这两部分

587
00:31:20,130 --> 00:31:21,700
首先我要证明

588
00:31:21,790 --> 00:31:23,310
训练误差是一个

589
00:31:23,420 --> 00:31:27,140
对一般误差的很好的近似

590
00:31:27,220 --> 00:31:28,590
这意味着如果我们使训练误差最小化

591
00:31:28,670 --> 00:31:29,400
那么一般误差

592
00:31:29,470 --> 00:31:30,330
也不会太大

593
00:31:30,410 --> 00:31:32,580
这会给出ERM输出的

594
00:31:32,660 --> 00:31:35,820
假设的一般误差的上界

595
00:31:35,940 --> 00:31:37,990
明白吗?

596
00:31:38,120 --> 00:31:40,790
证明过程是这样的

597
00:31:40,870 --> 00:31:54,510
首先我们不去考虑所有的假设

598
00:31:54,630 --> 00:31:56,660
让我们选定任意一个假设

599
00:31:56,740 --> 00:31:59,480
从H中选择h_j

600
00:31:59,560 --> 00:32:03,330
我们暂时先考虑一个固定的假设

601
00:32:03,420 --> 00:32:05,430
所以我们任意选定一个假设

602
00:32:05,530 --> 00:32:07,680
并且只考虑这个假设

603
00:32:07,760 --> 00:32:14,800
让我们将Z_i定义成

604
00:32:14,880 --> 00:32:25,880
表示第i个样本是否被

605
00:32:25,960 --> 00:32:30,170
错误分类的指示函数的值--

606
00:32:30,270 --> 00:32:31,840
对不起--这里是Z_i  明白吗?

607
00:32:31,920 --> 00:32:45,150
所以Z_i取0或1取决于

608
00:32:45,230 --> 00:32:47,860
对于我们现在考虑的这个假设

609
00:32:47,950 --> 00:32:51,110
它是否对该样本错误分类

610
00:32:51,210 --> 00:32:59,310
我们的训练样本是

611
00:32:59,390 --> 00:33:00,890
从某个分布D中随机生成的

612
00:33:00,970 --> 00:33:06,450
根据我们得到的训练样本

613
00:33:06,560 --> 00:33:08,900
Z_i取0或1

614
00:33:08,980 --> 00:33:14,270
所以让我们看看Z_i的概率分布

615
00:33:14,350 --> 00:33:18,680
由于Z_i只能取0或1

616
00:33:18,760 --> 00:33:19,610
所以很明显

617
00:33:19,690 --> 00:33:20,650
它是一个伯努利随机变量

618
00:33:20,730 --> 00:33:21,890
因为它只能取这两个值

619
00:33:21,970 --> 00:33:33,890
那么P(Z_i=1)应该是多少呢?

620
00:33:33,980 --> 00:33:34,700
换句话说

621
00:33:34,780 --> 00:33:38,950
对于给定的假设h_j

622
00:33:39,030 --> 00:33:44,170
当我从利用分布D生成一个样本时

623
00:33:44,240 --> 00:33:48,080
假设对该样本错误分类的概率是多少?

624
00:33:48,190 --> 00:33:51,240
根据定义

625
00:33:51,350 --> 00:33:57,780
它应该等于假设h_j的一般误差

626
00:33:57,890 --> 00:34:05,980
所以Z_i是一个伯努利随机变量

627
00:34:06,060 --> 00:34:07,940
其均值为假设的一般误差

628
00:34:08,050 --> 00:34:17,780
明白的话请举手 很好

629
00:34:17,860 --> 00:34:23,650
此外  对于所有的样本

630
00:34:23,730 --> 00:34:25,190
P(Z_i=1)都是相同的

631
00:34:25,280 --> 00:34:27,510
由于所有的样本都是独立同分布的

632
00:34:27,590 --> 00:34:29,920
所以Z_i也是独立的

633
00:34:29,990 --> 00:34:43,370
因此不同的Z_i之间是独立同分布的

634
00:34:43,500 --> 00:34:44,460
明白吗?

635
00:34:44,540 --> 00:34:45,520
因为根据假设

636
00:34:45,600 --> 00:34:46,750
我们的训练样本彼此之间都是独立的

637
00:34:46,830 --> 00:34:59,180
如果你记得训练误差的定义

638
00:34:59,260 --> 00:35:07,320
那么假设h_j的训练误差应该等于这个

639
00:35:07,410 --> 00:35:10,560
这个式子是Z_i的平均值--

640
00:35:10,640 --> 00:35:14,210
我之前将其定义成这样的形式

641
00:35:14,290 --> 00:35:22,100
对吗?

642
00:35:22,180 --> 00:35:33,860
所以ε ?(h_j)是m个独立同分布的

643
00:35:33,940 --> 00:35:35,080
伯努利随机变量的平均值

644
00:35:35,160 --> 00:35:37,690
每个样本都是

645
00:35:37,770 --> 00:35:41,210
由均值为一般误差的伯努利分布生成

646
00:35:41,300 --> 00:35:46,180
所以  这是m个独立同分布的伯努利

647
00:35:46,280 --> 00:35:47,160
随机变量的平均值

648
00:35:47,240 --> 00:35:50,110
每个变量的均值都是h_j的一般误差

649
00:35:50,460 --> 00:35:56,650
因此  根据Hoeffding不等式

650
00:35:56,750 --> 00:36:07,940
我们应该加上训练误差

651
00:36:08,020 --> 00:36:17,570
和一般误差之间的差异的概率

652
00:36:17,640 --> 00:36:18,870
它们之间的差异

653
00:36:18,950 --> 00:36:20,620
大于γ的概率应该小于这个式子

654
00:36:20,700 --> 00:36:26,610
明白吗?

655
00:36:26,690 --> 00:36:28,150
这就是Hoeffding不等式

656
00:36:28,230 --> 00:36:33,990
这个结论证明了

657
00:36:34,080 --> 00:36:36,310
对于给定的假设h_j

658
00:36:36,380 --> 00:36:38,250
我的训练误差  ε ?

659
00:36:38,320 --> 00:36:41,910
将会以很大的概率近似于一般误差

660
00:36:41,980 --> 00:36:44,120
假设m很大

661
00:36:44,200 --> 00:36:45,230
如果m很大

662
00:36:45,310 --> 00:36:46,640
右边这项就会很小

663
00:36:46,710 --> 00:36:48,960
因为它是

664
00:36:49,060 --> 00:36:52,080
随m指数减小的

665
00:36:52,160 --> 00:36:53,160
所以这说明如果我的训练集合足够大

666
00:36:53,240 --> 00:36:55,450
那么训练误差

667
00:36:55,530 --> 00:36:57,110
和一般误差差距很大

668
00:36:57,190 --> 00:36:58,880
(这意味着这一项大于γ)

669
00:36:58,990 --> 00:37:00,150
的概率将会很小

670
00:37:00,230 --> 00:37:03,180
其上界由右边这项限定  明白吗?

671
00:37:03,290 --> 00:37:10,860
这一部分比较巧妙

672
00:37:10,930 --> 00:37:13,560
我们刚刚证明了

673
00:37:13,670 --> 00:37:14,740
对于一个给定的假设

674
00:37:14,820 --> 00:37:15,800
h_j两种误差之间的差异存在上界

675
00:37:15,890 --> 00:37:18,580
我最终想证明的是训练误差是

676
00:37:18,650 --> 00:37:20,430
一个对于一般误差的很好的估计

677
00:37:20,540 --> 00:37:22,810
不仅仅是对于h_j

678
00:37:22,920 --> 00:37:25,800
而是对于假设类H中的

679
00:37:25,910 --> 00:37:28,670
所有k个假设

680
00:37:28,780 --> 00:37:37,330
所以--我需要换块黑板

681
00:37:37,400 --> 00:37:56,350
为了证明这个结论

682
00:37:56,420 --> 00:37:59,190
让我定义一个随机事件

683
00:37:59,270 --> 00:38:02,420
让我将A_j定义成这样的事件

684
00:38:02,520 --> 00:38:24,820
对于假设h_j  其训练误差

685
00:38:24,900 --> 00:38:27,280
和一般误差的

686
00:38:27,360 --> 00:38:29,260
差异大于γ

687
00:38:29,340 --> 00:38:34,260
这样我们之前黑板上的结论可以写成:

688
00:38:34,360 --> 00:38:36,240
P(A_j )

689
00:38:36,320 --> 00:38:39,450
≤2e^(-2γ^2 m)

690
00:38:39,530 --> 00:38:40,450
这样简洁了很多

691
00:38:40,530 --> 00:38:46,510
现在我希望证明

692
00:38:46,600 --> 00:38:53,120
对于整个假设类H

693
00:38:53,240 --> 00:39:05,310
我们的一般误差取很大值的概率

694
00:39:05,390 --> 00:39:07,300
存在着上界 明白吗?

695
00:39:07,380 --> 00:39:10,730
使得这个式子成立

696
00:39:10,800 --> 00:39:16,270
它表示

697
00:39:16,350 --> 00:39:17,820
存在一个假设使这个条件成立

698
00:39:17,910 --> 00:39:19,310
这个实际上就是

699
00:39:19,390 --> 00:39:26,510
P(A_1∪A_2…∪A_k)

700
00:39:26,590 --> 00:39:31,910
如果存在一个假设使得一般误差很大

701
00:39:32,010 --> 00:39:35,020
那么可能是

702
00:39:35,120 --> 00:39:36,570
假设1的一般误差很大

703
00:39:36,640 --> 00:39:39,560
也可能是假设2的一般误差很大

704
00:39:39,660 --> 00:39:41,070
依此类推

705
00:39:41,170 --> 00:39:45,070
所以根据联合界引理

706
00:39:45,180 --> 00:39:46,550
它应该小于等于这个式子

707
00:39:46,660 --> 00:39:58,910
因此要小于等于--

708
00:39:59,000 --> 00:40:16,330
因此等于它 明白吗?

709
00:40:16,440 --> 00:40:47,990
让我用1同时减去两边--

710
00:40:48,070 --> 00:40:48,880
对于我之前黑板上的不等式--

711
00:40:48,960 --> 00:40:50,070
我同时用1减去两边

712
00:40:50,140 --> 00:40:57,450
所以不存在假设使得这个成立的概率

713
00:40:57,530 --> 00:41:09,910
也就是不存在

714
00:41:10,020 --> 00:41:12,840
产生较大估计误差的假设的概率

715
00:41:12,950 --> 00:41:14,290
应该等于

716
00:41:14,370 --> 00:41:16,050
对于所有的假设

717
00:41:16,170 --> 00:41:25,630
我对一般误差的估计的误差都很小

718
00:41:25,750 --> 00:41:28,640
都小于等于γ

719
00:41:28,760 --> 00:41:33,520
这个概率应该大于等于

720
00:41:33,630 --> 00:41:41,960
1-2ke^(-2γ^2 m)

721
00:41:42,050 --> 00:41:42,950
明白吗?

722
00:41:43,040 --> 00:41:45,010
当我用1减去两边的时候

723
00:41:45,080 --> 00:41:47,930
不等式的方向发生了改变

724
00:41:48,010 --> 00:41:49,340
因为当我用1减去两边的时候

725
00:41:49,420 --> 00:41:51,410
负号改变了不等式的方向

726
00:41:51,490 --> 00:41:57,030
所以该结论表明

727
00:41:57,140 --> 00:42:02,900
在不小于某个概率的情况下

728
00:42:03,030 --> 00:42:03,960
这里简写为WP

729
00:42:04,080 --> 00:42:07,590
在不小于1-2ke^(-2γ^2 m)的概率下

730
00:42:07,700 --> 00:42:17,090
对于所有的H中的假设h

731
00:42:17,200 --> 00:42:23,440
ε ?(h)与ε(h)之间的差距

732
00:42:23,560 --> 00:42:36,680
将会在γ以内

733
00:42:36,760 --> 00:42:50,620
给这个结果取个名字

734
00:42:50,730 --> 00:42:58,330
它被称为一致收敛

735
00:42:58,440 --> 00:43:03,750
一致收敛暗示了

736
00:43:03,850 --> 00:43:05,310
一个事实

737
00:43:05,660 --> 00:43:07,620
当m很大时

738
00:43:07,700 --> 00:43:15,970
所有的这些ε ?(h)将会同时收敛到ε(h)

739
00:43:16,040 --> 00:43:17,730
此时对于所有的h

740
00:43:17,810 --> 00:43:20,010
其训练误差

741
00:43:20,140 --> 00:43:22,200
和一般误差都会非常接近

742
00:43:22,280 --> 00:43:26,760
这就是"一致"这个词的含义

743
00:43:26,840 --> 00:43:28,880
实际上所有的假设h都会收敛

744
00:43:28,950 --> 00:43:30,170
而不单单是一个假设

745
00:43:30,250 --> 00:43:33,190
所以我们展示的是

746
00:43:33,270 --> 00:43:34,350
一致收敛的例子

747
00:43:34,420 --> 00:43:35,280
明白吗?

748
00:43:35,360 --> 00:43:37,870
让我擦几块黑板

749
00:43:37,950 --> 00:43:38,790
我会回来

750
00:43:38,870 --> 00:43:39,900
看看你们有什么问题

751
00:43:39,960 --> 00:43:41,910
这些结论保证你们已经明白了

752
00:43:41,990 --> 00:43:42,980
好的

753
00:43:43,050 --> 00:44:22,160
你们有什么问题?

754
00:44:22,240 --> 00:44:28,800
S:γ的值是怎样计算的?

755
00:44:28,920 --> 00:44:33,080
I:好的 让我们看看

756
00:44:33,160 --> 00:44:36,180
你的问题是γ的值是如何计算的

757
00:44:36,300 --> 00:44:37,860
对于这个结论来说

758
00:44:37,940 --> 00:44:40,180
γ是一个固定值

759
00:44:40,260 --> 00:44:41,100
想象一下

760
00:44:41,160 --> 00:44:42,100
γ是一个我们预先选好的常量

761
00:44:42,180 --> 00:44:44,300
对于任意给定的γ值

762
00:44:44,380 --> 00:44:47,660
这个界都是存在的

763
00:44:47,770 --> 00:44:51,480
稍后  我们会利用这个界

764
00:44:51,560 --> 00:44:53,680
并且进一步地发展我们的结果

765
00:44:53,770 --> 00:44:56,780
那时我们会为γ选定一些具体的值

766
00:44:56,900 --> 00:44:59,130
现在我们只需要知道

767
00:44:59,200 --> 00:45:00,330
我们已经证明了对于任意的γ值

768
00:45:00,400 --> 00:45:01,760
这个结论都是成立的

769
00:45:01,840 --> 00:45:05,650
还有问题吗?什么?

770
00:45:05,730 --> 00:45:08,790
S:这个结论适用于无限假设类吗?

771
00:45:08,870 --> 00:45:09,610
I:好的

772
00:45:09,670 --> 00:45:10,980
对于无限假设类的情形

773
00:45:11,060 --> 00:45:15,170
这个简单的结果并不适用

774
00:45:15,250 --> 00:45:18,360
但是我们会对这种情形进行推广--

775
00:45:18,440 --> 00:45:20,900
可能不会在今天--

776
00:45:20,990 --> 00:45:21,860
我们会在下一节课一开始的时候

777
00:45:21,940 --> 00:45:23,300
对无限假设类的情形进行推广

778
00:45:23,380 --> 00:45:28,560
S:这个结论有哪些实际的用途?

779
00:45:28,670 --> 00:45:30,870
I:这个结论有哪些实际的用途?

780
00:45:30,950 --> 00:45:34,710
我可能稍后会讲到这些内容

781
00:45:34,820 --> 00:45:36,720
我们会具体地讲算法

782
00:45:36,810 --> 00:45:39,030
以及下节课中会讲到

783
00:45:39,150 --> 00:45:41,910
对于这些结论的理解

784
00:45:41,990 --> 00:45:44,780
明白吗?好的

785
00:45:44,860 --> 00:45:47,550
如果到现在为止我证明的东西

786
00:45:47,630 --> 00:45:49,660
你们都明白的话可以举下手吗?

787
00:45:49,740 --> 00:45:51,640
很好 谢谢

788
00:45:51,720 --> 00:45:57,300
好的  让我们来看一下

789
00:45:57,390 --> 00:46:01,320
一致收敛的其他几个表述形式

790
00:46:01,410 --> 00:46:06,920
这种形式实际上就是一个概率的界

791
00:46:07,000 --> 00:46:08,170
它说的是

792
00:46:08,250 --> 00:46:10,580
假设我固定训练集合

793
00:46:10,670 --> 00:46:14,420
并且固定我的误差阈值γ

794
00:46:14,500 --> 00:46:17,740
一致收敛成立的概率是多少?

795
00:46:17,850 --> 00:46:21,930
这个公式给出了这个答案

796
00:46:22,010 --> 00:46:23,390
它给出的是某件事发生的概率

797
00:46:23,550 --> 00:46:28,420
我们有三个感兴趣的参数

798
00:46:28,500 --> 00:46:30,760
一个是  "概率是多少?" 


799
00:46:30,840 --> 00:46:32,400
另外一个是

800
00:46:32,470 --> 00:46:33,780
"训练集合的大小m是多少?"

801
00:46:33,860 --> 00:46:35,440
第三个参数是

802
00:46:35,510 --> 00:46:39,060
"误差阈值γ是多少?"

803
00:46:39,150 --> 00:46:43,630
这里我不会改变k

804
00:46:43,710 --> 00:46:46,720
所以  另外两种等价的表述形式是

805
00:46:46,800 --> 00:46:48,440
你可以问这样的问题

806
00:46:48,520 --> 00:46:58,620
"给定γ 


807
00:46:58,700 --> 00:46:59,440
给定m

808
00:46:59,510 --> 00:47:00,420
一致收敛的概率是多少?"

809
00:47:00,520 --> 00:47:03,570
另外一种等价的表述形式是

810
00:47:03,650 --> 00:47:09,690
给定γ和表示出现了

811
00:47:09,770 --> 00:47:10,970
很大的错误的概率σ

812
00:47:11,060 --> 00:47:18,880
你需要多大的训练集合--

813
00:47:18,960 --> 00:47:26,040
你需要多大的训练集合

814
00:47:26,120 --> 00:47:28,370
以使得你可以获得符合γ

815
00:47:28,450 --> 00:47:29,190
和σ要求的界?

816
00:47:29,270 --> 00:47:30,340
好的

817
00:47:30,440 --> 00:47:36,050
你设

818
00:47:36,130 --> 00:47:38,460
σ=2ke^(-2γ^2 m)

819
00:47:38,540 --> 00:47:40,330
这是刚才结论的左边的式子

820
00:47:40,410 --> 00:47:44,170
如果你求解m

821
00:47:44,250 --> 00:47:49,840
你会发现一个等价的表述形式

822
00:47:49,920 --> 00:47:58,100
只要你的训练集合包含

823
00:47:58,180 --> 00:48:00,170
至少这么多的样本数

824
00:48:00,250 --> 00:48:02,290
这个式子是我对m求解得到的结果

825
00:48:02,360 --> 00:48:06,440
明白吗?

826
00:48:06,520 --> 00:48:07,950
只要m大于等于这个数

827
00:48:08,420 --> 00:48:09,890
那么概率至少为

828
00:48:09,960 --> 00:48:12,390
这里我简写为wp

829
00:48:12,470 --> 00:48:14,710
至少在1-σ的概率下

830
00:48:14,790 --> 00:48:24,600
我们有这个结论成立

831
00:48:24,680 --> 00:48:30,680
明白吗?

832
00:48:30,790 --> 00:48:34,430
所以这说明了

833
00:48:34,580 --> 00:48:35,530
为了保证对于所有的假设

834
00:48:35,640 --> 00:48:36,710
其训练误差与一般误差的差异

835
00:48:36,850 --> 00:48:38,080
都在γ以内的概率至少为1-σ

836
00:48:38,230 --> 00:48:39,400
我们需要多大的训练集合

837
00:48:39,500 --> 00:48:41,750
这个结论给出了答案

838
00:48:41,920 --> 00:48:43,360
我们给它一个名字

839
00:48:43,500 --> 00:48:47,880
称之为"样本复杂度界"

840
00:48:47,990 --> 00:48:50,040
如果你们上过本科生的计算机课程的换

841
00:48:50,110 --> 00:49:02,440
你们可能听说过算法复杂度

842
00:49:02,490 --> 00:49:04,180
它表示为了达到某个目的

843
00:49:04,250 --> 00:49:05,150
需要进行

844
00:49:05,220 --> 00:49:06,260
多少计算

845
00:49:06,340 --> 00:49:09,040
而样本复杂度说的是

846
00:49:09,130 --> 00:49:10,010
为了达到一个特定的错误的界

847
00:49:10,080 --> 00:49:11,860
你需要多大的训练集合

848
00:49:11,940 --> 00:49:15,830
或者多少训练样本

849
00:49:15,910 --> 00:49:19,090
实际上我们之前写的这个定理

850
00:49:19,180 --> 00:49:21,450
你们既可以将其表述成概率界的形式

851
00:49:21,560 --> 00:49:22,780
也可以表述成样本复杂度的界的形式

852
00:49:22,860 --> 00:49:25,240
或者一些其他的形式

853
00:49:25,380 --> 00:49:26,510
我个人觉得

854
00:49:26,660 --> 00:49:29,080
样本复杂度界是最容易解释的

855
00:49:29,220 --> 00:49:30,090
因为它说的是

856
00:49:30,190 --> 00:49:31,340
为了达到某个特定的错误界

857
00:49:31,490 --> 00:49:32,690
你需要多少训练样本

858
00:49:32,800 --> 00:49:37,950
实际上  我们之后会看到

859
00:49:38,030 --> 00:49:40,490
样本复杂度界

860
00:49:40,560 --> 00:49:43,240
经常可以指导我们做出选择

861
00:49:43,310 --> 00:49:45,230
如果你们尝试用

862
00:49:45,310 --> 00:49:46,130
机器学习算法解决某个问题

863
00:49:46,210 --> 00:49:47,750
它能够指导我们

864
00:49:47,820 --> 00:49:50,680
选择需要用到的样本数量

865
00:49:50,760 --> 00:49:57,370
需要注意的是

866
00:49:57,460 --> 00:50:00,830
m是和log k成正比的

867
00:50:00,910 --> 00:50:03,080
而log k作为k的函数

868
00:50:03,190 --> 00:50:04,980
增长速度是非常慢的

869
00:50:05,110 --> 00:50:07,550
log是增长速度最慢的函数之一

870
00:50:07,730 --> 00:50:12,790
它是--你们中可能有人听说过

871
00:50:12,960 --> 00:50:14,370
对于所有的k--

872
00:50:14,510 --> 00:50:19,600
我是从

873
00:50:19,770 --> 00:50:24,060
Carnegie Mellon的一个同事那儿学到的

874
00:50:24,200 --> 00:50:25,470
--在计算机科学中

875
00:50:25,610 --> 00:50:27,950
对于k的所有值

876
00:50:28,090 --> 00:50:31,230
log k不会超过30

877
00:50:31,380 --> 00:50:34,690
所以log是一种最慢的增长函数

878
00:50:34,830 --> 00:50:43,270
而m和log k

879
00:50:43,320 --> 00:50:44,440
是成正比的

880
00:50:44,530 --> 00:50:45,710
这意味着你们可以

881
00:50:45,790 --> 00:50:46,840
在假设类中增加很多的假设

882
00:50:46,920 --> 00:50:48,510
而所需的训练样本数

883
00:50:48,590 --> 00:50:51,400
却不会有太大的提高

884
00:50:51,470 --> 00:50:57,830
这个属性在之后

885
00:50:57,900 --> 00:50:59,130
我们讨论无限假设类的时候

886
00:50:59,210 --> 00:51:00,630
将会非常重要

887
00:51:00,700 --> 00:51:04,660
最后的一种形式是--

888
00:51:04,780 --> 00:51:08,470
有些时候被称为误差界

889
00:51:08,550 --> 00:51:11,790
也就是说给定m和σ不变

890
00:51:11,940 --> 00:51:12,690
求解γ

891
00:51:12,770 --> 00:51:27,200
你得到的结果是

892
00:51:27,320 --> 00:51:31,680
至少在概率1-σ下

893
00:51:31,790 --> 00:51:43,820
对于所有假设类中的假设

894
00:51:43,900 --> 00:51:47,140
训练误差和一般误差之间的差异

895
00:51:47,220 --> 00:51:48,460
将会不超过这个式子

896
00:51:48,540 --> 00:51:53,770
明白吗?

897
00:51:53,890 --> 00:51:55,940
我们仅仅是对γ进行求解

898
00:51:56,020 --> 00:51:59,370
并带入到这个式子中  明白吗?

899
00:51:59,450 --> 00:52:01,240
好的

900
00:52:01,350 --> 00:52:38,010
我们整个证明的

901
00:52:38,090 --> 00:52:39,680
第二部分的证明过程如下

902
00:52:39,770 --> 00:52:47,140
训练误差的结果是

903
00:52:47,230 --> 00:52:49,450
一致收敛的概率会很高

904
00:52:49,540 --> 00:52:52,250
我现在想展示的是

905
00:52:52,330 --> 00:52:54,960
假设一致收敛成立

906
00:52:55,070 --> 00:52:58,590
让我们假设对于所有的假设h

907
00:52:58,670 --> 00:53:01,810
我们都有

908
00:53:01,920 --> 00:53:06,150
|ε(h)-ε ?(h) |小于γ

909
00:53:06,270 --> 00:53:07,140
明白吗?

910
00:53:07,220 --> 00:53:12,720
我们希望做的是

911
00:53:12,890 --> 00:53:15,220
用这个结论来进一步看看

912
00:53:15,370 --> 00:53:34,950
我们对于一般误差能够证明出什么结论

913
00:53:35,090 --> 00:53:35,770
所以我想知道--

914
00:53:35,950 --> 00:53:36,710
假设这些成立--

915
00:53:36,850 --> 00:53:38,520
我想知道我们能不能证明出

916
00:53:38,610 --> 00:53:41,280
一些关于h ?的一般误差的结论

917
00:53:41,390 --> 00:53:48,310
这里h ?表示ERM选择的假设

918
00:53:48,430 --> 00:53:49,930
明白吗?

919
00:53:50,050 --> 00:53:54,610
为了证明  我需要再给一个定义

920
00:53:54,690 --> 00:53:56,020
让我定义h^

921
00:53:56,090 --> 00:54:07,430
将其定义为H中

922
00:54:07,540 --> 00:54:08,980
具有最小的一般误差的假设

923
00:54:09,060 --> 00:54:14,390
所以--如果我有无限多的训练数据

924
00:54:14,450 --> 00:54:16,060
或者说如果我可以从中找到

925
00:54:16,140 --> 00:54:18,290
尽可能好的假设--

926
00:54:18,570 --> 00:54:20,500
尽可能好的假设

927
00:54:20,610 --> 00:54:22,700
意味着其一般误差是最小的--

928
00:54:22,780 --> 00:54:26,670
我会得到哪个假设?明白吗?

929
00:54:26,780 --> 00:54:29,660
所以某种程度上说

930
00:54:29,730 --> 00:54:32,080
将我们的学习算法和h^*的性能

931
00:54:32,170 --> 00:54:34,550
进行对比是有意义的

932
00:54:34,630 --> 00:54:37,970
因为很显然我们不可能比

933
00:54:38,090 --> 00:54:38,870
h^*做的更好

934
00:54:38,940 --> 00:54:45,170
如果你的假设类中的假设

935
00:54:45,280 --> 00:54:46,600
全部是线性决策边界

936
00:54:46,670 --> 00:54:49,170
而且数据并不能够

937
00:54:49,280 --> 00:54:50,450
被任何线性函数划分

938
00:54:50,520 --> 00:54:52,910
那么即使h^*效果再差

939
00:54:52,990 --> 00:54:57,650
你的学习算法

940
00:54:57,730 --> 00:55:00,160
也不可能比

941
00:55:00,240 --> 00:55:01,230
h^*效果更

942
00:55:01,310 --> 00:55:06,090
所以我用三步证明这个结果

943
00:55:06,210 --> 00:55:11,000
h ?的一般误差

944
00:55:11,100 --> 00:55:11,940
也就是我选择的这个假设

945
00:55:12,010 --> 00:55:15,610
一定小于等于这个式子

946
00:55:15,680 --> 00:55:24,830
让我讲这些公式编上号

947
00:55:24,880 --> 00:55:28,150
因为公式(1)

948
00:55:28,250 --> 00:55:33,320
我这里将公式中的h替换成h ?

949
00:55:33,400 --> 00:55:39,350
现在  因为h^

950
00:55:39,730 --> 00:55:46,630
对不起  根据ERM的定义

951
00:55:46,680 --> 00:55:52,180
h ?是训练误差最小的假设

952
00:55:52,250 --> 00:55:54,860
所以不可能有任何假设的训练误差

953
00:55:54,940 --> 00:55:57,300
比h ?更小

954
00:55:57,370 --> 00:56:00,980
所以h ?的训练误差一定

955
00:56:01,060 --> 00:56:05,630
小于等于h^*的训练误差

956
00:56:05,800 --> 00:56:08,810
所以这是由公式(2)得到的

957
00:56:08,920 --> 00:56:10,480
或者说根据h ?的定义得到的

958
00:56:10,590 --> 00:56:13,910
它的训练误差是最小的

959
00:56:14,050 --> 00:56:19,560
最后一步

960
00:56:19,650 --> 00:56:22,640
我会再次用到一致收敛结果

961
00:56:22,750 --> 00:56:23,570
我们知道

962
00:56:23,660 --> 00:56:26,270
ε ?(h^*)与ε(h^*)之间的差异

963
00:56:26,390 --> 00:56:27,690
一定在γ以内

964
00:56:27,770 --> 00:56:35,100
所以它至多等于这项加上γ

965
00:56:35,210 --> 00:56:38,630
注意到我这里还有一个γ  对吗?

966
00:56:38,750 --> 00:56:41,990
所以根据公式(1)--

967
00:56:42,090 --> 00:56:43,320
对不起 


968
00:56:43,440 --> 00:56:46,360
因为我知道h^*的训练误差

969
00:56:46,470 --> 00:56:47,360
和一般误差之间的差异

970
00:56:47,470 --> 00:56:48,960
一定在γ之内

971
00:56:49,070 --> 00:57:00,390
所以  我们这里会写成2γ

972
00:57:00,460 --> 00:57:12,590
明白吗?什么问题?

973
00:57:12,700 --> 00:57:25,010
【听不清】

974
00:57:25,120 --> 00:57:26,660
I:好的

975
00:57:26,740 --> 00:57:28,350
让我将它写在这块黑板上

976
00:57:28,430 --> 00:57:46,010
让我想想  让我将它写在这里

977
00:57:46,320 --> 00:57:52,160
ε ?(h)表示假设h的训练误差

978
00:57:52,270 --> 00:57:54,020
换句话说  给定假设h--

979
00:57:54,100 --> 00:57:55,590
一个假设就是一个函数

980
00:57:55,670 --> 00:57:57,130
从x映射到y--

981
00:57:57,210 --> 00:57:58,840
所以ε ?(h)的意思就是

982
00:57:58,910 --> 00:58:00,570
给定假设h

983
00:58:00,650 --> 00:58:03,890
它分类错误的训练样本

984
00:58:03,960 --> 00:58:07,200
在总训练集合中所占的比例

985
00:58:07,310 --> 00:58:16,540
而h的一般误差的意思是

986
00:58:16,650 --> 00:58:22,230
如果根据分布D采样

987
00:58:22,310 --> 00:58:24,850
得到一个新的样本

988
00:58:24,920 --> 00:58:26,740
对该样本错误分类的概率

989
00:58:26,810 --> 00:58:29,200
明白吗?

990
00:58:29,270 --> 00:58:32,060
【听不清】

991
00:58:32,130 --> 00:58:33,020
I:好的

992
00:58:33,100 --> 00:58:35,770
h ?是由ERM选取的假设

993
00:58:35,850 --> 00:58:41,760
所以当我说到ERM的时候

994
00:58:41,830 --> 00:58:44,140
我的意思是它会选择

995
00:58:44,210 --> 00:58:46,170
具有最小训练误差的假设

996
00:58:46,240 --> 00:58:47,330
也就是ε ?(h)

997
00:58:47,410 --> 00:58:49,760
所以h ?被定义成假设类h中

998
00:58:49,840 --> 00:58:54,970
具有最小的训练误差的那个假设

999
00:58:55,040 --> 00:58:58,770
明白吗?

1000
00:58:58,880 --> 00:59:06,450
好的  什么问题?

1001
00:59:06,530 --> 00:59:12,640
S:h是H的一员是吗?

1002
00:59:12,720 --> 00:59:13,410
I:是的

1003
00:59:13,490 --> 00:59:16,750
【听不清】

1004
00:59:16,820 --> 00:59:20,780
I:我稍后会讲到

1005
00:59:20,860 --> 00:59:28,760
现在让我们将这些联系起来

1006
00:59:28,840 --> 00:59:44,080
提出一个定理

1007
00:59:44,200 --> 00:59:52,230
令H为一个有限的假设类

1008
00:59:52,310 --> 00:59:53,470
包含k个假设

1009
00:59:53,560 --> 00:59:54,800
令m和σ固定

1010
00:59:54,880 --> 01:00:00,340
因为我固定了m和σ

1011
01:00:00,420 --> 01:00:03,240
所以这个定理将会是错误界的形式

1012
01:00:03,310 --> 01:00:04,300
明白吗?

1013
01:00:04,380 --> 01:00:08,350
之后  至少在1-σ的概率下

1014
01:00:08,430 --> 01:00:10,350
我们有这个结论成立

1015
01:00:10,440 --> 01:00:12,710
h ?的一般误差小于等于最小的一般误差

1016
01:00:12,810 --> 01:00:21,320
加上两倍的这个式子

1017
01:00:21,380 --> 01:00:35,730
明白么?

1018
01:00:35,820 --> 01:00:40,300
为了证明这个

1019
01:00:40,380 --> 01:00:42,390
这一项就是ε(h^*)

1020
01:00:42,460 --> 01:00:44,030
为了证明这个结论

1021
01:00:44,090 --> 01:00:46,670
我们令γ等于这个式子--

1022
01:00:46,740 --> 01:00:50,200
这里是两倍的平方根

1023
01:00:50,270 --> 01:00:54,220
为了证明这个式子

1024
01:00:54,300 --> 01:00:56,650
我们令γ等于这个式子

1025
01:00:56,720 --> 01:00:58,160
能再说一遍吗?

1026
01:00:58,230 --> 01:01:04,460
【听不清】

1027
01:01:04,540 --> 01:01:08,380
I:等一下 能再说一遍吗?

1028
01:01:08,440 --> 01:01:11,770
学生:【听不清】

1029
01:01:11,830 --> 01:01:14,990
I:好的  谢谢

1030
01:01:15,100 --> 01:01:17,990
这样写确实没有意义

1031
01:01:18,060 --> 01:01:20,710
谢谢 好的

1032
01:01:20,820 --> 01:01:22,040
令γ等于这个根式

1033
01:01:22,110 --> 01:01:28,550
我们根据之前黑板上的公式(1)

1034
01:01:28,660 --> 01:01:31,080
我们可以得到

1035
01:01:31,160 --> 01:01:32,470
至少在1-σ的概率下

1036
01:01:32,580 --> 01:01:33,800
好的 公式(1)就是一致收敛结果

1037
01:01:33,880 --> 01:01:36,850
即-- 这是之前黑板上的公式(1)

1038
01:01:36,920 --> 01:01:38,550
好的  所以令γ等于这个式子

1039
01:01:38,620 --> 01:01:50,990
我们知道

1040
01:01:51,070 --> 01:01:54,030
至少在1-σ的概率下

1041
01:01:54,100 --> 01:01:55,570
一致收敛是成立的

1042
01:01:55,640 --> 01:01:57,150
当它成立时

1043
01:01:57,220 --> 01:01:59,050
这意味着--

1044
01:01:59,130 --> 01:02:01,550
你知道--

1045
01:02:01,620 --> 01:02:04,790
我们可以将这个公式标记为(*)

1046
01:02:04,870 --> 01:02:06,630
一旦一致收敛成立

1047
01:02:06,710 --> 01:02:09,030
我们可以证明

1048
01:02:09,130 --> 01:02:11,310
根据之前黑板上的结论

1049
01:02:11,380 --> 01:02:13,070
这个结论是成立的

1050
01:02:13,150 --> 01:02:14,880
即:h ?的一般误差

1051
01:02:14,940 --> 01:02:19,700
要小于ε(h^* )＋2γ

1052
01:02:19,780 --> 01:02:20,770
明白吗?

1053
01:02:20,840 --> 01:02:23,450
这样我们就证明了这个定理

1054
01:02:23,530 --> 01:02:43,800
这个结果可以帮助我们

1055
01:02:43,910 --> 01:02:46,860
更好地量化偏差方差权衡

1056
01:02:46,950 --> 01:02:52,680
我在这节课一开始的时候

1057
01:02:52,800 --> 01:02:54,030
提到过这个概念

1058
01:02:54,110 --> 01:02:58,960
具体地  比如说我有一个假设类H

1059
01:02:59,040 --> 01:03:01,860
可能这是一个仅包含线性函数

1060
01:03:01,940 --> 01:03:05,670
线性回归

1061
01:03:05,740 --> 01:03:09,240
或者仅有线性特征的logistic回归的类

1062
01:03:09,320 --> 01:03:12,920
比如说我考虑将这个类

1063
01:03:13,000 --> 01:03:16,510
换成一个包含了

1064
01:03:16,590 --> 01:03:18,770
更多特征的新的类H'

1065
01:03:18,820 --> 01:03:20,030
比如说这是线性的

1066
01:03:20,100 --> 01:03:24,110
这是二次的

1067
01:03:24,190 --> 01:03:29,130
所以所有线性函数构成的假设类是

1068
01:03:29,210 --> 01:03:32,990
所有二次函数构成的假设类的子集

1069
01:03:33,100 --> 01:03:38,370
也就是说H是H^' 的子集

1070
01:03:38,440 --> 01:03:42,800
比如说我考虑不再使用线性假设类

1071
01:03:42,870 --> 01:03:44,890
比如说我打算使用

1072
01:03:44,960 --> 01:03:48,760
一个二次假设类

1073
01:03:48,830 --> 01:03:50,180
或者一个更大的假设类

1074
01:03:50,250 --> 01:03:51,310
那么需要进行哪些权衡呢?

1075
01:03:51,410 --> 01:03:52,210
我们刚刚对于有限假设类证明了这些

1076
01:03:52,320 --> 01:03:54,310
但是我们之后会看到对于无限假设类

1077
01:03:54,380 --> 01:03:55,270
一些相似的结论也是成立的

1078
01:03:55,350 --> 01:03:56,670
权衡指的是

1079
01:03:56,740 --> 01:03:59,650
如果我将H切换成H'

1080
01:03:59,720 --> 01:04:00,600
或者说如果我将

1081
01:04:00,680 --> 01:04:05,650
线性假设类替换成二次假设类

1082
01:04:05,730 --> 01:04:06,480
ε(h^*)将会变得更好

1083
01:04:06,560 --> 01:04:11,680
因为假设类中的最好的假设只可能更好

1084
01:04:11,760 --> 01:04:13,460
最好的二次函数--

1085
01:04:13,560 --> 01:04:17,340
我所说的最好的是就一般误差而言的

1086
01:04:17,420 --> 01:04:18,390
--假设函数--

1087
01:04:18,470 --> 01:04:24,210
具有最小一般误差的二次函数

1088
01:04:24,290 --> 01:04:29,130
相比于最好的线性函数来说

1089
01:04:29,240 --> 01:04:30,800
一定具有相同的或更小的一般误差

1090
01:04:30,920 --> 01:04:33,570
所以通过切换到更复杂的假设类

1091
01:04:33,640 --> 01:04:35,490
你可以让第一项减小

1092
01:04:35,570 --> 01:04:41,380
但是代价是  k会增加

1093
01:04:41,450 --> 01:04:45,380
通过切换到一个更大的假设类

1094
01:04:45,490 --> 01:04:48,650
第一项会减小

1095
01:04:48,760 --> 01:04:49,900
但是第二项会增加

1096
01:04:49,970 --> 01:04:51,770
因为我们现在有了一个更大的假设类

1097
01:04:51,880 --> 01:04:55,490
所以第二项的k将会增加

1098
01:04:55,600 --> 01:05:00,030
这种现象通常被称之为偏差方差权衡

1099
01:05:00,110 --> 01:05:01,960
如果使用一个更大的假设类

1100
01:05:02,050 --> 01:05:04,600
也许我可以找到一个更好的函数

1101
01:05:04,670 --> 01:05:07,250
但是不能精确拟合出模型的风险

1102
01:05:07,330 --> 01:05:12,730
也随之提高

1103
01:05:12,840 --> 01:05:14,380
这是因为

1104
01:05:14,460 --> 01:05:18,150
第二项的值会随着

1105
01:05:18,230 --> 01:05:23,150
你的假设类的规模k的增加而增加

1106
01:05:23,230 --> 01:05:33,670
我们可以非正式地认为

1107
01:05:33,780 --> 01:05:41,290
第一项对应着学习算法的偏差

1108
01:05:41,370 --> 01:05:43,650
或者假设类的偏差

1109
01:05:43,730 --> 01:05:46,810
也可以非正式地

1110
01:05:46,890 --> 01:05:54,100
认为第二项对应着假设的方差

1111
01:05:54,190 --> 01:05:55,170
换句话说

1112
01:05:55,240 --> 01:05:57,090
它表示的是你的假设类

1113
01:05:57,170 --> 01:06:01,940
对于数据的拟合有多好

1114
01:06:02,020 --> 01:06:04,750
通过使用一个更为复杂的假设类

1115
01:06:04,890 --> 01:06:06,440
你的方差会增加

1116
01:06:06,590 --> 01:06:07,460
同时偏差会减小

1117
01:06:07,600 --> 01:06:10,450
提醒一下

1118
01:06:10,600 --> 01:06:12,930
实际上如果你上过统计学课程

1119
01:06:13,080 --> 01:06:14,630
你们可能见过偏差和方差的定义

1120
01:06:14,780 --> 01:06:16,870
通常都是定义成平方误差之类的东西

1121
01:06:17,010 --> 01:06:20,320
实际上  对于分类问题

1122
01:06:20,480 --> 01:06:23,200
实际上并没有统一的

1123
01:06:23,350 --> 01:06:27,700
被广泛接受的对偏差和方差的正式定义

1124
01:06:27,850 --> 01:06:28,970
对于回归问题

1125
01:06:29,070 --> 01:06:35,840
通常将它们定义成平方误差之类的东西

1126
01:06:35,950 --> 01:06:36,720
而对于分类问题

1127
01:06:36,800 --> 01:06:41,670
实际上对于偏差

1128
01:06:41,750 --> 01:06:43,010
和方差有好几种不同的定义方式

1129
01:06:43,100 --> 01:06:44,560
所以当我在课上提到偏差和方差时

1130
01:06:44,650 --> 01:06:46,580
你们可以将它们看成松散

1131
01:06:46,660 --> 01:06:48,360
非正式 直观的定义

1132
01:06:48,450 --> 01:06:49,960
而不是正式的定义

1133
01:06:50,060 --> 01:07:17,520
这两个概念的直观含义

1134
01:07:17,590 --> 01:07:20,090
我可以用一张图展示

1135
01:07:20,170 --> 01:07:28,570
所有的点都是对应一个固定的m

1136
01:07:28,680 --> 01:07:30,310
都是对应于一个固定的训练集合大小m

1137
01:07:30,400 --> 01:07:31,920
纵轴表示误差

1138
01:07:32,000 --> 01:07:36,120
横轴表示模型复杂度

1139
01:07:36,230 --> 01:07:38,240
模型复杂度表示一些诸如:

1140
01:07:38,320 --> 01:07:47,480
多项式的次数

1141
01:07:47,590 --> 01:08:01,760
假设类H的大小等

1142
01:08:01,870 --> 01:08:12,510
实际上  你们可能还记得

1143
01:08:12,590 --> 01:08:13,960
局部加权线性回归中的波长参数

1144
01:08:14,040 --> 01:08:15,700
它同样可以控制模型的复杂度

1145
01:08:15,780 --> 01:08:16,690
你可以简单地将模型复杂度认为是

1146
01:08:16,770 --> 01:08:19,280
多项式的次数

1147
01:08:19,350 --> 01:08:25,150
你的模型越复杂

1148
01:08:25,230 --> 01:08:26,220
你的训练误差就越低

1149
01:08:26,310 --> 01:08:31,860
当你增加模型复杂度的时候

1150
01:08:31,960 --> 01:08:33,820
你的训练误差可能趋近于0

1151
01:08:33,900 --> 01:08:35,450
因为你的模型越复杂

1152
01:08:35,530 --> 01:08:39,850
你对于训练集合的拟合就越好

1153
01:08:39,920 --> 01:08:45,910
由于偏差方差权衡的存在

1154
01:08:45,990 --> 01:08:54,670
你会发现一般误差会先下降

1155
01:08:54,790 --> 01:08:56,330
之后会再上升

1156
01:08:56,410 --> 01:08:58,660
左边的这部分表示数据欠拟合

1157
01:08:58,740 --> 01:09:00,240
同时伴随着较高的偏差

1158
01:09:00,330 --> 01:09:10,690
右边这部分表示数据过拟合

1159
01:09:10,800 --> 01:09:18,440
同时伴随着较高的方差

1160
01:09:18,510 --> 01:09:20,230
明白吗?

1161
01:09:20,320 --> 01:09:21,620
这就是为什么

1162
01:09:21,690 --> 01:09:24,180
如果你想最小化一般误差的话

1163
01:09:24,270 --> 01:09:28,650
一般会倾向于选取中间的模型复杂度

1164
01:09:28,770 --> 01:09:34,530
明白吗?

1165
01:09:34,610 --> 01:09:36,700
这只是一张示意图

1166
01:09:36,780 --> 01:09:38,900
在下讲中  我们会讲一些

1167
01:09:38,990 --> 01:09:40,690
可以自动选取模型复杂度的算法

1168
01:09:40,780 --> 01:09:42,460
从而一般误差

1169
01:09:42,530 --> 01:09:46,210
可以尽可能地接近最小值

1170
01:09:46,280 --> 01:09:50,290
也就是表示最小一般误差的这个区域

1171
01:09:50,410 --> 01:10:12,180
最后我要讲的是

1172
01:10:12,300 --> 01:10:16,820
回到之前我们讲的定理

1173
01:10:16,900 --> 01:10:19,830
我要做的是--

1174
01:10:19,930 --> 01:10:21,400
我写的这个定理是一个

1175
01:10:21,480 --> 01:10:22,440
关于错误界的定理

1176
01:10:22,520 --> 01:10:23,380
它说的是

1177
01:10:23,460 --> 01:10:29,560
对于给定的m和σ

1178
01:10:29,640 --> 01:10:30,770
那么至少在1-σ的概率下

1179
01:10:30,850 --> 01:10:31,760
我会得到γ的界

1180
01:10:31,840 --> 01:10:32,620
用这个式子表示

1181
01:10:32,700 --> 01:10:34,010
所以今天我最后要讲的是回到这个定理

1182
01:10:34,080 --> 01:10:35,430
写出这个定理的一个推论

1183
01:10:35,540 --> 01:10:36,570
我会固定γ

1184
01:10:36,680 --> 01:10:37,930
固定错误界

1185
01:10:38,040 --> 01:10:40,260
固定σ  求解m

1186
01:10:40,350 --> 01:10:42,160
如果你这样做

1187
01:10:42,250 --> 01:10:49,210
你会得到这样的推论

1188
01:10:49,290 --> 01:10:50,140
给定假设类H

1189
01:10:50,220 --> 01:10:54,460
包含k个假设

1190
01:10:54,560 --> 01:11:02,680
给定σ和γ

1191
01:11:02,790 --> 01:11:13,830
为了保证

1192
01:11:13,910 --> 01:11:26,720
我通过ERM选取的假设的一般误差

1193
01:11:26,830 --> 01:11:29,870
最多只会比我

1194
01:11:29,980 --> 01:11:35,320
能在假设类中选取的最好误差多2γ

1195
01:11:35,390 --> 01:11:37,080
我们希望至少在1-σ的概率下

1196
01:11:37,150 --> 01:11:41,220
这个结论是成立的

1197
01:11:41,300 --> 01:11:55,490
所以m需要满足这个条件

1198
01:11:55,580 --> 01:12:00,660
明白吗?

1199
01:12:00,750 --> 01:12:03,000
这样我们根据错误界解出了m

1200
01:12:03,080 --> 01:12:07,350
你们可以将这个定理想的容易一些

1201
01:12:07,420 --> 01:12:12,450
它就是在固定γ

1202
01:12:12,530 --> 01:12:13,930
σ这些量的时候求解m

1203
01:12:13,990 --> 01:12:17,100
从而得到这个结论

1204
01:12:17,170 --> 01:12:19,690
我希望你们回去

1205
01:12:19,760 --> 01:12:21,860
能够自己证实一下这个结论的正确性

1206
01:12:21,930 --> 01:12:22,990
它实际上和我们之前证明的定理

1207
01:12:23,100 --> 01:12:24,280
在逻辑上是直接关联的

1208
01:12:24,350 --> 01:12:24,970
换句话说

1209
01:12:25,040 --> 01:12:27,640
你可以根据那个公式自己解出m

1210
01:12:27,720 --> 01:12:28,970
因为这就是你得到m的公式

1211
01:12:29,040 --> 01:12:30,820
这很容易

1212
01:12:30,930 --> 01:12:33,170
你们应该回去搞清楚这个定理的正确性

1213
01:12:33,250 --> 01:12:36,510
并且明确它和之前的定理

1214
01:12:36,570 --> 01:12:37,820
是逻辑直接相关的

1215
01:12:37,870 --> 01:12:38,800
具体地

1216
01:12:38,910 --> 01:12:41,160
搞清楚如果自己求解的话

1217
01:12:41,230 --> 01:12:42,640
一定能够得到m大于等于这个式子

1218
01:12:42,720 --> 01:12:45,150
并且搞清楚为什么是m大于等于某项

1219
01:12:45,260 --> 01:12:46,270
而不是小于等于

1220
01:12:46,310 --> 01:12:48,300
我这里只是将结论写下来

1221
01:12:48,400 --> 01:12:49,770
请回去自己验证一下

1222
01:12:49,870 --> 01:12:51,840
好吗?

1223
01:12:51,910 --> 01:12:55,400
事实证明

1224
01:12:55,510 --> 01:12:57,840
当我们在学习理论中证明这些界时

1225
01:12:57,910 --> 01:13:00,990
这些常量通常是无关紧要的

1226
01:13:01,050 --> 01:13:02,050
事实证明当我们证明这些界时

1227
01:13:02,160 --> 01:13:04,780
我们通常不会对

1228
01:13:04,850 --> 01:13:08,040
这些常量感兴趣

1229
01:13:08,120 --> 01:13:11,220
所以我将它写成

1230
01:13:11,320 --> 01:13:16,920
O( 1/γ^2  log k/σ)

1231
01:13:16,990 --> 01:13:22,600
关键的部分是

1232
01:13:22,670 --> 01:13:25,620
m对于假设类大小k的依赖是对数函数

1233
01:13:25,720 --> 01:13:28,930
这一点在我们之后讨论

1234
01:13:29,000 --> 01:13:32,760
无限假设类的情形的时候会非常重要

1235
01:13:32,840 --> 01:13:34,280
明白吗?

1236
01:13:34,360 --> 01:13:49,830
还有问题吗?没有了吗?很好

1237
01:13:49,910 --> 01:13:52,460
下一讲我们会回来

1238
01:13:52,540 --> 01:13:54,110
从这个结果开始

1239
01:13:54,190 --> 01:13:54,890
记住这个结果

1240
01:13:54,970 --> 01:13:56,560
在下节课我首先要写的就是这个结果

1241
01:13:56,650 --> 01:13:59,680
之后我会将它推广到无限假设类的情形

1242
01:13:59,800 --> 01:14:01,070
之后我会讲一些

1243
01:14:01,150 --> 01:14:02,280
具体的可以进行模型选择的算法

1244
01:14:02,380 --> 01:14:03,610
几天后见

