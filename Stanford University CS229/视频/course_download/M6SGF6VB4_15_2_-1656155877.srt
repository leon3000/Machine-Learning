1
00:00:18,820 --> 00:00:29,250
Welcome back.

2
00:00:29,340 --> 00:00:31,910
What I want to do today is

3
00:00:32,560 --> 00:00:35,720
continue a discussion of principal components analysis,

4
00:00:35,820 --> 00:00:36,870
or PCA.

5
00:00:36,950 --> 00:00:40,480
In particular, there's one more application that

6
00:00:40,940 --> 00:00:41,820
I didn't get to in the last lecture

7
00:00:42,710 --> 00:00:43,510
on [inaudible] indexing, LSI.

8
00:00:44,430 --> 00:00:46,960
Then I want to spend just a little time

9
00:00:47,500 --> 00:00:49,030
talking about how to implement PCA,

10
00:00:49,950 --> 00:00:51,610
especially for very large problems.

11
00:00:52,280 --> 00:00:53,880
In particular, I'll spend just a little bit of time

12
00:00:54,490 --> 00:00:55,530
talking about singular value decomposition,

13
00:00:56,290 --> 00:00:57,150
or the SVD

14
00:00:57,820 --> 00:00:59,950
implementation of principal component analysis.

15
00:01:00,380 --> 00:01:03,700
So the second half of today's lecture,

16
00:01:03,900 --> 00:01:06,120
I want to talk about the different algorithm

17
00:01:06,680 --> 00:01:08,390
called independent component analysis,

18
00:01:08,930 --> 00:01:09,580
which is,

19
00:01:09,990 --> 00:01:10,820
in some ways, related to PCA,

20
00:01:11,300 --> 00:01:14,870
but in many other ways, it also manages to accomplish

21
00:01:15,330 --> 00:01:16,950
very different things than PCA

22
00:01:17,560 --> 00:01:20,610
So with this lecture, this will actually

23
00:01:21,190 --> 00:01:23,120
wrap up our discussion on unsupervised learning.

24
00:01:23,560 --> 00:01:25,170
The next lecture, we'll

25
00:01:25,750 --> 00:01:26,790
start to talk about reinforcement learning algorithms.

26
00:01:27,520 --> 00:01:34,070
Just to recap where we were with PCA,

27
00:01:34,770 --> 00:01:45,050
principal component analysis, I said that in PCA,

28
00:01:45,460 --> 00:01:47,110
we imagine that we have some very high

29
00:01:47,550 --> 00:01:49,350
dimensional data that perhaps

30
00:01:50,080 --> 00:01:53,240
lies approximately on some low dimensional subspace.

31
00:01:53,720 --> 00:01:54,980
So if you had the data set like this,

32
00:01:55,760 --> 00:01:56,770
you might find that

33
00:01:57,240 --> 00:01:59,380
that's the first principal component of the data,

34
00:01:59,840 --> 00:02:04,490
and that's the second component of this 2-D data.

35
00:02:04,900 --> 00:02:11,630
To summarize the algorithm, we have three steps.

36
00:02:12,000 --> 00:02:15,030
The first step of PCA was to

37
00:02:15,410 --> 00:02:21,300
normalize the data to zero mean and [inaudible].

38
00:02:21,940 --> 00:02:28,810
So tracked out the means of your training examples.

39
00:02:29,150 --> 00:02:31,260
So it now has zero means, and then normalize

40
00:02:31,670 --> 00:02:32,320
each of your features

41
00:02:32,890 --> 00:02:35,290
so that the variance of each feature is now one.

42
00:02:37,990 --> 00:02:40,100
The next step was [inaudible]

43
00:02:40,670 --> 00:02:43,710
computical variance matrix of your zero mean data.

44
00:02:44,250 --> 00:02:46,660
So you compute it as follows.

45
00:02:47,280 --> 00:02:50,030
The sum of all the products,

46
00:02:51,170 --> 00:03:02,190
and then you find the top K eigen vectors of sigma.

47
00:03:02,860 --> 00:03:11,070
So last time we saw the applications of this.

48
00:03:11,460 --> 00:03:15,130
For example, one of the applications was to eigen faces

49
00:03:15,650 --> 00:03:22,380
where each of your training examples, XI, is an image.

50
00:03:22,790 --> 00:03:28,750
So if you have 100 by 100 images,

51
00:03:29,240 --> 00:03:32,770
if your pictures of faces are 100 pixels by 100 pixels,

52
00:03:33,180 --> 00:03:37,810
then each of your training examples, XI, will be a

53
00:03:38,180 --> 00:03:40,190
10,000 dimensional vector, corresponding to

54
00:03:40,820 --> 00:03:44,740
the 10,000 grayscale intensity pixel values.

55
00:03:45,150 --> 00:03:47,260
There are 10,000 pixel values

56
00:03:47,670 --> 00:03:49,390
in each of your 100 by 100 images.

57
00:03:49,910 --> 00:03:52,880
So the eigen faces application was where

58
00:03:53,420 --> 00:03:55,230
the training example

59
00:03:55,620 --> 00:03:57,440
comprised pictures of faces of people.

60
00:03:58,270 --> 00:04:02,310
Then we ran PCA, and then to measure the distance

61
00:04:02,970 --> 00:04:05,640
between say a face here and a face there,

62
00:04:06,000 --> 00:04:06,900
we would project

63
00:04:07,320 --> 00:04:09,250
both of the face images onto the subspace

64
00:04:09,860 --> 00:04:13,180
and then measure the distance along the subspace.

65
00:04:13,600 --> 00:04:14,700
So in eigen faces,

66
00:04:15,380 --> 00:04:17,060
you use something like 50 principle components.

67
00:04:17,560 --> 00:04:21,430
So the difficulty of working

68
00:04:21,850 --> 00:04:22,850
with problems like these is that

69
00:04:23,690 --> 00:04:27,850
in step two of the algorithm, we construct

70
00:04:28,140 --> 00:04:29,520
the covariance matrix sigma.

71
00:04:29,970 --> 00:04:32,450
The covariance matrix now

72
00:04:32,950 --> 00:04:40,170
becomes a 10,000 by 10,000 dimensional matrix,

73
00:04:40,580 --> 00:04:41,170
which is huge.

74
00:04:42,130 --> 00:04:46,560
That has 100 million entries, which is huge.

75
00:04:47,250 --> 00:04:52,290
So let's apply PCA to very, very high dimensional data,

76
00:04:52,760 --> 00:04:54,630
used as a point of reducing the dimension.

77
00:04:55,050 --> 00:04:57,550
But step two of this algorithm had this step

78
00:04:57,820 --> 00:04:59,580
where you were constructing [inaudible].

79
00:04:59,770 --> 00:05:02,060
So this extremely large matrix, which you can't do.

80
00:05:02,150 --> 00:05:04,600
Come back to this in a second.

81
00:05:04,690 --> 00:05:05,850
It turns out one of

82
00:05:05,910 --> 00:05:11,300
the other frequently-used applications of PCA

83
00:05:11,430 --> 00:05:13,070
is actually to text data.

84
00:05:13,170 --> 00:05:15,430
So here's what I mean.

85
00:05:15,540 --> 00:05:20,270
Remember our vectorial representation of emails?

86
00:05:20,910 --> 00:05:23,670
So this is way back when we were talking about

87
00:05:24,040 --> 00:05:25,190
supervised learning algorithms

88
00:05:25,540 --> 00:05:27,540
for a stand classification.

89
00:05:27,960 --> 00:05:30,520
You remember I said that given a piece of email

90
00:05:30,900 --> 00:05:32,000
or given a piece of text document,

91
00:05:32,640 --> 00:05:33,950
you can represent it using

92
00:05:34,360 --> 00:05:35,060
a very high-dimensional vector

93
00:05:35,710 --> 00:05:36,880
by taking

94
00:05:37,510 --> 00:05:41,770
writing downa list of all the words in your dictionary.

95
00:05:42,130 --> 00:05:46,110
Somewhere you had the word learn,

96
00:05:46,580 --> 00:05:50,140
somewhere you have the word study and so on.

97
00:05:50,590 --> 00:05:55,920
Depending on whether each word

98
00:05:56,350 --> 00:05:57,880
appears or does not appear in your text document,

99
00:05:57,980 --> 00:06:00,370
you put either a one or a zero there.

100
00:06:00,970 --> 00:06:02,990
This is a representation we use on an electrode five

101
00:06:03,820 --> 00:06:05,670
or electrode six for representing text documents

102
00:06:06,110 --> 00:06:09,390
for when we're building [inaudible]

103
00:06:09,790 --> 00:06:10,890
based classifiers for [inaudible].

104
00:06:11,560 --> 00:06:14,760
So it turns out

105
00:06:15,390 --> 00:06:17,930
one of the common applications of PCA is

106
00:06:18,520 --> 00:06:21,910
actually this text data representations as well.

107
00:06:22,510 --> 00:06:24,720
When you apply PCA to this sort of data,

108
00:06:25,150 --> 00:06:26,930
the resulting algorithm,

109
00:06:27,500 --> 00:06:29,600
it often just goes by a different name,

110
00:06:29,970 --> 00:06:31,170
just latent semantic indexing.

111
00:06:32,180 --> 00:06:45,000
For the sake of completeness, I should say that in LSI,

112
00:06:45,600 --> 00:06:47,440
you usually skip the preprocessing step.

113
00:07:05,540 --> 00:07:08,110
For various reasons, in LSI,

114
00:07:08,690 --> 00:07:09,990
you usually don't normalize

115
00:07:10,540 --> 00:07:12,070
the mean of the data to one, and

116
00:07:12,670 --> 00:07:13,380
you usually don't normalize

117
00:07:13,980 --> 00:07:15,180
the variance of the features to one.

118
00:07:15,970 --> 00:07:18,880
These are relatively minor differences,

119
00:07:19,410 --> 00:07:22,150
it turns out, so it does something very similar to PCA.

120
00:07:22,670 --> 00:07:28,280
Normalizing the variance to one for text data

121
00:07:28,900 --> 00:07:31,280
would actually be a bad idea because all the words are

122
00:07:32,120 --> 00:07:34,590
because that would

123
00:07:34,690 --> 00:07:38,200
have the affect of dramatically scaling up

124
00:07:38,300 --> 00:07:41,240
the weight of rarely occurring words.

125
00:07:41,700 --> 00:07:43,400
So for example,  the word aardvark

126
00:07:43,770 --> 00:07:45,060
hardly ever appearsin any document.

127
00:07:45,770 --> 00:07:50,200
So to normalize  the variance of the second feature to one,

128
00:07:50,570 --> 00:07:52,220
you end up  you're scaling up

129
00:07:52,650 --> 00:07:53,800
the weight of the word aardvark dramatically.

130
00:07:54,320 --> 00:07:56,560
don't worry if you don't understand what i just said

131
00:07:57,270 --> 00:08:03,280
So let's see. [Inaudible] the language,

132
00:08:03,760 --> 00:08:07,260
something that we want to do quite often is,

133
00:08:07,790 --> 00:08:12,270
give it two documents, XI and XJ,

134
00:08:13,470 --> 00:08:15,690
to measure how similar they are.

135
00:08:20,360 --> 00:08:23,630
So for example, I may give you a document

136
00:08:24,090 --> 00:08:27,480
and ask you to find me more documents like this one.

137
00:08:27,880 --> 00:08:28,450
We're reading

138
00:08:28,940 --> 00:08:30,180
some article about some user event of today

139
00:08:31,140 --> 00:08:33,060
and want to find out  what other news articles there are.

140
00:08:33,190 --> 00:08:35,430
So I give you a document  and ask you

141
00:08:35,440 --> 00:08:37,680
to look at all the other documents

142
00:08:38,190 --> 00:08:39,810
you have in this large set of documents

143
00:08:40,100 --> 00:08:41,390
and find the documents similar to this.

144
00:08:41,840 --> 00:08:46,980
So this is typical text application,

145
00:08:47,370 --> 00:08:54,480
so to measure  the similarity between two documents in XI and XJ,

146
00:08:54,860 --> 00:08:59,450
where again each of these documents is represented

147
00:08:59,830 --> 00:09:01,770
as one of these high-dimensional vectors.

148
00:09:02,310 --> 00:09:04,570
One common way to do this is

149
00:09:04,990 --> 00:09:06,710
to view each of your documents

150
00:09:07,240 --> 00:09:10,730
as some sort of very high-dimensional vector.

151
00:09:11,090 --> 00:09:17,190
So these are vectors in the very high-dimensional space

152
00:09:17,570 --> 00:09:19,440
where the dimension of the vector is

153
00:09:19,780 --> 00:09:21,880
equal to the number of words in your dictionary.

154
00:09:22,260 --> 00:09:28,770
So maybe each of these documents

155
00:09:29,120 --> 00:09:31,260
lives in some 50,000-dimension space,

156
00:09:31,650 --> 00:09:33,150
if you have 50,000 words in your dictionary.

157
00:09:33,690 --> 00:09:36,130
So one nature of the similarity between these two documents

158
00:09:36,480 --> 00:09:39,510
that's often used is what's the angle

159
00:09:41,230 --> 00:09:43,820
between these two documents.

160
00:09:44,400 --> 00:09:55,220
In particular, if the angle between these two vectors is small,

161
00:09:55,600 --> 00:09:58,930
then the two documents, we'll onsider them to be similar.

162
00:09:59,410 --> 00:10:01,670
If the angle between these two vectors is large,

163
00:10:02,170 --> 00:10:03,950
then we consider the documents to be dissimilar.

164
00:10:04,530 --> 00:10:08,500
So more formally, one commonly used heuristic,

165
00:10:08,840 --> 00:10:10,590
the national language of processing, is to say

166
00:10:11,100 --> 00:10:13,350
that the similarity between the two documents is

167
00:10:13,690 --> 00:10:17,420
a co-sine of the angle theta between them. For similar values,

168
00:10:17,910 --> 00:10:24,480
anyway, the co-sine is a decreasing function of theta.

169
00:10:24,860 --> 00:10:27,390
So the smaller the angle between them, the larger the similarity.

170
00:10:27,840 --> 00:10:32,740
The co-sine between two vectors is, of course,

171
00:10:32,830 --> 00:10:44,090
just x(i)T x(j) divided by okay?

172
00:10:44,180 --> 00:10:53,180
That's just the linear algebra or the standard geometry definition

173
00:10:53,630 --> 00:10:55,860
of the co-sine between two vectors.

174
00:11:03,530 --> 00:11:07,080
Here's the intuition behind what LSI is doing.

175
00:11:07,180 --> 00:11:21,540
The hope, as usual, is that there may be some

176
00:11:21,650 --> 00:11:24,030
interesting axis of variations in the data,

177
00:11:24,620 --> 00:11:28,640
and there maybe some other axis that are just noise.

178
00:11:29,230 --> 00:11:30,840
So by projecting all of your data

179
00:11:30,840 --> 00:11:32,640
on lower-dimensional subspace,

180
00:11:33,020 --> 00:11:33,770
the hope is that

181
00:11:34,660 --> 00:11:36,480
by running PCA on your text data this way,

182
00:11:36,800 --> 00:11:38,360
you can remove some of the noise in the data

183
00:11:38,710 --> 00:11:39,590
and get better measures

184
00:11:40,020 --> 00:11:41,670
of the similarity between pairs of documents.

185
00:11:42,040 --> 00:11:43,960
Let's just delve a little deeper into those examples

186
00:11:44,420 --> 00:11:46,240
to convey more intuition about what LSI is doing.

187
00:11:46,670 --> 00:11:50,130
So look further in the definition of

188
00:11:50,520 --> 00:11:51,990
the co-sine similarity measure.

189
00:11:52,350 --> 00:12:05,310
So the numerator or the similarity

190
00:12:05,310 --> 00:12:13,090
between the two documents

191
00:12:13,480 --> 00:12:21,460
was this inner product, which is therefore

192
00:12:21,460 --> 00:12:24,890
sum over K, XIK, XJK.

193
00:12:25,550 --> 00:12:32,840
So this inner product would be equal to zero

194
00:12:33,320 --> 00:12:36,630
if the two documents have no words in common.

195
00:12:37,160 --> 00:12:43,020
So this is really sum over K indicator of

196
00:12:43,440 --> 00:12:56,650
whether documents,I and J, both contain the word, K,

197
00:12:57,090 --> 00:13:00,700
because I guess XIK indicates

198
00:13:01,070 --> 00:13:02,810
whether document I contains the word K,

199
00:13:03,260 --> 00:13:07,270
and XJK indicates whether document J contains the word, K.

200
00:13:07,780 --> 00:13:12,730
So the product would be one only if the word K appears

201
00:13:13,020 --> 00:13:14,330
in both documents.

202
00:13:14,740 --> 00:13:18,170
Therefore, the similarity between these two documents

203
00:13:18,490 --> 00:13:21,210
would be zero if the two documents have no words in common.

204
00:13:21,520 --> 00:13:35,860
For example, suppose your document, XI, has the word study

205
00:13:36,240 --> 00:13:42,570
and the word XJ, has the word learn.

206
00:13:42,950 --> 00:13:45,590
Then these two documents

207
00:13:45,900 --> 00:13:48,680
may be considered entirely dissimilar.

208
00:13:49,180 --> 00:13:52,040
[Inaudible] effective study strategies.

209
00:13:52,900 --> 00:13:54,520
Sometimes you read a news article about that.

210
00:13:55,160 --> 00:13:57,560
So you ask, what other documents are similar to this?

211
00:13:58,010 --> 00:14:00,640
If there are a bunch of other documents

212
00:14:01,000 --> 00:14:02,500
about good methods to learn,

213
00:14:02,850 --> 00:14:04,270
than there are words in common.

214
00:14:04,560 --> 00:14:06,130
So similarity of the two documents is zero.

215
00:14:07,010 --> 00:14:10,260
So here's a cartoon of

216
00:14:10,630 --> 00:14:12,370
what we hope or i hope PCA will do,

217
00:14:12,820 --> 00:14:16,090
which is suppose that on the horizontal axis,

218
00:14:16,850 --> 00:14:22,820
I plot the word learn, and on the vertical access,

219
00:14:23,220 --> 00:14:24,230
I plot the word study.

220
00:14:24,770 --> 00:14:28,630
So the values take on either the value of zero or one.

221
00:14:29,000 --> 00:14:31,110
So if a document contains the words learn

222
00:14:31,570 --> 00:14:34,720
but not study, then it'll plot that document there.

223
00:14:35,610 --> 00:14:36,940
If a document contains

224
00:14:37,420 --> 00:14:38,860
neither the word study nor learn,

225
00:14:39,210 --> 00:14:41,780
then it'll plot that at zero, zero.

226
00:14:42,290 --> 00:14:45,170
So here's a cartoon behind what PCA is doing,

227
00:14:45,760 --> 00:14:49,310
which is we identify lower dimensional subspace.

228
00:14:49,680 --> 00:14:54,030
That would be sum eigen vector, we get out of PCAs.

229
00:14:54,470 --> 00:14:59,890
Now, supposed we have a document about learning.

230
00:15:00,320 --> 00:15:02,110
We have a document about studying.

231
00:15:02,490 --> 00:15:06,090
The document about learning points to the right.

232
00:15:06,170 --> 00:15:07,410
Document about studying points up.

233
00:15:07,490 --> 00:15:09,730
So the inner product, or the co-sine angle

234
00:15:10,240 --> 00:15:13,020
between these two documents would be excuse me.

235
00:15:13,220 --> 00:15:15,700
The inner product between these two documents will be zero.

236
00:15:16,800 --> 00:15:21,800
So these two documents are entirely unrelated,

237
00:15:22,800 --> 00:15:24,830
which is not what we want.

238
00:15:24,830 --> 00:15:28,110
Documents about study,  documents about learning, they are related.

239
00:15:28,350 --> 00:15:31,160
But we take these two documents,

240
00:15:31,160 --> 00:15:33,740
and we project them onto this subspace.

241
00:15:34,290 --> 00:15:41,330
Then these two documents now become much closer together,

242
00:15:41,920 --> 00:15:44,140
and the algorithm will recognize that

243
00:15:44,530 --> 00:15:47,370
when you say the inner product between these two documents,

244
00:15:47,730 --> 00:15:49,520
you actually end up with a positive number.

245
00:15:49,940 --> 00:15:53,790
So LSI enables our algorithm to recognize that

246
00:15:54,070 --> 00:15:54,740
these two documents

247
00:15:55,140 --> 00:15:56,780
have some positive similarity between them.

248
00:15:57,340 --> 00:16:02,360
So that's just intuition about what PCA

249
00:16:02,900 --> 00:16:04,460
may be doing to text data.

250
00:16:04,840 --> 00:16:06,800
The same thing goes to other examples

251
00:16:07,090 --> 00:16:08,110
and the words study and learn.

252
00:16:08,460 --> 00:16:10,970
So you have you find a document about

253
00:16:11,400 --> 00:16:12,290
politicians

254
00:16:12,730 --> 00:16:15,330
and a document with the names of prominent politicians.

255
00:16:15,760 --> 00:16:20,620
That will also bring the documents closer together,

256
00:16:20,710 --> 00:16:26,080
or just any related topics, they end up mat points

257
00:16:26,180 --> 00:16:28,350
closer together and just lower dimensional space.

258
00:16:31,940 --> 00:16:34,570
Question about this?

259
00:16:36,590 --> 00:16:40,030
student: that sign over there i or k?

260
00:16:40,110 --> 00:16:42,850
Instructor (Andrew Ng):Which ones? This one?

261
00:16:43,480 --> 00:16:44,500
Interviewee: No, the line.

262
00:16:44,850 --> 00:16:51,790
Instructor (Andrew Ng):Oh, this one. Oh, yes. Thank you.

263
00:16:53,800 --> 00:17:07,370
[Inaudible]. So let's talk about

264
00:17:07,670 --> 00:17:16,530
how to actually implement this now. Okay.

265
00:17:16,980 --> 00:17:21,550
How many of you know what an SVD

266
00:17:21,860 --> 00:17:25,640
or single value decomposition is? Wow, that's a lot of you.

267
00:17:26,040 --> 00:17:28,550
That's a lot more than I thought. Curious.

268
00:17:28,640 --> 00:17:32,070
Did you guys learn it as under grads or as graduate students?

269
00:17:34,610 --> 00:17:41,850
All right. Let me talk about it anyway.

270
00:17:42,900 --> 00:17:46,870
I wasn't expecting so many of you to know what SVD is,

271
00:17:47,240 --> 00:17:49,000
but I want to get this on tape,

272
00:17:49,330 --> 00:17:51,890
just so everyone else can learn about this, too.

273
00:17:51,970 --> 00:17:59,020
So I'll say a little bit about how to implement PCA.

274
00:17:59,110 --> 00:18:03,290
The problem I was eluding to just now was that

275
00:18:04,370 --> 00:18:06,780
when you have these very high-dimensional vectors,

276
00:18:07,280 --> 00:18:08,190
than sigma is a large matrix.

277
00:18:08,760 --> 00:18:12,210
In particular, for our text example,

278
00:18:12,890 --> 00:18:16,620
if the vectors XI are 50,000 dimensional,

279
00:18:17,810 --> 00:18:19,620
then the covariance matrix will be

280
00:18:20,120 --> 00:18:26,930
50,000 dimensional by 50,000 dimensional,

281
00:18:27,440 --> 00:18:29,740
which is much too big to represent explicitly.

282
00:18:31,080 --> 00:18:37,970
I guess many of you already know this,

283
00:18:38,270 --> 00:18:39,810
but I'll just say it anyway.

284
00:18:40,800 --> 00:18:43,540
It turns out there's another way to implement PCA,

285
00:18:43,850 --> 00:18:51,950
which is if A is any N by N matrix,

286
00:18:52,670 --> 00:18:56,980
than one of the most remarkable results of linear algebra is that

287
00:18:57,370 --> 00:19:02,240
the matrix, A, can be decomposed into

288
00:19:02,860 --> 00:19:06,080
a singular value decomposition.

289
00:19:07,270 --> 00:19:12,080
What that means is that the matrix, A, which is N by N,

290
00:19:12,700 --> 00:19:16,650
can always be decomposed into a product of three matrixes.

291
00:19:17,080 --> 00:19:20,990
U is N by N, D is a square matrix, which is N by N,

292
00:19:21,430 --> 00:19:35,960
and V is also N by N. D is going to be diagonal.

293
00:19:36,470 --> 00:19:45,600
Zeros are on the off-diagonals, and

294
00:19:46,210 --> 00:19:47,680
the values sigma I are called

295
00:19:48,550 --> 00:19:55,410
the singular values of the matrix A.

296
00:19:55,500 --> 00:20:04,000
Almost all of you said you learned this as a graduate student,

297
00:20:04,650 --> 00:20:06,540
rather than as an under grad,  so it turns out that

298
00:20:07,130 --> 00:20:09,350
when you take a class in undergraduate linear algebra,

299
00:20:09,730 --> 00:20:11,240
usually you learn a bunch of decomposition.

300
00:20:11,630 --> 00:20:13,930
So you usually learn about the QLD composition,

301
00:20:14,270 --> 00:20:15,810
maybe the LU factorization of the matrixes.

302
00:20:16,940 --> 00:20:18,130
Most under grad courses

303
00:20:18,530 --> 00:20:20,540
don't get to talk about singular value decompositions,

304
00:20:20,830 --> 00:20:26,010
but at least in almost everything I do in machine learning,

305
00:20:26,420 --> 00:20:29,700
you actually find that you end up using SVDs

306
00:20:29,990 --> 00:20:31,470
much more than any of the decompositions

307
00:20:31,880 --> 00:20:35,100
you learned in typical under grad linear algebra class.

308
00:20:35,460 --> 00:20:37,190
So personally,

309
00:20:37,590 --> 00:20:40,260
I have already used an SVD dozens of times in the last year,

310
00:20:41,030 --> 00:20:43,580
but LU and QRD compositions,

311
00:20:44,010 --> 00:20:45,010
I think I used the

312
00:20:45,280 --> 00:20:46,280
QRD composition once

313
00:20:46,710 --> 00:20:48,000
and an LU decomposition in the last year.

314
00:20:48,800 --> 00:20:55,850
So let's see. I'll say a bit more about this.

315
00:20:56,290 --> 00:21:03,630
So I'm going to draw the picture, I guess.

316
00:21:04,090 --> 00:21:09,870
For example, if A is an N by N matrix,

317
00:21:10,240 --> 00:21:13,260
it can be decomposed into another matrix, U,

318
00:21:13,560 --> 00:21:14,570
which is also N by N.

319
00:21:14,970 --> 00:21:25,390
It's the same size, D, which is N by N.

320
00:21:25,760 --> 00:21:32,340
Another square matrix, V transpose,

321
00:21:32,720 --> 00:21:33,760
which is also N by N.

322
00:21:34,190 --> 00:21:39,710
Furthermore, in a singular value decomposition,

323
00:21:40,160 --> 00:21:44,200
the columns of the matrix, U,

324
00:21:44,620 --> 00:21:52,040
will be the eigen vectors of A transpose,

325
00:21:52,400 --> 00:21:56,640
and the columns of V will

326
00:21:57,100 --> 00:22:06,750
be the eigen vectors of A transpose A.

327
00:22:07,560 --> 00:22:13,090
To compute it,

328
00:22:13,460 --> 00:22:18,220
you just use the SVD commands in Matlab or Octave.

329
00:22:18,880 --> 00:22:25,950
Today, say the art in numerical linear algebra is that

330
00:22:26,450 --> 00:22:29,390
SVD, singular value decompositions,

331
00:22:29,750 --> 00:22:32,130
and matrixes can be computed extremely stably.

332
00:22:33,040 --> 00:22:37,270
We've used a package like Matlab or Octave to compute,

333
00:22:38,250 --> 00:22:39,940
say, the eigen vectors of a matrix.

334
00:22:40,450 --> 00:22:46,530
So if SVD routines are even more numerically stable than

335
00:22:46,970 --> 00:22:48,330
eigen vector routines for finding

336
00:22:48,710 --> 00:22:49,630
eigen vector in the matrix.

337
00:22:50,360 --> 00:22:52,650
So you can safely use a routine like this,

338
00:22:52,990 --> 00:22:54,530
and similar to the way they use a square root command

339
00:22:54,830 --> 00:22:56,090
without thinking about how it's computed.

340
00:22:56,420 --> 00:22:57,890
You can compute the square root of something

341
00:22:58,290 --> 00:22:59,910
and just not worry about it.

342
00:23:00,490 --> 00:23:02,000
You know the computer will give you the right answer.

343
00:23:02,410 --> 00:23:05,500
For most reasonably-sized matrixes,

344
00:23:05,890 --> 00:23:06,580
even up to

345
00:23:06,940 --> 00:23:09,000
thousands by thousands matrixes, the SVD routine,

346
00:23:09,370 --> 00:23:11,260
I think of it as a square root function.

347
00:23:11,610 --> 00:23:13,770
If you call it, it'll give you back the right answer.

348
00:23:14,150 --> 00:23:16,050
You don't have to worry too much about it.

349
00:23:16,420 --> 00:23:18,340
If you have extremely large matrixes,

350
00:23:18,670 --> 00:23:19,550
like a million by a million matrixes,

351
00:23:20,200 --> 00:23:21,900
I might start to worry a bit,

352
00:23:22,300 --> 00:23:23,940
but a few thousand by a few thousand matrixes,

353
00:23:24,290 --> 00:23:26,700
this is implemented very well today.

354
00:23:28,630 --> 00:23:30,050
Interviewee: [Inaudible].

355
00:23:30,410 --> 00:23:32,140
Instructor (Andrew Ng):What's the complexity of SVD?

356
00:23:32,230 --> 00:23:33,090
That's a good question. I actually don't know.

357
00:23:33,180 --> 00:23:35,990
I want to guess it's roughly on the order of N-cubed.

358
00:23:36,080 --> 00:23:37,440
I'm not sure.

359
00:23:37,440 --> 00:23:38,440
[Inaudible] algorithms, so I think –I don't know

360
00:23:44,710 --> 00:23:46,850
what's known about the conversion of these algorithms.

361
00:23:49,270 --> 00:23:57,350
The example I drew out was for a fat matrix,

362
00:23:57,730 --> 00:23:59,430
the matrix is wider than its tall.

363
00:23:59,930 --> 00:24:05,630
In the same way, you can also call SVD on the tall matrix,

364
00:24:06,140 --> 00:24:07,630
so it's taller than it's wide.

365
00:24:07,960 --> 00:24:20,370
It would decompose it into okay?

366
00:24:20,750 --> 00:24:22,660
A product of three matrixes like that.

367
00:24:23,580 --> 00:24:32,340
The nice thing about this is that  we can use it to

368
00:24:32,660 --> 00:24:36,040
compute eigen vectors and PCA very efficiently.

369
00:24:38,530 --> 00:24:49,250
In particular, a covariance matrix sigma was this.

370
00:24:49,990 --> 00:24:54,450
It was the sum of all the products,

371
00:24:55,550 --> 00:24:57,250
so if you go back

372
00:24:57,680 --> 00:25:01,170
and recall the definition of the design matrix

373
00:25:01,730 --> 00:25:06,700
I think I described this in lecture two

374
00:25:07,110 --> 00:25:09,850
when we derived the close form solution to

375
00:25:10,220 --> 00:25:11,880
these squares [inaudible] these squares.

376
00:25:12,280 --> 00:25:14,670
The design matrix was this matrix

377
00:25:15,130 --> 00:25:26,280
where I took my examples and stacked them in rows.

378
00:25:26,790 --> 00:25:29,570
They call this the design matrix [inaudible].

379
00:25:30,220 --> 00:25:35,070
So if you construct the design matrix,

380
00:25:35,500 --> 00:25:40,180
then the covariance matrix sigma

381
00:25:41,010 --> 00:25:45,330
can be written just X transposing.

382
00:26:01,060 --> 00:26:03,060
That's X transposed, and [inaudible].

383
00:26:10,530 --> 00:26:19,710
Okay? I hope you see why the X transpose X

384
00:26:20,120 --> 00:26:21,680
gives you the sum of products of vectors.

385
00:26:22,980 --> 00:26:24,490
If you aren't seeing this right now,

386
00:26:24,830 --> 00:26:27,490
just go home and convince yourself [inaudible] if it's true.

387
00:26:36,530 --> 00:26:42,780
To get the top K eigen vectors of sigma,

388
00:26:50,800 --> 00:27:02,470
you would take sigma and decompose it using the excuse me.

389
00:27:03,270 --> 00:27:09,710
You would take the matrix X, and you would compute as SVD.

390
00:27:10,290 --> 00:27:11,760
So you get USV transpose.

391
00:27:12,370 --> 00:27:19,350
Then the top three columns of U are

392
00:27:19,780 --> 00:27:32,390
the top K eigen vectors of X transpose X, which is therefore,

393
00:27:32,990 --> 00:27:38,730
the top K eigen vectors of your covariance matrix sigma.

394
00:27:39,460 --> 00:27:48,460
So in our examples, the design matrix may be, say R.

395
00:27:49,090 --> 00:27:51,390
If you have 50,000 words in your dictionary,

396
00:27:52,140 --> 00:27:56,230
than the design matrix would be RM by 50,000.

397
00:27:56,800 --> 00:28:01,230
[Inaudible] say 100 by 50,000, if you have 100 examples.

398
00:28:01,900 --> 00:28:05,660
So X would be quite tractable to represent and compute the SVD,

399
00:28:06,040 --> 00:28:10,420
whereas the matrix sigma would be much harder to represent.

400
00:28:10,890 --> 00:28:12,430
This is 50,000 by 50,000.

401
00:28:12,840 --> 00:28:16,720
So this gives you an efficient way to implement PCA.

402
00:28:17,830 --> 00:28:21,330
The reason I want to talk about this is in previous years,

403
00:28:21,410 --> 00:28:24,410
i didn't talk about it in...

404
00:28:25,280 --> 00:28:27,280
in the class projects, I found a number of students trying to

405
00:28:27,280 --> 00:28:30,540
implement SVD on huge problems and [inaudible],

406
00:28:31,170 --> 00:28:34,320
so this is a much better to implement PCA

407
00:28:35,020 --> 00:28:36,990
if you have extremely high dimensional data.

408
00:28:37,510 --> 00:28:39,260
If you have low dimensional data,

409
00:28:39,670 --> 00:28:41,380
if you have 50 or 100 dimensional data,

410
00:28:41,670 --> 00:28:44,620
then computing sigma's no problem.

411
00:28:44,970 --> 00:28:46,560
You can do it the old way, but otherwise,

412
00:28:47,010 --> 00:28:48,010
use the SVD to implement this.

413
00:28:48,860 --> 00:28:53,890
Questions about this?

414
00:29:25,630 --> 00:29:29,120
The last thing I want to say is that in practice,

415
00:29:29,540 --> 00:29:31,670
when you want to implement this,I want to say a note of caution.

416
00:29:32,500 --> 00:29:38,740
It turns out that for many applications of let's see.

417
00:29:39,400 --> 00:29:41,700
When you apply SVD to these wide yeah.

418
00:29:42,370 --> 00:29:43,550
Interviewee: Just a quick question.

419
00:29:43,940 --> 00:29:45,670
Are the top K columns of U or V

420
00:29:45,980 --> 00:29:50,830
because X transposed X is V transpose, right?

421
00:29:51,280 --> 00:29:54,710
Instructor (Andrew Ng):Let's see. Oh, yes.

422
00:29:55,520 --> 00:29:59,890
I think you're right.I think you're right.

423
00:30:00,670 --> 00:30:08,400
Let's see. Is it top K columns of U or top K of V?

424
00:30:09,920 --> 00:30:33,080
Yeah, I think you're right. Is that right?

425
00:30:33,640 --> 00:30:37,110
Something bothers me about that, but I think you're right.

426
00:30:42,080 --> 00:30:44,160
Interviewee: [Inaudible], so then X transpose X should be VDD.

427
00:30:45,070 --> 00:30:50,220
X is UDV, so X transpose X would be

428
00:30:51,080 --> 00:30:53,350
Instructor (Andrew Ng):[Inaudible].

429
00:30:59,170 --> 00:31:04,040
If anyone thinks about this and has another opinion,

430
00:31:04,600 --> 00:31:05,830
let me know, but I think you're right.

431
00:31:06,750 --> 00:31:08,750
I'll make sure I get the details and let you know.

432
00:31:15,940 --> 00:31:19,920
Everyone's still looking at that.

433
00:31:22,090 --> 00:31:24,390
Tom, can you figure out the right answer and let me know?

434
00:31:25,090 --> 00:31:26,400
Male Speaker: That sounds right.

435
00:31:27,940 --> 00:31:29,940
Instructor (Andrew Ng):Okay. Cool. Okay.

436
00:31:30,530 --> 00:31:32,920
So just one last note, a note of caution.

437
00:31:33,600 --> 00:31:34,930
It turns out that in this example,

438
00:31:35,540 --> 00:31:38,250
I was implementing SVD with a wide matrix.

439
00:31:38,750 --> 00:31:44,550
So the matrix X was N by N.

440
00:31:45,280 --> 00:31:49,570
It turns out when you find the SVD decomposition of this,

441
00:31:50,120 --> 00:31:57,190
it turns out that let's see.

442
00:31:57,890 --> 00:32:00,630
Yeah, I think you're definitely right.

443
00:32:01,270 --> 00:32:03,680
So it turns out that we find the SVD of this,

444
00:32:04,130 --> 00:32:07,260
the right-most portion of this block of this matrix

445
00:32:07,650 --> 00:32:08,830
would be all zeros.

446
00:32:09,320 --> 00:32:17,160
Also, when you compute the matrix, D,

447
00:32:17,790 --> 00:32:21,010
a large part of this matrix would be zeros.

448
00:32:21,890 --> 00:32:23,800
You have the matrix D transpose.

449
00:32:24,390 --> 00:32:29,900
So depending on what convention you use, for example,

450
00:32:30,350 --> 00:32:33,160
I think Matlab actually uses a convention of just

451
00:32:33,780 --> 00:32:37,770
cutting off the zero elements.

452
00:32:47,250 --> 00:32:50,320
So the Matlab uses the convention of

453
00:32:50,770 --> 00:32:53,450
chopping off the right-most half of the U matrix and

454
00:32:53,530 --> 00:32:55,400
chopping off the bottom portion of the D matrix.

455
00:32:55,490 --> 00:32:59,950
I'm not sure if this even depends on the version of Matlab,

456
00:33:00,170 --> 00:33:01,400
but when you call SVD on Matlab

457
00:33:01,400 --> 00:33:03,740
or some other numerical algebra packages,

458
00:33:04,240 --> 00:33:07,510
there's slightly different conventions of how to define your SVD

459
00:33:07,870 --> 00:33:10,030
when the matrix is wider than it is tall.

460
00:33:10,470 --> 00:33:12,490
So just watch out for this and make sure

461
00:33:12,850 --> 00:33:15,440
you map whatever convention your numerical algebra library

462
00:33:15,870 --> 00:33:19,960
uses to the original computations.

463
00:33:21,590 --> 00:33:24,780
It turns out if you turn Matlab [inaudible]

464
00:33:25,170 --> 00:33:27,530
or you're writing C code.

465
00:33:27,970 --> 00:33:29,760
There are many scientific libraries that

466
00:33:30,250 --> 00:33:32,250
can compute SVDs for you,

467
00:33:32,740 --> 00:33:35,500
but they're just slightly different in conventions

468
00:33:36,390 --> 00:33:37,870
for the dimensions of these matrixes.

469
00:33:38,350 --> 00:33:39,040
So just make sure

470
00:33:39,430 --> 00:33:40,760
you figure this out for the package that you use.

471
00:33:41,370 --> 00:33:45,800
Finally, I just want to take

472
00:33:46,170 --> 00:33:48,050
the unsupervised learning algorithms we talked about

473
00:33:48,460 --> 00:33:51,010
and just put a little bit of broader context.

474
00:33:51,390 --> 00:33:53,750
This is partly in response to the questions

475
00:33:54,100 --> 00:33:55,660
I've gotten from students in office hours and elsewhere

476
00:33:56,420 --> 00:34:00,300
about when to use each of these algorithms.

477
00:34:01,540 --> 00:34:03,040
So I'm going to draw a two by two matrix.

478
00:34:03,460 --> 00:34:09,210
This is a little cartoon that I find useful.

479
00:34:09,850 --> 00:34:17,940
One of the algorithms we talked about earlier, right before this,

480
00:34:18,290 --> 00:34:21,340
was factor analysis, which was it was

481
00:34:21,940 --> 00:34:25,760
I hope you remember that picture I drew

482
00:34:26,110 --> 00:34:27,960
where I would have a bunch of point Z on the line.

483
00:34:28,560 --> 00:34:30,980
Then I had these ellipses that I drew.

484
00:34:31,350 --> 00:34:36,260
I hope you remember that picture.

485
00:34:36,650 --> 00:34:37,970
This was a factor analysis model

486
00:34:38,390 --> 00:34:41,030
which models the density effects of gaussian, right?

487
00:34:41,510 --> 00:34:43,860
It was also a PCA, just now.

488
00:34:44,370 --> 00:34:47,800
So the difference between factor analysis and PCA,

489
00:34:48,190 --> 00:34:50,020
the way I think about it,

490
00:34:50,280 --> 00:34:53,870
is that factor analysis is a density estimation algorithm.

491
00:34:54,510 --> 00:34:57,560
It tries to model the density of the training example's X.

492
00:34:58,180 --> 00:35:03,640
Whereas PCA is not a probabilistic algorithm.

493
00:35:04,380 --> 00:35:07,300
In particular, it does not endow

494
00:35:07,740 --> 00:35:11,660
your training examples of any probabilistic distributions

495
00:35:11,940 --> 00:35:13,910
and directly goes to find the subspace.

496
00:35:14,210 --> 00:35:16,350
So in terms of when to use factor analysis

497
00:35:16,810 --> 00:35:17,950
and when to use PCA,

498
00:35:18,230 --> 00:35:21,290
if your goal is to reducethe dimension of the data,

499
00:35:21,810 --> 00:35:24,910
if your goal is to find the subspace that the data lies on,

500
00:35:25,270 --> 00:35:30,330
then PCA directly tries to find the subspace.

501
00:35:30,730 --> 00:35:33,220
I think I would tend to use PCA.

502
00:35:33,580 --> 00:35:39,920
Factor analysis, it sort of assumes the data lies on a subspace.

503
00:35:40,520 --> 00:35:42,090
Let me write a subspace here.

504
00:35:42,750 --> 00:35:47,830
So both of these algorithms sort of assume the data

505
00:35:48,100 --> 00:35:51,700
maybe lies close or on some low dimensional subspace,

506
00:35:52,060 --> 00:35:53,780
but fundamentally, factor analysis,

507
00:35:54,170 --> 00:35:56,290
I think of it as a density estimation algorithm.

508
00:35:56,580 --> 00:35:58,870
So that has some very high dimensional distribution.

509
00:35:59,160 --> 00:36:01,430
I want to model P of X,

510
00:36:02,070 --> 00:36:03,270
then the factor analysis is the algorithm

511
00:36:03,270 --> 00:36:04,600
I'm more inclined to use.

512
00:36:05,960 --> 00:36:07,560
So even though you could in theory,

513
00:36:08,050 --> 00:36:11,890
I would tend to avoid trying to use factor analysis to

514
00:36:12,210 --> 00:36:14,690
identify a subspace the data set lies on.

515
00:36:15,200 --> 00:36:19,740
So gaussian, if you want to do anomaly detection,

516
00:36:20,100 --> 00:36:21,390
if you want to model P of X

517
00:36:21,630 --> 00:36:23,500
so that if you have a very low probability of N,

518
00:36:23,500 --> 00:36:25,740
you can factor an anomaly, then I would

519
00:36:26,970 --> 00:36:29,550
tend to use factor analysis to do that density estimation.

520
00:36:30,490 --> 00:36:34,270
So factor analysis and PCA are both algorithms

521
00:36:34,930 --> 00:36:37,280
that assume that your data lies in the subspace.

522
00:36:37,820 --> 00:36:40,360
The other cause of algorithms we talked about was

523
00:36:40,880 --> 00:36:47,630
algorithms that assumes the data lies in clumps

524
00:36:48,010 --> 00:36:53,940
or that the data has a few coherence to groups.

525
00:36:54,420 --> 00:36:56,400
So let me just fill in the rest of this picture.

526
00:37:07,930 --> 00:37:10,960
So if you think your data lies in clumps

527
00:37:11,360 --> 00:37:12,200
or lies in groups,

528
00:37:12,720 --> 00:37:15,600
and if it goes [inaudible] density estimation,

529
00:37:15,920 --> 00:37:18,500
then I would tend to use a mixture of [inaudible] algorithm.

530
00:37:18,930 --> 00:37:21,840
But again, you don't necessarily want to

531
00:37:22,200 --> 00:37:24,190
endow your data of any probably semantics,

532
00:37:24,670 --> 00:37:26,390
so if you just want to find the clumps of the groups,

533
00:37:26,810 --> 00:37:28,540
then I'd be inclined to use a K-Means algorithm.

534
00:37:29,110 --> 00:37:31,910
So haven't seen anyone else draw this picture before,

535
00:37:32,300 --> 00:37:34,320
but I tend to organize these things this way in my brain.

536
00:37:34,890 --> 00:37:36,190
Hopefully this helps guide

537
00:37:36,800 --> 00:37:39,270
when you might use each of these algorithms as well,

538
00:37:39,650 --> 00:37:42,300
depending on whether you believe the data

539
00:37:42,590 --> 00:37:43,650
might lie in the subspace

540
00:37:44,010 --> 00:37:45,790
or whether it might bind in clumps or groups.

541
00:37:51,480 --> 00:37:56,190
All right. That wraps up the discussion on PCA.

542
00:38:00,770 --> 00:38:04,940
What I want to do next is talk about

543
00:38:07,840 --> 00:38:13,530
independent component analysis, or ICA. Yeah.

544
00:38:13,650 --> 00:38:16,290
Interviewee: I have a question about the upper right corner.

545
00:38:16,390 --> 00:38:19,540
So once you have all of the eigen vectors,

546
00:38:19,640 --> 00:38:23,020
is that anyone knows something like OK

547
00:38:23,130 --> 00:38:24,530
[inaudible] how similar is feature I to feature J.

548
00:38:24,650 --> 00:38:28,450
You pick some eigen vector,

549
00:38:28,550 --> 00:38:29,960
and you take some dot products between

550
00:38:30,330 --> 00:38:34,390
the feature I and feature J and the eigen vector.

551
00:38:35,060 --> 00:38:37,040
But there's a lot of eigen vectors to choose from.

552
00:38:38,020 --> 00:38:38,660
Instructor (Andrew Ng):Right.

553
00:38:39,260 --> 00:38:41,430
So Justin's question was

554
00:38:41,930 --> 00:38:43,720
having found my eigen vectors,

555
00:38:44,050 --> 00:38:46,630
how do I choose what eigen vector to use to measure distance.

556
00:38:47,020 --> 00:38:54,140
I'm going to start this up.

557
00:38:55,410 --> 00:38:59,240
So the answer is really in this cartoon,

558
00:38:59,760 --> 00:39:03,490
I would avoid thinking about eigen vectors one other time.

559
00:39:03,850 --> 00:39:06,650
A better way to view this cartoon is that this is actually

560
00:39:07,240 --> 00:39:10,290
if I decide to choose 100 eigen vectors,

561
00:39:10,700 --> 00:39:13,060
this is really 100 D subspace.

562
00:39:18,880 --> 00:39:23,360
So I'm not actually projecting my data onto one eigen vector.

563
00:39:23,790 --> 00:39:27,250
This arrow, this cartoon,

564
00:39:27,640 --> 00:39:29,070
this denotes

565
00:39:29,340 --> 00:39:31,150
the 100-dimensional subspace[inaudible] by all my eigen vectors.

566
00:39:31,410 --> 00:39:35,290
So what I actually do is project my data onto the span,

567
00:39:36,340 --> 00:39:38,170
the linear span of eigen vectors.

568
00:39:39,500 --> 00:39:42,150
Then I measure distance or take inner products of the distance

569
00:39:42,150 --> 00:39:45,930
between the projections of the two points of the eigen vectors.

570
00:39:46,830 --> 00:39:49,100
Okay.

571
00:39:49,830 --> 00:39:56,290
So let's talk about ICA, independent component analysis.

572
00:39:56,920 --> 00:40:01,990
So whereas PCA was an algorithm for finding

573
00:40:02,310 --> 00:40:05,550
what I call the main axis of variations of data,

574
00:40:05,880 --> 00:40:08,730
in ICA, we're going to try find

575
00:40:09,040 --> 00:40:11,520
the independent of components of variations in the data.

576
00:40:12,050 --> 00:40:14,510
So switch it to the laptop there, please.

577
00:40:15,010 --> 00:40:17,750
We'll just take a second to motivate that.

578
00:40:18,110 --> 00:40:30,840
I'm going to do so by although if you put on the okay.

579
00:40:31,240 --> 00:40:34,240
This is actually a slide that

580
00:40:34,730 --> 00:40:38,760
I showed in lecture one of the cocktail party problem.

581
00:40:39,150 --> 00:40:42,100
Suppose you have two speakers at a cocktail party,

582
00:40:42,620 --> 00:40:44,440
and you have two microphones in the room,

583
00:40:44,710 --> 00:40:47,010
overlapping sets of two conversations.

584
00:40:47,980 --> 00:40:49,860
Then can you separate out t

585
00:40:50,180 --> 00:40:51,820
he two original speaker sources?

586
00:40:52,070 --> 00:40:53,580
So I actually played this audio

587
00:40:54,520 --> 00:40:55,910
as well in the very first lecture,

588
00:40:56,220 --> 00:41:12,860
which is suppose microphone one records this. [Recording]

589
00:41:12,860 --> 00:41:14,140
Instructor (Andrew Ng):So the question is,

590
00:41:14,630 --> 00:41:15,990
these are really two speakers,

591
00:41:16,530 --> 00:41:17,950
speaking independently of each other.

592
00:41:18,620 --> 00:41:22,910
So each speaker is outputting a series of sound signals

593
00:41:23,490 --> 00:41:26,250
as independent of the other conversation going on in the room.

594
00:41:26,700 --> 00:41:30,040
So this being an supervised learning problem, the question is,

595
00:41:30,440 --> 00:41:32,470
can we take these two microphone recordings

596
00:41:33,190 --> 00:41:35,260
and feed it to an algorithm to find

597
00:41:35,550 --> 00:41:37,760
the independent components in this data?

598
00:41:38,040 --> 00:41:47,200
This is the output when we do so. [Recording]

599
00:41:47,900 --> 00:41:55,910
Instructor (Andrew Ng):This is the other one. [Recording]

600
00:41:56,270 --> 00:41:58,040
Instructor (Andrew Ng):Just for fun. [Inaudible].

601
00:41:58,460 --> 00:41:59,920
These are audio clips I got from [inaudible].

602
00:42:00,410 --> 00:42:03,520
Just for fun, let me play the other ones as well.

603
00:42:03,910 --> 00:42:13,200
This is overlapping microphone one. [Recording]

604
00:42:13,600 --> 00:42:21,560
Instructor (Andrew Ng):Here's microphone two. [Recording]

605
00:42:21,970 --> 00:42:23,130
Instructor (Andrew Ng):So given this as input,

606
00:42:23,470 --> 00:42:30,880
here's output one.[Recording]

607
00:42:31,200 --> 00:42:31,990
Instructor (Andrew Ng):It's not perfect,

608
00:42:32,330 --> 00:42:33,640
but it's largely cleaned up the music.

609
00:42:34,000 --> 00:42:41,730
Here's number two. [Recording]

610
00:42:42,190 --> 00:42:43,710
Instructor (Andrew Ng):Okay. Switch back

611
00:42:43,710 --> 00:42:44,850
to [inaudible], please.

612
00:42:45,930 --> 00:42:48,440
So what I want to do now is

613
00:42:49,000 --> 00:42:50,810
describe an algorithm that does that.

614
00:42:51,840 --> 00:43:00,380
Before I actually jump into the algorithm,

615
00:43:00,700 --> 00:43:04,810
I want to say two minutes of CDF,

616
00:43:05,150 --> 00:43:06,700
so cumulative distribution functions.

617
00:43:18,380 --> 00:43:20,140
I know most of you know what these are,

618
00:43:20,430 --> 00:43:22,830
but I'm just going to remind you of what they are.

619
00:43:23,240 --> 00:43:26,820
Let's say you have a one-D random variable S.

620
00:43:27,270 --> 00:43:33,260
So suppose you have a random variable, S,

621
00:43:33,930 --> 00:43:38,850
and suppose it has a property density function [inaudible].

622
00:43:39,060 --> 00:43:47,470
Then the CDF is defined as a function, or rather as F,

623
00:43:49,950 --> 00:43:52,620
which is the probability that the random variable, S,

624
00:43:53,860 --> 00:43:57,840
is less than the value given by that lower-case value, S.

625
00:44:01,700 --> 00:44:05,230
For example, if this is your [inaudible] density,

626
00:44:06,200 --> 00:44:07,730
than the density of the [inaudible]

627
00:44:08,730 --> 00:44:11,120
usually to note it lower-case phi.

628
00:44:11,610 --> 00:44:13,330
That's roughly a bell-shaped density.

629
00:44:13,930 --> 00:44:21,240
Then the CDF or the Gaussian will look something like this.

630
00:44:21,900 --> 00:44:23,950
There'll be a capital function pi.

631
00:44:24,770 --> 00:44:30,050
So if I pick a value S like that, then the height of this

632
00:44:30,490 --> 00:44:32,540
this is [inaudible] probability that

633
00:44:33,010 --> 00:44:35,020
my Gaussian random variableis less than that value there.

634
00:44:35,850 --> 00:44:39,110
In other words, the height of the function at that point is

635
00:44:39,680 --> 00:44:46,840
less than the area of the Gaussian density, up to the point S.

636
00:44:47,800 --> 00:44:50,640
As you move further and further to the right,

637
00:44:50,990 --> 00:44:52,240
this function will approach one,

638
00:44:52,520 --> 00:44:54,710
as you integrate more and more of this area of the Gaussian.

639
00:45:28,650 --> 00:45:32,850
So another way to write F of S is the integral,

640
00:45:33,080 --> 00:45:37,130
the minus infinity to S of the density, DT.

641
00:45:40,730 --> 00:45:43,690
So something that'll come later is

642
00:45:44,150 --> 00:45:45,630
suppose I have a random variable, S,

643
00:45:46,030 --> 00:45:48,950
and I want to model the distribution of the random variable, S.

644
00:45:49,390 --> 00:45:51,230
So one thing I could do is

645
00:45:51,720 --> 00:45:54,820
I can specify what I think the density is.

646
00:45:55,260 --> 00:46:04,270
Or I can specify what the CDF is.

647
00:46:04,810 --> 00:46:11,220
These are related by this equation. F is the integral of P of S.

648
00:46:11,700 --> 00:46:17,350
You can also recover the density by taking the CDF

649
00:46:17,770 --> 00:46:19,290
and taking the derivative.

650
00:46:19,530 --> 00:46:21,310
So F prime, take the derivative of the CDF,

651
00:46:21,640 --> 00:46:22,920
you get back the density.

652
00:46:23,410 --> 00:46:26,650
So this has come up in the middle of when I derive ICA,

653
00:46:27,040 --> 00:46:28,880
which is that there'll be a step

654
00:46:29,240 --> 00:46:31,570
where they need to assume a distribution for random variable, S.

655
00:46:31,990 --> 00:46:33,890
I can either specify the density for S directly,

656
00:46:34,780 --> 00:46:38,300
or I can specify the CDF. I choose to specify the CDF.

657
00:46:40,730 --> 00:46:45,040
It has to be some function increasing from zero to one.

658
00:46:45,520 --> 00:46:49,580
So you can choose any function that looks like that,

659
00:46:49,950 --> 00:46:53,130
and in particular, pulling functions out of

660
00:46:53,520 --> 00:46:54,760
a hat that look like that.

661
00:46:55,080 --> 00:46:57,050
You can, for instance, choose a sigmoid function of CDF.

662
00:46:57,430 --> 00:47:01,250
That would be one way of specifying

663
00:47:01,580 --> 00:47:04,530
the distribution of the densities for the random variable S.

664
00:47:05,340 --> 00:47:09,780
So this will come up later. Just [inaudible],

665
00:47:30,390 --> 00:47:32,480
just raise your hand if that is familiar to you,

666
00:47:33,640 --> 00:47:34,840
if you've seen that before. Great.

667
00:47:36,580 --> 00:47:43,360
So let's start to

668
00:47:43,740 --> 00:47:45,390
derive our RCA,

669
00:47:45,850 --> 00:47:47,770
or our independent component analysis algorithm.

670
00:47:48,640 --> 00:48:00,950
Let's assume that the data comes from N original sources.

671
00:48:01,180 --> 00:48:04,390
So let's say

672
00:48:04,810 --> 00:48:06,400
there are N speakers in a cocktail party.

673
00:48:06,860 --> 00:48:12,270
So the original sources, I'm going to write as a vector, S as in RN.

674
00:48:13,070 --> 00:48:15,610
So just to be concrete about what I mean about that,

675
00:48:16,110 --> 00:48:19,690
I'm going to use SIJ to denote the signal

676
00:48:20,340 --> 00:48:31,480
from speaker J at time I.

677
00:48:32,620 --> 00:48:34,970
Here's what I mean. So what is sound?

678
00:48:35,640 --> 00:48:38,110
When you hear sound waves, sound is created

679
00:48:38,430 --> 00:48:41,460
by a pattern of expansions and compressions in air.

680
00:48:41,950 --> 00:48:43,340
So the way you're hearing my voice is

681
00:48:43,740 --> 00:48:48,890
my mouth is causing certain changes in the air pressure,

682
00:48:49,300 --> 00:48:51,420
and then your ear is hearing my voice

683
00:48:51,920 --> 00:48:53,500
as detecting those changes in air pressure.

684
00:48:53,900 --> 00:48:57,340
So what a microphone records, what my mouth is generating,

685
00:48:57,910 --> 00:49:00,670
is a pattern. I'm going to draw a cartoon, I guess.

686
00:49:01,820 --> 00:49:08,220
Changes in air pressure. So this is what sound is.

687
00:49:08,580 --> 00:49:09,930
You look at a microphone recording,

688
00:49:10,310 --> 00:49:12,780
you see these roughly periodic signals that

689
00:49:13,380 --> 00:49:15,710
comprise of changes in air pressure over time

690
00:49:16,090 --> 00:49:17,070
as the air pressure

691
00:49:17,420 --> 00:49:18,880
goes above and below some baseline air pressure.

692
00:49:19,570 --> 00:49:21,680
So this is what the speech signal looks like, say.

693
00:49:22,470 --> 00:49:24,210
So this is speaker one.

694
00:49:25,300 --> 00:49:30,280
Then what I'm saying is that this is some time, T.

695
00:49:31,210 --> 00:49:33,350
What I'm saying is that the value of that point,

696
00:49:34,750 --> 00:49:38,510
I'm going to denote as S, super script T, sub script one.

697
00:49:40,190 --> 00:49:45,740
Similarly, speaker two, it's outputting some sound wave.

698
00:49:46,580 --> 00:49:48,180
Speaker voice will play that.

699
00:49:48,650 --> 00:49:51,600
It'll actually sound like a single tone, I guess.

700
00:49:52,330 --> 00:49:55,350
So in the same way, at the same time, T,

701
00:49:55,910 --> 00:50:00,230
the value of the air pressure generated by speaker two,

702
00:50:00,740 --> 00:50:03,540
I'll denote as ST 2.

703
00:50:05,950 --> 00:50:39,970
So we observe XI equals A times SI,

704
00:50:40,690 --> 00:50:44,960
where these XIs are vectors in RN.

705
00:50:45,500 --> 00:50:52,370
So I'm going to assume that I have N microphones,

706
00:50:52,880 --> 00:50:55,730
and each of my microphones records

707
00:50:56,250 --> 00:50:59,680
some linear combination of what the speakers are saying.

708
00:51:00,070 --> 00:51:01,040
So each microphone records

709
00:51:01,440 --> 00:51:03,980
some overlapping combination of what the speakers are saying.

710
00:51:04,360 --> 00:51:12,380
For example, XIJ, which is

711
00:51:12,770 --> 00:51:15,520
this is what microphone J records at time, I.

712
00:51:15,950 --> 00:51:18,360
So by definition of the matrix multiplication,

713
00:51:18,950 --> 00:51:28,970
this is sum of AIKSJ. Oh, excuse me. Okay?

714
00:51:29,590 --> 00:51:34,230
So what my J sorry.

715
00:51:34,880 --> 00:51:39,740
So what my J microphone is recording is

716
00:51:40,230 --> 00:51:44,670
some linear combination of all of the speakers.

717
00:51:44,760 --> 00:51:48,880
So at time I, what microphone J is recording is some linear

718
00:51:49,390 --> 00:51:52,080
combination ofwhat all the speakers are saying at time I.

719
00:51:52,900 --> 00:51:55,760
So K here indexes over the N speakers.

720
00:51:56,210 --> 00:52:03,320
So our goal is to find

721
00:52:03,760 --> 00:52:07,140
the matrix, W, equals A inverse,

722
00:52:07,580 --> 00:52:08,930
and just defining W that way.

723
00:52:10,000 --> 00:52:20,040
So we can recover the original sources

724
00:52:20,390 --> 00:52:24,810
as a linear combination of our microphone recordings, XI.

725
00:52:32,920 --> 00:52:35,040
Just as a point of notation,

726
00:52:36,020 --> 00:52:38,180
I'm going to write the matrix W this way.

727
00:52:38,770 --> 00:52:53,490
I'm going to use lower case W subscript one, subscript two

728
00:52:53,910 --> 00:52:56,610
and so on to denote the roles of this matrix, W.

729
00:53:03,220 --> 00:53:05,900
Let's see.

730
00:53:18,310 --> 00:53:20,640
So let's look at why IC is possible.

731
00:53:21,140 --> 00:53:24,450
Given these overlapping voices, let's think briefly why

732
00:53:24,800 --> 00:53:29,720
it might be possible to recover the original sources.

733
00:53:30,220 --> 00:53:32,810
So for the next example, I want to say

734
00:53:33,270 --> 00:53:46,860
let's say that each of my speakers outputs

735
00:53:47,430 --> 00:53:49,200
this will sound like white noise.

736
00:53:49,670 --> 00:53:52,170
Can I switch the laptop display, please?

737
00:53:53,550 --> 00:53:55,510
For this example,let's say that

738
00:53:56,590 --> 00:54:00,250
each of my speakers outputs uniform white noise.

739
00:54:00,780 --> 00:54:02,180
So if that's the case,

740
00:54:02,730 --> 00:54:04,840
these are my axis, S1 and S2.

741
00:54:05,280 --> 00:54:07,610
This is what my two speakers would be uttering.

742
00:54:08,040 --> 00:54:11,990
The parts of what they're uttering will

743
00:54:12,310 --> 00:54:13,640
look like a line in a square box

744
00:54:14,000 --> 00:54:14,850
if the two speakers are

745
00:54:15,240 --> 00:54:18,090
independently outputting uniform minus one random variables.

746
00:54:18,810 --> 00:54:22,400
So this is part of S1 and S2, my original sources.

747
00:54:23,920 --> 00:54:27,580
This would be a typical sample of what my microphones record.

748
00:54:27,970 --> 00:54:29,930
Here, at the axis, are X1 and X2.

749
00:54:31,390 --> 00:54:35,390
So these are images I got from [inaudible] on ICA.

750
00:54:35,990 --> 00:54:41,120
Given a picture like this, you can sort of look at this box,

751
00:54:41,490 --> 00:54:45,140
and you can sort of tell what the axis of this parallelogram are.

752
00:54:45,870 --> 00:54:49,670
You can figure out what linear transformation

753
00:54:50,160 --> 00:54:52,640
would transform the parallelogram back to a box.

754
00:54:53,030 --> 00:54:57,530
So it turns out there are some inherent ambiguities in ICA.

755
00:54:57,980 --> 00:55:00,040
I'll just say what they are.

756
00:55:00,440 --> 00:55:04,050
One is that you can't recover the original indexing of the sources.

757
00:55:04,880 --> 00:55:08,510
In particular, if I generated the data

758
00:55:08,870 --> 00:55:09,960
for speaker oneand speaker two,

759
00:55:10,530 --> 00:55:11,810
you can run ICA,

760
00:55:12,250 --> 00:55:13,430
and then you may end up

761
00:55:13,780 --> 00:55:14,810
with the order of the speakers reversed.

762
00:55:15,290 --> 00:55:17,740
What that corresponds to is if you take this picture

763
00:55:18,090 --> 00:55:21,120
and you flip this picture along a 45-degree axis.

764
00:55:21,710 --> 00:55:24,220
You take a 45-degree axis

765
00:55:24,640 --> 00:55:26,410
and reflect this picture across the 45-degree axis,

766
00:55:26,930 --> 00:55:27,840
you'll still get a box.

767
00:55:28,460 --> 00:55:30,260
So there's no way for the algorithms to

768
00:55:30,620 --> 00:55:31,560
tell which was speaker No. 1

769
00:55:31,910 --> 00:55:32,890
and which was speaker No. 2.

770
00:55:33,450 --> 00:55:35,690
The numbering or the ordering of the speakers is ambiguous.

771
00:55:38,060 --> 00:55:39,570
The other source of ambiguity,

772
00:55:39,990 --> 00:55:41,580
and these are the only ambiguities in this example,

773
00:55:42,110 --> 00:55:43,660
is the sign of the sources.

774
00:55:44,460 --> 00:55:46,590
So given my speakers' recordings,

775
00:55:46,990 --> 00:55:52,450
you can't tell whether you got a positive SI

776
00:55:52,930 --> 00:55:54,670
or whether you got back a negative SI.

777
00:55:55,310 --> 00:55:58,070
In this picture, what that corresponds to is

778
00:55:58,610 --> 00:55:59,770
if you take this picture,

779
00:56:00,300 --> 00:56:02,290
and you reflect it along the vertical axis,

780
00:56:02,660 --> 00:56:04,340
if you reflect it along the horizontal axis,

781
00:56:04,340 --> 00:56:05,850
you still get a box.

782
00:56:06,560 --> 00:56:07,540
You still get back [inaudible] speakers.

783
00:56:07,750 --> 00:56:12,040
So it turns out that in this example,

784
00:56:12,990 --> 00:56:15,920
you can't guarantee that you've recovered positive SI

785
00:56:16,560 --> 00:56:17,630
rather than negative SI.

786
00:56:18,170 --> 00:56:20,670
So it turns out that

787
00:56:21,090 --> 00:56:23,400
these are the only two ambiguities in this example.

788
00:56:23,930 --> 00:56:25,600
What is the permutation of the speakers,

789
00:56:25,980 --> 00:56:27,650
and the other is the sign of the speakers.

790
00:56:28,340 --> 00:56:29,460
Permutation of the speakers,

791
00:56:29,900 --> 00:56:31,190
there's not much you can do about that.

792
00:56:31,570 --> 00:56:33,320
It turns out that if you take the audio source and

793
00:56:34,010 --> 00:56:37,720
if you flip the sign, and you take negative S,

794
00:56:38,280 --> 00:56:40,010
and if you play that through a microphone

795
00:56:40,470 --> 00:56:41,470
it'll sound indistinguishable.

796
00:56:42,110 --> 00:56:47,300
So for many of the applications we care about,

797
00:56:47,690 --> 00:56:51,030
the sign as well as the permutation is ambiguous,

798
00:56:52,110 --> 00:56:54,600
but you don't really care about it.

799
00:56:55,050 --> 00:56:57,020
Let's switch back to chalk board, please.

800
00:57:11,370 --> 00:57:14,220
It turns out, and I don't want to spend too much time on this,

801
00:57:14,810 --> 00:57:15,810
but I do want to say it briefly.

802
00:57:16,330 --> 00:57:17,320
It turns out the reason

803
00:57:17,670 --> 00:57:20,850
why those are the only sources of ambiguity

804
00:57:21,530 --> 00:57:22,530
so the ambiguities were

805
00:57:25,830 --> 00:57:30,270
the permutation of the speakers and the signs.

806
00:57:31,370 --> 00:57:37,950
It turns out that the reason these were the only ambiguities was

807
00:57:38,260 --> 00:57:42,010
because the SIJs were non-Gaussian.

808
00:57:42,570 --> 00:57:49,920
I don't want to spend too much time on this, but I'll say it briefly.

809
00:57:50,420 --> 00:57:53,360
Suppose my original sources, S1 and S2, were Gaussian.

810
00:57:54,660 --> 00:58:02,440
So suppose SI is Gaussian,

811
00:58:02,890 --> 00:58:05,400
would mean zero and identity covariance.

812
00:58:07,250 --> 00:58:09,250
That just means that each of my speakers

813
00:58:10,240 --> 00:58:11,370
outputs a Gaussian random variable.

814
00:58:12,100 --> 00:58:14,280
Here's a typical example of Gaussian data.

815
00:58:17,410 --> 00:58:21,510
You will recall the contours of a Gaussian distribution

816
00:58:22,020 --> 00:58:26,200
with identity covariants looks like this, right?

817
00:58:26,550 --> 00:58:29,400
The Gaussian is a spherically symmetric distribution.

818
00:58:30,130 --> 00:58:33,130
So if my speakers were outputting Gaussian random variables,

819
00:58:33,640 --> 00:58:37,120
than if I observe a linear combination of this,

820
00:58:38,310 --> 00:58:41,530
there's actually no way to recover the original distribution

821
00:58:41,880 --> 00:58:43,230
because there's no way for me to tell

822
00:58:43,630 --> 00:58:45,170
if the axis are at this angle

823
00:58:46,000 --> 00:58:48,120
or if they're at that angle and so on.

824
00:58:48,640 --> 00:58:52,560
The Gaussian is a rotationally symmetric distribution,

825
00:58:53,090 --> 00:58:54,710
so I would no be able to

826
00:58:55,090 --> 00:58:57,340
recover the orientation in the rotation of this.

827
00:58:58,250 --> 00:59:00,120
So I don't want to prove this too much.

828
00:59:00,450 --> 00:59:01,980
I don't want to spend too much time dwelling on this,

829
00:59:02,240 --> 00:59:04,500
but it turns out if your source is a Gaussian,

830
00:59:04,850 --> 00:59:07,110
then it's actually impossible to do ICA.

831
00:59:07,530 --> 00:59:11,080
ICA relies critically on your data being non-Gaussian

832
00:59:11,630 --> 00:59:12,730
because if the data were Gaussian,

833
00:59:13,100 --> 00:59:15,220
then the rotation of the data would be ambiguous.

834
00:59:15,640 --> 00:59:18,530
So regardless of how much data you have,

835
00:59:18,950 --> 00:59:22,150
even if you had infinitely large amounts of data,

836
00:59:22,450 --> 00:59:24,890
you would not be able to recover the matrix A or W.

837
00:59:32,850 --> 00:59:34,990
Let's go ahead and divide the algorithm.

838
00:59:49,380 --> 00:59:59,650
To do this, I need just one more result,

839
00:59:59,980 --> 01:00:01,620
and then the derivation will be three lines.

840
01:00:02,210 --> 01:00:04,660
[Inaudible] many variables as N,

841
01:00:05,120 --> 01:00:07,950
which is the joint vector of the sound that all of my speakers

842
01:00:08,330 --> 01:00:09,730
that are emitting at any time.

843
01:00:11,500 --> 01:00:17,920
So let's say the density of S is P subscript S, capital S.

844
01:00:19,260 --> 01:00:22,610
So my microphone recording records S equals AS,

845
01:00:23,380 --> 01:00:28,240
equals W inverse S. Equivalently, S equals W sign of X.

846
01:00:29,070 --> 01:00:34,640
So let's think about what is the density of X.

847
01:00:35,420 --> 01:00:37,420
So I have P of S. I know the density of S,

848
01:00:38,080 --> 01:00:41,030
and X is a linear combination of the S's.

849
01:00:41,460 --> 01:00:44,230
So let's figure out what is the density of X.

850
01:00:45,260 --> 01:00:49,740
One thing we could do is figure out what S is.

851
01:00:50,260 --> 01:00:56,670
So this is just apply the density of S to W of S.

852
01:00:57,040 --> 01:00:59,650
So let's see. This is the probability of S,

853
01:01:00,030 --> 01:01:03,980
so we just figure out what S is.

854
01:01:04,590 --> 01:01:07,390
S is W times X, so the probability of S is W times X,

855
01:01:08,040 --> 01:01:09,760
so the probability of X must be [inaudible].

856
01:01:10,320 --> 01:01:11,310
So this is wrong.

857
01:01:11,730 --> 01:01:14,070
It turns out you can do this for probably mass functions

858
01:01:14,410 --> 01:01:15,690
but not for continuous density.

859
01:01:16,010 --> 01:01:18,880
So in particular, it's not correct to say that the probability of X is

860
01:01:22,650 --> 01:01:25,860
Then you say the probability of S is applied to that. This is wrong.

861
01:01:25,860 --> 01:01:26,860
– well, you just figure out what S is.

862
01:01:26,260 --> 01:01:27,620
You can't do this with densities.

863
01:01:28,190 --> 01:01:29,640
You can't say the probability of S is that

864
01:01:30,060 --> 01:01:33,090
because it's a property density function. In particular,

865
01:01:33,540 --> 01:01:39,710
the right formula is the density of S applied to W times X,

866
01:01:40,530 --> 01:01:42,800
times the determinant of the matrix, W.

867
01:01:44,500 --> 01:01:46,850
Let me just illustrate that with an example.

868
01:01:47,370 --> 01:01:56,090
Let's say the density for S is that.

869
01:01:56,570 --> 01:02:04,250
In this example, S is uniform over the unit interval.

870
01:02:05,430 --> 01:02:14,820
So the density for S looks like that.

871
01:02:15,540 --> 01:02:19,010
It's just density for the uniform distribution of zero one.

872
01:02:20,670 --> 01:02:22,940
So let me let X be equal to two times S.

873
01:02:24,280 --> 01:02:31,050
So this means A equals two. W equals one half.

874
01:02:33,350 --> 01:02:35,910
So if S is a uniform distribution over zero, one,

875
01:02:36,710 --> 01:02:38,490
then X, which is two times that,

876
01:02:38,840 --> 01:02:41,780
will be the uniform distribution over the range from zero to two.

877
01:02:42,960 --> 01:02:45,370
So the density for X will be

878
01:02:53,590 --> 01:02:59,570
that's one, that's two, that's one half, and that's one. Okay?

879
01:03:02,510 --> 01:03:09,680
Density for X will be indicator zero so these for X [inaudible]

880
01:03:10,500 --> 01:03:14,300
two times W, times one half.

881
01:03:15,640 --> 01:03:19,190
So does that make sense?

882
01:03:19,560 --> 01:03:23,650
[Inaudible] computer density for X because

883
01:03:23,990 --> 01:03:26,260
X isnow spread out across a wider range.

884
01:03:26,720 --> 01:03:28,970
The density of X is now smaller, and therefore,

885
01:03:29,360 --> 01:03:37,520
the density of X has this one half term here. Okay?

886
01:03:38,230 --> 01:03:40,360
This is an illustration for

887
01:03:40,930 --> 01:03:43,060
the case of one-dimensional random variables,

888
01:03:43,420 --> 01:03:44,520
or S and X of one D.

889
01:03:45,080 --> 01:03:46,770
I'm not going to show it,

890
01:03:47,100 --> 01:03:48,280
but the generalization of this

891
01:03:48,590 --> 01:03:49,520
to vector value random variables is

892
01:03:49,840 --> 01:03:51,020
that the density of X is given

893
01:03:51,390 --> 01:03:53,530
by this times the determinant of the matrix, W.

894
01:03:54,070 --> 01:03:55,060
Over here, I showed

895
01:03:55,380 --> 01:03:56,340
the one dimensional [inaudible] generalization.

896
01:04:15,770 --> 01:04:22,990
So we're nearly there.

897
01:04:25,040 --> 01:04:30,330
Here's how I can implement ICA.

898
01:04:32,350 --> 01:04:51,540
So my distribution on S, so I'm going to assume that

899
01:04:51,940 --> 01:04:53,800
my density on S is given by this

900
01:04:54,280 --> 01:04:57,300
as a product over the N speakers of the density

901
01:04:57,750 --> 01:05:01,950
the product of speaker I emitting a certain sound.

902
01:05:02,290 --> 01:05:03,300
This is a product of densities.

903
01:05:03,960 --> 01:05:05,940
This is a product of distributions

904
01:05:06,320 --> 01:05:08,410
because I'm going to assume that the speakers are

905
01:05:08,770 --> 01:05:09,870
having independent conversations.

906
01:05:10,280 --> 01:05:12,750
So the SI's independent for different values of I.

907
01:05:15,930 --> 01:05:18,100
So by the formula we just worked out,

908
01:05:18,480 --> 01:05:36,550
the density for X would be equal to that.

909
01:05:36,950 --> 01:05:40,540
I'll just remind you, W was A inverse.

910
01:05:42,580 --> 01:05:46,340
It was this matrix I defined previously

911
01:05:47,700 --> 01:05:51,970
so that SI equals WI [inaudible] X.

912
01:05:52,900 --> 01:05:55,330
So that's what's in there.

913
01:05:56,780 --> 01:06:01,710
To complete my formulation for this model,

914
01:06:02,480 --> 01:06:11,630
the final thing I need to do is choose a density for

915
01:06:11,980 --> 01:06:13,500
what I think each speaker is saying.

916
01:06:14,250 --> 01:06:18,890
I need to assume some density over the sounds emitted

917
01:06:19,220 --> 01:06:20,500
by an individual speaker.

918
01:06:20,990 --> 01:06:24,610
So following the discussion I had right

919
01:06:25,030 --> 01:06:28,510
when the [inaudible] ICA, one thing I could do is

920
01:06:29,060 --> 01:06:31,480
I could choose the density for S,

921
01:06:32,150 --> 01:06:34,320
or equivalently, I could choose the CDF,

922
01:06:34,680 --> 01:06:36,070
the cumulative distribution function for S.

923
01:06:37,970 --> 01:06:39,950
In this case, I'm going to choose a CDF,

924
01:06:41,030 --> 01:06:45,050
probably for historical reasons  and probably for convenience.

925
01:06:47,210 --> 01:06:48,720
I need to choose the CDF for S,

926
01:06:49,540 --> 01:06:51,010
so what that means is

927
01:06:51,310 --> 01:06:52,590
I just need to choose some function that

928
01:06:52,900 --> 01:06:54,060
increases from zero to what.

929
01:06:54,570 --> 01:06:57,330
I know I can't choose a Gaussian

930
01:06:57,720 --> 01:07:01,430
because we know you can't do ICA on Gaussian data.

931
01:07:01,980 --> 01:07:04,220
So I need some function increasing from zero to one

932
01:07:04,930 --> 01:07:05,490
that is not

933
01:07:05,900 --> 01:07:07,180
the cumulative distribution function

934
01:07:07,510 --> 01:07:09,650
for a Gaussian distribution.

935
01:07:10,340 --> 01:07:12,060
So what other functions do I know that

936
01:07:12,530 --> 01:07:13,330
increase from zero to one?

937
01:07:14,260 --> 01:07:19,560
I just choose the CDF to be the sigmoid function.

938
01:07:21,850 --> 01:07:25,460
This is a commonly-made choice

939
01:07:26,020 --> 01:07:28,140
that is made for convenience.

940
01:07:28,720 --> 01:07:30,540
There is actually no great reason for

941
01:07:30,950 --> 01:07:32,030
why you choose a sigmoid function.

942
01:07:32,440 --> 01:07:33,850
It's just a convenient function that we all know

943
01:07:34,360 --> 01:07:35,860
and are familiar with that happens

944
01:07:36,240 --> 01:07:37,640
to increase from zero to one.

945
01:07:37,990 --> 01:07:41,380
When you take the derivative of the sigmoid,

946
01:07:46,110 --> 01:07:49,410
and that will give you back your density.

947
01:07:50,520 --> 01:07:51,940
This is just not Gaussian.

948
01:07:52,590 --> 01:07:54,200
This is the main virtue of choosing the sigmoid.

949
01:08:18,680 --> 01:08:21,350
So there's really no rational for the choice of sigma.

950
01:08:21,840 --> 01:08:23,440
Lots of other things will work fine, too.

951
01:08:23,910 --> 01:08:25,360
It's just a common, reasonable default.

952
01:08:38,080 --> 01:08:41,860
It turns out that one reason the sigma works well

953
01:08:42,360 --> 01:08:45,390
for a lot of data sources is that if this is the Gaussian.

954
01:08:49,130 --> 01:08:51,600
If you actually take the sigmoid and you take its derivative,

955
01:08:52,330 --> 01:09:03,820
you find that the sigmoid has

956
01:09:04,160 --> 01:09:05,480
father tails than the Gaussian.

957
01:09:05,840 --> 01:09:07,740
By this I mean the density of the sigmoid

958
01:09:08,170 --> 01:09:10,880
dies down to zero much more slowly than the Gaussian.

959
01:09:12,020 --> 01:09:14,480
The magnitudes of the tails

960
01:09:14,900 --> 01:09:17,640
dies down as E to the minus S squared.

961
01:09:18,180 --> 01:09:21,160
For the sigmoid, the tails look like E to the minus S.

962
01:09:21,630 --> 01:09:24,850
So the tails die down as E to the minus S,

963
01:09:25,260 --> 01:09:26,750
around E to the minus S squared.

964
01:09:27,130 --> 01:09:27,980
It turns out that

965
01:09:28,390 --> 01:09:31,440
most distributions of this property with [inaudible] tails,

966
01:09:31,800 --> 01:09:34,040
where the distribution decays to zero relatively slowly

967
01:09:34,510 --> 01:09:36,510
compared to Gaussian will work fine for your data.

968
01:09:39,920 --> 01:09:42,490
Actually, one other choice you can sometimes us is

969
01:09:42,810 --> 01:09:46,240
what's called the Laplacian distribution, which is that.

970
01:09:46,950 --> 01:09:48,780
This will work fine, too, for many data sources.

971
01:10:06,090 --> 01:10:07,730
Sticking with the sigmoid for now,

972
01:10:09,350 --> 01:10:11,320
I'll just write down the algorithm in two steps.

973
01:10:11,700 --> 01:10:17,750
So given my training set, and as you show,

974
01:10:18,100 --> 01:10:19,590
this is an unlabeled training set,

975
01:10:20,940 --> 01:10:23,120
I can write down the log likelihood of my parameters.

976
01:10:24,010 --> 01:10:24,560
So that's

977
01:10:25,100 --> 01:10:42,230
assembled my training examples, log of times that.

978
01:10:42,980 --> 01:10:44,320
So that's my log likelihood.

979
01:10:53,280 --> 01:10:56,450
To learn the parameters, W, of this model,

980
01:10:56,790 --> 01:11:06,980
I can use the [inaudible] assent, which is just that.

981
01:11:07,910 --> 01:11:11,750
It turns out, if you work through the math, let's see.

982
01:11:12,180 --> 01:11:16,280
If P of S is equal to the derivative of the sigmoid,

983
01:11:19,380 --> 01:11:21,040
then if you just work through

984
01:11:21,650 --> 01:11:22,860
the math to compute the [inaudible] there.

985
01:11:24,170 --> 01:11:25,610
You've all done this a lot of times.

986
01:11:26,080 --> 01:11:27,540
I won't bother to show the details.

987
01:11:28,050 --> 01:11:29,720
You find that is equal to this.

988
01:11:42,820 --> 01:11:49,220
Okay? That's just you can work those out yourself.

989
01:11:49,960 --> 01:11:52,150
It's just math to compute the derivative of this with respect to W.

990
01:11:53,320 --> 01:11:57,430
So to summarize, given the training set,

991
01:11:57,870 --> 01:12:01,090
here's my [inaudible] update rule.

992
01:12:01,520 --> 01:12:05,200
So you run the [inaudible] to learn the parameters W.

993
01:12:06,430 --> 01:12:14,620
After you're done, you then output SI equals WXI,

994
01:12:15,050 --> 01:12:18,520
and you've separated your sources of your data

995
01:12:18,880 --> 01:12:20,730
back out into the original independent sources.

996
01:12:21,250 --> 01:12:24,420
Hopefully up to only a permutation

997
01:12:24,950 --> 01:12:28,900
and a plus/minus sign ambiguity. Okay?

998
01:12:29,760 --> 01:12:35,250
So just switch back to the laptop, please? So we'll

999
01:12:35,600 --> 01:12:38,370
just wrap up with a couple of examples of applications of ICA.

1000
01:12:42,150 --> 01:12:45,140
This is actually a picture of our TA, Katie.

1001
01:12:45,860 --> 01:12:48,990
So one of the applications of ICA is

1002
01:12:49,290 --> 01:12:53,550
to process various types of [inaudible] recording data,

1003
01:12:54,060 --> 01:12:54,910
so [inaudible].

1004
01:12:55,250 --> 01:12:59,740
This is a picture of a EEG cap,

1005
01:13:02,820 --> 01:13:07,570
in this case,  on Katie's brain, on Katie's scalp.

1006
01:13:07,570 --> 01:13:08,570
in which there are a number of electrodes you place on the –

1007
01:13:08,310 --> 01:13:10,400
So where each electrode measures changes

1008
01:13:10,480 --> 01:13:14,450
in voltage over time on the scalp.

1009
01:13:15,110 --> 01:13:18,440
On the right, it's a typical example of [inaudible] data

1010
01:13:18,770 --> 01:13:20,180
where each electrode measures

1011
01:13:20,560 --> 01:13:21,900
just changes in voltage over time.

1012
01:13:22,280 --> 01:13:24,110
So the horizontal axis is time,

1013
01:13:24,110 --> 01:13:25,130
and the vertical axis is voltage.

1014
01:13:26,500 --> 01:13:28,740
So here's the same thing, blown up a little bit.

1015
01:13:29,580 --> 01:13:31,310
You notice there are artifacts in this data.

1016
01:13:31,610 --> 01:13:34,770
Where the circle is, where the data is circled,

1017
01:13:35,060 --> 01:13:37,940
all the electrodes seem to

1018
01:13:38,330 --> 01:13:40,130
measure in these very synchronized recordings.

1019
01:13:40,950 --> 01:13:43,050
It turns out that we look at [inaudible] data

1020
01:13:43,590 --> 01:13:45,260
as well as a number of other types of data,

1021
01:13:45,610 --> 01:13:47,930
there are artifacts from heartbeats

1022
01:13:48,380 --> 01:13:49,740
and from human eye blinks and so on.

1023
01:13:50,590 --> 01:13:55,900
So the cartoonist, if you imagine, placing the electrodes,

1024
01:13:56,330 --> 01:13:59,350
or microphones, on my scalp, then each microphone is

1025
01:13:59,640 --> 01:14:02,140
recording some overlapping combination of all the things

1026
01:14:02,450 --> 01:14:03,660
happening in my brain or in my body.

1027
01:14:04,160 --> 01:14:06,970
My brain has a number of different processes going on.

1028
01:14:07,380 --> 01:14:08,780
My body's [inaudible] going on,

1029
01:14:09,400 --> 01:14:10,740
and each electrode measures

1030
01:14:11,590 --> 01:14:14,640
a sum ofthe different voices in my brain.

1031
01:14:15,270 --> 01:14:17,630
That didn't quite come out the way I wanted it to.

1032
01:14:18,210 --> 01:14:22,770
So we can just take this data and run ICA on it

1033
01:14:23,150 --> 01:14:24,860
and find out one of the independent components,

1034
01:14:25,190 --> 01:14:27,350
what the independent process are going on in my brain.

1035
01:14:27,670 --> 01:14:29,610
This is an example of running ICA.

1036
01:14:30,850 --> 01:14:32,620
So you find that a small number of components,

1037
01:14:32,940 --> 01:14:34,720
like those shown up there, they correspond to heartbeat,

1038
01:14:35,090 --> 01:14:37,980
where the arrows so those are very periodic signals.

1039
01:14:38,370 --> 01:14:39,590
They come on occasionally

1040
01:14:39,860 --> 01:14:42,480
and correspond to [inaudible] components of heartbeat.

1041
01:14:42,870 --> 01:14:45,950
You also find things like an eye blink component,

1042
01:14:46,370 --> 01:14:48,150
corresponding to a sigmoid generated

1043
01:14:48,550 --> 01:14:49,320
when you blink your eyes.

1044
01:14:50,170 --> 01:14:52,850
By doing this, you can then subtract out the heartbeat

1045
01:14:53,250 --> 01:14:54,840
and the eye blink artifacts from the data,

1046
01:14:55,280 --> 01:14:58,510
and now you get much cleaner ICA data

1047
01:14:58,850 --> 01:15:00,420
get much cleaner EEG readings.

1048
01:15:00,820 --> 01:15:02,780
You can do further scientific studies.

1049
01:15:03,320 --> 01:15:06,090
So this is a pretty commonly used preprocessing step

1050
01:15:06,640 --> 01:15:08,840
that is a common application of ICA.

1051
01:15:10,100 --> 01:15:15,380
[Inaudible] example is the application, again, from [inaudible].

1052
01:15:16,110 --> 01:15:20,970
As a result of running ICA on natural small image patches.

1053
01:15:21,380 --> 01:15:23,690
Suppose I take natural images and run ICA on the data

1054
01:15:24,080 --> 01:15:25,730
and ask what are the independent components of data.

1055
01:15:26,510 --> 01:15:28,290
It turns out that these are the bases you get.

1056
01:15:28,820 --> 01:15:31,730
So this is a plot of the sources you get.

1057
01:15:33,000 --> 01:15:35,630
This algorithm is saying that a natural image patch shown

1058
01:15:36,530 --> 01:15:42,320
on the left is often expressed

1059
01:15:42,810 --> 01:15:44,060
as a sum, or a linear combination,

1060
01:15:44,630 --> 01:15:47,600
of independent sources of things that make up images.

1061
01:15:48,400 --> 01:15:51,490
So this model's natural images is generated

1062
01:15:51,890 --> 01:15:53,150
by independent objects that

1063
01:15:53,570 --> 01:15:54,820
generate different ages in the image.

1064
01:15:55,500 --> 01:15:58,530
One of the fascinating things about this is that,

1065
01:15:58,850 --> 01:15:59,670
similar to neuroscience,

1066
01:16:00,050 --> 01:16:02,140
this has also been hypothesized as a method

1067
01:16:02,550 --> 01:16:04,880
for how the human brain processes image data.

1068
01:16:05,340 --> 01:16:08,860
It turns out, this is similar, in many ways,

1069
01:16:09,270 --> 01:16:13,510
to computations happening in early visual processing

1070
01:16:13,960 --> 01:16:15,940
in the human brain, in the mammalian brain.

1071
01:16:16,800 --> 01:16:21,260
It's just interesting to see ages are

1072
01:16:21,560 --> 01:16:23,940
the independent components of images.

1073
01:16:24,560 --> 01:16:27,300
Are there quick questions, because I'm running late.

1074
01:16:27,810 --> 01:16:29,910
Quick questions before I close?

1075
01:16:30,250 --> 01:16:31,580
Interviewee: [Inaudible] square matrix?

1076
01:16:31,660 --> 01:16:32,460
Instructor (Andrew Ng):Oh, yes.

1077
01:16:32,500 --> 01:16:33,660
For the algorithms I describe,

1078
01:16:34,060 --> 01:16:35,010
I assume A is a square matrix.

1079
01:16:35,580 --> 01:16:37,460
It turns out if you have more microphones than speakers,

1080
01:16:37,840 --> 01:16:39,340
you can also apply very similar algorithms.

1081
01:16:39,790 --> 01:16:41,680
If you have fewer microphones than speakers,

1082
01:16:42,030 --> 01:16:43,530
there's sort of an open research problem.

1083
01:16:43,890 --> 01:16:45,520
The odds are that if you have one male

1084
01:16:45,850 --> 01:16:47,770
and one female speaker, but one microphone,

1085
01:16:48,140 --> 01:16:49,590
you can sometimes sort of separate them

1086
01:16:49,920 --> 01:16:50,860
because one is high, one is low.

1087
01:16:51,190 --> 01:16:53,240
If you have two male speakers or two female speakers,

1088
01:16:53,710 --> 01:16:55,810
then it's beyond the state of the art now

1089
01:16:56,250 --> 01:16:57,860
to separate them with one microphone.

1090
01:16:58,200 --> 01:16:59,400
It's a great research program.

1091
01:17:00,150 --> 01:17:01,820
Okay. Sorry about running late again.

1092
01:17:02,180 --> 01:17:02,970
Let's close now,

1093
01:17:03,480 --> 01:17:05,180
and we'll continue

1094
01:17:05,260 --> 01:17:06,520
reinforcement learning next [inaudible].

