1
00:00:24,060 --> 00:00:26,220
因此来到这个课程中的最后一个课时

2
00:00:26,930 --> 00:00:29,200
今天我要做的是

3
00:00:29,400 --> 00:00:30,250
给你们上

4
00:00:30,410 --> 00:00:34,070
强化学习算法的最后一节课

5
00:00:34,280 --> 00:00:36,860
我将介绍一点点POMDPs

6
00:00:37,050 --> 00:00:39,730
今天主要讨论的是

7
00:00:40,030 --> 00:00:43,000
Policy search的算法

8
00:00:43,190 --> 00:00:45,260
我将谈到两种特别的算法

9
00:00:45,540 --> 00:00:48,010
实质上叫Reinforced和Pegasus

10
00:00:48,270 --> 00:00:50,530
这覆盖整节课

11
00:00:50,890 --> 00:00:54,360
如果你回忆上一节课内容

12
00:00:54,540 --> 00:00:57,040
实际上我要谈论一个

13
00:00:57,260 --> 00:00:58,830
POMDP的特别的例子

14
00:00:59,100 --> 00:01:02,530
这基于线性动态系统的

15
00:01:02,740 --> 00:01:10,190
这是基于LQR  线性二次相关问题

16
00:01:10,380 --> 00:01:13,040
但是我改变它如果我们

17
00:01:13,200 --> 00:01:19,710
只有观察值Y_t

18
00:01:19,910 --> 00:01:28,010
那我们就不能直接观察系统的状态

19
00:01:28,170 --> 00:01:31,300
但是得选择一些行动  基于一些噪音观测值

20
00:01:31,810 --> 00:01:35,390
那可能是一些状态的函数

21
00:01:35,550 --> 00:01:40,110
所以我们上次的策略  我们已说过

22
00:01:40,270 --> 00:01:42,230
在这个完全可观察的例子中

23
00:01:42,410 --> 00:01:50,930
我们选择动作----At=L_tS_t

24
00:01:51,100 --> 00:01:56,010
Lt是这个矩阵参数

25
00:01:56,160 --> 00:01:59,430
【inaudible】描述了LQR问题中

26
00:01:59,610 --> 00:02:01,780
有限边界MDPs的动态程序算法

27
00:02:01,980 --> 00:02:05,480
因此我们说

28
00:02:05,700 --> 00:02:09,110
如果我们知道这状态是什么  我们依据

29
00:02:09,290 --> 00:02:14,880
Lt乘以这个状态来选择相应行为动作

30
00:02:15,050 --> 00:02:22,600
我说过在这个部分观测值 的例子中

31
00:02:22,810 --> 00:02:33,090
我们将计算这些估算值  我写成St|t

32
00:02:33,250 --> 00:02:35,140
这是对这个状态方法所有观察值的最好计算

33
00:02:35,330 --> 00:02:39,150
特别的  我将谈论(Kalman滤波)

34
00:02:39,360 --> 00:02:47,280
我们算出它们状态的后分布

35
00:02:47,610 --> 00:02:51,170
给定某个时间点和所有观测值

36
00:02:51,360 --> 00:02:57,620
所以这个从上次得出来的

37
00:02:57,810 --> 00:03:01,300
因此给出的观察值Y1…..Yt

38
00:03:01,510 --> 00:03:04,650
我们的现在的St的后分布

39
00:03:04,810 --> 00:03:06,150
是高斯的

40
00:03:06,300 --> 00:03:09,560
这个意思是St|t  Et|t

41
00:03:09,700 --> 00:03:13,200
因此我说过我们用Kalman 过滤器

42
00:03:13,380 --> 00:03:16,270
来计算这个St|t

43
00:03:16,460 --> 00:03:19,850
St|t是我们现在这个情形下最好的猜测值

44
00:03:20,680 --> 00:03:31,160
然后我们用我们的这个情形下的估算值

45
00:03:31,400 --> 00:03:32,280
来选择行为

46
00:03:32,430 --> 00:03:33,540
而不是用这个真实的状态

47
00:03:33,750 --> 00:03:37,210
因为在POMDP中我们并不知道这个真实的状态

48
00:03:37,420 --> 00:03:42,310
所以结果证明是这个具体的策略

49
00:03:42,520 --> 00:03:48,180
实际上允许你选择最佳的行为

50
00:03:48,340 --> 00:03:49,920
允许你选择你的行为

51
00:03:50,070 --> 00:03:50,780
给定它

52
00:03:50,950 --> 00:03:52,110
这就是一个POMDP

53
00:03:52,310 --> 00:03:53,610
并给出有这些噪音(noisy)观察值

54
00:03:53,810 --> 00:03:57,490
结果证明POMDPs 在一般的寻找最佳策略中

55
00:03:57,660 --> 00:04:01,530
为部分观可观察MDPs的这些种类中

56
00:04:01,770 --> 00:04:04,220
寻找最佳策略  是一个NP-hard问题

57
00:04:04,400 --> 00:04:08,920
具体的POMDP的形式

58
00:04:09,070 --> 00:04:20,610
我写在这边  一个PODMP是一个元组


59
00:04:20,840 --> 00:04:32,670
(S  A  Y      T  R) 就像这些

60
00:04:32,850 --> 00:04:37,110
这里的改变 是设置Y是一个可能的观察值

61
00:04:46,170 --> 00:04:51,760
这个Os是观察分布

62
00:04:52,440 --> 00:05:00,340
我们观察 YtN Ost

63
00:05:00,610 --> 00:05:19,380
在POMDP中的每一步

64
00:05:19,590 --> 00:05:21,430
如果在St的情形下

65
00:05:21,630 --> 00:05:24,490
我们观察一些观察值Yt

66
00:05:24,680 --> 00:05:26,940
从观察分布Ost中

67
00:05:27,140 --> 00:05:29,520
有一个现在这个状态的索引

68
00:05:29,730 --> 00:05:34,190
结果证明在一个POMDP中计算最佳策略

69
00:05:34,370 --> 00:05:35,170
是一个非常难的问题

70
00:05:35,320 --> 00:05:38,330
在线性动力系统中的

71
00:05:38,520 --> 00:05:40,250
Kalman过滤模型的具体例子中

72
00:05:40,420 --> 00:05:41,940
我们有这个策略

73
00:05:42,130 --> 00:05:45,140
它能计算最佳策略充分可观察性

74
00:05:45,300 --> 00:05:48,100
和从观察值中估算这种状态

75
00:05:48,340 --> 00:05:50,330
并把这两个合成一起

76
00:05:50,520 --> 00:05:52,390
这证明是实质上

77
00:05:52,660 --> 00:05:55,390
仅仅对那个具体POMDP例子才是最佳的

78
00:05:55,580 --> 00:05:57,440
在更多的一般例子中

79
00:05:57,680 --> 00:06:01,020
设计一个控制器的策略

80
00:06:01,370 --> 00:06:03,180
假设是完全可观测的

81
00:06:03,360 --> 00:06:05,610
估算这个状态  将二者结合

82
00:06:05,810 --> 00:06:07,740
对于一般的POMDPs

83
00:06:08,200 --> 00:06:11,660
相同的策略是通常是合理的策略

84
00:06:11,870 --> 00:06:13,460
但是并不总是保证是最佳的

85
00:06:13,770 --> 00:06:16,580
一般处理这些问题  NP-hard

86
00:06:16,730 --> 00:06:26,120
因此今天的课我要谈

87
00:06:26,280 --> 00:06:28,930
不同于强化学习算法的

88
00:06:29,100 --> 00:06:31,040
它们叫做策略搜索算法

89
00:06:31,230 --> 00:06:37,760
特别地  策略搜索算法

90
00:06:37,920 --> 00:06:41,060
可以在MDP中

91
00:06:41,210 --> 00:06:43,340
完全观察的Markov决定过程中

92
00:06:43,550 --> 00:06:48,260
这些POMDPs和部分可观察的MPDs中 都可以应用的很好

93
00:06:48,440 --> 00:06:51,090
现在我要做的是

94
00:06:51,270 --> 00:06:52,850
将描述策略

95
00:06:52,990 --> 00:06:56,010
搜索算法应用到MDPs

96
00:06:56,210 --> 00:06:57,880
应用到完全观察性的例子中

97
00:06:58,060 --> 00:07:00,330
到最后  我将简要的介绍你们

98
00:07:00,540 --> 00:07:02,290
如何使用策略搜索算法

99
00:07:02,500 --> 00:07:03,580
并将它们应用到POMDPs中

100
00:07:03,740 --> 00:07:06,910
在后面的例子中

101
00:07:07,050 --> 00:07:09,010
当你将策略搜索算法应用到POMDP中

102
00:07:10,140 --> 00:07:14,670
这很难保证你的获得的是全局最佳策略

103
00:07:14,950 --> 00:07:17,990
因为一般来讲

104
00:07:18,200 --> 00:07:20,910
解决POMDPs非常困难

105
00:07:21,110 --> 00:07:24,310
但是我认为策略搜索算法被证明

106
00:07:24,530 --> 00:07:26,010
是在强化学习算法的课堂中 最有效率的

107
00:07:26,190 --> 00:07:27,780
对于MDPs和POMDPs都是的

108
00:07:28,020 --> 00:07:35,730
所以这就是我们将要做的

109
00:07:35,930 --> 00:07:42,640
在策略性搜索中  我们将定义一些设置

110
00:07:42,860 --> 00:07:50,860
我将其表示策略性的大Pi

111
00:07:51,110 --> 00:08:04,110
我们的策略是搜索一个的目的策略

112
00:08:04,370 --> 00:08:07,630
小π属于大Pi

113
00:08:11,040 --> 00:08:13,230
通过类推  我想说-


114
00:08:13,490 --> 00:08:15,930
通过同样的方法

115
00:08:16,090 --> 00:08:18,330
回到当我们谈论的指导性学习

116
00:08:18,550 --> 00:08:21,910
我们定义设定大PI策略的方式

117
00:08:22,090 --> 00:08:24,740
在策略搜索中

118
00:08:24,930 --> 00:08:29,350
在大PI的设置中

119
00:08:31,730 --> 00:08:39,400
这是和监督学习是类似的  在监督学习中

120
00:08:39,600 --> 00:08:51,320
我们定义一个设置脚本H的假设和搜索


121
00:08:51,540 --> 00:08:52,630
然后在这个策略脚本H中搜索一个好的假设

122
00:08:52,800 --> 00:09:01,560
策略搜索有时也叫直接策略搜索

123
00:09:03,290 --> 00:09:05,100
为将这个与我们目前讨论的算法

124
00:09:05,270 --> 00:09:05,820
来源相对比

125
00:09:05,980 --> 00:09:08,670
在迄今为止我们谈论的所有算法中

126
00:09:08,840 --> 00:09:10,550
我们尝试去找到V

127
00:09:10,740 --> 00:09:12,700
我们尝试找到最佳的值函数

128
00:09:12,910 --> 00:09:16,090
然后我们用V

129
00:09:16,270 --> 00:09:17,660
---最佳值函数尝试计算


130
00:09:17,780 --> 00:09:20,140
或者尝试接近大概的π

131
00:09:20,330 --> 00:09:22,750
因此我们先前谈论的所有的方法

132
00:09:22,970 --> 00:09:25,570
都是为找到一个好的策略的策略

133
00:09:25,750 --> 00:09:27,380
一旦我们计算出值函数

134
00:09:27,580 --> 00:09:28,980
然后我们将从那到策略

135
00:09:29,170 --> 00:09:31,630
通过对比在策略搜索算法

136
00:09:31,800 --> 00:09:33,720
和某些叫做直接策略搜索算法

137
00:09:33,900 --> 00:09:37,730
这个构想是  我们将引用"直接的"

138
00:09:37,880 --> 00:09:41,070
试着估计的一个好的策略

139
00:09:41,170 --> 00:09:44,370
不用通过中间尝试 找到值函数的阶段

140
00:09:44,550 --> 00:09:52,180
让我想想  同时在我开展策略搜索----

141
00:09:52,420 --> 00:09:55,290
只要一步  有时候有些迷惑的

142
00:09:55,500 --> 00:09:59,440
再次类比监督学习

143
00:09:59,630 --> 00:10:02,170
当我们谈论逻辑回归

144
00:10:02,390 --> 00:10:07,600
我说过我们已经输入特征X和一些标签Y

145
00:10:07,790 --> 00:10:09,950
我说过让我们通过

146
00:10:10,130 --> 00:10:12,060
使用输入X的逻辑函数 来估计Y

147
00:10:12,290 --> 00:10:14,440
至少最开始

148
00:10:14,590 --> 00:10:16,850
这个逻辑函数有那么点无中生有

149
00:10:17,020 --> 00:10:21,060
在同样的方法  当我定义策略搜索算法

150
00:10:21,310 --> 00:10:23,040
这将有那么一个阶段  我说

151
00:10:23,210 --> 00:10:25,640
"好  让我们来尝试计算这个行为

152
00:10:25,850 --> 00:10:27,790
让我们来通过这个状态中一个逻辑函数

153
00:10:27,960 --> 00:10:30,550
来估计所谓的一个好的行为 "

154
00:10:30,760 --> 00:10:34,530
因此同样  有那么点一个方法无中生有

155
00:10:34,710 --> 00:10:36,620
我将说  "让我们选择一个函数

156
00:10:36,780 --> 00:10:39,180
那将是我们策略花费的选择"

157
00:10:39,360 --> 00:10:42,570
然后我将说  "让我们吧这个输入状态里面

158
00:10:42,740 --> 00:10:44,300
然后我们将通过逻辑方函数描图

159
00:10:44,450 --> 00:10:46,900
然后希望  我们将估计所谓的一个好的函数

160
00:10:47,160 --> 00:10:49,970
抱歉  我们将使用一个逻辑状态方法


161
00:10:50,240 --> 00:10:52,970
来估计一个所谓的好行为 "

162
00:10:53,170 --> 00:10:54,670
因此有那么点

163
00:10:54,880 --> 00:10:56,550
这策略性成本选择的函数

164
00:10:56,700 --> 00:11:00,400
又一次有点任意的

165
00:11:00,570 --> 00:11:02,660
但是当我们谈了监督学习时候它也就这样的

166
00:11:02,930 --> 00:11:11,850
因此为开展我们第一个策略搜索算法

167
00:11:12,000 --> 00:11:13,900
我实际上需要新的定义

168
00:11:14,070 --> 00:12:07,840
所以我们第一个策略搜索算法

169
00:12:08,060 --> 00:12:11,720
我们实际上将需要随机策略

170
00:12:11,980 --> 00:12:14,160
我说的随机策略的意思

171
00:12:14,370 --> 00:12:16,440
是这将由一个函数

172
00:12:16,660 --> 00:12:20,670
它们是实数它是状态和行为的映射

173
00:12:20,850 --> 00:12:25,740
π(s  a)将被解释为一种可能

174
00:12:25,920 --> 00:12:29,830
将在状态S中  采取这个行为A

175
00:12:30,030 --> 00:12:32,790
因此我们必须在A上面加

176
00:12:32,980 --> 00:12:48,070
---换句话说 对每个状态一个随机的策略

177
00:12:48,290 --> 00:12:52,370
制定一个可能的行为分布

178
00:12:52,610 --> 00:13:02,660
因此具体来讲  假设你正执行一些策略π

179
00:13:02,840 --> 00:13:04,460
我有一些随机策略π

180
00:13:04,620 --> 00:13:06,490
我想要执行这个策略π

181
00:13:06,670 --> 00:13:08,510
意思就是---


182
00:13:08,710 --> 00:13:11,320
在这个例子中我有三个行为

183
00:13:11,510 --> 00:13:14,200
在那的意思是  假设我处在某个状态S

184
00:13:14,410 --> 00:13:18,920
我将计算π(s  a1)

185
00:13:19,150 --> 00:13:26,690
π(s  a2)  π(s  a3)

186
00:13:26,890 --> 00:13:28,620
如果我有一个三次行为的MDP

187
00:13:28,830 --> 00:13:33,010
这将会有三个数字总和到一个

188
00:13:33,220 --> 00:13:37,430
然后可能采取行动A1将会等于这个

189
00:13:37,610 --> 00:13:39,040
采取行动A2 将和π(s  a2)等同

190
00:13:39,220 --> 00:13:44,090
采取A3动作将与这个相等

191
00:13:44,360 --> 00:13:48,700
因此这就是所谓的执行一个随机的策略

192
00:13:48,950 --> 00:13:52,010
所以作为一个具体的例子

193
00:13:52,180 --> 00:13:53,900
让我来做这个-----

194
00:13:54,060 --> 00:13:56,390
为什么你要用随机策略

195
00:13:56,500 --> 00:13:57,610
它的概念可能有点难理解

196
00:13:57,780 --> 00:14:05,880
所以让我继续并给一个具体的随机策略

197
00:14:06,060 --> 00:14:08,400
可能是什么样子的例子

198
00:14:08,570 --> 00:14:12,560
对于这个例子  我将用倒立摆

199
00:14:12,710 --> 00:14:15,410
作为刺激例子

200
00:14:15,720 --> 00:14:17,740
这是一个杆子平衡的例子

201
00:14:20,040 --> 00:14:23,610
我们由一个摆动自如的倒立摆

202
00:14:23,780 --> 00:14:25,310
你想左右移动车

203
00:14:25,470 --> 00:14:25,980
来保持杆子垂直

204
00:14:26,160 --> 00:14:30,330
我说一下我的行为-----

205
00:14:30,530 --> 00:14:37,970
对于今天的例子  我将用这个角度

206
00:14:38,190 --> 00:14:41,930
来表示这个杆子phi.

207
00:14:42,140 --> 00:14:49,120
我有两个行动  A1是向左加速

208
00:14:49,320 --> 00:14:52,100
A2是向右加速

209
00:14:55,100 --> 00:14:57,470
实际上  让我写成其他另外一种方式

210
00:14:57,640 --> 00:15:01,640
A1是向左加速  A2是向右加速

211
00:15:01,800 --> 00:15:07,100
所以我们看到  选择一个奖励函数

212
00:15:07,270 --> 00:15:10,980
杆子不管倒向哪边都受惩罚

213
00:15:11,150 --> 00:15:13,840
让我们对这个问题想出一个随机策略

214
00:15:14,090 --> 00:15:20,480
提出一类随机策略  意味着

215
00:15:20,660 --> 00:15:22,440
提出一类方法

216
00:15:22,630 --> 00:15:23,870
来估计在这种状态下

217
00:15:24,070 --> 00:15:25,700
你将要采取行动的函数

218
00:15:25,900 --> 00:15:28,650
因此这是一些任意的选择

219
00:15:28,810 --> 00:15:29,610
我想说

220
00:15:29,780 --> 00:15:34,830
动作A1的可能性π(S  A1)

221
00:15:35,120 --> 00:15:40,680
我写成----OK?

222
00:15:40,860 --> 00:15:42,400
我选择逻辑函数

223
00:15:42,560 --> 00:15:44,550
因为它是我们使用非常多的一个方便的函数

224
00:15:44,670 --> 00:15:45,520
所以我想说

225
00:15:45,680 --> 00:15:48,840
我的策略是一组参数θ

226
00:15:49,010 --> 00:15:51,020
对于给出的

227
00:15:51,200 --> 00:15:51,800
一个参数θ

228
00:15:51,980 --> 00:15:55,230
它就给出一个随机策略

229
00:15:55,380 --> 00:15:58,990
如果我执行带参数θ的策略

230
00:15:59,140 --> 00:16:00,770
那就意味着我选择

231
00:16:00,910 --> 00:16:04,380
一组由这个数字给出正确的A的几率 是由这个数字给出的

232
00:16:04,580 --> 00:16:21,180
因为我的选择执行行为A1和A2

233
00:16:21,320 --> 00:16:23,800
加总成一个  这给出π(S  A2)

234
00:16:24,010 --> 00:16:26,680
因此【不可见的】  这意味着

235
00:16:26,840 --> 00:16:30,530
当我加总状态S  我将计算这个数字

236
00:16:30,710 --> 00:16:33,840
计算一个一个加E到一个负的转置S状态

237
00:16:33,990 --> 00:16:35,460
然后带着这个可能性

238
00:16:35,590 --> 00:16:38,490
我将执行向右加速的行为

239
00:16:38,700 --> 00:16:41,670
用1减去这个可能性

240
00:16:41,840 --> 00:16:43,900
我将执行向左加速的行为

241
00:16:44,090 --> 00:16:47,240
又一次  只是让你明白

242
00:16:47,390 --> 00:16:48,920
为什么这可能是一个合理的事情

243
00:16:49,090 --> 00:16:54,030
我们说我的状态矢量是


244
00:16:54,220 --> 00:17:02,560
这是个【不可听见的】状态

245
00:17:02,690 --> 00:17:04,950
我添加了一个额外的拦截器

246
00:17:05,140 --> 00:17:07,190
只是为给我的逻辑函数一个额外的特征

247
00:17:07,390 --> 00:17:13,640
如果我选择参数和我的策略是  比如说

248
00:17:19,820 --> 00:17:22,030
那意味着在任何情形状态下

249
00:17:23,280 --> 00:17:32,890
采取A1行动的可能性---

250
00:17:33,160 --> 00:17:35,400
采取向右加速的行动的可能性是

251
00:17:35,580 --> 00:17:37,180
这个1/1+E到一个负的转置S

252
00:17:37,490 --> 00:17:38,490
的状态(1/1+e^-φ)

253
00:17:38,850 --> 00:17:43,020
这把θ和S做内积

254
00:17:43,170 --> 00:17:50,140
你就得到φ

255
00:17:50,340 --> 00:17:54,430
等于1/1+e^-φ

256
00:17:54,640 --> 00:18:01,680
如果我选择参数θ如下

257
00:18:02,100 --> 00:18:04,630
这意味着仅仅依赖

258
00:18:04,860 --> 00:18:11,850
倒立摆的角度φ

259
00:18:12,080 --> 00:18:16,790
我加速向右的机会

260
00:18:16,990 --> 00:18:18,260
正是这个倒立摆的角度的函数

261
00:18:18,540 --> 00:18:22,200
因此它的意思是  例如

262
00:18:22,390 --> 00:18:24,480
如果我的倒立摆倾向右边

263
00:18:24,640 --> 00:18:25,760
然后我可能向右加速去hold住它

264
00:18:26,080 --> 00:18:28,100
我希望物理学上倒立摆的事

265
00:18:28,280 --> 00:18:28,780
能起到作用

266
00:18:29,160 --> 00:18:30,910
如果我的杆倾向右边

267
00:18:31,090 --> 00:18:32,800
然后我加速到右去hold住它

268
00:18:32,970 --> 00:18:34,930
相反的如果φ是负数的

269
00:18:35,150 --> 00:18:36,380
它就倾向左边

270
00:18:36,540 --> 00:18:38,140
我将加速左边去hold住它

271
00:18:38,370 --> 00:18:40,100
因此这是一个

272
00:18:40,350 --> 00:18:43,090
参数θ具体选择的一个例子

273
00:18:43,260 --> 00:18:46,240
很显然  这不是一个非常好的策略

274
00:18:46,430 --> 00:18:48,160
因为它忽略了其余的特征

275
00:18:48,380 --> 00:18:50,850
如果这个车到右端更远

276
00:18:51,400 --> 00:18:52,960
你想要它没那么可能去向右加速

277
00:18:53,150 --> 00:18:56,510
你可以通过改变其中一个系数

278
00:18:56,680 --> 00:18:58,870
来考虑捕捉实际这车的位置

279
00:18:59,050 --> 00:19:01,160
然后根据不同的车的速度

280
00:19:01,320 --> 00:19:02,790
和角速度

281
00:19:02,960 --> 00:19:04,590
你也可能想要改变θ

282
00:19:04,740 --> 00:19:06,780
来加以考虑其他影响

283
00:19:06,950 --> 00:19:09,450
也许如果杆子靠得右端太远

284
00:19:09,600 --> 00:19:11,760
但是实际上在摇摆回来

285
00:19:12,930 --> 00:19:14,760
它指定角速度

286
00:19:14,930 --> 00:19:16,770
然后你可能不用那么担心

287
00:19:16,940 --> 00:19:18,220
向右加速太过厉害

288
00:19:18,380 --> 00:19:20,380
因此通过改变参数θ

289
00:19:20,540 --> 00:19:22,300
来获得这些行为

290
00:19:22,500 --> 00:19:26,920
所以我们的目标是调整参数θ---

291
00:19:27,070 --> 00:19:30,380
我们在策略搜索中的目标是调整参数θ

292
00:19:30,570 --> 00:19:36,320
这样当我们执行策略π_θ

293
00:19:36,560 --> 00:19:39,140
这杆尽可能长的保持向上的

294
00:19:39,390 --> 00:19:45,030
换句话说  我们的目标是最大化θ的函数

295
00:19:45,250 --> 00:20:02,590
我们的目标是最大化预期回报价值

296
00:20:02,810 --> 00:20:09,190
当我们执行策略π_θ

297
00:20:09,430 --> 00:20:12,520
我们想要选择参数θ来最大化它

298
00:20:16,320 --> 00:20:20,750
对于问题的建立

299
00:20:20,970 --> 00:20:22,060
和策略搜索

300
00:20:22,360 --> 00:20:23,870
和策略课程或者其他的有什么问题吗?

301
00:20:25,420 --> 00:20:27,690
学生:在一个例子中我们有两个以上的行为

302
00:20:27,850 --> 00:20:28,970
我们是对每个分布都用一个不同的θ

303
00:20:29,130 --> 00:20:32,450
还是仍然使用同一个参数

304
00:20:32,630 --> 00:20:33,630
教导员(Andew Ng):哦  是的

305
00:20:33,760 --> 00:20:35,230
好的  如果我们有两个以上的行为

306
00:20:35,370 --> 00:20:38,200
结果证明是在我们的策略类

307
00:20:38,370 --> 00:20:39,260
我们可以选择几乎任何东西你想要的

308
00:20:39,450 --> 00:20:43,450
但是你说有一个固定的离散行动的数字

309
00:20:43,660 --> 00:20:46,720
我有时候使用一个Softmax参数

310
00:20:47,040 --> 00:20:51,470
类似早些时候

311
00:20:51,660 --> 00:20:53,440
我们看到的softmax回归

312
00:20:53,640 --> 00:20:56,880
你可能说---黑板上没地方了

313
00:20:57,080 --> 00:21:11,160
你可能有一系列参数θ1到θd

314
00:21:11,400 --> 00:21:13,940
如果有D这个行为---

315
00:21:14,180 --> 00:21:20,730
πθ(s  a)*(eθTs)/Σ_1eθTs; ---

316
00:21:20,950 --> 00:21:26,030
这样将是一个

317
00:21:26,220 --> 00:21:28,290
为多个动作的softmax参数化的例子

318
00:21:28,470 --> 00:21:31,560
结果证明是如果你有连续的行动

319
00:21:31,720 --> 00:21:35,150
你实际上可以使这个成为一个密度的行动A

320
00:21:35,340 --> 00:21:37,880
也被其他东西参数化

321
00:21:38,090 --> 00:21:41,580
但是策略类的选择是部分由你决定

322
00:21:41,740 --> 00:21:45,010
是否我们选择线性函数

323
00:21:45,170 --> 00:21:46,910
或者二次特征的线性函数

324
00:21:47,100 --> 00:21:50,510
或者监督学习似乎由我们决定

325
00:21:50,680 --> 00:21:53,670
是同样的方式

326
00:21:53,860 --> 00:21:58,190
还有其他的问题吗?好的

327
00:21:58,380 --> 00:22:02,030
学生:【不可听见的】随机的?

328
00:22:02,200 --> 00:22:02,900
教导员(Andrew Ng):是的

329
00:22:03,410 --> 00:22:04,210
学生:所以

330
00:22:04,400 --> 00:22:07,680
一个可能的策略

331
00:22:07,930 --> 00:22:11,100
用【不可听见的】数字是随机的

332
00:22:11,460 --> 00:22:13,090
教导员(Andrew Ng):我明白

333
00:22:13,470 --> 00:22:16,500
给出MDP有随机转移概率

334
00:22:16,720 --> 00:22:20,380
很可能是使用策略和状态

335
00:22:20,550 --> 00:22:22,720
转移概率的随机数

336
00:22:22,930 --> 00:22:24,160
这个答案是"yes"

337
00:22:24,300 --> 00:22:27,700
但是在我稍后展示的目的中

338
00:22:27,890 --> 00:22:29,150
那将不会有作用的

339
00:22:29,300 --> 00:22:30,550
但正式而言  是有可能的

340
00:22:30,710 --> 00:22:33,490
如果你已经有一个固定的


341
00:22:33,740 --> 00:22:37,600
如果你有一个固定策略  然后你可能那样做

342
00:22:37,770 --> 00:22:41,170
还有其他的吗?Yeah  不

343
00:22:41,310 --> 00:22:43,390
我才甚至【听不清】一个策略类可以那样做

344
00:22:43,550 --> 00:22:45,790
但是稍后的导数的

345
00:22:45,960 --> 00:22:47,730
我实际上需要将它保持分离的

346
00:22:47,900 --> 00:22:50,730
实际上  你们可以


347
00:22:50,860 --> 00:22:52,940
我知道策略搜索概念

348
00:22:53,030 --> 00:22:54,040
有些时候是有点令人迷糊的

349
00:22:54,170 --> 00:22:56,450
如果明白话你们可以举起手?

350
00:22:56,630 --> 00:23:02,770
好吧 谢谢 因此我们讨论一个算法

351
00:23:02,960 --> 00:23:07,800
我想要谈论的---第一个算法

352
00:23:07,990 --> 00:23:08,900
我将要展示的是

353
00:23:09,060 --> 00:23:11,650
有时候叫做加强算法

354
00:23:11,840 --> 00:23:15,650
我所谓要展示的其实并不是确切的加强算法

355
00:23:15,840 --> 00:23:19,560
因为它最初是由作者Ron Williams展示的

356
00:23:19,720 --> 00:23:21,660
但是本质上是差不多的

357
00:23:21,820 --> 00:23:25,500
这就是想法

358
00:23:25,660 --> 00:23:32,660
在这续集里---我即将要做

359
00:23:32,820 --> 00:23:38,750
我将假设S0是个固定的初始状态

360
00:23:38,960 --> 00:23:46,800
或者如果S0

361
00:23:47,010 --> 00:23:49,600
从一些固定初始状态分布

362
00:23:49,920 --> 00:23:51,070
然后其他任何事都将迎刃而解

363
00:23:51,240 --> 00:23:53,330
但是我们仅仅说S0是某个固定的初始状态

364
00:23:54,250 --> 00:24:06,230
所以我的目标是

365
00:24:06,420 --> 00:24:08,470
最大化这个期望的加和【不可听见的】

366
00:24:08,670 --> 00:24:12,660
给出策略和

367
00:24:12,870 --> 00:24:15,670
原始状态和行为:S0

368
00:24:15,870 --> 00:24:19,430
A0  S1  A1等等一直到ST  AT

369
00:24:19,620 --> 00:24:23,300
它们是随机的变量

370
00:24:23,510 --> 00:24:30,260
因此让我写成这个期望明确作为一个总结

371
00:24:30,530 --> 00:24:47,870
所以可能的状态和行为序列---

372
00:24:48,060 --> 00:24:49,790
因此那就是所谓的期望值

373
00:24:49,790 --> 00:24:51,020
它是随机变量乘以那个

374
00:24:51,250 --> 00:24:54,860
让我来扩展这一概率

375
00:24:55,030 --> 00:25:04,070
因此看到这一确切序列状态和行为的概率

376
00:25:04,230 --> 00:25:09,590
是所有MDP开始

377
00:25:09,740 --> 00:25:10,430
在那个状态里的概率

378
00:25:10,570 --> 00:25:12,680
如果这是确定型初始状态

379
00:25:12,900 --> 00:25:14,820
那么所有状态会混合成一个状态

380
00:25:14,950 --> 00:25:16,810
然而  在原始状态有一些分布

381
00:25:18,270 --> 00:25:24,750
乘以概率  即你选择行动

382
00:25:24,960 --> 00:25:31,000
从A0到那种为0的状态的概率

383
00:25:31,190 --> 00:25:36,830
乘以MDP的转移概率  发生在转移

384
00:25:37,030 --> 00:25:41,310
从你选择行为A0到状态S0的状态S1

385
00:25:41,520 --> 00:25:51,380
乘以你从选择等等的概率

386
00:25:51,610 --> 00:26:11,500
最后一项是  乘以那个

387
00:26:11,750 --> 00:26:13,290
因此我做的是

388
00:26:13,500 --> 00:26:17,110
把这个观察到的这个状态

389
00:26:17,270 --> 00:26:19,620
和行为的序列概率和这个循环

390
00:26:19,930 --> 00:26:21,210
确切的或者像这样明确扩展的

391
00:26:21,380 --> 00:26:23,890
事实证明

392
00:26:24,070 --> 00:26:27,670
后来我要去需要写很多奖励总和

393
00:26:27,830 --> 00:26:31,760
所以我只是要去调用现在的回报

394
00:26:31,990 --> 00:26:33,650
所以  只要以后在此课堂上

395
00:26:33,870 --> 00:26:35,330
我写的单词回报(payoff)

396
00:26:35,760 --> 00:26:47,240
我意味着这个总和  因此   我们的目标是

397
00:26:47,670 --> 00:26:49,660
最大限度地实现预期的回报

398
00:26:49,800 --> 00:26:51,590
所以  我们的目标是最大化这个总和

399
00:26:51,810 --> 00:26:53,910
让我实际上跳到前头

400
00:26:54,090 --> 00:26:55,840
我要写下最后的答案是什么

401
00:26:55,970 --> 00:26:58,530
然后我会回来  并证明该算法

402
00:26:59,900 --> 00:28:13,930
因此  这里的算法

403
00:28:14,130 --> 00:28:15,600
这是我们要如何

404
00:28:15,850 --> 00:28:17,750
更新算法的参数

405
00:28:17,930 --> 00:28:20,720
我们将样本化状态行动序列

406
00:28:20,890 --> 00:28:22,140
你这样做的方式是

407
00:28:22,310 --> 00:28:23,960
你只需要当前的随机策略

408
00:28:24,180 --> 00:28:26,130
并在MDP中执行

409
00:28:26,340 --> 00:28:28,420
所以继续

410
00:28:28,660 --> 00:28:30,370
并且从一些初始状态开始

411
00:28:30,530 --> 00:28:32,090
根据你当前的随机策略采取一个随机行动

412
00:28:32,240 --> 00:28:34,320
看看那里的状态转换可能让你

413
00:28:34,500 --> 00:28:36,610
所以你只是做T次的步骤  并且

414
00:28:36,740 --> 00:28:38,010
这就是如何对状态序列进行取样的

415
00:28:38,180 --> 00:28:39,860
然后你计算回报

416
00:28:40,060 --> 00:28:43,020
然后执行此更新

417
00:28:45,280 --> 00:28:45,470
因此

418
00:28:45,900 --> 00:28:48,320
让我们回去搞清楚这个算法是做什么

419
00:28:48,590 --> 00:28:52,070
请注意  该算法进行随机更新

420
00:28:52,250 --> 00:28:53,630
因为它的每一步根据右侧的东西

421
00:28:53,760 --> 00:28:55,360
来更新数据

422
00:28:55,530 --> 00:28:56,450
这个右侧的东西

423
00:28:56,630 --> 00:28:59,340
很大程度上取决于你的回报

424
00:28:59,510 --> 00:29:01,950
和你看到的状态行动序列

425
00:29:02,180 --> 00:29:03,910
你的状态行动顺序是随机的

426
00:29:04,120 --> 00:29:08,760
所以我想要做的是弄清楚 – 在每一步中

427
00:29:08,950 --> 00:29:11,790
我将采取一个步骤  采取随机选择

428
00:29:12,000 --> 00:29:15,730
因为它取决于这个随机状态的动作序列

429
00:29:15,950 --> 00:29:18,740
所以我想要做的是找出平均数字

430
00:29:18,940 --> 00:29:20,920
它是如何改变的参数θ

431
00:29:21,140 --> 00:29:24,120
我特别想知道

432
00:29:24,320 --> 00:29:27,460
什么是参数的变化的预期值

433
00:29:58,200 --> 00:29:59,120
所以  我想知道

434
00:29:59,300 --> 00:30:04,790
我的参数θ的变化的预期值是什么

435
00:30:05,020 --> 00:30:08,700
我们的目标是最大化总和[听不清] – 


436
00:30:08,890 --> 00:30:10,520
我们的目标是最大化价值回报

437
00:30:11,090 --> 00:30:16,560
只要期望的更新  能平均提高

438
00:30:16,750 --> 00:30:21,150
我们的预期回报  然后我们很高兴

439
00:30:21,350 --> 00:30:23,080
原来

440
00:30:23,320 --> 00:30:28,630
该算法是一种随机梯度上升的形式

441
00:30:28,820 --> 00:30:32,940
记得  当我谈到


442
00:30:33,140 --> 00:30:36,020
关于最小二乘回归的随机梯度下降

443
00:30:36,280 --> 00:30:39,320
我说你有一些参数 - 也许你想

444
00:30:39,500 --> 00:30:41,570
最小化二次函数 那么

445
00:30:41,740 --> 00:30:47,900
你可能随机四处徘徊

446
00:30:48,080 --> 00:30:50,620
直到它得到接近

447
00:30:50,840 --> 00:30:53,760
[无声]二次曲面的最佳参数

448
00:30:53,970 --> 00:30:57,570
事实证明  加强算法

449
00:30:57,710 --> 00:30:58,740
将非常像它

450
00:30:59,010 --> 00:31:01,640
这将是一个随机梯度上升算法

451
00:31:01,820 --> 00:31:02,560
在其中的每一步 –

452
00:31:02,700 --> 00:31:04,730
我们采取的步骤是一点点随机

453
00:31:04,950 --> 00:31:06,740
它由随机状态的动作序列决定

454
00:31:06,950 --> 00:31:08,620
但期望

455
00:31:08,790 --> 00:31:11,500
这基本上是梯度上升算法

456
00:31:11,690 --> 00:31:13,300
因此  我们将这样做

457
00:31:13,510 --> 00:31:14,570
它会随机四处徘徊

458
00:31:14,730 --> 00:31:16,530
但平均采取最佳

459
00:31:16,790 --> 00:31:19,390
因此  让我继续前进  现在证明它

460
00:31:19,590 --> 00:31:39,110
让我们来看看 我将要做的是

461
00:31:39,320 --> 00:31:43,890
我要获得

462
00:31:44,050 --> 00:31:47,950
预期收益最大化的梯度上升的更新规则

463
00:31:48,170 --> 00:31:49,700
然后  我将希望展示

464
00:31:49,900 --> 00:31:52,690
导出梯度上升的更新规则

465
00:31:52,850 --> 00:31:56,020
我将以这个期望的事情而结束 所以

466
00:31:56,190 --> 00:31:59,780
在我做求导之前

467
00:31:59,970 --> 00:32:04,450
让我提醒你的链式法则– 微分的乘积法则

468
00:32:04,720 --> 00:32:13,410
如果我有一个函数的乘积

469
00:32:13,640 --> 00:32:17,270
然后乘积的导数

470
00:32:17,470 --> 00:32:39,980
是通过在一个时间 对这些东西进行求导

471
00:32:40,500 --> 00:32:42,470
所以我首先区分为 F prime(F’偏导数)

472
00:32:42,650 --> 00:32:43,210
让其他两个固定

473
00:32:43,410 --> 00:32:44,690
然后我区分G

474
00:32:44,900 --> 00:32:46,340
让其他两个固定

475
00:32:46,510 --> 00:32:47,960
然后我区分为H

476
00:32:48,140 --> 00:32:49,900
所以我得到H prime  让其他两个固定

477
00:32:50,080 --> 00:32:51,500
这就是求导的乘积规则

478
00:32:51,650 --> 00:33:05,650
如果你回顾前面我们写的这个方程

479
00:33:05,820 --> 00:33:09,640
这个方程的预期回报

480
00:33:11,080 --> 00:33:13,930
这对所有状态的概率

481
00:33:14,250 --> 00:33:14,880
乘以回报的总和

482
00:33:15,080 --> 00:33:18,700
所以我将要做的是

483
00:33:18,930 --> 00:33:22,180
关于参数Θ的导数

484
00:33:22,420 --> 00:33:25,690
是因为我想要做的这个函数的梯度上升

485
00:33:25,920 --> 00:33:27,210
所以我要为这个函数

486
00:33:27,410 --> 00:33:28,340
对Θ的导数

487
00:33:28,550 --> 00:33:29,840
然后再尝试用此函数上坡

488
00:33:30,040 --> 00:33:32,450
因此  使用该乘积规则

489
00:33:32,630 --> 00:33:34,250
当我对这个函数的Θ

490
00:33:34,460 --> 00:33:35,160
进行求导数的时候

491
00:33:35,450 --> 00:33:36,400
我得到的是

492
00:33:36,530 --> 00:33:38,490
我们将最终得到这些项的总和

493
00:33:38,670 --> 00:33:41,260
在这里  有很多项取决于对Θ

494
00:33:41,480 --> 00:33:46,850
所以我将最终得到的是  拥有一个总和 –

495
00:33:47,040 --> 00:33:48,870
有一个项  对应这个导数

496
00:33:49,050 --> 00:33:50,610
而其他的东西都是固定的  一个项的导数

497
00:33:50,780 --> 00:33:52,640
其他一切都是固定的

498
00:33:52,840 --> 00:33:54,020
我得到一个项

499
00:33:54,160 --> 00:33:56,090
对最后那个进行求导

500
00:33:56,250 --> 00:33:58,160
其他一切保持固定的

501
00:33:58,370 --> 00:33:59,590
所以只要对它应用乘积法则

502
00:33:59,780 --> 00:34:11,230
让我们写下来 所以  我有 回报的预期值的

503
00:34:11,380 --> 00:34:13,910
对于Θ的导数 –

504
00:34:14,130 --> 00:34:17,400
事实证明  其实我可以通过整整四个步骤

505
00:34:17,580 --> 00:34:21,330
完成整个推导

506
00:34:21,530 --> 00:34:23,360
但每个步骤

507
00:34:23,520 --> 00:34:24,770
需要大量的书写

508
00:34:25,180 --> 00:34:29,120
所以我开始写

509
00:34:29,290 --> 00:34:31,980
看看它是如何推导的  但这是一个四步推导

510
00:34:38,460 --> 00:34:42,110
因此  在状态行动序列的总和中

511
00:34:42,390 --> 00:34:43,370
如我们以前看到的

512
00:36:03,480 --> 00:36:03,880
右括号

513
00:36:04,100 --> 00:36:09,950
然后乘以回报 因此  大量的书写

514
00:36:10,150 --> 00:36:11,930
只是我前面的公式

515
00:36:12,200 --> 00:36:14,430
和区分这些项Θ之一

516
00:36:14,680 --> 00:36:16,020
取决于一次

517
00:36:16,190 --> 00:36:21,280
这是 PI Θ S0 A0 PI导数的项

518
00:36:21,460 --> 00:36:23,930
所以第一个微分项

519
00:36:24,110 --> 00:36:25,080
有第二个

520
00:36:25,270 --> 00:36:26,470
然后你有加点  点

521
00:36:26,650 --> 00:36:28,320
点  [听不清]

522
00:36:28,550 --> 00:36:29,180
这是我的最后一项

523
00:36:31,310 --> 00:36:37,190
所以这是四个步骤中的第一个

524
00:36:37,560 --> 00:36:53,620
通过代数 –

525
00:37:00,120 --> 00:37:01,260
让我写下来  并说服我们

526
00:37:01,510 --> 00:37:02,460
这是正确的

527
00:38:06,960 --> 00:38:08,330
这是第二个步骤

528
00:38:08,680 --> 00:38:12,180
它刚刚说服自己  如果我扩大了-

529
00:38:12,370 --> 00:38:15,200
使用那个总和  再乘以前面的乘积

530
00:38:15,380 --> 00:38:17,220
然后我回去  我得到项总和

531
00:38:17,410 --> 00:38:20,760
本质上 - 例如  当我乘这个

532
00:38:21,020 --> 00:38:23,840
顶端乘积的比例  第一个分数

533
00:38:24,100 --> 00:38:27,430
然后PI_Θ S0A0

534
00:38:27,610 --> 00:38:30,720
即会取消这个PI_Θ S0A0

535
00:38:30,970 --> 00:38:33,370
并且更换为:

536
00:38:33,580 --> 00:38:34,730
PI_Θ S0A0关于θ的导数

537
00:38:34,940 --> 00:38:37,540
[无声]代数是第二次

538
00:38:37,810 --> 00:38:49,060
但是  上面的项只是我以前算出的 –

539
00:38:49,320 --> 00:38:57,750
状态行动序列的联合概率

540
00:38:58,120 --> 00:39:25,720
现在我已经乘以它再乘以回报

541
00:39:26,190 --> 00:39:33,910
通过期望的定义

542
00:39:34,160 --> 00:40:03,360
这仅仅是等于那个东西乘以回报

543
00:40:07,520 --> 00:40:11,560
所以这期望里面的东西

544
00:40:11,990 --> 00:40:16,970
这正是我们在内部组中

545
00:40:17,210 --> 00:40:20,010
采取的加强算法的步骤

546
00:40:20,250 --> 00:40:22,020
这证明了

547
00:40:22,220 --> 00:40:26,370
我们改变Θ的预期值 是完全

548
00:40:26,610 --> 00:40:30,010
在我们预期回报的梯度方向

549
00:40:30,250 --> 00:40:31,870
这就是我如何开始这个整个推导

550
00:40:32,070 --> 00:40:33,890
我说过  让我们看一下我们的预期回报

551
00:40:34,080 --> 00:40:36,220
并采用对于Θ的导数

552
00:40:36,450 --> 00:40:39,420
我们已经证明上的期望

553
00:40:39,630 --> 00:40:43,010
我采取加强的步骤方向

554
00:40:43,210 --> 00:40:45,210
是我想要优化的梯度

555
00:40:45,470 --> 00:40:47,350
这表明

556
00:40:47,520 --> 00:40:51,280
该算法是一个随机梯度上升算法

557
00:40:51,450 --> 00:40:54,760
我写了很多

558
00:40:54,950 --> 00:40:57,550
你为什么不花一分钟在方程的

559
00:40:57,730 --> 00:40:59,300
[无声]检查看看是否一切都明白

560
00:40:59,490 --> 00:41:00,260
我会擦除黑板

561
00:41:00,410 --> 00:41:42,970
然后看看你是否有问题 有问题吗?

562
00:41:53,450 --> 00:41:56,440
请举手  如果明白了?很好

563
00:41:56,750 --> 00:42:07,120
一些评论  我们谈到

564
00:42:07,420 --> 00:42:10,000
这些价值函数的近似方法

565
00:42:10,220 --> 00:42:12,990
当你估计V*  然后你去从V*到PI

566
00:42:13,210 --> 00:42:15,150
那么这里也是策略的搜索方法

567
00:42:15,500 --> 00:42:17,020
你尝试直接估计的策略

568
00:42:17,290 --> 00:42:20,000
因此 让我们简要谈谈

569
00:42:20,000 --> 00:42:21,930
哪一个可能是更可取的

570
00:42:22,140 --> 00:42:24,920
事实证明  策略的搜索算法

571
00:42:25,140 --> 00:42:26,520
是特别有效的

572
00:42:26,730 --> 00:42:29,520
你可以选择一个简单的策略类PI

573
00:42:29,790 --> 00:42:33,230
因此  真正的问题是

574
00:42:33,460 --> 00:42:37,560
对于你的问题

575
00:42:37,800 --> 00:42:40,160
是否存在这样的一个简单线性函数

576
00:42:40,330 --> 00:42:41,840
或逻辑函数  是映射到状态特征

577
00:42:42,060 --> 00:42:44,460
到行动的  并且运行得很好

578
00:42:44,680 --> 00:42:47,290
倒立摆的问题 –

579
00:42:47,480 --> 00:42:48,580
这是很可能是真实的

580
00:42:48,770 --> 00:42:51,460
通过各种不同的参数选择

581
00:42:51,670 --> 00:42:52,380
可以说

582
00:42:52,550 --> 00:42:54,540
像杆子的向右倾斜

583
00:42:54,720 --> 00:42:56,690
然后朝着右边加速   以试图hold住它

584
00:42:56,880 --> 00:42:59,990
由于倒立摆  这可能是真实的

585
00:43:00,220 --> 00:43:02,260
对于很多所谓的低级别控制任务

586
00:43:02,500 --> 00:43:04,360
如驾驶汽车

587
00:43:04,590 --> 00:43:07,890
在低级别反射是  你让车左拐

588
00:43:08,090 --> 00:43:10,030
来避免另一辆汽车  你控制你的车

589
00:43:10,210 --> 00:43:12,930
沿着公路驾驶  飞行的直升机

590
00:43:13,130 --> 00:43:16,350
非常短的时间内的决策类型 –

591
00:43:16,580 --> 00:43:19,670
我想象这些一个训练有素的驾驶员

592
00:43:19,860 --> 00:43:22,560
或一个训练有素的飞行员

593
00:43:22,810 --> 00:43:24,920
这些像训练有素的操作员做出的决定

594
00:43:25,140 --> 00:43:26,620
这几乎是一种条件反射

595
00:43:27,050 --> 00:43:28,630
这些各种本能的东西非常快

596
00:43:28,780 --> 00:43:30,120
你的直接从输入  数据  行为做出反应

597
00:43:30,330 --> 00:43:31,840
这些问题

598
00:43:32,070 --> 00:43:34,010
你或许可以选择一个合理策略类

599
00:43:34,270 --> 00:43:35,610
如逻辑函数或之类的东西

600
00:43:35,810 --> 00:43:38,910
它往往是有效的 相反

601
00:43:39,220 --> 00:43:43,220
如果你有问题   需要很长的多步推理的

602
00:43:43,470 --> 00:43:44,960
所以  比如你有一盘棋

603
00:43:45,240 --> 00:43:46,760
你必须仔细考虑前因后果

604
00:43:47,050 --> 00:43:48,720
如果我这样做  那么他们就会做

605
00:43:48,920 --> 00:43:49,690
那么他们就会做到这一点

606
00:43:49,900 --> 00:43:51,500
那么他们就会做到这一点

607
00:43:52,000 --> 00:43:53,310
我觉得这不是那么本能的

608
00:43:53,510 --> 00:43:55,110
需要非常高的水平决策

609
00:43:55,330 --> 00:44:00,760
对于这样的问题  我有时会使用

610
00:44:00,960 --> 00:44:02,950
一个值函数逼近的方法

611
00:44:03,170 --> 00:44:07,270
让我说稍后再说

612
00:44:07,500 --> 00:44:13,190
我想要做的最后一件事情其实是 告诉你们\ 


613
00:44:13,660 --> 00:44:20,910
我认为只是一个侧面意见

614
00:44:21,140 --> 00:44:24,480
事实证明  如果你有POMDP

615
00:44:24,700 --> 00:44:25,920
如果你有一个部分观察到的MDP –

616
00:44:26,240 --> 00:44:27,560
我不希望说太多 - 这

617
00:44:27,780 --> 00:44:33,030
如果你只拥有一个近似 –

618
00:44:33,320 --> 00:44:40,810
让我们叫它为S^的真实状态

619
00:44:41,050 --> 00:44:44,820
并因此这可能是S^

620
00:44:45,030 --> 00:44:48,900
等于S-t|t给予卡尔曼滤波  然后

621
00:44:49,130 --> 00:45:01,070
你可以仍然使用这些策略的搜索算法

622
00:45:01,310 --> 00:45:05,040
在那里你可以说PI θ的S^  A –

623
00:45:05,310 --> 00:45:15,270
有各种方式可以使用

624
00:45:15,500 --> 00:45:17,430
POMDPs策略的搜索算法

625
00:45:17,650 --> 00:45:18,410
但是这是其中之一

626
00:45:18,600 --> 00:45:20,560
如果你只有估计的状态

627
00:45:20,740 --> 00:45:22,480
你就可以选择一个策略类

628
00:45:22,690 --> 00:45:24,350
仅仅看着你的状态估计

629
00:45:24,550 --> 00:45:25,230
来选择行为

630
00:45:25,390 --> 00:45:28,370
使用同样的方法估算状态

631
00:45:28,580 --> 00:45:29,780
在培训和测试中

632
00:45:30,020 --> 00:45:31,730
这通常会做一些

633
00:45:31,940 --> 00:45:35,550
所以这种策略的搜索算法经常可以应用

634
00:45:35,750 --> 00:45:40,590
于合理有效地POMDPs中

635
00:45:40,810 --> 00:45:50,010
有一个算法  我想谈谈

636
00:45:50,210 --> 00:45:51,870
但关于加强算法的最后一些话

637
00:45:52,080 --> 00:45:55,590
事实证明  加强算法往往效果很好

638
00:45:55,800 --> 00:45:58,020
但往往是极其缓慢的 因此

639
00:45:58,200 --> 00:46:02,390
[无声]的工作的  但有一点需要注意的是

640
00:46:02,560 --> 00:46:04,790
因为你采取梯度上升的这些步骤

641
00:46:04,990 --> 00:46:07,200
是非常嘈杂  你状态行动序列进行采样

642
00:46:07,210 --> 00:46:08,000
然后你采取梯度上升在本质上

643
00:46:08,290 --> 00:46:11,530
是一种随机的方向排序的步骤

644
00:46:11,710 --> 00:46:13,330
只是在期望上是正确的

645
00:46:13,470 --> 00:46:16,630
对于加强算法

646
00:46:16,780 --> 00:46:19,480
梯度上升方向有时可能会有点偏差

647
00:46:19,650 --> 00:46:22,250
所以通常需要一百万个迭代的帝都上升

648
00:46:22,410 --> 00:46:25,500
或一千万个  或1亿个梯度上升迭代

649
00:46:25,680 --> 00:46:28,070
对于加强 [听不清]  所以这是一些

650
00:46:28,210 --> 00:46:28,740
需要注意的东西

651
00:46:28,940 --> 00:46:40,430
其中的后果之一是  在加强算法中  


652
00:46:40,630 --> 00:46:42,760
我真的不应该把它叫做"加强"

653
00:46:42,910 --> 00:46:44,710
在本质上加强算法

654
00:46:44,880 --> 00:46:45,810
有这一步

655
00:46:45,970 --> 00:46:47,600
你需要来样一个状态的行动序列

656
00:46:47,850 --> 00:46:52,590
因此在原则上可以在机器人上这样做

657
00:46:52,780 --> 00:46:54,470
如果你试图控制一个机器人

658
00:46:54,650 --> 00:46:56,910
你可以在物理上初始化为某些状态

659
00:46:57,090 --> 00:46:58,170
挑一个动作等等

660
00:46:58,320 --> 00:47:00,250
并从那里取样状态行动序列

661
00:47:00,410 --> 00:47:03,590
但是  如果你需要做这个1000万次

662
00:47:03,760 --> 00:47:05,300
你可能不希望[听不清]

663
00:47:05,490 --> 00:47:06,470
你的机器人1000万次

664
00:47:06,650 --> 00:47:09,120
我个人在加强模拟中

665
00:47:09,260 --> 00:47:11,440
看到的更多的应用

666
00:47:11,620 --> 00:47:15,270
也许你可以轻松地运行1万个模拟仿真

667
00:47:15,400 --> 00:47:17,870
或1000万个机器人模拟

668
00:47:18,040 --> 00:47:20,380
但你可能并不想这样做 –


669
00:47:20,560 --> 00:47:22,200
让你的机器人重复

670
00:47:22,420 --> 00:47:23,860
一些动作1000万次 所以

671
00:47:24,030 --> 00:47:27,220
我个人看到更多加强的应用

672
00:47:27,390 --> 00:47:30,020
学习使用一个模拟器

673
00:47:30,200 --> 00:47:32,600
而不是实际物理设备

674
00:47:32,780 --> 00:47:37,730
我想做的最后一件事情是

675
00:47:37,890 --> 00:47:38,760
告诉你另外一个算法

676
00:47:38,910 --> 00:47:40,580
最后一个策略的搜索算法 [听不清]

677
00:47:40,780 --> 00:47:42,300
笔记本电脑的显示屏

678
00:47:52,520 --> 00:47:54,380
这是策略的搜索算法  称为Pegasus

679
00:47:54,610 --> 00:47:58,750
实际上在我们自主直升机飞行上

680
00:47:59,140 --> 00:48:00,880
使用多年了

681
00:48:01,080 --> 00:48:02,910
还有一些其他事情  我们现在要做的

682
00:48:03,100 --> 00:48:06,090
因此  这里的想法

683
00:48:06,260 --> 00:48:10,010
这是RL形式体系上的一个提醒幻灯片

684
00:48:10,170 --> 00:48:11,270
在这里  没有你不知道的

685
00:48:11,480 --> 00:48:15,430
但我只想形象地描述RL形式体系

686
00:48:15,590 --> 00:48:17,650
因为我以后将使用它 我要去画

687
00:48:17,840 --> 00:48:20,060
强化学习图片如下

688
00:48:20,230 --> 00:48:22,810
初始化[听不清]系统

689
00:48:22,960 --> 00:48:25,380
说直升机或任何总和状态S0

690
00:48:25,570 --> 00:48:28,510
你选择一个动作A0  然后你会说

691
00:48:28,690 --> 00:48:31,030
直升机动力带你到一些新的状态S1

692
00:48:31,220 --> 00:48:33,800
你可以选择其他一些行动A1  等等

693
00:48:33,990 --> 00:48:36,280
然后你一定的奖励函数

694
00:48:36,290 --> 00:48:38,810
你答复你总结出来的状态序列

695
00:48:39,030 --> 00:48:40,270
这 就是你的总回报

696
00:48:40,450 --> 00:48:42,520
因此  这仅仅是一张图片

697
00:48:42,650 --> 00:48:45,920
我要使用总结RL问题

698
00:48:46,110 --> 00:48:51,540
我们的目标是最大化预期的回报

699
00:48:51,700 --> 00:48:52,720
这是预期的回报总额

700
00:48:52,870 --> 00:48:54,630
我们的目标是学习策略

701
00:48:54,790 --> 00:48:56,300
我用一个绿色方块表示

702
00:48:56,450 --> 00:48:58,110
因此  我们的策略--

703
00:48:58,320 --> 00:49:01,580
目前我会切换回确定的策略

704
00:49:01,770 --> 00:49:04,460
所以我的确定的策略将是

705
00:49:04,610 --> 00:49:07,800
一些从状态的行动的函数映射

706
00:49:07,990 --> 00:49:12,570
作为一个具体的例子  你能想象

707
00:49:12,790 --> 00:49:14,270
在策略的搜寻设定中

708
00:49:14,430 --> 00:49:16,720
你可能有一个线性的策略类

709
00:49:16,890 --> 00:49:19,780
所以你可以想象行动A

710
00:49:19,920 --> 00:49:23,300
将是状态的线性函数  并且你的目标

711
00:49:23,480 --> 00:49:27,660
是学习线性函数的参数

712
00:49:27,840 --> 00:49:30,230
所以想象尝试做线性发展的策略

713
00:49:30,410 --> 00:49:31,150
除非你想

714
00:49:31,280 --> 00:49:32,790
优化强化学习目标

715
00:49:32,970 --> 00:49:35,060
所以只[听不清]想象

716
00:49:35,250 --> 00:49:37,810
行动A是转置S的状态

717
00:49:38,010 --> 00:49:39,460
你去和策略搜索

718
00:49:39,650 --> 00:49:41,440
好的θ好参数

719
00:49:41,610 --> 00:49:43,590
以便最大化预期回报

720
00:49:43,790 --> 00:49:46,520
这将是一个在该图片上应用的设置

721
00:49:46,700 --> 00:49:49,300
这个想法 很多时候

722
00:49:49,490 --> 00:49:53,050
我们提出一个MDP模型或模拟器

723
00:49:53,210 --> 00:49:57,710
并像以前那样  一个模式或模拟器

724
00:49:57,840 --> 00:50:00,020
只是一个对话框  需要输入一些状态ST

725
00:50:00,200 --> 00:50:02,290
输入需要的一些动作

726
00:50:02,590 --> 00:50:05,360
然后输出一些[无声]状态ST+1

727
00:50:05,560 --> 00:50:06,650
你可能希望采取MDP

728
00:50:06,860 --> 00:50:09,650
这个ST+1将是一个随机的状态

729
00:50:09,840 --> 00:50:10,540
这将是来自

730
00:50:10,790 --> 00:50:13,050
MDP的随机状态转移概率

731
00:50:13,270 --> 00:50:15,730
这是很重要的 非常重要

732
00:50:15,940 --> 00:50:19,180
ST+1将是一个随机函数ST和AT

733
00:50:19,360 --> 00:50:21,450
在模拟器上  这是[听不清]

734
00:50:21,640 --> 00:50:24,070
因此  例如  对于自主的直升机飞行

735
00:50:24,270 --> 00:50:28,880
你使用监督学习建立一个模拟器

736
00:50:29,060 --> 00:50:31,160
像线性回归算法[听不清]线性回归

737
00:50:31,370 --> 00:50:34,480
所以我们可以得到一个非线性动力学模型

738
00:50:34,720 --> 00:50:37,730
ST+1作为ST和AT的随机函数

739
00:50:37,940 --> 00:50:42,670
现在  一旦你有一个模拟器

740
00:50:42,890 --> 00:50:48,750
任何固定的策略  你可以很直截的

741
00:50:48,930 --> 00:50:51,080
在模拟器中计算任何策略

742
00:50:51,300 --> 00:50:55,660
具体地说  我们的目标是

743
00:50:55,860 --> 00:50:57,610
找到各状态行动的策略PI映射

744
00:50:57,820 --> 00:51:01,520
因此我们的目标是要找到这样的绿色方块

745
00:51:01,740 --> 00:51:02,460
它工作得很好

746
00:51:02,630 --> 00:51:06,370
所以如果你有任何一个固定的策略PI

747
00:51:06,590 --> 00:51:10,760
你可以计算策略PI  只使用模拟器

748
00:51:10,960 --> 00:51:15,320
通过在幻灯片的底部显示的图片

749
00:51:15,570 --> 00:51:17,020
因此  具体地讲  你可以

750
00:51:17,170 --> 00:51:20,110
把你的初始状态S0  送入PI策略

751
00:51:20,320 --> 00:51:22,960
策略PI输出一些行动A0

752
00:51:23,160 --> 00:51:24,330
你嵌入模拟器

753
00:51:24,490 --> 00:51:26,920
模拟器输出一个随机的状态S1

754
00:51:27,150 --> 00:51:29,560
你给S1到策略中  等等

755
00:51:29,760 --> 00:51:33,100
你获得状态序列  S0到ST

756
00:51:33,270 --> 00:51:34,810
直升机在模拟器中飞行

757
00:51:35,010 --> 00:51:38,000
然后计算回报总和

758
00:51:38,160 --> 00:51:41,780
这给你一个策略的预期收益的估计

759
00:51:41,950 --> 00:51:44,110
这张照片仅仅是一个奇特的方式

760
00:51:44,260 --> 00:51:46,940
说你的直升机在模拟器中飞行

761
00:51:47,130 --> 00:51:48,500
看看它飞的有多好  并测量

762
00:51:48,680 --> 00:51:51,040
平均在模拟器上得到的回报的总和

763
00:51:51,250 --> 00:51:55,410
我在这里绘制的图片  假设

764
00:51:55,590 --> 00:51:57,940
你通过模拟器运行你的策略一次

765
00:51:58,130 --> 00:52:00,710
在一般情况下  你可以通过模拟器

766
00:52:00,930 --> 00:52:03,150
运行一定次数的策略  然后平均获得

767
00:52:03,350 --> 00:52:06,230
通过M模拟平均  以获得更好的

768
00:52:06,400 --> 00:52:13,090
策略的预期回报的估计 如果

769
00:52:13,270 --> 00:52:17,140
我有办法  给定的任何一个固定的策略

770
00:52:17,340 --> 00:52:18,860
这给了我一个方法

771
00:52:19,040 --> 00:52:22,300
以评估该策略的预期回报

772
00:52:22,500 --> 00:52:27,350
所以你可以尝试做一个相当明显的事情

773
00:52:27,540 --> 00:52:30,470
然后只是寻找一个策略  换句话说

774
00:52:30,680 --> 00:52:32,790
为你的策略寻找参数θ

775
00:52:32,970 --> 00:52:36,700
给你高的估计回报 明白吗?

776
00:52:36,880 --> 00:52:38,310
所以  我的策略有一些参数θ

777
00:52:38,500 --> 00:52:43,650
使我的策略是我的行动A等于θ转置S

778
00:52:43,870 --> 00:52:45,210
比如说  如果有一个线性的策略

779
00:52:45,380 --> 00:52:48,770
对于任何参数θ的固定值

780
00:52:48,990 --> 00:52:52,530
我可以估计  我可以得到 一个良好的估计

781
00:52:52,710 --> 00:52:53,650
这个策略使用模拟器有多好

782
00:52:54,940 --> 00:52:58,770
我可能会尝试做的一件事情是  搜索参数θ

783
00:52:58,980 --> 00:52:59,890
尝试使我的估计收益最大化

784
00:53:00,200 --> 00:53:04,950
事实证明  你可以做到这一点  但是

785
00:53:05,850 --> 00:53:09,550
正如我刚才说的想法是很难实现

786
00:53:09,730 --> 00:53:14,200
这里的原因是 模拟器使我们能够

787
00:53:14,390 --> 00:53:16,980
估计策略  让我们寻找高价值的策略

788
00:53:17,190 --> 00:53:19,860
困难的是  模拟器是随机的

789
00:53:20,080 --> 00:53:22,480
所以每次我们估计一个策略

790
00:53:22,660 --> 00:53:24,940
我们得到一个稍许不同的答案

791
00:53:25,200 --> 00:53:30,260
因此  在下面的图片中  我希望你能想象

792
00:53:30,430 --> 00:53:32,970
横轴是策略的空间

793
00:53:33,180 --> 00:53:37,800
换言之  正如我改变不相同的策略参数

794
00:53:38,030 --> 00:53:40,540
我在横轴上  得到不同的点

795
00:53:40,740 --> 00:53:42,080
如果我采用不同的参数θ

796
00:53:42,230 --> 00:53:43,110
我得到了不同的策略

797
00:53:43,290 --> 00:53:45,500
所以我沿着X轴移动

798
00:53:45,700 --> 00:53:49,470
并且我的总回报  我要在纵轴上描绘出来

799
00:53:49,680 --> 00:53:52,250
在这张图片中   红线

800
00:53:52,440 --> 00:53:54,350
是不同的策略的预期回报

801
00:53:54,540 --> 00:53:57,640
我的目标是要找到

802
00:53:57,800 --> 00:54:00,120
具有最高预期收益的策略

803
00:54:00,340 --> 00:54:04,090
你可以搜索预期收益高的策略

804
00:54:04,280 --> 00:54:06,410
但你每次估算策略 – 比如说

805
00:54:06,580 --> 00:54:07,750
我对一些策略进行估算

806
00:54:07,910 --> 00:54:09,810
然后我可能会得到一个点  只是

807
00:54:10,030 --> 00:54:12,660
一个偶然的机会看上去比它应该有更好一点

808
00:54:12,850 --> 00:54:14,570
如果我对第二个策略  只是一个偶然的机会

809
00:54:14,720 --> 00:54:16,410
它看上去有点更糟

810
00:54:16,560 --> 00:54:22,010
我评估第三个策略  第四  有时你看这里-


811
00:54:22,220 --> 00:54:24,320
有时我实际上 可能准确评估

812
00:54:24,570 --> 00:54:25,580
两次同样的策略

813
00:54:25,870 --> 00:54:28,280
而得到略有不同的答案  只是因为

814
00:54:28,540 --> 00:54:30,530
我的模拟器是随机的  所以当我

815
00:54:30,700 --> 00:54:33,440
在模拟器中应用两次同样的策略

816
00:54:33,590 --> 00:54:35,270
我可能会得到稍微不同的答案

817
00:54:35,480 --> 00:54:40,320
因此  当我计算越来越多的策略

818
00:54:40,460 --> 00:54:41,530
这些都是我得到的图片

819
00:54:41,710 --> 00:54:43,870
我的目标是尽量优化这个红线

820
00:54:44,090 --> 00:54:46,430
我希望你明白这是一个很难的问题

821
00:54:47,120 --> 00:54:49,150
尤其是当所有的[听不清]优化算法

822
00:54:49,440 --> 00:54:51,250
都能看到这些黑点  并且

823
00:54:51,500 --> 00:54:53,610
他们没有直接访问红线

824
00:54:53,800 --> 00:54:55,820
所以  当我的输入空间

825
00:54:56,000 --> 00:54:57,410
是一些相当高维空间

826
00:54:57,590 --> 00:54:58,860
如果我有10个参数

827
00:54:59,100 --> 00:55:01,700
横轴实际上是一个10 - D空间  我得到的红线是

828
00:55:01,920 --> 00:55:06,080
这些偏差估计

829
00:55:06,300 --> 00:55:08,720
这是一个非常困难的随机优化问题

830
00:55:08,950 --> 00:55:11,520
所以有一种方式

831
00:55:11,710 --> 00:55:13,260
使这个优化问题容易得多

832
00:55:13,430 --> 00:55:18,020
这里的想法和方法被称为Pegasus

833
00:55:18,200 --> 00:55:21,800
这是它的首字母缩写 稍后我会告诉你

834
00:55:21,990 --> 00:55:24,920
所以模拟器反复

835
00:55:25,120 --> 00:55:27,760
调用一个随机数生成器

836
00:55:27,960 --> 00:55:30,520
来生成随机数字RT

837
00:55:30,750 --> 00:55:32,730
这是用来模拟随机动力学

838
00:55:32,930 --> 00:55:35,330
我的意思是  模拟器

839
00:55:35,510 --> 00:55:37,320
用这种状态和行动的输入

840
00:55:37,490 --> 00:55:39,180
随机输出混合状态

841
00:55:39,350 --> 00:55:41,850
而如果你同步到模拟器

842
00:55:42,020 --> 00:55:43,820
如果你打开模拟器的那个盒子  并问我

843
00:55:44,000 --> 00:55:45,620
模拟器是如何产生

844
00:55:45,800 --> 00:55:48,010
这些随机混合态ST+1

845
00:55:48,180 --> 00:55:53,870
通常的唯一途径是这样做 – 几乎唯一的

846
00:55:54,010 --> 00:55:56,390
随机输出的方式来写一个模拟器是

847
00:55:56,570 --> 00:55:59,710
我们要去调用随机数发生器

848
00:55:59,910 --> 00:56:02,740
并获得随机数   这些RTS

849
00:56:02,980 --> 00:56:04,730
然后你有一些函数

850
00:56:04,870 --> 00:56:06,950
需要输入S0  A0

851
00:56:07,140 --> 00:56:11,670
和你的随机数发生器的结果  并计算

852
00:56:11,870 --> 00:56:12,870
一些混合状态

853
00:56:13,010 --> 00:56:14,620
作为函数的输入  并且

854
00:56:14,810 --> 00:56:16,430
从 随机数发生器获得随机数

855
00:56:16,580 --> 00:56:18,880
这几乎是唯一途径

856
00:56:19,040 --> 00:56:21,500
任何人实现任何随机码

857
00:56:21,660 --> 00:56:23,920
随机产生的输出的任何代码

858
00:56:24,070 --> 00:56:25,630
你调用一个随机数发生器

859
00:56:25,800 --> 00:56:28,280
并且你计算一些随机数

860
00:56:28,440 --> 00:56:29,510
和你的其他输入的函数

861
00:56:29,670 --> 00:56:34,540
当你估算不同的策略

862
00:56:34,730 --> 00:56:36,630
你得到不同的答案的原因是

863
00:56:36,820 --> 00:56:39,530
因为每次您重新运行模拟器

864
00:56:39,690 --> 00:56:41,530
你得到一个不同 随机数

865
00:56:41,740 --> 00:56:43,000
发生器 的随机数序列

866
00:56:43,190 --> 00:56:45,280
所以你每次得到了不同的答案

867
00:56:45,470 --> 00:56:47,180
即使你评估两次同样的策略

868
00:56:47,390 --> 00:56:51,190
因此  这里的想法 比方说

869
00:56:51,380 --> 00:56:54,130
我们提前固定随机数序列

870
00:56:54,330 --> 00:56:58,800
选择R1  R2到RT-1

871
00:56:58,990 --> 00:57:03,310
提前固定的随机数序列

872
00:57:03,520 --> 00:57:07,100
我们将始终使用相同的随机数序列

873
00:57:07,290 --> 00:57:08,840
来评估不同的策略

874
00:57:09,030 --> 00:57:14,190
进入你的代码  并固定R1  R2到RT-1

875
00:57:14,370 --> 00:57:16,850
他们随机选择一次  然后永远固定它们

876
00:57:17,030 --> 00:57:19,890
如果你总是使用相同的随机数序列

877
00:57:20,070 --> 00:57:22,620
然后系统不再是随机的

878
00:57:22,810 --> 00:57:24,970
如果对两次同样的策略进行评估

879
00:57:25,160 --> 00:57:27,120
你得到完全相同的答案 因此

880
00:57:27,320 --> 00:57:32,300
这些随机数序列  [听不清]

881
00:57:32,490 --> 00:57:34,020
叫它们为场景  并且Pegasus 是

882
00:57:34,160 --> 00:57:35,650
策略的梯度和搜索

883
00:57:35,950 --> 00:57:37,940
使用方案评价的缩写

884
00:57:38,180 --> 00:57:44,710
因此  当你这样做  这是你的图片

885
00:57:44,950 --> 00:57:48,190
像以前一样  红线是你的预期收益

886
00:57:48,390 --> 00:57:50,730
并通过固定的随机数  你已经定义了

887
00:57:50,950 --> 00:57:52,770
一些预期收益的估计

888
00:57:52,980 --> 00:57:57,140
而通过你评估不同的策略  他们仍然只是

889
00:57:57,350 --> 00:57:59,820
他们真正的预期收益的近似值

890
00:58:00,000 --> 00:58:01,060
但至少现在

891
00:58:01,210 --> 00:58:02,870
你有一个确定性优化的函数

892
00:58:03,040 --> 00:58:05,510
现在您可以应用你最喜爱的算法

893
00:58:05,700 --> 00:58:08,580
它梯度上升或一些地方

894
00:58:08,760 --> 00:58:13,860
[听不清]搜索尝试优化的黑色曲线

895
00:58:14,140 --> 00:58:16,970
这给你一个更简单的优化问题

896
00:58:17,180 --> 00:58:20,870
你可以优化它  希望找到一个好的策略

897
00:58:21,140 --> 00:58:25,110
因此  这就是Pegasus策略搜索方法

898
00:58:25,330 --> 00:58:29,920
所以当我开始谈论强化学习的时候

899
00:58:30,140 --> 00:58:33,560
我展示了一个直升机从上到下飞行的视频

900
00:58:33,760 --> 00:58:35,720
这实际上是使用完全相同的方法来完成的

901
00:58:35,950 --> 00:58:37,880
使用的正是这种策略的搜索算法

902
00:58:38,150 --> 00:58:43,330
这似乎处理得很好   甚至规模相当大的问题

903
00:58:43,520 --> 00:58:45,630
甚至相当高维状态空间

904
00:58:45,830 --> 00:58:49,230
典型的Pegasus策略搜索算法 –

905
00:58:49,500 --> 00:58:52,180
优化问题依然 – 


906
00:58:52,420 --> 00:58:55,290
比的随机版本要容易得多

907
00:58:55,510 --> 00:58:57,740
但有时它不是完全不重要的

908
00:58:57,950 --> 00:59:00,040
因此你需要对函数应用

909
00:59:00,270 --> 00:59:03,460
也许十个参数或几十个参数

910
00:59:03,730 --> 00:59:04,890
因此30个  40个参数

911
00:59:05,090 --> 00:59:06,870
但没有成千上万的参数

912
00:59:07,090 --> 00:59:10,200
至少在这些东西中

913
00:59:10,400 --> 00:59:12,680
学生:所以那是不同的方法

914
00:59:12,870 --> 00:59:15,430
相对于假设你确切地知道模拟器

915
00:59:15,660 --> 00:59:17,410
只要扔掉所有的随机数?

916
00:59:17,610 --> 00:59:22,250
教导员(安德鲁吴):因此   这是不是不同的呢?

917
00:59:22,450 --> 00:59:24,520
相对于假设 我们有一个确定的模拟器呢?

918
00:59:24,960 --> 00:59:29,570
答案是否定的 你这样做的方式

919
00:59:29,790 --> 00:59:33,020
为简便起见  我谈了

920
00:59:33,190 --> 00:59:34,810
一个随机数序列

921
00:59:35,060 --> 00:59:38,470
你要做的就是 - 想象随机数

922
00:59:38,650 --> 00:59:41,690
是模拟对你的直升机不同的阵风

923
00:59:41,910 --> 00:59:44,220
那么  你想要做的是  不是真的

924
00:59:44,460 --> 00:59:46,520
对一个阵风模型的评估

925
00:59:46,740 --> 00:59:47,760
你想要做的是

926
00:59:48,000 --> 00:59:51,290
采样一些不同模式的阵风

927
00:59:51,480 --> 00:59:53,320
平均对他们的估算

928
00:59:53,530 --> 00:59:55,720
所以你要做的就是你实际取样

929
00:59:55,940 --> 00:59:58,930
比如100 – 我用一些

930
00:59:59,140 --> 01:00:00,580
比如100的随机数序列

931
01:00:00,830 --> 01:00:03,210
每次你要评估一个策略

932
01:00:03,420 --> 01:00:06,720
对所有100个随机数序列

933
01:00:06,950 --> 01:00:08,300
进行评估  然后计算平均

934
01:00:08,500 --> 01:00:10,560
这是在完全一样的方式

935
01:00:10,720 --> 01:00:12,040
在此以前的照片

936
01:00:12,250 --> 01:00:15,740
你不一定只是一次评估策略

937
01:00:15,950 --> 01:00:17,950
你可能在模拟中评估100次

938
01:00:18,200 --> 01:00:18,900
然后平均

939
01:00:19,070 --> 01:00:20,730
得到预期的回报更好的估计

940
01:00:20,910 --> 01:00:23,520
同样的方式  你在这里处理

941
01:00:23,740 --> 01:00:26,740
但使用100的随机数固定序列

942
01:00:26,950 --> 01:00:29,880
明白吗?有任何其他的问题吗?

943
01:00:30,080 --> 01:00:34,540
学生:如果我们用100的情景

944
01:00:34,770 --> 01:00:36,910
并对策略进行估计

945
01:00:37,090 --> 01:00:41,570
[无声] 100次[无声]的随机数[无声]

946
01:00:41,770 --> 01:00:47,130
你不会得到类似的想法[无声]?

947
01:00:48,040 --> 01:00:48,860
教导员(安德鲁吴):是啊 我猜你说得对

948
01:00:49,250 --> 01:00:51,140
所以质量 – 对于一个固定的策略

949
01:00:51,410 --> 01:00:54,910
评估的质量同样是

950
01:00:55,170 --> 01:00:56,200
对这两种情况都很好

951
01:00:56,360 --> 01:00:59,290
固定随机数的好处是

952
01:00:59,480 --> 01:01:02,820
你最终均和使一个优化的问题更容易些

953
01:01:03,040 --> 01:01:07,870
我有一些搜索问题  并在横轴上

954
01:01:08,060 --> 01:01:09,490
有一个控制策略的空间

955
01:01:09,680 --> 01:01:12,150
我的目标是找到一种控制策略

956
01:01:12,400 --> 01:01:13,930
最大限度地得到回报

957
01:01:14,110 --> 01:01:17,320
这种早期设置的问题是

958
01:01:17,510 --> 01:01:21,240
当我评估的策略  我得到这些偏差的估计

959
01:01:21,480 --> 01:01:25,130
然后这很难优化这红色曲线

960
01:01:25,320 --> 01:01:26,700
如果我有这点  在所有的地方

961
01:01:26,880 --> 01:01:28,130
如果我两次评估相同的策略

962
01:01:28,350 --> 01:01:29,580
我甚至不会得到相同的答案

963
01:01:29,780 --> 01:01:31,910
通过固定的随机数

964
01:01:32,120 --> 01:01:35,870
算法仍然没有能看到红色的曲线

965
01:01:36,060 --> 01:01:38,880
但至少它现在是一个确定性函数优化

966
01:01:39,120 --> 01:01:41,440
这使得优化问题容易得多

967
01:01:41,650 --> 01:01:42,830
明白么?

968
01:01:43,020 --> 01:01:49,580
学生:所以每次你固定随机数

969
01:01:49,750 --> 01:01:51,200
你会得到一个很好的曲线进行优化

970
01:01:51,380 --> 01:01:52,810
然后你改变随机数

971
01:01:52,990 --> 01:01:54,270
得到一堆不同的曲线

972
01:01:54,540 --> 01:01:57,370
使得更容易优化 然后你把它们结合在一起?

973
01:01:57,630 --> 01:02:00,030
教导员(安德鲁吴):让我们来看看 我只有

974
01:02:00,210 --> 01:02:02,500
一个漂亮的黑色曲线  我想优化它

975
01:02:02,700 --> 01:02:03,890
学生:对于每一个场景

976
01:02:04,070 --> 01:02:04,820
教导员(安德鲁吴):我明白了

977
01:02:05,200 --> 01:02:06,550
所以我要去平均在M情景中

978
01:02:06,770 --> 01:02:09,570
所以我很要去平均超过100个场景

979
01:02:09,730 --> 01:02:10,570
因此  这里的黑色曲线是指

980
01:02:10,790 --> 01:02:13,430
在大量的场景设置下取平均

981
01:02:13,630 --> 01:02:17,140
明白吗?因此  而不是只有一个 –

982
01:02:17,340 --> 01:02:19,760
如果平均的事情不明白  可以想象

983
01:02:19,950 --> 01:02:21,660
这里有一个随机数序列

984
01:02:21,820 --> 01:02:22,820
这可能是更容易思考

985
01:02:22,990 --> 01:02:25,470
固定一个序列的随机数

986
01:02:25,630 --> 01:02:27,300
每次我评价的另一项策略

987
01:02:27,440 --> 01:02:30,300
我对相同的随机数序列进行评估

988
01:02:30,460 --> 01:02:32,980
这给了我一个很好的确定性函数进行优化

989
01:02:33,180 --> 01:02:40,470
有其他的问题吗?它的优点其实是 –

990
01:02:40,650 --> 01:02:42,470
思考的方式之一是

991
01:02:42,650 --> 01:02:44,700
当我评估两次同样的策略

992
01:02:44,900 --> 01:02:47,040
至少我得到了同样的答案

993
01:02:47,270 --> 01:02:49,700
这给了我一个确定性函数

994
01:02:49,930 --> 01:02:52,460
从我的策略中的参数

995
01:02:52,680 --> 01:02:55,550
映射到我的预期收益的估计

996
01:02:55,890 --> 01:02:58,870
这只是一个函数  我可以尝试使用

997
01:02:59,120 --> 01:03:00,480
搜索算法进行优化

998
01:03:00,710 --> 01:03:15,940
因此  我们使用这种算法反向重复计算

999
01:03:16,170 --> 01:03:19,150
再次策略搜索算法往往工作得很好

1000
01:03:19,350 --> 01:03:21,880
你可以找到从状态到行为的

1001
01:03:22,130 --> 01:03:24,470
一个相当简单的策略映射

1002
01:03:24,710 --> 01:03:29,000
这是特别是低级别的控制任务

1003
01:03:29,160 --> 01:03:31,590
我认为这几乎是条件反射之类的

1004
01:03:31,800 --> 01:03:35,750
完全的  如果解决一个如

1005
01:03:35,940 --> 01:03:38,190
"俄罗斯方块"(Tetris)的问题

1006
01:03:38,410 --> 01:03:40,770
你可能提前几步计划一个板的形状

1007
01:03:41,050 --> 01:03:44,280
像一盘棋  或长路径规划问题

1008
01:03:44,490 --> 01:03:45,930
关于走这里  然后去那里  然后去到那里

1009
01:03:46,350 --> 01:03:47,800
然后有时你可能

1010
01:03:47,980 --> 01:03:49,710
应用值函数的方法

1011
01:03:49,940 --> 01:03:52,660
但对于像直升机的飞行任务

1012
01:03:52,880 --> 01:03:56,470
低级别的控制任务

1013
01:03:56,660 --> 01:03:59,340
对于驾驶或控制各种机器人的条件反射

1014
01:03:59,540 --> 01:04:04,020
策略的搜索算法更容易 –

1015
01:04:04,240 --> 01:04:07,360
我们有时可以直接地

1016
01:04:07,600 --> 01:04:09,270
更容易地估计策略工作得很好

1017
01:04:09,500 --> 01:04:13,700
一些[听不清]今天的RL的状态

1018
01:04:13,910 --> 01:04:17,090
RL算法应用到各种各样的问题

1019
01:04:17,290 --> 01:04:20,900
关键是序贯决策的制定

1020
01:04:21,150 --> 01:04:22,960
你认为应用

1021
01:04:23,170 --> 01:04:24,410
强化学习算法的地方是

1022
01:04:24,620 --> 01:04:26,360
当你需要做出决定  然后作另外一个决定

1023
01:04:26,550 --> 01:04:27,140
然后再决定

1024
01:04:27,330 --> 01:04:29,420
和你的一些行动可能有长期的后果

1025
01:04:29,580 --> 01:04:33,070
我认为那是RL的顺序 决策的核心

1026
01:04:33,300 --> 01:04:35,690
在那里你做出多个决定

1027
01:04:35,900 --> 01:04:37,930
和你的一些行动

1028
01:04:38,110 --> 01:04:40,010
可能有长期的后果

1029
01:04:40,200 --> 01:04:42,600
我已经向你展示了一堆机器人的例子

1030
01:04:42,870 --> 01:04:47,190
RL是也适用于  比如医疗决策

1031
01:04:47,400 --> 01:04:48,570
你可能有一个病人

1032
01:04:48,770 --> 01:04:50,460
你要选择一个序列治疗

1033
01:04:50,730 --> 01:04:51,880
并为病人处理

1034
01:04:52,090 --> 01:04:53,710
然后患者可能会在其他一些状态

1035
01:04:53,880 --> 01:04:55,340
你再选择这样做  依此类推

1036
01:04:55,530 --> 01:04:58,490
原来有一大群的人运用

1037
01:04:58,680 --> 01:04:59,870
各种工具来处理这些队列

1038
01:05:00,030 --> 01:05:02,160
因此  假设你有一个银行

1039
01:05:02,520 --> 01:05:04,040
让大家排队

1040
01:05:04,210 --> 01:05:06,890
他们去到一个出纳后  他们中的一些人

1041
01:05:07,080 --> 01:05:09,100
要去找经理去处理别的事情

1042
01:05:10,340 --> 01:05:11,040
你有多人

1043
01:05:11,410 --> 01:05:13,300
站在多个队列系统

1044
01:05:13,500 --> 01:05:15,020
以及如何安排你的人

1045
01:05:15,210 --> 01:05:19,610
以尽量减少等待时间 不仅仅是人

1046
01:05:19,890 --> 01:05:21,930
在装配生产线中的对象等等

1047
01:05:22,110 --> 01:05:24,140
原来有一个大得出奇的社群的人

1048
01:05:24,350 --> 01:05:25,540
在处理优化队列

1049
01:05:25,760 --> 01:05:28,630
我提到一点玩游戏的知识

1050
01:05:28,840 --> 01:05:31,210
如财务决策制定的事情

1051
01:05:31,420 --> 01:05:33,010
如果你有大量的股票

1052
01:05:33,230 --> 01:05:35,330
怎么你抛售大量 -

1053
01:05:35,590 --> 01:05:38,210
你怎么卖你的股票

1054
01:05:38,440 --> 01:05:40,430
以不影响不利的市场价格太多?

1055
01:05:40,640 --> 01:05:43,530
有许多运筹学问题

1056
01:05:43,730 --> 01:05:44,810
如工厂自动化的东西

1057
01:05:45,000 --> 01:05:48,040
你能不能设计一个工厂  以优化生成量

1058
01:05:48,220 --> 01:05:49,040
或最大限度地降低成本  或其他的情况

1059
01:05:49,250 --> 01:05:51,750
这些都是人们应用

1060
01:05:51,940 --> 01:05:53,080
强化学习算法的各种问题

1061
01:05:53,300 --> 01:05:56,550
让我接近几个机器人例子

1062
01:05:56,760 --> 01:05:57,870
因为他们总是有趣的

1063
01:05:58,040 --> 01:05:58,840
并且我们只有这些视频

1064
01:05:59,100 --> 01:06:03,650
该视频是Ziko Coulter 和Peter Abiel的作品

1065
01:06:03,960 --> 01:06:05,300
他是一个博士研究生

1066
01:06:05,540 --> 01:06:09,710
他们的工作得到一个机器狗

1067
01:06:09,940 --> 01:06:13,230
翻越困难崎岖的地形

1068
01:06:13,440 --> 01:06:15,600
使用强化学习算法

1069
01:06:15,830 --> 01:06:20,230
他们采用的类似的

1070
01:06:20,430 --> 01:06:21,950
值函数逼近的方法

1071
01:06:22,150 --> 01:06:23,140
不是相同的  但类似的做法

1072
01:06:23,320 --> 01:06:25,270
他们允许机器狗

1073
01:06:25,530 --> 01:06:28,120
预先计划多个步骤

1074
01:06:28,310 --> 01:06:30,670
并谨慎地选择自己的脚步

1075
01:06:30,870 --> 01:06:33,240
穿越崎岖的地形

1076
01:06:33,450 --> 01:06:36,440
这是真正的艺术状态

1077
01:06:36,650 --> 01:06:40,820
你可以让一个机器狗这样做

1078
01:06:41,030 --> 01:06:46,110
这时另一种乐趣之一 事实证明

1079
01:06:46,380 --> 01:06:51,180
轮式机器人非常省油 汽车和卡车

1080
01:06:51,380 --> 01:06:53,010
都在世界上几乎最省油的机器人

1081
01:06:53,540 --> 01:06:56,920
汽车和卡车都非常省油

1082
01:06:57,100 --> 01:06:58,790
但更大的机器人有能力

1083
01:06:58,970 --> 01:07:00,490
穿过更崎岖的地形

1084
01:07:00,720 --> 01:07:02,030
因此  这是一个机器人

1085
01:07:02,240 --> 01:07:03,860
这其实是一个小型由洛克希德马丁公司

1086
01:07:04,020 --> 01:07:06,020
建立了一个较大的车辆的样机

1087
01:07:06,200 --> 01:07:09,240
但你能设计

1088
01:07:09,470 --> 01:07:12,970
有轮子的车辆和轮式机器人的燃油效率

1089
01:07:13,180 --> 01:07:16,540
但也有腿  所以它可以穿过障碍 同样

1090
01:07:16,860 --> 01:07:22,200
一个加强算法来设计这种机器人的控制器

1091
01:07:22,390 --> 01:07:26,210
它穿过障碍

1092
01:07:26,400 --> 01:07:28,260
有点复杂的步态

1093
01:07:28,470 --> 01:07:31,040
将很难用手工来设计

1094
01:07:31,280 --> 01:07:32,760
但选择一个奖励函数

1095
01:07:32,960 --> 01:07:35,320
告诉机器人这是一个+1奖励

1096
01:07:35,510 --> 01:07:37,600
这是目标最高点  和几个其他的东西

1097
01:07:37,810 --> 01:07:40,230
它会自动学习这些策略

1098
01:07:40,430 --> 01:07:47,770
最后几个有趣的 –

1099
01:07:48,030 --> 01:07:52,600
我会给你们展示最后几组直升机视频

1100
01:07:52,810 --> 01:07:59,700
这也是博士学位的学生在这里的作品

1101
01:07:59,930 --> 01:08:01,780
Peter Abiel和 Adam Coates

1102
01:08:02,010 --> 01:08:14,160
一直致力于自主飞行

1103
01:08:27,160 --> 01:08:28,140
我只让你看

1104
01:09:49,600 --> 01:09:50,160
我会再展示一次

1105
01:09:50,470 --> 01:09:52,900
学生:[听不清] 一个真正的直升机[听不清]?

1106
01:09:53,290 --> 01:09:54,530
不是一个完整大小的直升机

1107
01:09:54,710 --> 01:09:56,900
而是小的无线电控制直升机

1108
01:09:57,110 --> 01:10:00,120
学生:[听不清]

1109
01:10:00,440 --> 01:10:03,100
教导员(安德鲁吴):全尺寸直升机

1110
01:10:03,280 --> 01:10:03,930
这是错误的设计

1111
01:10:04,100 --> 01:10:05,230
你不应该做一个全尺寸的直升机

1112
01:10:05,390 --> 01:10:07,370
许多全尺寸的直升机可能会四分五裂

1113
01:10:07,580 --> 01:10:08,320
如果你试图这样做

1114
01:10:08,490 --> 01:10:13,060
让我们来看一次

1115
01:10:13,250 --> 01:10:19,270
学生:有没有人[无声]?

1116
01:10:19,430 --> 01:10:19,790
讲师(安德鲁吴):是的

1117
01:10:19,970 --> 01:10:21,090
有很好的人类飞行员

1118
01:10:21,280 --> 01:10:30,030
这仅仅是一个演习

1119
01:10:44,240 --> 01:10:44,400
这是一种乐趣 因此

1120
01:10:45,000 --> 01:10:47,440
这是Peter Abiel和Adam Coates的作品

1121
01:10:47,630 --> 01:10:52,850
所以这是它

1122
01:10:53,040 --> 01:10:55,220
这是我目前在想这个课程中展示的

1123
01:10:55,400 --> 01:10:57,410
所有的技术材料

1124
01:10:57,600 --> 01:11:05,960
我猜你现在都是机器学习专家

1125
01:11:06,150 --> 01:11:06,850
恭喜你

1126
01:11:07,030 --> 01:11:12,610
我希望你有意识通过这门课程

1127
01:11:12,820 --> 01:11:15,440
这是真的有一个科学的工程

1128
01:11:15,660 --> 01:11:18,490
和工业的巨大影响的技术之一

1129
01:11:18,690 --> 01:11:22,700
正如我在第一场演讲中说  我认为

1130
01:11:22,900 --> 01:11:24,950
许多人使用一天使用几十次

1131
01:11:25,140 --> 01:11:26,500
甚至不用知道机器学习算法

1132
01:11:26,670 --> 01:11:31,550
你所做的项目的基础上  我希望

1133
01:11:31,780 --> 01:11:36,260
你们其中大多数将能够

1134
01:11:36,490 --> 01:11:39,490
想象自己从这个教室走出去后

1135
01:11:39,680 --> 01:11:41,910
并运用这些东西来解决各种各样的问题

1136
01:11:42,120 --> 01:11:45,290
但愿  你也会想象自己

1137
01:11:45,500 --> 01:11:48,280
在课程之后写研究论文

1138
01:11:48,480 --> 01:11:50,780
作为做机器学习的一种新颖的方式

1139
01:11:50,990 --> 01:11:53,610
或在一些你所关心的一个问题上

1140
01:11:53,800 --> 01:11:56,290
应用机器学习的方法 事实上

1141
01:11:56,490 --> 01:11:58,220
看看项目里程碑  我确信

1142
01:11:58,430 --> 01:12:00,180
这课程项目中 的很大一部分

1143
01:12:00,360 --> 01:12:01,610
将出版   所以我认为这很棒

1144
01:12:01,790 --> 01:12:10,200
我想你们中许多人也将去社会  制作新产品

1145
01:12:10,370 --> 01:12:12,330
用这种算法并赚很多的钱

1146
01:12:12,520 --> 01:12:14,330
如果出现这种情况  记住我

1147
01:12:14,520 --> 01:12:17,940
我很兴奋的事情 之一

1148
01:12:18,140 --> 01:12:19,760
是通过研究或行业

1149
01:12:19,960 --> 01:12:21,590
我其实完全有把握这一类的人

1150
01:12:21,810 --> 01:12:23,590
在未来几个月

1151
01:12:23,810 --> 01:12:25,490
将使用机器学习算法

1152
01:12:25,690 --> 01:12:27,780
来解决大量的产业化经营中存在的问题

1153
01:12:27,980 --> 01:12:28,810
在计算机科学中

1154
01:12:29,040 --> 01:12:30,770
像计算机体系结构优化的问题

1155
01:12:31,070 --> 01:12:34,970
网络安全  机器人  计算机视觉

1156
01:12:35,170 --> 01:12:37,600
计算生物学

1157
01:12:37,800 --> 01:12:41,900
航空航天的问题  或理解自然语言

1158
01:12:42,080 --> 01:12:43,640
还有更多的这样的事情之中

1159
01:12:43,840 --> 01:12:49,320
所以现在我不知道你们将要去做什么

1160
01:12:49,470 --> 01:12:51,080
使用你们了解的学习算法

1161
01:12:51,260 --> 01:12:53,220
但每一次  当我结束这一课

1162
01:12:53,410 --> 01:12:56,920
我总是觉得很兴奋  乐观  希望

1163
01:12:57,110 --> 01:12:59,660
有关的各种令人惊奇的事情你就可以做

1164
01:12:59,810 --> 01:13:07,930
最后一件事  我就要发放这份讲义

1165
01:13:08,130 --> 01:13:31,890
最后一件事是  机械学习已经成长

1166
01:13:32,090 --> 01:13:37,290
一个人工智能较大的文学

1167
01:13:37,480 --> 01:13:39,890
其中 这种构建系统出现的智能行为

1168
01:13:40,110 --> 01:13:43,700
和机器学习的愿望是的AI工具之一

1169
01:13:43,890 --> 01:13:45,710
也许是一个  有非常非常大的影响

1170
01:13:45,910 --> 01:13:49,840
但有许多其他的想法  在人工智能中

1171
01:13:50,020 --> 01:13:52,990
我希望你去继续了解

1172
01:13:53,200 --> 01:13:54,880
幸运的是

1173
01:13:55,090 --> 01:13:58,380
斯坦福大学 的AI课程是最好的和最广泛的

1174
01:13:58,580 --> 01:14:00,950
我希望你采用这些课程中的一些优势

1175
01:14:01,190 --> 01:14:02,640
去学更多的AI

1176
01:14:02,840 --> 01:14:05,980
并通常适用于学习算法的问题视野

1177
01:14:06,160 --> 01:14:08,020
在机器人的自然语言处理的问题

1178
01:14:08,220 --> 01:14:10,220
等其他领域

1179
01:14:10,430 --> 01:14:12,440
所以我只给了

1180
01:14:12,650 --> 01:14:14,040
讲义AI相关课程列表

1181
01:14:14,260 --> 01:14:16,460
只是速度非常快  我想

1182
01:14:16,660 --> 01:14:18,880
CS221是我教的课程的一个概述

1183
01:14:19,090 --> 01:14:21,210
有大量的机器人课程:

1184
01:14:21,430 --> 01:14:26,530
223A  225A  225B - 很多机器人课程

1185
01:14:26,740 --> 01:14:28,520
学习算法的机器人

1186
01:14:28,700 --> 01:14:31,060
今天有这么多的应用

1187
01:14:31,280 --> 01:14:34,810
222和227是知识表示

1188
01:14:35,050 --> 01:14:35,870
和推理课程

1189
01:14:36,050 --> 01:14:38,580
228 - 在此列表中的所有课程中

1190
01:14:38,770 --> 01:14:41,000
228  Daphne Koller教的

1191
01:14:41,220 --> 01:14:42,800
大概是在精神上最接近229

1192
01:14:43,020 --> 01:14:45,550
这是我极力推荐

1193
01:14:45,740 --> 01:14:47,010
给我的博士生的课程之一

1194
01:14:47,210 --> 01:14:51,870
许多其他问题与机器学习有关

1195
01:14:52,050 --> 01:14:54,980
在接下来的页面  计算机视觉

1196
01:14:55,160 --> 01:14:57,110
语音识别  自然语言处理

1197
01:14:57,310 --> 01:15:00,900
不只是机器学习的各种工具

1198
01:15:01,050 --> 01:15:02,610
但课程往往涉及在许多方面的机器学习

1199
01:15:02,820 --> 01:15:04,490
其他方面的人工智能

1200
01:15:04,710 --> 01:15:07,940
多智能体系系统是由[听不清]教的

1201
01:15:08,180 --> 01:15:11,970
EE364A是凸优化

1202
01:15:12,160 --> 01:15:13,740
这是由Steve Boyd办班

1203
01:15:13,890 --> 01:15:16,100
和凸优化在这个课程上出现了很多次

1204
01:15:16,320 --> 01:15:18,040
如果你想真正学好它

1205
01:15:18,200 --> 01:15:19,810
EE364是一个伟大的课程

1206
01:15:20,020 --> 01:15:22,880
如果你对项目课程感兴趣

1207
01:15:23,040 --> 01:15:25,150
下一个季度我也教工程课程

1208
01:15:25,350 --> 01:15:26,860
我们用整个季度来

1209
01:15:27,060 --> 01:15:28,460
研究项目工作

1210
01:15:28,730 --> 01:15:34,200
所以  我希望你去  并选一些这些课程

1211
01:15:34,430 --> 01:15:44,620
最后  我只想说  教这门课程真的很好玩

1212
01:15:44,810 --> 01:15:47,770
我个人非常满意

1213
01:15:47,970 --> 01:15:50,680
当我们处理这些疯狂的困难点

1214
01:15:50,880 --> 01:15:53,000
然后我们会看到一个解决方案  我会想

1215
01:15:53,190 --> 01:15:54,610
"噢  天 实际上  他们想出了解决方案 "

1216
01:15:54,780 --> 01:15:57,210
当我看到这个  实际上是非常令人满意的

1217
01:15:57,400 --> 01:16:01,000
或寻找在里程碑  我经常去

1218
01:16:01,240 --> 01:16:02,150
"哇  这真的很酷

1219
01:16:02,330 --> 01:16:03,420
我敢打赌这是可以出版的 "

1220
01:16:03,600 --> 01:16:06,850
因此  我希望你把你学到的东西

1221
01:16:07,050 --> 01:16:10,660
向前进  并用学习算法  做出令人惊奇的事情

1222
01:16:10,850 --> 01:16:13,200
我知道这是一个繁重的工作

1223
01:16:13,410 --> 01:16:16,660
所以谢谢你们

1224
01:16:16,850 --> 01:16:17,660
在这个课程上的辛勤工作

1225
01:16:17,920 --> 01:16:19,590
和学习这种材料的辛勤工作

1226
01:16:19,790 --> 01:16:22,230
非常感谢你们

1227
01:16:22,430 --> 01:16:23,750
上了这门课程

1228
01:16:23,940 --> 01:16:25,200
[音频完]

1229
01:16:28,500 --> 01:16:28,670
时间:78分钟

