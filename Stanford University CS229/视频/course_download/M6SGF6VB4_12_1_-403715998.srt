1
00:00:24,130 --> 00:00:26,420
好了 早上好 欢迎回来

2
00:00:27,420 --> 00:00:28,630
今天我要

3
00:00:28,630 --> 00:00:31,280
开始新的一章

4
00:00:32,070 --> 00:00:35,880
开始讲无监督学习

5
00:00:35,880 --> 00:00:41,270
首先我要简要地讲一个

6
00:00:41,270 --> 00:00:47,100
被称为k-means的聚类算法

7
00:00:47,100 --> 00:00:50,340
之后我会讲混合高斯模型

8
00:00:50,340 --> 00:00:52,030
它是最大期望算法

9
00:00:52,030 --> 00:00:55,390
(EM算法)的

10
00:00:55,390 --> 00:00:56,270
一种特例

11
00:00:56,270 --> 00:00:59,860
我们会用它来描述Jensen不等式

12
00:00:59,860 --> 00:01:01,240
从而帮助我们

13
00:01:01,240 --> 00:01:02,680
导出EM算法的一般形式

14
00:01:03,160 --> 00:01:04,560
它是一种非常有用的算法

15
00:01:04,790 --> 00:01:06,160
我们在各种不同的

16
00:01:06,160 --> 00:01:07,530
无监督学习应用中

17
00:01:07,530 --> 00:01:10,200
都会用到它

18
00:01:11,600 --> 00:01:19,930
在讲监督学习的时候

19
00:01:19,930 --> 00:01:23,710
我经常会画这样的一张图

20
00:01:23,710 --> 00:01:27,590
你需要用logistic回归

21
00:01:27,590 --> 00:01:30,890
或者SVM将这些数据

22
00:01:30,890 --> 00:01:31,590
分成正负两类

23
00:01:32,230 --> 00:01:33,730
我们称之为监督学习

24
00:01:33,730 --> 00:01:35,770
是因为对于每一个训练样本

25
00:01:35,770 --> 00:01:37,800
你都给出了正确的类标签

26
00:01:39,150 --> 00:01:40,200
这就是监督

27
00:01:41,870 --> 00:01:43,240
在无监督学习中

28
00:01:43,240 --> 00:01:44,390
我们会研究一类不同的问题

29
00:01:47,390 --> 00:01:48,270
给定一个由若干点

30
00:01:54,880 --> 00:01:57,120
组成的数据集合

31
00:01:57,120 --> 00:01:59,730
所有的点都没有像监督学习那样

32
00:01:59,730 --> 00:02:02,670
给出类标签和所谓的正确答案

33
00:02:02,670 --> 00:02:06,470
我们需要靠算法本身

34
00:02:06,470 --> 00:02:09,040
来发现数据中的结构

35
00:02:10,790 --> 00:02:13,140
这节课以及未来几周

36
00:02:13,140 --> 00:02:14,400
我们会介绍几个

37
00:02:14,400 --> 00:02:15,920
不同的无监督学习算法

38
00:02:15,920 --> 00:02:18,560
可以在这样的数据中

39
00:02:18,560 --> 00:02:20,710
发现不同的结构

40
00:02:22,060 --> 00:02:23,840
我画的这张图里

41
00:02:23,840 --> 00:02:25,840
你可以发现的一个结构是

42
00:02:25,840 --> 00:02:29,320
这些数据被分成了两簇

43
00:02:29,320 --> 00:02:33,510
所以我要讲的

44
00:02:33,510 --> 00:02:34,980
第一个无监督学习算法

45
00:02:34,980 --> 00:02:36,240
将会是聚类算法

46
00:02:36,240 --> 00:02:38,820
这样的算法会将

47
00:02:38,820 --> 00:02:41,950
这样的数据

48
00:02:41,950 --> 00:02:44,660
聚集成几个不同的类

49
00:02:47,330 --> 00:02:50,160
让我们看看

50
00:02:51,320 --> 00:02:52,450
在我的笔记本准备好以后

51
00:02:52,450 --> 00:02:53,860
我会让你们看一个例子

52
00:02:53,860 --> 00:02:56,110
像这样的聚类算法

53
00:02:56,110 --> 00:02:59,450
有很多应用

54
00:02:59,450 --> 00:03:03,340
举几个最常用的

55
00:03:03,340 --> 00:03:04,980
在生物学应用中

56
00:03:04,980 --> 00:03:06,690
你经常需要对不同的东西进行聚类

57
00:03:06,690 --> 00:03:08,420
你有很多基因的数据

58
00:03:08,420 --> 00:03:09,750
你希望对它们进行聚类

59
00:03:09,750 --> 00:03:10,870
这样可以更好地理解

60
00:03:10,870 --> 00:03:12,800
不同种类的基因对应的生物功能

61
00:03:13,540 --> 00:03:16,740
另外一个常见的应用

62
00:03:16,740 --> 00:03:17,610
是市场调查

63
00:03:17,610 --> 00:03:20,790
假设你有一个数据库

64
00:03:20,790 --> 00:03:22,230
里面保存了不同顾客的行为

65
00:03:23,210 --> 00:03:25,850
经常需要做的一件事是

66
00:03:25,850 --> 00:03:27,750
对这些数据执行聚类算法

67
00:03:27,750 --> 00:03:29,680
将市场分为

68
00:03:29,680 --> 00:03:32,830
几个不同的部分

69
00:03:32,830 --> 00:03:33,850
从而你可以面对

70
00:03:33,850 --> 00:03:35,030
不同的部分

71
00:03:35,030 --> 00:03:36,840
制定相应的销售策略

72
00:03:38,180 --> 00:03:39,200
我们之后会讲到

73
00:03:39,200 --> 00:03:41,140
--我不想现在说了

74
00:03:41,140 --> 00:03:42,970
如果你上过

75
00:03:42,970 --> 00:03:44,780
news.google.com

76
00:03:46,920 --> 00:03:49,550
这个网站会利用聚类算法

77
00:03:49,550 --> 00:03:53,940
将每天的新闻

78
00:03:53,940 --> 00:03:56,540
按照相关性分组

79
00:03:56,540 --> 00:03:59,440
所以每一天你都可以看到

80
00:03:59,440 --> 00:04:01,840
上千篇讲述同样事情的新闻中的一篇

81
00:04:01,840 --> 00:04:03,760
或者看到来自500个网站的讲述

82
00:04:03,760 --> 00:04:06,320
另一件事的新闻

83
00:04:07,480 --> 00:04:09,360
另外一个应用是

84
00:04:09,360 --> 00:04:10,560
进行图片分割

85
00:04:10,560 --> 00:04:14,930
你可以将一副照片分成

86
00:04:14,930 --> 00:04:17,700
若干个一致的像素的子集

87
00:04:17,700 --> 00:04:21,640
尝试理解

88
00:04:21,640 --> 00:04:23,290
照片的内容

89
00:04:23,290 --> 00:04:26,670
这是另外一个聚类的应用

90
00:04:27,710 --> 00:04:30,360
基本的思想是

91
00:04:30,360 --> 00:04:31,360
给定这样一个数据集合

92
00:04:32,120 --> 00:04:35,480
你能否将这些数据

93
00:04:35,480 --> 00:04:39,030
聚集成若干一致的类

94
00:04:39,950 --> 00:04:42,950
让我看看

95
00:04:43,940 --> 00:04:45,510
我还在等我的笔记本

96
00:04:45,510 --> 00:04:46,630
我想给你们展示一个例子

97
00:04:47,860 --> 00:04:51,780
我可以先将

98
00:04:51,780 --> 00:04:53,580
这个聚类算法写出来

99
00:04:53,580 --> 00:04:55,010
之后再给你们展示动画

100
00:05:00,940 --> 00:05:03,140
这个算法称之为k-means算法

101
00:05:03,140 --> 00:05:05,450
用于寻找数据集合中的类

102
00:05:08,840 --> 00:05:11,150
算法的输入是

103
00:05:11,150 --> 00:05:12,630
一个无标记的数据集合

104
00:05:13,970 --> 00:05:20,590
我将其写作

105
00:05:20,590 --> 00:05:22,320
因为我们现在讲的

106
00:05:22,320 --> 00:05:23,510
是无监督学习

107
00:05:23,510 --> 00:05:25,500
所以在数据集合中

108
00:05:25,500 --> 00:05:26,270
你只能看到x

109
00:05:26,270 --> 00:05:27,700
而看不到类标记y

110
00:05:28,990 --> 00:05:30,550
k-means算法

111
00:05:30,550 --> 00:05:31,770
会这样做

112
00:05:32,400 --> 00:05:33,740
看到我的在电脑上展示的动画之后

113
00:05:33,740 --> 00:05:38,020
你们可能理解的更清楚

114
00:05:39,990 --> 00:05:43,180
首先初始化一组数据点

115
00:05:43,180 --> 00:05:44,450
我们称之为类重心

116
00:05:52,310 --> 00:05:55,470
随机地从数据点中选取

117
00:05:55,470 --> 00:05:59,230
如果你的数据集合

118
00:05:59,230 --> 00:06:01,470
都是n维向量

119
00:06:01,470 --> 00:06:04,870
那么这些类重心也是n维向量

120
00:06:06,280 --> 00:06:07,480
之后你需要重复这两步

121
00:06:11,100 --> 00:06:17,150
直到算法收敛

122
00:06:37,960 --> 00:06:40,920
这些类重心是你对于

123
00:06:40,920 --> 00:06:42,870
每个类的中心的猜测

124
00:06:42,870 --> 00:06:46,820
第一步 对于每个x^((i) ) 你需要查看

125
00:06:46,820 --> 00:06:50,480
哪个重心j离它最近

126
00:06:50,480 --> 00:06:53,390
这一步称之为

127
00:06:53,390 --> 00:06:56,820
你需要将x^((i) )分配给类j

128
00:06:57,760 --> 00:06:59,160
这一步你做的是

129
00:06:59,160 --> 00:07:01,000
对于所有的点

130
00:07:01,000 --> 00:07:02,160
选择离它最近的那个类重心并分配给它

131
00:07:03,330 --> 00:07:10,320
另一步要做的是将类重心

132
00:07:28,190 --> 00:07:30,600
更新为分配给

133
00:07:30,600 --> 00:07:31,550
该类的所有点的均值

134
00:07:46,580 --> 00:07:47,920
好的

135
00:07:47,920 --> 00:07:49,610
能把我的电脑放出来吗?

136
00:08:56,680 --> 00:08:57,510
我们开始

137
00:09:01,720 --> 00:09:03,440
这是一个k-means算法

138
00:09:03,440 --> 00:09:04,510
的演示

139
00:09:04,510 --> 00:09:07,540
希望你们看了之后能够理解更深刻

140
00:09:07,540 --> 00:09:09,420
下面一点看不到了

141
00:09:09,420 --> 00:09:10,980
这个例子是我从Berkley的

142
00:09:10,980 --> 00:09:13,750
Michael Jordan那里获得的

143
00:09:14,470 --> 00:09:17,980
这些绿色的点是我的数据点

144
00:09:17,980 --> 00:09:20,640
我随机地初始化

145
00:09:20,640 --> 00:09:21,750
两个类重心

146
00:09:21,750 --> 00:09:25,650
所以红叉和兰叉代表了

147
00:09:25,650 --> 00:09:27,830
μ_1 和μ_2的位置

148
00:09:27,830 --> 00:09:29,230
这是我假设的

149
00:09:29,230 --> 00:09:30,650
数据中的两个

150
00:09:33,830 --> 00:09:37,390
类 k-means算法的步骤是这样的

151
00:09:37,390 --> 00:09:40,170
我会重复地进行这样的步骤

152
00:09:40,170 --> 00:09:44,530
根据数据点离哪个重心更近

153
00:09:44,530 --> 00:09:47,090
将这些点联系起来

154
00:09:47,090 --> 00:09:50,940
出于视觉考虑

155
00:09:50,940 --> 00:09:54,820
基于们离哪个

156
00:09:54,820 --> 00:09:57,190
重心更近

157
00:09:57,880 --> 00:09:59,520
我会将数据点

158
00:09:59,520 --> 00:10:00,960
画成红色或蓝色

159
00:10:00,960 --> 00:10:02,100
所有离蓝叉更近的点会被画成蓝色反之亦然

160
00:10:03,030 --> 00:10:06,330
下一步是更新类重心

161
00:10:06,330 --> 00:10:09,800
我会计算

162
00:10:09,800 --> 00:10:10,930
所有蓝色点的平均值

163
00:10:11,500 --> 00:10:14,810
之后再计算

164
00:10:15,510 --> 00:10:18,120
所有红色点

165
00:10:18,120 --> 00:10:19,990
的平均值

166
00:10:19,990 --> 00:10:22,810
之后我用计算的平均值

167
00:10:22,810 --> 00:10:25,710
作为新的类重心

168
00:10:25,710 --> 00:10:29,000
这是经过一次迭代之后的结果

169
00:10:29,950 --> 00:10:31,430
现在我会重复相同的过程

170
00:10:31,430 --> 00:10:33,930
我会将离蓝叉近的点

171
00:10:33,930 --> 00:10:36,580
画成蓝色

172
00:10:36,580 --> 00:10:37,960
同理画出红色点

173
00:10:37,960 --> 00:10:41,070
之后我会

174
00:10:41,070 --> 00:10:45,820
计算出

175
00:10:45,820 --> 00:10:48,030
红色点和蓝色点的均值

176
00:10:48,030 --> 00:10:50,280
并再次更新类重心

177
00:10:50,280 --> 00:10:52,300
现在k-means算法

178
00:10:53,280 --> 00:10:56,450
已经收敛了

179
00:10:56,450 --> 00:10:58,920
即使你再

180
00:10:58,920 --> 00:11:01,000
进行更多次的迭代

181
00:11:01,000 --> 00:11:02,940
点的颜色和类重心

182
00:11:02,940 --> 00:11:04,140
都不会再改变了

183
00:11:04,140 --> 00:11:05,010
什么问题?

184
00:11:05,010 --> 00:11:06,570
(不可闻)

185
00:11:06,570 --> 00:11:07,430
我稍后会讲

186
00:11:07,430 --> 00:11:10,570
好的 请切回黑板

187
00:11:32,910 --> 00:11:35,540
花一会儿时间看看这个

188
00:11:35,540 --> 00:11:42,200
确保你明白

189
00:11:42,200 --> 00:11:43,520
我写的算法和演示动画

190
00:11:43,520 --> 00:11:44,290
是如何对应的

191
00:11:44,290 --> 00:11:45,070
有问题吗?

192
00:11:45,070 --> 00:11:48,400
(不可闻)

193
00:11:48,400 --> 00:11:50,560
让我稍后回答这个问题

194
00:11:52,010 --> 00:11:53,880
好的 这就是算法的两个步骤

195
00:11:53,880 --> 00:11:56,900
步骤2.1将点分配给

196
00:11:56,900 --> 00:11:58,120
离得最近的类重心

197
00:11:58,120 --> 00:12:01,660
2.2将类重心变为

198
00:12:01,660 --> 00:12:03,750
分配给它的

199
00:12:03,750 --> 00:12:04,600
所有点的均值

200
00:12:06,000 --> 00:12:09,900
接下来让我来回答你们问的两个问题

201
00:12:09,900 --> 00:12:11,500
第一个是 算法收敛吗?

202
00:12:12,270 --> 00:12:13,480
答案是肯定的

203
00:12:13,480 --> 00:12:14,660
k-means算法

204
00:12:14,660 --> 00:12:15,980
可以保证收敛

205
00:12:15,980 --> 00:12:31,060
实际上 你可以这样定义

206
00:12:31,710 --> 00:12:44,660
失真函数J(C  μ)

207
00:12:46,330 --> 00:12:48,710
你可以将

208
00:12:48,710 --> 00:12:50,830
失真函数定义

209
00:12:50,830 --> 00:12:53,690
为一个类分配和类重心的函数

210
00:12:53,690 --> 00:12:55,640
将它定义成所有点

211
00:12:55,640 --> 00:12:56,850
到它的重心的距离平方和

212
00:12:57,690 --> 00:13:00,270
之后你可以证明--我这里就不证了

213
00:13:00,790 --> 00:13:03,670
你可以证明 k-means算法

214
00:13:03,670 --> 00:13:15,000
实际上就是函数J的坐标上升过程

215
00:13:15,000 --> 00:13:17,650
你们应该还记得

216
00:13:17,650 --> 00:13:19,610
坐标上升是一个优化算法

217
00:13:19,610 --> 00:13:20,670
大概两周之前讲过

218
00:13:21,420 --> 00:13:23,960
我们会交替地固定C 相对于μ令函数最优化

219
00:13:23,960 --> 00:13:30,190
之后固定μ 相对于C令函数最优化

220
00:13:30,190 --> 00:13:31,570
这个过程被称为坐标上升

221
00:13:32,090 --> 00:13:33,910
你可以证明

222
00:13:33,910 --> 00:13:39,820
k-means的两个步骤

223
00:13:39,820 --> 00:13:43,430
实际上就是分别相对于

224
00:13:43,430 --> 00:13:45,570
C和μ令函数最优化

225
00:13:46,360 --> 00:13:52,270
因此 函数J(C  μ)

226
00:13:52,270 --> 00:13:54,400
一定是随

227
00:13:54,400 --> 00:13:56,220
迭代次数单调下降的

228
00:13:56,220 --> 00:13:59,120
这意味着k-means算法

229
00:13:59,120 --> 00:14:02,130
一定会收敛

230
00:14:02,130 --> 00:14:04,750
因为函数J的值只能减小

231
00:14:04,750 --> 00:14:06,710
J的值不再减小的时候

232
00:14:06,710 --> 00:14:08,090
就是算法收敛的时候

233
00:14:08,090 --> 00:14:12,060
实际上也许

234
00:14:12,060 --> 00:14:15,990
对于相同的J(C)的值

235
00:14:15,990 --> 00:14:19,150
可能得到多个聚类方式

236
00:14:19,150 --> 00:14:21,760
可能k-means算法在不同的类聚方式中交替出现

237
00:14:21,760 --> 00:14:24,700
它们在极端的情况下  这里可能有

238
00:14:24,700 --> 00:14:25,810
多个类聚方式

239
00:14:25,810 --> 00:14:27,340
它们对于这个目标函数给定相同的值

240
00:14:27,340 --> 00:14:29,410
K-means可能也会【无声】

241
00:14:30,450 --> 00:14:31,070
这种情况不可能出现

242
00:14:31,070 --> 00:14:34,570
即使出现这种情况

243
00:14:34,570 --> 00:14:37,400
函数J也会收敛

244
00:14:38,870 --> 00:14:40,240
另外一个问题是

245
00:14:40,240 --> 00:14:41,590
应该怎样选择类的数量?

246
00:14:41,590 --> 00:14:45,840
事实证明

247
00:14:45,840 --> 00:14:47,070
多数人在应用k-means算法时

248
00:14:47,070 --> 00:14:49,660
会随机地选择类的数量

249
00:14:49,660 --> 00:14:51,450
你可以随机

250
00:14:51,450 --> 00:14:52,630
地选取几个数

251
00:14:52,630 --> 00:14:57,160
并选择效果

252
00:14:57,160 --> 00:14:58,240
最好的那个

253
00:14:59,180 --> 00:15:00,730
类的数量对于算法来说

254
00:15:00,730 --> 00:15:01,750
只是一个参数

255
00:15:01,750 --> 00:15:02,680
所以我认为自动选择

256
00:15:02,680 --> 00:15:04,500
并不会非常困难

257
00:15:05,360 --> 00:15:07,200
有很多

258
00:15:07,200 --> 00:15:08,750
自动选择类数目的方法

259
00:15:08,750 --> 00:15:10,970
我不打算讲了

260
00:15:10,970 --> 00:15:12,200
在我用这个算法的时候

261
00:15:12,200 --> 00:15:13,200
我通常会随机

262
00:15:13,200 --> 00:15:14,500
地选择一个类的数量

263
00:15:14,500 --> 00:15:15,710
我这样想的原因是因为

264
00:15:17,110 --> 00:15:20,630
对于许多聚类问题

265
00:15:20,630 --> 00:15:22,740
"真正的"类数量是非常模糊的一个概念

266
00:15:22,740 --> 00:15:23,640
例如

267
00:15:23,640 --> 00:15:26,020
如果你的数据集合看起来是这样的

268
00:15:30,330 --> 00:15:34,390
你们有些人可能会看到4个类

269
00:15:34,390 --> 00:15:35,950
但是有些人会看到2个类

270
00:15:35,950 --> 00:15:38,610
所以类数量这个概念

271
00:15:38,610 --> 00:15:40,280
是非常模糊的

272
00:15:41,620 --> 00:15:43,640
什么问题?

273
00:15:46,130 --> 00:15:52,070
如果你用离得很远的点初始化类重心

274
00:15:52,070 --> 00:15:52,920
最后也会得到

275
00:15:52,920 --> 00:15:59,510
相同的聚类结果吗?

276
00:16:01,240 --> 00:16:01,760
让我看看

277
00:16:01,760 --> 00:16:04,890
k-means算法可能会

278
00:16:04,890 --> 00:16:08,780
得到局部最优值

279
00:16:08,780 --> 00:16:12,770
函数J并不是一个凸函数

280
00:16:12,770 --> 00:16:16,530
所以k-means算法

281
00:16:16,530 --> 00:16:18,830
并不保证收敛

282
00:16:18,830 --> 00:16:20,300
到全局最优值

283
00:16:20,300 --> 00:16:21,910
所以k-means算法可能

284
00:16:24,550 --> 00:16:31,650
会得到局部最优值

285
00:16:31,650 --> 00:16:32,860
如果在你的应用中k-means算法

286
00:16:32,860 --> 00:16:35,020
可能会得到局部最优值

287
00:16:35,020 --> 00:16:37,670
那么你可以做的是

288
00:16:37,670 --> 00:16:39,560
尝试多种随机的初始化方式

289
00:16:39,560 --> 00:16:40,390
进行多次聚类

290
00:16:40,390 --> 00:16:42,100
从中选取失真函数最小的初始化方式

291
00:16:42,100 --> 00:16:44,320
什么问题?

292
00:16:44,320 --> 00:16:52,170
(不可闻)

293
00:16:52,170 --> 00:16:57,230
让我看看

294
00:16:57,230 --> 00:16:59,230
如果某个类重心没有

295
00:16:59,230 --> 00:17:00,740
分配到任何点应该怎么办

296
00:17:00,740 --> 00:17:02,050
你可以

297
00:17:02,050 --> 00:17:03,930
直接去掉它

298
00:17:03,930 --> 00:17:07,090
或者如果你一定需要k个类的话

299
00:17:07,090 --> 00:17:09,760
你可以尝试重新进行初始化

300
00:17:12,180 --> 00:17:13,020
什么问题?

301
00:17:13,020 --> 00:17:18,000
(不可闻)

302
00:17:19,290 --> 00:17:20,320
让我看看

303
00:17:20,320 --> 00:17:22,540
这里是二次函数

304
00:17:24,590 --> 00:17:28,060
对于绝大多数的

305
00:17:28,060 --> 00:17:30,330
k-means的应用

306
00:17:30,330 --> 00:17:31,520
都可以使用二次模

307
00:17:32,340 --> 00:17:33,980
虽然也可以使用无限次的模

308
00:17:33,980 --> 00:17:35,890
或者一次的模

309
00:17:35,890 --> 00:17:38,010
但是我看到的并不多

310
00:17:38,010 --> 00:17:42,010
尽管这里可以

311
00:17:42,010 --> 00:17:43,640
用不同的次数

312
00:17:43,640 --> 00:17:45,420
但是我这里写的

313
00:17:45,420 --> 00:17:46,460
是最为常见的

314
00:17:50,250 --> 00:17:59,300
好的 这就是k-means聚类算法

315
00:18:05,620 --> 00:18:08,440
我接下来

316
00:18:08,440 --> 00:18:09,770
要做的是

317
00:18:09,770 --> 00:18:11,130
讲一个高度相关的问题

318
00:18:18,080 --> 00:18:20,290
我要讲的问题

319
00:18:20,290 --> 00:18:21,680
称为密度估计

320
00:18:31,290 --> 00:18:33,750
这个例子是我

321
00:18:33,750 --> 00:18:35,150
从某些人的工作中想到的

322
00:18:36,190 --> 00:18:37,690
假设你需要检测

323
00:18:37,690 --> 00:18:38,880
从组装线上生产出的飞机引擎

324
00:18:38,880 --> 00:18:41,400
假设你为某个航空公司工作

325
00:18:41,400 --> 00:18:42,850
你希望检测从组装线上

326
00:18:42,850 --> 00:18:47,060
生产出的飞机引擎

327
00:18:47,060 --> 00:18:49,150
你会检测它们的各种属性

328
00:18:49,150 --> 00:18:50,990
出于简化

329
00:18:50,990 --> 00:18:54,370
我只考虑

330
00:18:54,370 --> 00:18:56,080
两个属性:

331
00:18:56,080 --> 00:18:58,070
发热和振动

332
00:18:58,730 --> 00:19:00,360
实际上 你会检测振动的频率

333
00:19:00,360 --> 00:19:01,800
诸如此类的属性

334
00:19:01,800 --> 00:19:03,220
我们只考虑

335
00:19:03,220 --> 00:19:04,910
热量和振动这两个属性

336
00:19:07,060 --> 00:19:27,610
如果你得到这样的一个数据集合

337
00:19:27,610 --> 00:19:30,810
你就可以执行聚类算法

338
00:19:30,810 --> 00:19:33,110
将数据聚成两类

339
00:19:33,110 --> 00:19:35,640
也有可能你的数据是这样的

340
00:19:35,640 --> 00:19:36,760
这时你希望估计出这些数据的概率密度即:

341
00:19:36,760 --> 00:19:37,830
热量和振动的joint概率分布

342
00:19:38,780 --> 00:19:42,640
因为你希望检测出异常的样本

343
00:19:42,640 --> 00:19:44,930
当一个新的飞机引擎

344
00:19:44,930 --> 00:19:46,920
从组装线上生产出来时

345
00:19:46,920 --> 00:19:49,110
你照例测量它的热量和振动属性

346
00:19:50,540 --> 00:19:51,590
如果你得到这样的一个点

347
00:19:51,590 --> 00:19:53,270
你可能会问

348
00:19:53,270 --> 00:19:55,180
"这个引擎是不是

349
00:19:55,180 --> 00:19:57,270
有没有检测出的错误?

350
00:19:57,270 --> 00:20:00,330
它是不是需要进行进一步的检查?"

351
00:20:00,860 --> 00:20:02,650
如果我们考虑

352
00:20:02,650 --> 00:20:06,050
这些特征的概率分布

353
00:20:07,690 --> 00:20:09,500
并建立P(x)的模型

354
00:20:11,160 --> 00:20:14,120
如果我们发现对于一个新的引擎

355
00:20:14,120 --> 00:20:15,700
P(x)非常小

356
00:20:15,700 --> 00:20:17,900
我们可能会亮起红灯

357
00:20:17,900 --> 00:20:20,010
认为这是个异常的产品

358
00:20:20,010 --> 00:20:22,870
所以它需要在正式使用之前

359
00:20:22,870 --> 00:20:24,270
进行进一步的检查

360
00:20:25,680 --> 00:20:31,350
我讲的这个问题是

361
00:20:31,350 --> 00:20:33,280
一个异常检测的例子

362
00:20:33,280 --> 00:20:35,250
进行异常检测

363
00:20:35,250 --> 00:20:36,960
的一般方法是

364
00:20:36,960 --> 00:20:38,270
利用这组数据集合

365
00:20:43,610 --> 00:20:48,060
建立一个常规数据

366
00:20:48,060 --> 00:20:51,980
的概率分布模型P(x)

367
00:20:51,980 --> 00:20:54,210
如果对于一个新样本

368
00:20:54,210 --> 00:20:57,090
x P(x)的值很小

369
00:20:57,090 --> 00:20:59,980
那么你可以会报告发现了一个异常样本

370
00:20:59,980 --> 00:21:03,960
异常检测通常

371
00:21:03,960 --> 00:21:06,040
被应用在安全领域

372
00:21:16,590 --> 00:21:20,020
如果我的信用卡上

373
00:21:20,020 --> 00:21:21,110
出现了很多异常交易行为

374
00:21:21,110 --> 00:21:22,410
那么也许有人

375
00:21:22,410 --> 00:21:23,060
盗用了我的信用卡

376
00:21:24,510 --> 00:21:27,690
我要介绍的

377
00:21:27,690 --> 00:21:29,530
就是一些

378
00:21:29,530 --> 00:21:31,620
进行密度估计

379
00:21:31,620 --> 00:21:33,560
的算法

380
00:21:33,560 --> 00:21:34,510
你知道

381
00:21:34,510 --> 00:21:40,790
这样的概率分布并不属于

382
00:21:40,790 --> 00:21:42,890
任何标准教科书上介绍的概率分布

383
00:21:42,890 --> 00:21:44,480
它不属于高斯分布

384
00:21:44,480 --> 00:21:46,300
或泊松分布

385
00:21:46,300 --> 00:21:49,310
所以我们能否对这样

386
00:21:49,310 --> 00:21:51,070
的一个奇怪的区域

387
00:21:51,070 --> 00:21:52,300
估计出其概率分布?

388
00:21:53,410 --> 00:22:02,840
为了描述我们的算法

389
00:22:02,840 --> 00:22:04,560
我要举一个一维的例子

390
00:22:04,560 --> 00:22:05,880
而不是二维的例子

391
00:22:06,820 --> 00:22:11,340
这个例子中

392
00:22:11,340 --> 00:22:21,750
我要用到这样的

393
00:22:21,750 --> 00:22:22,630
一个数据集合

394
00:22:23,760 --> 00:22:26,740
水平轴是x轴

395
00:22:26,740 --> 00:22:28,680
这些点表示了

396
00:22:28,680 --> 00:22:31,390
我的数据集合

397
00:22:31,390 --> 00:22:32,240
好的

398
00:22:32,240 --> 00:22:33,720
数据集合看起来是这样的

399
00:22:33,720 --> 00:22:35,070
似乎是两个

400
00:22:38,280 --> 00:22:40,930
高斯分布合在一起

401
00:22:40,930 --> 00:22:43,220
所以我要描述

402
00:22:43,220 --> 00:22:48,310
的这个模型

403
00:22:48,310 --> 00:22:49,620
称为混合高斯模型

404
00:22:50,660 --> 00:22:55,150
为了让你们看的更清楚

405
00:22:55,150 --> 00:22:59,840
这里我将产生数据

406
00:22:59,840 --> 00:23:11,520
的两个高斯分布画出来

407
00:23:11,520 --> 00:23:13,530
如果我知道两个高斯分布

408
00:23:13,530 --> 00:23:16,010
我就可以让一个分布生成"X"

409
00:23:16,010 --> 00:23:17,390
一个分布生成"O"

410
00:23:17,390 --> 00:23:21,420
最终的概率分布是

411
00:23:21,420 --> 00:23:22,610
两个概率分布之和

412
00:23:23,590 --> 00:23:26,150
但是问题是

413
00:23:26,150 --> 00:23:29,700
我不知道数据的标签

414
00:23:29,700 --> 00:23:31,090
我不知道生成数据的

415
00:23:31,090 --> 00:23:33,010
两个高斯分布是怎样的

416
00:23:33,010 --> 00:23:36,910
我要做的是用一个算法

417
00:23:36,910 --> 00:23:40,380
拟合出这两个混合在一起的高斯分布

418
00:23:40,380 --> 00:23:43,480
尽管我并不知道哪些数据

419
00:23:43,480 --> 00:23:45,990
是由哪个分布生成的

420
00:23:54,450 --> 00:23:56,060
好的 我们是这样做的

421
00:24:02,060 --> 00:24:03,260
在这个模型中

422
00:24:03,260 --> 00:24:04,370
我想像着

423
00:24:09,930 --> 00:24:12,100
有一个隐含的随机变量

424
00:24:19,970 --> 00:24:22,730
"隐含"意味着

425
00:24:22,730 --> 00:24:24,800
"隐藏" "未被发现"

426
00:24:35,040 --> 00:24:36,510
所以我么假设

427
00:24:36,510 --> 00:24:39,130
有一个隐含的随机变量z

428
00:24:39,130 --> 00:24:45,530
这样x^((i) ) 与z^((i) )

429
00:24:53,710 --> 00:24:55,070
之间有一个joint概率

430
00:24:57,180 --> 00:25:03,240
所以P(x^((i) )  z^((i) )) 根据链式规则

431
00:25:03,240 --> 00:25:05,910
应该等于这个式子

432
00:25:05,910 --> 00:25:07,460
这个公式永远成立

433
00:25:10,860 --> 00:25:14,430
我们假设z^((i) )服从

434
00:25:14,430 --> 00:25:16,260
参数为φ的

435
00:25:16,260 --> 00:25:21,820
伯努利分布

436
00:25:32,360 --> 00:25:34,140
在假设有两个高斯分布的特例下

437
00:25:34,140 --> 00:25:36,160
z^((i) )应该服从伯努利分布

438
00:25:46,690 --> 00:25:48,510
这些参数表示

439
00:25:48,510 --> 00:25:50,580
多项式分布的参数

440
00:25:51,290 --> 00:25:56,800
那么x^((i) ) |z^((i) )=j

441
00:25:56,800 --> 00:26:02,260
应该服从这样的高斯分布

442
00:26:02,880 --> 00:26:04,890
均值是μ_j

443
00:26:06,930 --> 00:26:11,970
协方差是Σ_j

444
00:26:13,100 --> 00:26:17,220
这个式子

445
00:26:17,220 --> 00:26:18,640
看起来非常熟悉

446
00:26:18,980 --> 00:26:23,870
我在讲高斯判别

447
00:26:23,870 --> 00:26:25,480
分析算法时

448
00:26:25,480 --> 00:26:28,600
写的也差不多

449
00:26:28,600 --> 00:26:29,910
就是这个公式

450
00:26:29,910 --> 00:26:33,680
除了一点

451
00:26:33,680 --> 00:26:36,220
之前我讲的是监督学习算法

452
00:26:36,220 --> 00:26:39,070
用到了类标记y

453
00:26:40,000 --> 00:26:42,960
但是这里我将y

454
00:26:42,960 --> 00:26:45,610
换成了隐含随机变量

455
00:26:45,610 --> 00:26:47,320
未被观察到的随机变量z

456
00:26:47,320 --> 00:26:48,350
实际上我们

457
00:26:48,350 --> 00:26:49,450
并不知道它的值

458
00:26:50,680 --> 00:27:03,410
为了让它与高斯判别分析的联系

459
00:27:03,410 --> 00:27:05,710
更为明显

460
00:27:06,890 --> 00:27:08,560
如果我们知道z的话

461
00:27:13,400 --> 00:27:15,100
实际上我们并不知道

462
00:27:15,100 --> 00:27:16,860
假设我们

463
00:27:16,860 --> 00:27:18,900
知道z

464
00:27:18,900 --> 00:27:20,460
即我们知道每个数据点

465
00:27:20,460 --> 00:27:22,190
来自于哪个高斯分布

466
00:27:23,470 --> 00:27:26,750
那么你可以利用极大似然估计


467
00:27:29,590 --> 00:27:31,890
你可以将参数的

468
00:27:31,890 --> 00:27:33,880
似然性写出来

469
00:27:43,330 --> 00:27:46,430
之后利用极大似然估计

470
00:27:48,210 --> 00:27:50,170
你可以得到

471
00:27:50,170 --> 00:27:52,060
和高斯判别分析相同的公式

472
00:28:24,360 --> 00:28:26,650
如果你知道z的值

473
00:28:26,650 --> 00:28:28,900
你可以用

474
00:28:30,630 --> 00:28:32,250
极大似然估计

475
00:28:33,020 --> 00:28:34,390
估计出

476
00:28:34,390 --> 00:28:35,560
模型的所有参数

477
00:28:38,230 --> 00:28:39,030
明白了吗?

478
00:28:39,030 --> 00:28:40,450
明白的话请举手

479
00:28:42,780 --> 00:28:45,170
很好 还有问题吗?

480
00:28:45,170 --> 00:28:46,140
有问题请举手

481
00:28:46,140 --> 00:28:46,730
什么问题?

482
00:28:46,730 --> 00:28:50,150
z^((i) )是标记吗?表示"X"和"O"?

483
00:28:50,150 --> 00:28:51,670
基本上是这个意思

484
00:28:52,320 --> 00:28:54,420
还有问题吗?

485
00:29:05,080 --> 00:29:15,000
好的 如果你知道z的值

486
00:29:15,000 --> 00:29:17,230
z的角色和高斯判别分析中

487
00:29:17,230 --> 00:29:18,730
的类标记是一样的

488
00:29:18,730 --> 00:29:21,430
之后你可以用极大似然估计

489
00:29:21,430 --> 00:29:22,660
估计出参数的值

490
00:29:23,620 --> 00:29:25,170
但是实际上

491
00:29:25,170 --> 00:29:27,110
我们并不知道z的值

492
00:29:27,110 --> 00:29:27,900
我们知道的只有一个

493
00:29:27,900 --> 00:29:28,930
没有任何标记的数据集合

494
00:29:28,930 --> 00:29:32,320
所以我们需要

495
00:29:32,320 --> 00:29:34,160
用到一个自举过程

496
00:29:35,480 --> 00:29:36,480
思想是:

497
00:29:36,480 --> 00:29:40,780
我们先用模型

498
00:29:40,780 --> 00:29:43,040
尝试猜测z的值

499
00:29:43,040 --> 00:29:44,320
我们不知道z的值

500
00:29:44,320 --> 00:29:46,550
但是我们会尝试猜测z的值

501
00:29:46,550 --> 00:29:50,770
之后我们会用猜测的z的值

502
00:29:50,770 --> 00:29:52,950
拟合出参数的值 之后

503
00:29:52,950 --> 00:29:55,490
我们进行迭代

504
00:29:55,490 --> 00:29:57,270
所以可以得到一个

505
00:29:57,270 --> 00:29:59,140
对参数值的更好的估计

506
00:29:59,140 --> 00:30:00,750
我们之后再去猜测z的值

507
00:30:00,750 --> 00:30:02,630
之后我们用

508
00:30:02,630 --> 00:30:03,590
极大似然估计

509
00:30:03,590 --> 00:30:07,580
求出参数的值

510
00:30:31,030 --> 00:30:33,000
我接下来要写的这个算法

511
00:30:33,000 --> 00:30:35,430
称为最大期望算法

512
00:30:38,300 --> 00:30:40,260
它的过程是这样的

513
00:30:42,580 --> 00:30:43,700
重复下列步骤直至算法收敛

514
00:30:48,650 --> 00:30:49,520
这一步称为E-step

515
00:30:49,520 --> 00:30:59,270
我们会猜测未知的z^((i) )的值

516
00:30:59,270 --> 00:31:20,760
我会计算w_j^((i) )

517
00:31:20,760 --> 00:31:22,230
我会计算

518
00:31:22,230 --> 00:31:24,520
z^((i) )=j的概率

519
00:31:24,520 --> 00:31:26,200
我会用到

520
00:31:26,200 --> 00:31:27,420
模型的参数

521
00:31:27,420 --> 00:31:29,160
并且计算出x^((i) )

522
00:31:29,160 --> 00:31:35,430
由第j个高斯分布生成的概率

523
00:31:37,600 --> 00:31:39,600
具体地

524
00:31:39,600 --> 00:31:41,750
我要去计算

525
00:31:41,750 --> 00:32:03,070
这一步是由

526
00:32:17,420 --> 00:32:19,070
贝叶斯公式得到的

527
00:32:19,070 --> 00:32:21,270
具体的

528
00:32:21,270 --> 00:32:24,310
分子的这个部分

529
00:32:24,310 --> 00:32:26,950
是高斯分布 对吗?

530
00:32:27,090 --> 00:33:00,160
它应该等于这个式子

531
00:33:00,160 --> 00:33:07,690
它等于φ_j 再除以相同的东西

532
00:33:07,690 --> 00:33:11,080
将j替换成l

533
00:33:11,080 --> 00:33:14,230
将分子替换成高斯分布

534
00:33:14,230 --> 00:33:16,030
分母也类似

535
00:33:17,680 --> 00:33:25,070
对不起 这里应该是l

536
00:33:25,070 --> 00:33:26,600
这是算法的前半部分

537
00:33:46,220 --> 00:33:59,680
剩下的部分称为M-step

538
00:33:59,680 --> 00:34:01,530
这里你需要更新

539
00:34:01,530 --> 00:34:03,380
对参数的估计

540
00:34:03,380 --> 00:34:05,340
我把公式写下来

541
00:34:06,580 --> 00:34:08,980
你需要将它们

542
00:34:08,980 --> 00:34:12,220
与极大似然估计的结果

543
00:34:12,220 --> 00:34:13,680
进行比较

544
00:35:03,490 --> 00:35:11,210
上面的这两个公式

545
00:35:11,210 --> 00:35:13,790
和之前高斯判别分析的结果非常相似

546
00:35:13,790 --> 00:35:17,360
除了这里用到的是w_i^((j) )

547
00:35:17,360 --> 00:35:21,480
它表示

548
00:35:21,480 --> 00:35:25,570
我们刚刚计算的第i个点

549
00:35:27,330 --> 00:35:29,010
由第j个

550
00:35:29,010 --> 00:35:33,280
高斯分布生成的概率

551
00:35:34,320 --> 00:35:35,940
而不是第i个点由第j个高斯分布

552
00:35:35,940 --> 00:35:37,370
生成的指示函数

553
00:35:39,000 --> 00:35:42,030
另外一点

554
00:35:42,030 --> 00:35:46,550
和高斯判别分析不同

555
00:35:46,550 --> 00:35:47,890
的地方是

556
00:35:47,890 --> 00:35:49,390
对于混合高斯模型

557
00:35:49,390 --> 00:35:52,800
我们对不同的高斯分布

558
00:35:52,800 --> 00:35:54,310
使用了不同的协方差矩阵

559
00:35:54,310 --> 00:35:56,710
在高斯判别分析中

560
00:35:56,710 --> 00:35:57,980
出于习惯

561
00:35:57,980 --> 00:36:01,180
我们通常会用同样的协方差矩阵

562
00:36:01,180 --> 00:36:03,590
对所有的类进行建模

563
00:36:03,730 --> 00:36:05,710
但是有些时候

564
00:36:05,800 --> 00:36:09,230
你也可以

565
00:36:09,320 --> 00:36:11,890
对不同的高斯分布使用不同的协方差矩阵

566
00:36:11,990 --> 00:36:13,730
这里我们对不同的高斯分布

567
00:36:13,830 --> 00:36:16,350
使用了不同的协方差矩阵Σ_j

568
00:36:18,310 --> 00:36:22,280
所以它和高斯判别分析是对应的

569
00:36:23,600 --> 00:36:26,330
为什么不再看看

570
00:36:26,330 --> 00:36:28,240
确保你们都明白了呢?

571
00:36:49,970 --> 00:36:51,640
有问题吗?

572
00:36:59,830 --> 00:37:02,230
如果明白的话请举手

573
00:37:02,230 --> 00:37:08,510
只有一部分人

574
00:37:10,830 --> 00:37:12,810
让我再

575
00:37:14,750 --> 00:37:16,580
解释一下

576
00:37:16,580 --> 00:37:18,420
回忆一下

577
00:37:18,420 --> 00:37:20,440
在高斯判别分析(GDA)中

578
00:37:22,530 --> 00:37:32,070
我们知道z^((i) )的值

579
00:37:33,470 --> 00:37:35,790
假设我给你们一个有标记的数据集合

580
00:37:35,790 --> 00:37:36,860
假设我告诉你们了

581
00:37:36,860 --> 00:37:38,960
每个样本的z^((i) )的值

582
00:37:39,930 --> 00:37:43,330
那么我给你的数据集合会是这样的

583
00:37:44,390 --> 00:37:48,650
这些是一维数据

584
00:37:48,650 --> 00:37:50,200
所以这是一个典型

585
00:37:50,200 --> 00:37:51,820
的一维GDA

586
00:37:52,900 --> 00:37:55,950
对于GDA

587
00:37:55,950 --> 00:37:58,820
我们用

588
00:37:58,820 --> 00:38:00,350
极大似然估计

589
00:38:00,350 --> 00:38:01,980
得到GDA的参数

590
00:38:02,820 --> 00:38:05,410
其中的一个参数

591
00:38:05,410 --> 00:38:10,010
是φ_j

592
00:38:10,010 --> 00:38:17,240
它表示z^((i) )=j的概率

593
00:38:19,860 --> 00:38:21,110
我们估计的值是:

594
00:38:21,110 --> 00:38:26,650
1/m ∑_(i=1)

595
00:38:26,650 --> 00:38:32,570
^m?〖I〗

596
00:38:32,570 --> 00:38:36,890
当我们在推导GDA时

597
00:38:36,890 --> 00:38:39,260
如果你知道

598
00:38:39,260 --> 00:38:40,630
每个样本的类标记

599
00:38:40,630 --> 00:38:43,630
那么这样我们就得到了样本

600
00:38:43,630 --> 00:38:47,570
由正样本类生成

601
00:38:47,570 --> 00:38:49,120
或由负样本类生成的概率

602
00:38:49,120 --> 00:38:50,510
这是一个比率

603
00:38:50,510 --> 00:38:53,980
通过极大似然估计得到的

604
00:38:53,980 --> 00:38:58,300
样本来自第j类的概率

605
00:38:58,300 --> 00:38:59,710
就等于训练集合中属于

606
00:38:59,710 --> 00:39:01,150
第j类的样本所占的比例

607
00:39:01,150 --> 00:39:03,520
所以这是

608
00:39:03,520 --> 00:39:04,690
对GDA的极大似然估计

609
00:39:07,560 --> 00:39:10,560
在混合高斯模型

610
00:39:10,560 --> 00:39:11,720
和EM问题中

611
00:39:11,720 --> 00:39:14,090
我们不知道类标记

612
00:39:14,090 --> 00:39:15,690
所以我们的数据集合是

613
00:39:15,690 --> 00:39:16,630
这样的一组无标记的数据

614
00:39:16,630 --> 00:39:17,730
我只有一些点

615
00:39:20,880 --> 00:39:22,120
这些点和

616
00:39:22,120 --> 00:39:22,800
之前的数据集合是一样的

617
00:39:22,800 --> 00:39:24,030
但是我们不知道类标记

618
00:39:25,690 --> 00:39:30,340
现在你观测到x^((i) )

619
00:39:31,830 --> 00:39:37,250
但是z^((i) )未知

620
00:39:41,750 --> 00:39:43,280
所以类标记是未知的

621
00:39:45,500 --> 00:39:47,230
在EM算法中

622
00:39:49,230 --> 00:39:52,700
我们尝试对

623
00:39:52,700 --> 00:39:54,250
z^((i) )的值进行猜测

624
00:39:55,930 --> 00:40:02,910
在E-step 我们计算出w_j^((i) )

625
00:40:02,910 --> 00:40:05,470
表示我们当前对

626
00:40:05,470 --> 00:40:10,050
P(z^((i) )=j)的最好的猜测

627
00:40:10,050 --> 00:40:14,020
它表示在目前的假设和参数下

628
00:40:14,020 --> 00:40:16,280
计算出的x^((i) )

629
00:40:16,280 --> 00:40:19,170
由第j个高斯分布产生的后验概率

630
00:40:19,170 --> 00:40:22,130
当前数据是"X"的概率

631
00:40:22,130 --> 00:40:26,160
是多少?

632
00:40:26,160 --> 00:40:28,740
当前数据是"Y"的概率

633
00:40:28,740 --> 00:40:31,870
是多少?

634
00:40:34,200 --> 00:40:35,530
在M-step

635
00:40:37,560 --> 00:40:42,370
我们对φ_j的估计将会是这个式子

636
00:40:42,370 --> 00:40:45,830
这个概率是我当前能够

637
00:40:45,830 --> 00:41:01,720
对第i个点由第j个高斯分布产生

638
00:41:01,720 --> 00:41:03,960
的概率能够做出的最好的猜测

639
00:41:03,960 --> 00:41:08,070
所以我们使用这个公式

640
00:41:08,870 --> 00:41:15,440
而不是这个

641
00:41:17,760 --> 00:41:20,140
类似的

642
00:41:20,140 --> 00:41:22,450
在我对μ_j的估计中

643
00:41:22,450 --> 00:41:25,660
我用w_j^((i) )代替了

644
00:41:25,660 --> 00:41:28,230
原来的指示函数

645
00:41:28,230 --> 00:41:30,610
这和GDA的公式

646
00:41:30,610 --> 00:41:33,070
是类似的

647
00:41:35,310 --> 00:41:38,560
我在尝试直观地阐述

648
00:41:38,560 --> 00:41:39,770
这个算法的意义

649
00:41:39,770 --> 00:41:41,830
如果明白的话请举手

650
00:41:41,830 --> 00:41:44,120
很好

651
00:41:49,800 --> 00:41:52,440
所以这就是EM算法的介绍

652
00:41:52,530 --> 00:41:56,460
原来我推导这个公式的方法

653
00:41:56,570 --> 00:42:02,320
这和EM算法的生成是一样的

654
00:42:02,410 --> 00:42:03,740
也是我们接下来将要讨论的内容

655
00:42:03,830 --> 00:42:10,090
我希望我们能处理这个观念

656
00:42:10,190 --> 00:42:11,420
但是它恰好来自于

657
00:42:11,510 --> 00:42:14,420
这是一个数据问题的答案

658
00:42:38,910 --> 00:42:40,210
这个例子展示了

659
00:42:40,210 --> 00:42:44,720
EM算法的直观含义

660
00:42:44,720 --> 00:42:48,030
你们刚才看到的例子是

661
00:42:48,030 --> 00:42:50,910
EM算法对于

662
00:42:50,910 --> 00:42:52,140
高斯模型的一个特例

663
00:42:52,140 --> 00:42:55,230
在剩下的半个小时里

664
00:42:55,230 --> 00:43:00,910
我要介绍EM算法的一般形式

665
00:43:00,910 --> 00:43:03,460
刚才你们所看

666
00:43:03,460 --> 00:43:05,420
到的是EM算法

667
00:43:05,420 --> 00:43:06,720
的一个特例

668
00:43:08,570 --> 00:43:17,750
在我们介绍EM算法

669
00:43:17,750 --> 00:43:20,570
的一般形式之前

670
00:43:21,680 --> 00:43:23,810
我会先讲

671
00:43:23,810 --> 00:43:25,200
Jensen不等式

672
00:43:25,200 --> 00:43:26,730
我们的推导过程中会用到它

673
00:43:27,780 --> 00:43:30,060
Jensen不等式是这样的

674
00:43:32,170 --> 00:43:34,000
令f是一个凸函数

675
00:43:45,360 --> 00:43:48,180
如果f是一个凸函数而且有二阶导数

676
00:43:48,180 --> 00:43:51,850
那么其二阶导数恒大于等于0

677
00:43:51,850 --> 00:43:53,540
虽然一个凸函数

678
00:43:53,540 --> 00:43:54,870
并不一定可导

679
00:43:54,870 --> 00:43:56,850
但是如果它有二阶导数的话

680
00:43:56,850 --> 00:43:58,620
其值一定恒大于等于0

681
00:44:00,590 --> 00:44:02,380
令x为一个随机变量

682
00:44:12,760 --> 00:44:24,940
所以:

683
00:44:24,940 --> 00:44:26,940
f(Ex)≤E[f(x)]

684
00:44:27,710 --> 00:44:30,630
我这里省略了

685
00:44:30,630 --> 00:44:32,050
方括号

686
00:44:32,050 --> 00:44:36,660
所以Ex实际上表示E[x]

687
00:44:37,810 --> 00:44:40,010
我将方括号省略掉了

688
00:44:43,640 --> 00:44:45,460
让我画一张图解释一下这个式子--

689
00:44:48,420 --> 00:44:52,270
很多我的朋友都记不清

690
00:44:52,270 --> 00:44:55,100
这个不等式的方向

691
00:44:55,100 --> 00:45:00,930
他们记忆不等式方向的方法

692
00:45:00,930 --> 00:45:04,890
就是画这样的一张图

693
00:45:30,970 --> 00:45:31,810
对于这个例子

694
00:45:31,810 --> 00:45:36,920
比如说 x=1的概率是1/2

695
00:45:36,920 --> 00:45:40,290
x=6的概率也是1/2

696
00:45:40,290 --> 00:45:44,720
我接下来要用这个例子来说明

697
00:45:47,420 --> 00:45:54,810
所以x=1的概率是1/2

698
00:45:54,810 --> 00:45:56,520
x=6的概率也是1/2

699
00:45:56,520 --> 00:45:59,600
所以x的期望值是3.5

700
00:45:59,600 --> 00:46:01,310
是中间的这个值

701
00:46:01,310 --> 00:46:04,290
这是x的期望值

702
00:46:04,950 --> 00:46:07,170
水平轴是x轴

703
00:46:08,900 --> 00:46:13,740
所以f(Ex)会对应这个点

704
00:46:13,740 --> 00:46:16,270
这个值

705
00:46:16,270 --> 00:46:20,470
是f(Ex)

706
00:46:22,160 --> 00:46:24,650
让我们反过来看看

707
00:46:25,140 --> 00:46:32,830
对于x=1 这里是f(1)

708
00:46:34,180 --> 00:46:39,370
对于x=6 这里是f(6) f

709
00:46:41,200 --> 00:46:46,380
(x)的期望值

710
00:46:48,000 --> 00:46:53,410
是纵轴的平均值

711
00:47:08,040 --> 00:47:11,040
你有50%的概率得到f(1)

712
00:47:11,040 --> 00:47:12,550
有50%的概率得到f(6)

713
00:47:12,550 --> 00:47:15,240
所以f(x)的期望值是

714
00:47:15,240 --> 00:47:17,220
f(1)和f(6)的平均值

715
00:47:17,220 --> 00:47:19,570
会是中间的这个值

716
00:47:21,210 --> 00:47:24,930
在这个例子中

717
00:47:24,930 --> 00:47:26,700
你会看到E[f(x)]大于

718
00:47:26,700 --> 00:47:30,270
等于f(Ex)

719
00:47:30,270 --> 00:47:31,640
明白吗?

720
00:47:33,860 --> 00:47:37,120
所以这就是这个情况下的它的原因的图解

721
00:47:54,810 --> 00:47:58,690
事实上 我们还可以进一步推出

722
00:48:00,930 --> 00:48:05,820
如果f^'' (x)

723
00:48:05,820 --> 00:48:06,940
严格大于0

724
00:48:06,940 --> 00:48:08,150
如果这个条件成立

725
00:48:08,150 --> 00:48:09,860
我们说f是一个严格的凸函数

726
00:48:18,240 --> 00:48:20,070
在这个

727
00:48:20,070 --> 00:48:21,810
条件下

728
00:48:21,810 --> 00:48:26,240
不等式满足等式:E[f(x) ]=f(Ex)

729
00:48:29,380 --> 00:48:38,130
当且仅当x为常量的概率为1

730
00:48:40,910 --> 00:48:44,650
另一种写法是 x=E[x]

731
00:48:54,200 --> 00:48:55,400
换句话说

732
00:48:55,400 --> 00:48:57,410
如果f是一个严格凸函数

733
00:48:58,230 --> 00:49:01,590
那么不等式取等号的唯一方式

734
00:49:01,590 --> 00:49:04,090
是随机变量x

735
00:49:04,090 --> 00:49:06,380
在概率1下取相同的值

736
00:49:06,480 --> 00:49:10,910
如果X=E[x]  可能其中一个身份

737
00:49:13,550 --> 00:49:20,570
嗯  另外  对于凹函数的这个状态

738
00:49:20,660 --> 00:49:27,470
如果f''小于等于0  concave的成就

739
00:49:29,560 --> 00:49:36,130
然后所有东西保留verse质量的方向

740
00:49:36,220 --> 00:49:46,250
所以f''(Ex)>=E[f(x)]  等等

741
00:49:46,350 --> 00:49:47,990
所以如果你有一个凹函数

742
00:49:48,080 --> 00:49:51,240
由于所有东西保留verse质量的方向

743
00:49:51,330 --> 00:49:53,410
一切质量都是同一方向

744
00:49:53,500 --> 00:49:57,470
然后我们会介绍EM算法

745
00:49:57,560 --> 00:50:01,970
为什么使用这个verse质量凹函数版本

746
00:50:05,360 --> 00:50:18,180
好的 有问题吗?什么问题?

747
00:50:18,270 --> 00:50:20,080
(不可闻)

748
00:50:20,080 --> 00:50:21,610
能再说一遍吗?

749
00:50:21,610 --> 00:50:24,840
(不可闻)

750
00:50:24,840 --> 00:50:26,570
我还是听不见 

751
00:50:26,570 --> 00:50:29,310
什么是严格凸函数?

752
00:50:29,310 --> 00:50:30,750
哦

753
00:50:30,750 --> 00:50:33,540
如果f^'' (x)严格大于0

754
00:50:33,540 --> 00:50:35,500
那么我就将f定义为严格的凸函数

755
00:50:37,340 --> 00:50:40,790
如果x的二阶导数严格大于0

756
00:50:40,790 --> 00:50:43,520
那么这意味着f是一个严格的凸函数

757
00:50:43,520 --> 00:50:45,180
(不可闻)

758
00:50:45,180 --> 00:50:46,100
当然

759
00:50:46,100 --> 00:50:47,770
例如

760
00:50:50,250 --> 00:50:52,710
这个例子表示一个凸函数

761
00:50:52,710 --> 00:50:53,980
但它不是一个严格的凸函数

762
00:50:53,980 --> 00:50:57,000
因为这个部分是一条直线

763
00:50:57,000 --> 00:51:01,790
所以这里f''将会取0

764
00:51:09,760 --> 00:51:10,480
是的

765
00:51:10,480 --> 00:51:13,180
这种解释方法稍微有点不正式

766
00:51:13,180 --> 00:51:14,200
它意味着你的凸函数

767
00:51:14,200 --> 00:51:17,290
不能包含直线部分

768
00:51:17,290 --> 00:51:20,960
这种说法非常不正式

769
00:51:20,960 --> 00:51:22,050
但是可以

770
00:51:22,050 --> 00:51:26,070
帮助你理解

771
00:51:46,150 --> 00:51:50,640
接下来我们正式

772
00:51:50,640 --> 00:51:53,930
推导EM的一般形式

773
00:51:57,470 --> 00:52:00,720
我们的问题是这样的

774
00:52:13,590 --> 00:52:14,830
我们有关于

775
00:52:14,830 --> 00:52:16,830
P(x  z;?)的概率分布模型

776
00:52:18,520 --> 00:52:24,700
但是我们只能观察到x

777
00:52:26,080 --> 00:52:31,310
我们的目标是

778
00:52:41,770 --> 00:52:43,810
使参数的对数似然性最大化

779
00:52:52,690 --> 00:52:54,890
我们有x和z的

780
00:52:54,890 --> 00:52:56,710
联合概率分布

781
00:52:56,710 --> 00:52:59,210
我们的目标是

782
00:52:59,210 --> 00:53:01,230
找到参数的极大似然估计

783
00:53:01,230 --> 00:53:06,020
参数应该等于对于1到m

784
00:53:06,020 --> 00:53:08,730
对P(x^((i) );?)求和

785
00:53:13,250 --> 00:53:17,230
这里的P(x^((i) );?)

786
00:53:17,230 --> 00:53:21,320
应该等于

787
00:53:21,320 --> 00:53:30,960
对所有z^((i) )的概率的求和

788
00:53:30,960 --> 00:53:33,860
所以我们的模型考虑

789
00:53:33,860 --> 00:53:35,120
x和z的联合概率分布

790
00:53:35,120 --> 00:53:38,370
并将z^((i) )

791
00:53:38,370 --> 00:53:41,780
独立出去

792
00:53:44,860 --> 00:53:58,430
所以EM算法是一个进行

793
00:53:58,430 --> 00:54:01,510
极大似然估计的过程

794
00:54:01,510 --> 00:54:04,620
难点在于我们模型中的z^((i) )

795
00:54:04,620 --> 00:54:07,050
是不知道的

796
00:54:24,340 --> 00:54:25,570
在我开始进行数学推导之前

797
00:54:25,570 --> 00:54:27,490
我希望你们记住这张有用的图

798
00:54:31,910 --> 00:54:33,890
水平轴表示参数?

799
00:54:33,890 --> 00:54:36,760
这个函数表示对数似然性

800
00:54:36,760 --> 00:54:38,250
我们尝试

801
00:54:38,250 --> 00:54:39,670
使它最大化

802
00:54:46,990 --> 00:54:51,940
通常情况下通过对?求导

803
00:54:51,940 --> 00:54:52,950
并令它为0

804
00:54:52,950 --> 00:54:53,950
这样计算是非常困难的

805
00:54:53,950 --> 00:54:57,600
EM算法是这样做的

806
00:54:58,540 --> 00:55:00,770
它先初始化参数?^((0) )

807
00:55:01,920 --> 00:55:04,210
EM算法做的是

808
00:55:04,210 --> 00:55:07,330
建立了一个对

809
00:55:07,330 --> 00:55:09,500
数似然性函数的比较紧的下界

810
00:55:09,500 --> 00:55:14,370
在猜测参数之后

811
00:55:14,370 --> 00:55:18,870
会满足某个不等式的约束

812
00:55:18,870 --> 00:55:21,890
我们会相对于?

813
00:55:21,890 --> 00:55:22,710
使这个下界函数最大化

814
00:55:22,710 --> 00:55:25,060
所以我们得到了?^((1) )

815
00:55:25,500 --> 00:55:27,370
EM算法之后会根据?^((1) )

816
00:55:28,540 --> 00:55:30,740
创建一个

817
00:55:30,740 --> 00:55:36,720
新的下界

818
00:55:36,720 --> 00:55:38,650
之后求它的最大值

819
00:55:39,380 --> 00:55:40,170
所以这里

820
00:55:40,170 --> 00:55:42,330
你得到了?^((2) )

821
00:55:42,330 --> 00:55:44,150
继续这样做

822
00:55:45,390 --> 00:55:49,050
你会得到?^((3) )和?^((4) )

823
00:55:49,050 --> 00:55:51,010
所以最后

824
00:55:51,010 --> 00:55:53,930
你会收敛到函数的一个局部最优值

825
00:55:54,680 --> 00:55:58,340
这张图展示了

826
00:55:58,340 --> 00:55:59,600
EM算法的原理

827
00:56:00,320 --> 00:56:02,100
接下来让我们形式化地论述它

828
00:56:20,110 --> 00:56:27,420
你希望相对于?令这个式子取最大值

829
00:56:27,420 --> 00:56:36,400
它应该等于相对于

830
00:56:36,400 --> 00:56:41,500
所有的z^((i) )求和

831
00:56:50,470 --> 00:56:57,980
之后我们要乘以

832
00:56:57,980 --> 00:56:59,380
再除以同样的东西

833
00:56:59,380 --> 00:57:06,710
我将其写作--

834
00:57:14,820 --> 00:57:17,940
接下来我要建立Q_i

835
00:57:17,940 --> 00:57:22,640
在隐含随机变量z^((i) )上

836
00:57:22,640 --> 00:57:24,280
的概率分布

837
00:57:24,280 --> 00:57:27,460
这些Q_i都是概率分布

838
00:57:27,460 --> 00:57:30,600
所有的Q_i (z^((i) ))

839
00:57:30,600 --> 00:57:37,700
加起来应该等于1

840
00:57:37,700 --> 00:57:40,250
所以这些Q是我

841
00:57:40,250 --> 00:57:41,390
要建立的概率分布

842
00:57:42,100 --> 00:57:46,130
我稍后会讲怎样

843
00:57:46,130 --> 00:57:48,600
对Q进行选择

844
00:57:54,370 --> 00:57:58,490
Q_i是一个z_i上

845
00:57:58,490 --> 00:58:01,420
的概率分布

846
00:58:01,520 --> 00:58:04,660
所以这个式子应该等于

847
00:58:04,750 --> 00:58:08,690
Q_i (z^((i) ))乘上一个z^((i) )的函数

848
00:58:08,800 --> 00:58:12,700
所以整个求和式是一个期望

849
00:58:12,810 --> 00:58:25,360
也就是这个式子的期望

850
00:58:34,660 --> 00:58:36,330
这个期望是相对

851
00:58:36,430 --> 00:58:40,110
于随机变量z^((i) )的概率分布Q_i的

852
00:58:40,210 --> 00:58:42,970
这里之所以可以看成一个期望

853
00:58:43,090 --> 00:58:48,190
是因为这里是对所有的z^((i) )

854
00:58:48,310 --> 00:58:57,240
对它的概率分布乘以它的函数进行求和

855
00:58:59,070 --> 00:59:01,670
我看到你们有些人在皱眉头

856
00:59:01,670 --> 00:59:02,750
有问题吗?

857
00:59:05,600 --> 00:59:06,390
没有 好的

858
00:59:32,290 --> 00:59:43,390
对数函数看起来是这样的

859
00:59:43,390 --> 00:59:45,770
它是一个凹函数

860
00:59:46,960 --> 00:59:52,110
那么根据凹函数版本的Jesen不等式

861
00:59:52,110 --> 00:59:58,320
我们有

862
00:59:58,320 --> 01:00:01,570
log?〖E[x]〗≥E[log?x]

863
01:00:03,820 --> 01:00:06,560
根据我们之前的公式

864
01:00:06,560 --> 01:00:15,350
它是对期望的对数求和

865
01:00:15,350 --> 01:00:19,040
它应该大于等于

866
01:00:19,040 --> 01:00:29,510
这个式子

867
01:00:45,490 --> 01:00:46,220
明白吗?

868
01:00:46,980 --> 01:00:48,220
这是根据Jesen不等式得到的

869
01:00:51,380 --> 01:00:54,530
之后展开这个公式

870
01:01:15,620 --> 01:01:16,590
它等于

871
01:01:19,250 --> 01:01:22,160
说吧

872
01:01:22,160 --> 01:01:29,590
(不可闻)

873
01:01:29,590 --> 01:01:31,370
我是怎么从这一步到这一步的?

874
01:01:31,630 --> 01:01:32,530
好的

875
01:01:32,530 --> 01:01:47,340
假设我们有一个随机变量z

876
01:01:47,340 --> 01:01:51,510
服从某个概率分布

877
01:01:51,920 --> 01:01:54,380
将其写为P

878
01:01:54,380 --> 01:01:57,070
有某个函数g(z)

879
01:01:58,090 --> 01:02:00,530
那么根据定义

880
01:02:00,530 --> 01:02:07,430
g(z)的期望值

881
01:02:07,430 --> 01:02:11,040
应该等于

882
01:02:11,040 --> 01:02:14,180
∑_z?〖P(z)g(z)〗 对吗?

883
01:02:15,220 --> 01:02:18,360
这是期望值的定义

884
01:02:20,650 --> 01:02:23,810
所以我从这一步

885
01:02:23,810 --> 01:02:28,230
推到这一步就是根据这个定义

886
01:02:28,880 --> 01:02:29,690
具体地

887
01:02:29,690 --> 01:02:35,100
我这里用Q_i

888
01:02:35,100 --> 01:02:36,580
表示z的概率分布

889
01:02:36,580 --> 01:02:41,720
所以这个公式可以看成是∑_z?〖P(z)g(z)〗

890
01:02:56,870 --> 01:03:03,030
这一部分代表了P(z)

891
01:03:03,030 --> 01:03:04,460
这一部分代表了g(z)

892
01:03:04,460 --> 01:03:08,020
所以这里就是E_(z~Q) [g(z)]

893
01:03:11,420 --> 01:03:12,250
还有问题吗?

894
01:03:15,460 --> 01:03:16,800
通常情况下在进行

895
01:03:16,800 --> 01:03:19,220
极大似然估计的时候

896
01:03:19,220 --> 01:03:23,210
数据的似然性是包含x和z的

897
01:03:23,210 --> 01:03:27,280
但是在这个例子中你只写了x

898
01:03:27,280 --> 01:03:30,230
是因为你没有

899
01:03:30,230 --> 01:03:32,510
观察到x的标记吗?

900
01:03:32,510 --> 01:03:33,880
是的 正是这样

901
01:03:33,880 --> 01:03:38,170
在极大似然估计中

902
01:03:38,170 --> 01:03:39,610
我们希望选择参数

903
01:03:39,610 --> 01:03:40,530
使得数据的概率最大化

904
01:03:40,530 --> 01:03:45,050
这里我们的数据仅仅由x组成

905
01:03:45,520 --> 01:03:46,770
因为我没有观察到z

906
01:03:46,770 --> 01:03:53,560
因此参数的似然性

907
01:03:53,560 --> 01:03:57,560
只包含x

908
01:04:05,800 --> 01:04:08,020
这是我们的做法

909
01:04:08,020 --> 01:04:12,000
我们希望做的是使?的对数似然性最大化

910
01:04:12,000 --> 01:04:16,220
通过我们的数学处理

911
01:04:16,220 --> 01:04:17,820
我们得到了数据的

912
01:04:17,820 --> 01:04:19,220
对数似然性的下界

913
01:04:20,420 --> 01:04:22,850
具体地看一下我们得到的这个公式

914
01:04:22,850 --> 01:04:28,530
我们应该将其看成?的函数 ?是模型的

915
01:04:29,020 --> 01:04:31,020
参数对吗?

916
01:04:31,020 --> 01:04:31,720
如果

917
01:04:31,720 --> 01:04:34,040
你这样做的话

918
01:04:34,040 --> 01:04:35,910
我们所证明的结论说的就是

919
01:04:35,910 --> 01:04:40,150
参数?的对数似然性存在

920
01:04:40,150 --> 01:04:45,790
着这样的一个下界

921
01:05:04,140 --> 01:05:07,510
记得刚才的图中

922
01:05:07,510 --> 01:05:08,770
我们会重复构建这样的下界

923
01:05:08,770 --> 01:05:10,120
并且最优化这个下界

924
01:05:10,120 --> 01:05:14,030
所以我们刚刚构建了

925
01:05:14,030 --> 01:05:16,180
?的对数似然性的下界

926
01:05:18,200 --> 01:05:22,650
现在我们所要做的最后一步

927
01:05:22,650 --> 01:05:27,830
是让这个不等式的等号

928
01:05:27,830 --> 01:05:29,020
对当前的?成立

929
01:05:29,020 --> 01:05:32,140
回到我们刚才画的图中

930
01:05:33,650 --> 01:05:36,570
如果这个函数代表了?的对数似然性

931
01:05:36,570 --> 01:05:38,980
那么我们构建出了它的下界

932
01:05:38,980 --> 01:05:40,040
如果这是我们目前的?值

933
01:05:40,040 --> 01:05:44,580
那么下界就会是?的一个函数的值

934
01:05:44,580 --> 01:05:46,920
我们希望下界尽可能地紧

935
01:05:46,920 --> 01:05:48,810
我希望下界

936
01:05:48,810 --> 01:05:49,910
等于?的对数似然性

937
01:05:49,910 --> 01:05:52,070
因为这样我才能保证

938
01:05:52,070 --> 01:05:54,060
当我最优化我的下界时

939
01:05:54,060 --> 01:05:56,530
我的真正的优化目标

940
01:05:56,530 --> 01:05:58,560
也会取更好的值

941
01:06:02,530 --> 01:06:05,890
所以要那样做来保证

942
01:06:05,950 --> 01:06:07,960
任何等式都保留了这个特性

943
01:06:08,050 --> 01:06:12,800
我们需要回到【无声】

944
01:06:12,880 --> 01:06:14,910
我需要的使用这个【无声】的特性

945
01:06:14,990 --> 01:06:18,970
让这些保持相等

946
01:06:19,060 --> 01:06:24,770
特别的  我将用分布--

947
01:06:24,880 --> 01:06:26,660
所以对于任何我可能选择的分配

948
01:06:26,760 --> 01:06:28,470
这些保持为真

949
01:06:28,550 --> 01:06:31,610
所以我将做的是

950
01:06:31,710 --> 01:06:34,770
选择可能的分配Q来保证

951
01:06:34,850 --> 01:06:39,400
对于我当前的数据

952
01:06:39,470 --> 01:06:41,280
这个等式为真

953
01:06:41,360 --> 01:06:42,320
什么问题?

954
01:06:42,390 --> 01:06:43,980
(不可闻)

955
01:06:45,060 --> 01:06:47,860
好问题

956
01:06:47,860 --> 01:06:49,510
我怎么知道这个函数是凹函数

957
01:06:51,960 --> 01:06:53,330
实际上我并没有证明

958
01:06:53,330 --> 01:06:54,390
但是对于我们目前所用到的模型

959
01:06:54,390 --> 01:06:55,350
这个结论都是成立的

960
01:06:55,350 --> 01:06:59,730
我能确定下界

961
01:06:59,730 --> 01:07:00,940
是一个?的凹函数吗?

962
01:07:00,940 --> 01:07:02,260
我想你是对的

963
01:07:02,260 --> 01:07:04,400
一般情况下它并不总是?的凹函数

964
01:07:04,400 --> 01:07:05,870
但是对于我们所遇到的许多模型

965
01:07:05,870 --> 01:07:07,430
它是凹函数

966
01:07:07,430 --> 01:07:08,220
但是这一点并不总是成立的

967
01:07:14,040 --> 01:07:16,140
好的 让我们继续来选择Q

968
01:07:16,140 --> 01:07:19,140
我们说不等式

969
01:07:19,140 --> 01:07:22,770
的等号成立

970
01:07:22,770 --> 01:07:27,310
当且仅当里面的这个随机变量是个常量时

971
01:07:27,790 --> 01:07:29,270
这里就是在对

972
01:07:29,270 --> 01:07:30,360
一个常量求期望值

973
01:07:30,360 --> 01:07:32,780
我们需要选择概率分布Q_i

974
01:07:58,800 --> 01:07:59,880
所以我们想要的是

975
01:07:59,960 --> 01:08:02,520
选择这个Qi(Z^i)的分配

976
01:08:02,620 --> 01:08:16,630
所以那个等于常数

977
01:08:16,730 --> 01:08:20,840
常数意味着对于所有的Z^i

978
01:08:20,910 --> 01:08:27,830
它得到的是相同的值

979
01:08:27,930 --> 01:08:30,160
不管你插入的Z^i是什么值

980
01:08:30,240 --> 01:08:32,770
最终得到相同的结果

981
01:08:32,870 --> 01:08:47,270
所以你将设置Qi(Z^i)为那个

982
01:08:47,360 --> 01:08:49,890
来保证它为这个常数

983
01:08:53,300 --> 01:08:58,980
但同样Z^i应该为-- 哦  对不起

984
01:08:59,090 --> 01:09:00,900
Qi对于这个分配

985
01:09:00,980 --> 01:09:08,290
所以Z^i的总和等于1.

986
01:09:08,390 --> 01:09:12,940
正如大家所知 这个期望避开这里

987
01:09:13,040 --> 01:09:19,350
那个选择实际上等于--

988
01:10:18,540 --> 01:10:22,110
使得这个式子为常量这个求和式应该等于1

989
01:10:22,110 --> 01:10:24,890
你选择的这个值

990
01:10:24,890 --> 01:10:28,180
应该等于这里我将P(x^((i) )  z^((i) );?)

991
01:10:28,180 --> 01:10:29,790
归一化之后加起来应该等于1

992
01:10:29,790 --> 01:10:31,810
这中间

993
01:10:31,810 --> 01:10:34,460
我跳步了

994
01:10:34,460 --> 01:10:35,980
但是希望你们能够相信它的正确性

995
01:10:35,980 --> 01:10:39,090
中间我跳了几步

996
01:10:39,090 --> 01:10:40,900
这些步骤写在了讲义上

997
01:10:44,110 --> 01:10:53,240
分母是这个式子

998
01:10:56,380 --> 01:11:06,740
根据条件概率的定义

999
01:11:06,740 --> 01:11:12,000
Q_i (z^((i) ) )=P(z^((i) ) |x^((i) );?)

1000
01:11:37,150 --> 01:11:39,110
总结一下算法

1001
01:11:45,920 --> 01:11:48,270
EM算法有两步

1002
01:11:48,270 --> 01:11:49,990
E-step中

1003
01:11:52,650 --> 01:11:56,540
我们会选取Q_i

1004
01:11:56,540 --> 01:12:01,800
所以Q_i (z^((i) )

1005
01:12:09,550 --> 01:12:12,420
)应该被设为:P(z^((i) ) |x^((i) );?)

1006
01:12:12,420 --> 01:12:13,750
这是我们刚刚得到的公式

1007
01:12:16,630 --> 01:12:20,050
所以通过这一步

1008
01:12:20,050 --> 01:12:22,740
我们得到了由当前?值

1009
01:12:22,740 --> 01:12:24,200
确定的对数似然函数的非常紧的下界

1010
01:12:25,810 --> 01:12:26,890
在M-step中

1011
01:12:26,890 --> 01:12:30,800
我们会相对于

1012
01:12:30,800 --> 01:12:35,410
更新?

1013
01:12:35,410 --> 01:12:36,790
使这个式子最大化

1014
01:12:57,890 --> 01:13:02,960
这就是EM算法

1015
01:13:03,710 --> 01:13:05,270
今天没有时间了

1016
01:13:05,270 --> 01:13:08,470
但是下一讲我会给你们证明

1017
01:13:08,470 --> 01:13:11,220
混合高斯模型对应的EM算法

1018
01:13:11,220 --> 01:13:13,040
实际上是一般化的EM算法模板

1019
01:13:13,040 --> 01:13:16,410
的一个特例

1020
01:13:16,410 --> 01:13:20,130
E-step和M-step都是一一对应的

1021
01:13:20,490 --> 01:13:23,120
混合高斯模型的E-step和M-step

1022
01:13:23,120 --> 01:13:23,670
实际上就是这里的步骤

1023
01:13:32,730 --> 01:13:35,620
函数表示?的对数似然函数

1024
01:13:35,620 --> 01:13:38,000
E-step构造这样的下界

1025
01:13:38,000 --> 01:13:39,000
并且确保下界是紧的

1026
01:13:39,960 --> 01:13:41,070
这是我选择的Q

1027
01:13:41,070 --> 01:13:45,140
M-step相对于

1028
01:13:45,140 --> 01:13:47,300
?使下界最大化

1029
01:13:48,260 --> 01:13:52,350
下节课还有内容要讲

1030
01:13:52,350 --> 01:13:54,350
让我看看你们有什么问题

1031
01:14:03,490 --> 01:14:04,430
很好

1032
01:14:04,430 --> 01:14:05,510
今天就到这里

1033
01:14:05,510 --> 01:14:07,410
我们下次课继续讲

